# -*- coding: utf-8 -*-
"""HPI cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAj6AYxuWMZdKOU570c1W_EctvCI9UHf
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


nltk.download('punkt')
nltk.download('stopwords')


df = pd.read_csv("/content/drive/MyDrive/research/hpi_Extract.csv")
hpis = df["hpi"].astype(str).tolist()

# med dict
with open("/content/wordlist.txt", "r") as f:
    med_dict = set(line.strip().lower() for line in f if line.strip())

stop_words = set(stopwords.words('english'))


cleaned_hpi = []
total_tokens = 0
total_removed_stopwords = 0
total_removed_nondict = 0
skipped_lines = 0

for line in hpis:
    if pd.isna(line) or not str(line).strip() or str(line).strip().upper() == "NA":
        cleaned_hpi.append("")
        skipped_lines += 1
        continue


    tokens = word_tokenize(line.lower())


    tokens = [re.sub(r'\W+', '', t) for t in tokens if t.isalnum()]
    total_tokens += len(tokens)

    # stopwords removal
    tokens_nostop = [t for t in tokens if t not in stop_words]
    total_removed_stopwords += len(tokens) - len(tokens_nostop)

    # keep what is present in the med dict
    final_tokens = [t for t in tokens_nostop if t in med_dict]
    total_removed_nondict += len(tokens_nostop) - len(final_tokens)

    cleaned_hpi.append(" ".join(sorted(set(final_tokens))))


df["cleaned_hpi"] = cleaned_hpi
df["cleaned_hpi"] = df["cleaned_hpi"].fillna("")
df["cleaned_hpi"] = df["cleaned_hpi"].replace(["nan", "NA"], "").fillna("")


out_path = "/content/drive/MyDrive/research/hpi_cleaned_for_embed.csv"
df.to_csv(out_path, index=False)