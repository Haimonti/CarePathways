# -*- coding: utf-8 -*-
"""CC cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAj6AYxuWMZdKOU570c1W_EctvCI9UHf
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


nltk.download('punkt')
nltk.download('stopwords')


df = pd.read_csv("/content/drive/MyDrive/research/chief_complaint-3.csv")
complaints = df["chief_complaint"].astype(str).tolist()


with open("/content/wordlist.txt", "r") as f:
    med_dict = set(line.strip().lower() for line in f if line.strip())


stop_words = set(stopwords.words('english'))


cleaned_complaints = []
total_tokens = 0
total_removed_stopwords = 0
total_removed_nondict = 0
skipped_lines = 0

for line in complaints:
    if pd.isna(line) or not str(line).strip():
        cleaned_complaints.append("")
        skipped_lines += 1
        continue

    tokens = word_tokenize(line.lower())

    tokens = [re.sub(r'\W+', '', t) for t in tokens if t.isalnum()]
    total_tokens += len(tokens)

    # stopwords removal
    tokens_nostop = [t for t in tokens if t not in stop_words]
    total_removed_stopwords += len(tokens) - len(tokens_nostop)

    # med dict filtering
    final_tokens = [t for t in tokens_nostop if t in med_dict]
    total_removed_nondict += len(tokens_nostop) - len(final_tokens)


    cleaned_complaints.append(" ".join(sorted(set(final_tokens))))

df["cleaned_complaint"] = cleaned_complaints


df.to_csv("/content/drive/MyDrive/research/chief_complaint_cleaned_for_embed.csv", index=False)