{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ba83a5",
   "metadata": {},
   "source": [
    "### Implementing RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4592fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9508f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/df_over_14.csv\")\n",
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799c48b",
   "metadata": {},
   "source": [
    "### Initially we will calculate the days remaining in the hospital which is given by the hospital length of stay - the day in the hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24d3a4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>highest_mean_arterial_pressure</th>\n",
       "      <th>lowest_mean_arterial_pressure</th>\n",
       "      <th>highest_heart_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Bilateral Ground Glass</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Unilateral Consolidation</th>\n",
       "      <th>Bilateral Ground Glass Opacities</th>\n",
       "      <th>Bilateral consolidationinfiltration</th>\n",
       "      <th>Pulmonary Embolism</th>\n",
       "      <th>Scarring or Fibrosis</th>\n",
       "      <th>days_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>79.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>121.333333</td>\n",
       "      <td>61.833333</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  systolic_blood_pressure  diastolic_blood_pressure  heart_rate  \\\n",
       "0           6               127.000000                 76.000000        68.0   \n",
       "1           6                97.000000                 60.000000        68.0   \n",
       "2           6               140.000000                 68.000000        72.0   \n",
       "3           6               108.000000                 63.000000        98.0   \n",
       "4           6               126.000000                 77.000000        68.0   \n",
       "5           6               120.000000                 82.000000        76.0   \n",
       "6           6               128.000000                 62.000000        73.0   \n",
       "7           6               116.000000                 74.000000        82.0   \n",
       "8          14               107.000000                 64.000000        60.0   \n",
       "9          14               153.000000                 72.000000        56.0   \n",
       "10         14               168.000000                 66.000000        58.0   \n",
       "11         14               160.000000                 60.000000        58.0   \n",
       "12         14               188.000000                 75.000000        60.0   \n",
       "13         14               172.000000                 65.000000        68.0   \n",
       "14         14               153.000000                 60.000000        62.0   \n",
       "15         14               132.000000                 79.000000        83.0   \n",
       "16         15               157.000000                100.000000        89.0   \n",
       "17         15               168.000000                 93.000000        85.0   \n",
       "18         15               161.000000                100.000000        88.0   \n",
       "19         15               139.000000                 87.000000        91.0   \n",
       "20         15               139.000000                 89.000000        67.0   \n",
       "21         15               131.000000                 89.000000        73.0   \n",
       "22         15               133.000000                 85.000000        67.0   \n",
       "23         15               103.000000                 66.000000        66.0   \n",
       "24         25               105.000000                 56.000000        84.0   \n",
       "25         25               118.000000                 66.000000        60.0   \n",
       "26         25               115.000000                 61.000000        79.0   \n",
       "27         25               121.333333                 61.833333        90.0   \n",
       "28         25               133.000000                 71.000000        91.0   \n",
       "29         25               116.000000                 59.000000        92.0   \n",
       "\n",
       "    respiratory_rate  oxygen_saturation  temperature  \\\n",
       "0               19.0               95.0         36.6   \n",
       "1               22.0               98.0         36.4   \n",
       "2               22.0               99.0         36.5   \n",
       "3               22.0               95.0         36.5   \n",
       "4               24.0               98.0         36.5   \n",
       "5               24.0               93.0         36.6   \n",
       "6               26.0               93.0         36.8   \n",
       "7               20.0               96.0         37.6   \n",
       "8               20.0               94.0         36.9   \n",
       "9               18.0               93.0         36.7   \n",
       "10              18.0               94.0         36.3   \n",
       "11              18.0               93.0         36.4   \n",
       "12              20.0               93.0         36.6   \n",
       "13              18.0               99.0         36.5   \n",
       "14              18.0               95.0         36.8   \n",
       "15              20.0               97.0         37.0   \n",
       "16              18.0               95.0         36.8   \n",
       "17              20.0               93.0         37.6   \n",
       "18              18.0               93.0         37.1   \n",
       "19              17.0               84.0         36.9   \n",
       "20              18.0               92.0         36.8   \n",
       "21              19.0               93.0         36.5   \n",
       "22              19.0               93.0         36.9   \n",
       "23              20.0               95.0         37.3   \n",
       "24              22.0               88.0         37.7   \n",
       "25              30.0               97.0         36.7   \n",
       "26              34.0               95.0         36.3   \n",
       "27              30.0               96.0         37.3   \n",
       "28              30.0               98.0         37.0   \n",
       "29              30.0               96.0         37.2   \n",
       "\n",
       "    highest_mean_arterial_pressure  lowest_mean_arterial_pressure  \\\n",
       "0                        92.000000                      80.000000   \n",
       "1                        71.000000                      67.000000   \n",
       "2                        84.000000                      84.000000   \n",
       "3                        77.000000                      77.000000   \n",
       "4                        92.000000                      92.000000   \n",
       "5                        93.000000                      93.000000   \n",
       "6                        93.000000                      81.000000   \n",
       "7                        86.000000                      66.000000   \n",
       "8                         0.000000                       0.000000   \n",
       "9                         0.000000                       0.000000   \n",
       "10                        0.000000                       0.000000   \n",
       "11                        0.000000                       0.000000   \n",
       "12                        0.000000                       0.000000   \n",
       "13                        0.000000                       0.000000   \n",
       "14                        0.000000                       0.000000   \n",
       "15                        0.000000                       0.000000   \n",
       "16                        0.000000                       0.000000   \n",
       "17                        0.000000                       0.000000   \n",
       "18                        0.000000                       0.000000   \n",
       "19                        0.000000                       0.000000   \n",
       "20                        0.000000                       0.000000   \n",
       "21                        0.000000                       0.000000   \n",
       "22                        0.000000                       0.000000   \n",
       "23                        0.000000                       0.000000   \n",
       "24                       78.000000                      71.000000   \n",
       "25                      107.000000                      72.000000   \n",
       "26                      122.000000                      71.000000   \n",
       "27                       93.166667                      72.333333   \n",
       "28                       91.000000                      75.000000   \n",
       "29                       82.000000                      76.000000   \n",
       "\n",
       "    highest_heart_rate  ...  Bilateral Ground Glass  Cardiomegaly  Edema  \\\n",
       "0                 77.0  ...                       0             0      0   \n",
       "1                 77.0  ...                       0             0      0   \n",
       "2                 97.0  ...                       0             0      0   \n",
       "3                107.0  ...                       0             0      0   \n",
       "4                 77.0  ...                       0             0      0   \n",
       "5                 82.0  ...                       0             0      0   \n",
       "6                 78.0  ...                       0             0      0   \n",
       "7                 94.0  ...                       0             0      0   \n",
       "8                 64.0  ...                       0             0      0   \n",
       "9                 83.0  ...                       0             0      0   \n",
       "10                58.0  ...                       0             0      0   \n",
       "11                60.0  ...                       0             0      0   \n",
       "12                81.0  ...                       0             0      0   \n",
       "13                70.0  ...                       0             0      0   \n",
       "14                73.0  ...                       0             0      0   \n",
       "15                83.0  ...                       0             0      0   \n",
       "16                89.0  ...                       0             0      0   \n",
       "17                94.0  ...                       0             0      0   \n",
       "18                92.0  ...                       0             0      0   \n",
       "19                91.0  ...                       0             0      0   \n",
       "20                75.0  ...                       0             0      0   \n",
       "21                73.0  ...                       0             0      0   \n",
       "22                73.0  ...                       0             0      0   \n",
       "23                78.0  ...                       0             0      0   \n",
       "24                92.0  ...                       0             1      0   \n",
       "25               114.0  ...                       0             1      0   \n",
       "26                90.0  ...                       0             1      0   \n",
       "27                98.0  ...                       0             1      0   \n",
       "28               101.0  ...                       0             0      0   \n",
       "29                99.0  ...                       0             1      0   \n",
       "\n",
       "    Effusion  Unilateral Consolidation  Bilateral Ground Glass Opacities  \\\n",
       "0          0                         0                                 0   \n",
       "1          0                         0                                 0   \n",
       "2          0                         0                                 0   \n",
       "3          1                         0                                 0   \n",
       "4          0                         0                                 1   \n",
       "5          0                         0                                 0   \n",
       "6          0                         0                                 0   \n",
       "7          0                         0                                 0   \n",
       "8          0                         0                                 0   \n",
       "9          0                         0                                 0   \n",
       "10         0                         0                                 0   \n",
       "11         0                         0                                 0   \n",
       "12         0                         0                                 0   \n",
       "13         0                         0                                 0   \n",
       "14         0                         0                                 0   \n",
       "15         0                         0                                 0   \n",
       "16         0                         0                                 0   \n",
       "17         0                         0                                 0   \n",
       "18         0                         0                                 0   \n",
       "19         0                         0                                 0   \n",
       "20         0                         0                                 0   \n",
       "21         0                         0                                 0   \n",
       "22         1                         1                                 0   \n",
       "23         0                         0                                 0   \n",
       "24         0                         0                                 0   \n",
       "25         0                         0                                 0   \n",
       "26         0                         0                                 0   \n",
       "27         0                         0                                 0   \n",
       "28         0                         0                                 0   \n",
       "29         0                         0                                 0   \n",
       "\n",
       "    Bilateral consolidationinfiltration  Pulmonary Embolism  \\\n",
       "0                                     0                   0   \n",
       "1                                     0                   0   \n",
       "2                                     0                   0   \n",
       "3                                     0                   0   \n",
       "4                                     1                   0   \n",
       "5                                     0                   0   \n",
       "6                                     0                   0   \n",
       "7                                     0                   0   \n",
       "8                                     0                   0   \n",
       "9                                     0                   0   \n",
       "10                                    0                   0   \n",
       "11                                    0                   0   \n",
       "12                                    0                   0   \n",
       "13                                    0                   0   \n",
       "14                                    0                   0   \n",
       "15                                    0                   0   \n",
       "16                                    0                   0   \n",
       "17                                    0                   0   \n",
       "18                                    0                   0   \n",
       "19                                    0                   0   \n",
       "20                                    0                   0   \n",
       "21                                    0                   0   \n",
       "22                                    0                   0   \n",
       "23                                    0                   0   \n",
       "24                                    0                   0   \n",
       "25                                    0                   0   \n",
       "26                                    0                   0   \n",
       "27                                    0                   0   \n",
       "28                                    0                   0   \n",
       "29                                    0                   0   \n",
       "\n",
       "    Scarring or Fibrosis  days_remaining  \n",
       "0                      0              31  \n",
       "1                      0              30  \n",
       "2                      0              29  \n",
       "3                      0              28  \n",
       "4                      0              27  \n",
       "5                      0              26  \n",
       "6                      0              25  \n",
       "7                      0              18  \n",
       "8                      0              32  \n",
       "9                      0              31  \n",
       "10                     0              30  \n",
       "11                     0              29  \n",
       "12                     0              28  \n",
       "13                     0              27  \n",
       "14                     0              26  \n",
       "15                     0              19  \n",
       "16                     0              33  \n",
       "17                     1              32  \n",
       "18                     0              31  \n",
       "19                     0              30  \n",
       "20                     0              29  \n",
       "21                     0              28  \n",
       "22                     0              27  \n",
       "23                     0              20  \n",
       "24                     0              71  \n",
       "25                     0              70  \n",
       "26                     0              69  \n",
       "27                     0              68  \n",
       "28                     0              67  \n",
       "29                     0              66  \n",
       "\n",
       "[30 rows x 67 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['days_remaining'] = df['hospital_length_of_stay']-df['day']\n",
    "df = df.drop(['hospital_length_of_stay','day'],axis=1)\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a799b4",
   "metadata": {},
   "source": [
    "### We will scale the columns which arent encoded and have continious values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff559a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>highest_mean_arterial_pressure</th>\n",
       "      <th>lowest_mean_arterial_pressure</th>\n",
       "      <th>highest_heart_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Bilateral Ground Glass</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Unilateral Consolidation</th>\n",
       "      <th>Bilateral Ground Glass Opacities</th>\n",
       "      <th>Bilateral consolidationinfiltration</th>\n",
       "      <th>Pulmonary Embolism</th>\n",
       "      <th>Scarring or Fibrosis</th>\n",
       "      <th>days_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.471299</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>-0.426718</td>\n",
       "      <td>0.274956</td>\n",
       "      <td>-0.415526</td>\n",
       "      <td>0.775369</td>\n",
       "      <td>1.002965</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.416503</td>\n",
       "      <td>-0.741557</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>1.190958</td>\n",
       "      <td>-0.847667</td>\n",
       "      <td>0.355284</td>\n",
       "      <td>0.668960</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.615945</td>\n",
       "      <td>-0.135129</td>\n",
       "      <td>-0.352470</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>1.496292</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.615336</td>\n",
       "      <td>1.105736</td>\n",
       "      <td>0.443514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.896574</td>\n",
       "      <td>-0.514146</td>\n",
       "      <td>1.254138</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>0.274956</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.475308</td>\n",
       "      <td>0.925887</td>\n",
       "      <td>1.055964</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.045782</td>\n",
       "      <td>0.547103</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>0.644958</td>\n",
       "      <td>1.190958</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.775369</td>\n",
       "      <td>1.311277</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  systolic_blood_pressure  diastolic_blood_pressure  heart_rate  \\\n",
       "0          6                 0.001484                  0.471299   -0.599641   \n",
       "1          6                -1.416503                 -0.741557   -0.599641   \n",
       "2          6                 0.615945                 -0.135129   -0.352470   \n",
       "3          6                -0.896574                 -0.514146    1.254138   \n",
       "4          6                -0.045782                  0.547103   -0.599641   \n",
       "\n",
       "   respiratory_rate  oxygen_saturation  temperature  \\\n",
       "0         -0.426718           0.274956    -0.415526   \n",
       "1          0.216288           1.190958    -0.847667   \n",
       "2          0.216288           1.496292    -0.631597   \n",
       "3          0.216288           0.274956    -0.631597   \n",
       "4          0.644958           1.190958    -0.631597   \n",
       "\n",
       "   highest_mean_arterial_pressure  lowest_mean_arterial_pressure  \\\n",
       "0                        0.775369                       1.002965   \n",
       "1                        0.355284                       0.668960   \n",
       "2                        0.615336                       1.105736   \n",
       "3                        0.475308                       0.925887   \n",
       "4                        0.775369                       1.311277   \n",
       "\n",
       "   highest_heart_rate  ...  Bilateral Ground Glass  Cardiomegaly  Edema  \\\n",
       "0           -0.781388  ...                       0             0      0   \n",
       "1           -0.781388  ...                       0             0      0   \n",
       "2            0.443514  ...                       0             0      0   \n",
       "3            1.055964  ...                       0             0      0   \n",
       "4           -0.781388  ...                       0             0      0   \n",
       "\n",
       "   Effusion  Unilateral Consolidation  Bilateral Ground Glass Opacities  \\\n",
       "0         0                         0                                 0   \n",
       "1         0                         0                                 0   \n",
       "2         0                         0                                 0   \n",
       "3         1                         0                                 0   \n",
       "4         0                         0                                 1   \n",
       "\n",
       "   Bilateral consolidationinfiltration  Pulmonary Embolism  \\\n",
       "0                                    0                   0   \n",
       "1                                    0                   0   \n",
       "2                                    0                   0   \n",
       "3                                    0                   0   \n",
       "4                                    1                   0   \n",
       "\n",
       "   Scarring or Fibrosis  days_remaining  \n",
       "0                     0              31  \n",
       "1                     0              30  \n",
       "2                     0              29  \n",
       "3                     0              28  \n",
       "4                     0              27  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_binary = [\n",
    "    'intubated', 'cardiac_arrest', 'arrested_time', 'major_cardiac_events', \n",
    "    'clinically_diagnosed_infections', 'mechanical_ventilation', 'antiarrhythmic_therapies', \n",
    "    'renal_replacement_therapy_dialysis', 'cardiovascular_mechanical_support', 'echocardiogram', \n",
    "    'chest_x_ray', 'chest_ct', 'head_ct', 'antimicrobial', 'anticoagulation', 'steroid',\n",
    "    'Bilateral Consolidation', 'Bilateral Ground Glass', 'Cardiomegaly', 'Edema', 'Effusion', \n",
    "    'Pneumothorax', 'Unilateral Consolidation', 'Unilateral Ground Glass', 'Bilateral Ground Glass Opacities',\n",
    "    'Bilateral consolidationinfiltration', 'Subarachnoid Hemorrhage', 'Subdural Hemorrhage',\n",
    "    'Emphysematous or Bronchiectasis changes', 'Emphysematous or Bronchiectatic changes', \n",
    "    'Pulmonary Embolism', 'Scarring or Fibrosis', 'Unilateral Ground Glass Opacities', \n",
    "    'Unilateral consolidationinfiltration'\n",
    "]\n",
    "\n",
    "# Define your columns to exclude from scaling\n",
    "columns_to_exclude = ['parent_id', 'days_remaining'] + columns_binary\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the columns that need scaling\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Check the scaled data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b533d7a",
   "metadata": {},
   "source": [
    "### Define a simple RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d8352ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# This line defines a class SimpleRNN that inherits from torch.nn.Module. \n",
    "#In PyTorch, the Module class is the base class for all neural network modules,\n",
    "#and it provides essential functions for building and training models.\n",
    "class SimpleRNN(nn.Module):\n",
    "    ## constructor that initiaes once the class SimpleRNN is called\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        ## this is an RNN layer that takes inputsize, hiddensize\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        ## this is a fully connected layer which will give us the output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "   ## this is the way in which our RNN will operate in forward\n",
    "    def forward(self, x):\n",
    "        ## first the input values will be sent to the RNN\n",
    "        ## it will give out two values out: This is the output of the RNN layer. \n",
    "            # It contains the hidden states of the RNN at all time steps.\n",
    "        ##_: The second value returned by self.rnn(x) is the hidden state for the next time step\n",
    "            #(which is usually not needed in a basic RNN setup like this, so we use _ to ignore it).\n",
    "        out, _ = self.rnn(x)\n",
    "        ## the out consists of the output from each of the timestamp \n",
    "        ## which is also the hidden layer for the next timestamp\n",
    "        ## out[:, -1, :] gives the ouptut of the last layer \n",
    "        ## which is also the hidden layer of the next timestamp\n",
    "        ## but in this case it will only be used to produce the output\n",
    "        ## _ in the above gives the hidden state produced at timestamp t for the next timestamp (t+1)\n",
    "        ## it is not used so discarded\n",
    "        ## out[:,-1,:] and _ will give the same output\n",
    "        out = out[:, -1, :] \n",
    "        ## below we will pass the output of the last timestep and pass to a fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76f5bf",
   "metadata": {},
   "source": [
    "### Function to extract the dataset for one patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "399fc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data extraction function for each patient\n",
    "def get_data_for_parent(df, parent_id):\n",
    "    data = df[df['parent_id'] == parent_id]\n",
    "    features = data.drop(columns=['parent_id', 'days_remaining'])\n",
    "    target = data['days_remaining'].values\n",
    "    features_tensor = torch.tensor(features.values).float().unsqueeze(0)  # Add batch dimension\n",
    "    target_tensor = torch.tensor(target).float().unsqueeze(0)  # Add batch dimension\n",
    "    return features_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "641a4419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for parent_id 6...\n",
      "Epoch [1/200] Loss: 717.6364135742188\n",
      "Epoch [2/200] Loss: 701.8953247070312\n",
      "Epoch [3/200] Loss: 686.76611328125\n",
      "Epoch [4/200] Loss: 672.5366821289062\n",
      "Epoch [5/200] Loss: 659.3148193359375\n",
      "Epoch [6/200] Loss: 647.0418090820312\n",
      "Epoch [7/200] Loss: 635.591552734375\n",
      "Epoch [8/200] Loss: 624.823486328125\n",
      "Epoch [9/200] Loss: 614.61572265625\n",
      "Epoch [10/200] Loss: 604.8864135742188\n",
      "Epoch [11/200] Loss: 595.5907592773438\n",
      "Epoch [12/200] Loss: 586.7081298828125\n",
      "Epoch [13/200] Loss: 578.2283935546875\n",
      "Epoch [14/200] Loss: 570.144287109375\n",
      "Epoch [15/200] Loss: 562.4466552734375\n",
      "Epoch [16/200] Loss: 555.1224365234375\n",
      "Epoch [17/200] Loss: 548.1543579101562\n",
      "Epoch [18/200] Loss: 541.5216064453125\n",
      "Epoch [19/200] Loss: 535.2023315429688\n",
      "Epoch [20/200] Loss: 529.1746826171875\n",
      "Epoch [21/200] Loss: 523.41796875\n",
      "Epoch [22/200] Loss: 517.9137573242188\n",
      "Epoch [23/200] Loss: 512.6459350585938\n",
      "Epoch [24/200] Loss: 507.6014404296875\n",
      "Epoch [25/200] Loss: 502.76922607421875\n",
      "Epoch [26/200] Loss: 498.1382141113281\n",
      "Epoch [27/200] Loss: 493.69573974609375\n",
      "Epoch [28/200] Loss: 489.4270324707031\n",
      "Epoch [29/200] Loss: 485.314697265625\n",
      "Epoch [30/200] Loss: 481.3399353027344\n",
      "Epoch [31/200] Loss: 477.4842529296875\n",
      "Epoch [32/200] Loss: 473.7303466796875\n",
      "Epoch [33/200] Loss: 470.06317138671875\n",
      "Epoch [34/200] Loss: 466.4701232910156\n",
      "Epoch [35/200] Loss: 462.94134521484375\n",
      "Epoch [36/200] Loss: 459.4692687988281\n",
      "Epoch [37/200] Loss: 456.0480651855469\n",
      "Epoch [38/200] Loss: 452.67364501953125\n",
      "Epoch [39/200] Loss: 449.34307861328125\n",
      "Epoch [40/200] Loss: 446.0537109375\n",
      "Epoch [41/200] Loss: 442.8040771484375\n",
      "Epoch [42/200] Loss: 439.592529296875\n",
      "Epoch [43/200] Loss: 436.41754150390625\n",
      "Epoch [44/200] Loss: 433.2779541015625\n",
      "Epoch [45/200] Loss: 430.17230224609375\n",
      "Epoch [46/200] Loss: 427.0994873046875\n",
      "Epoch [47/200] Loss: 424.0580139160156\n",
      "Epoch [48/200] Loss: 421.0469055175781\n",
      "Epoch [49/200] Loss: 418.0649108886719\n",
      "Epoch [50/200] Loss: 415.1111755371094\n",
      "Epoch [51/200] Loss: 412.1844787597656\n",
      "Epoch [52/200] Loss: 409.2843933105469\n",
      "Epoch [53/200] Loss: 406.4096984863281\n",
      "Epoch [54/200] Loss: 403.5598449707031\n",
      "Epoch [55/200] Loss: 400.73431396484375\n",
      "Epoch [56/200] Loss: 397.93231201171875\n",
      "Epoch [57/200] Loss: 395.1533203125\n",
      "Epoch [58/200] Loss: 392.3968811035156\n",
      "Epoch [59/200] Loss: 389.6625671386719\n",
      "Epoch [60/200] Loss: 386.9498596191406\n",
      "Epoch [61/200] Loss: 384.2584228515625\n",
      "Epoch [62/200] Loss: 381.5877380371094\n",
      "Epoch [63/200] Loss: 378.93756103515625\n",
      "Epoch [64/200] Loss: 376.3075866699219\n",
      "Epoch [65/200] Loss: 373.6972961425781\n",
      "Epoch [66/200] Loss: 371.1065368652344\n",
      "Epoch [67/200] Loss: 368.534912109375\n",
      "Epoch [68/200] Loss: 365.982177734375\n",
      "Epoch [69/200] Loss: 363.447998046875\n",
      "Epoch [70/200] Loss: 360.9323425292969\n",
      "Epoch [71/200] Loss: 358.434814453125\n",
      "Epoch [72/200] Loss: 355.95501708984375\n",
      "Epoch [73/200] Loss: 353.49285888671875\n",
      "Epoch [74/200] Loss: 351.048095703125\n",
      "Epoch [75/200] Loss: 348.62054443359375\n",
      "Epoch [76/200] Loss: 346.2099609375\n",
      "Epoch [77/200] Loss: 343.816162109375\n",
      "Epoch [78/200] Loss: 341.4388732910156\n",
      "Epoch [79/200] Loss: 339.0780334472656\n",
      "Epoch [80/200] Loss: 336.7333068847656\n",
      "Epoch [81/200] Loss: 334.40460205078125\n",
      "Epoch [82/200] Loss: 332.0917053222656\n",
      "Epoch [83/200] Loss: 329.7944641113281\n",
      "Epoch [84/200] Loss: 327.51275634765625\n",
      "Epoch [85/200] Loss: 325.2463684082031\n",
      "Epoch [86/200] Loss: 322.9951477050781\n",
      "Epoch [87/200] Loss: 320.7589111328125\n",
      "Epoch [88/200] Loss: 318.5376892089844\n",
      "Epoch [89/200] Loss: 316.3310241699219\n",
      "Epoch [90/200] Loss: 314.1390075683594\n",
      "Epoch [91/200] Loss: 311.96142578125\n",
      "Epoch [92/200] Loss: 309.798095703125\n",
      "Epoch [93/200] Loss: 307.6490478515625\n",
      "Epoch [94/200] Loss: 305.51397705078125\n",
      "Epoch [95/200] Loss: 303.39288330078125\n",
      "Epoch [96/200] Loss: 301.28558349609375\n",
      "Epoch [97/200] Loss: 299.19195556640625\n",
      "Epoch [98/200] Loss: 297.1118469238281\n",
      "Epoch [99/200] Loss: 295.04522705078125\n",
      "Epoch [100/200] Loss: 292.9919128417969\n",
      "Epoch [101/200] Loss: 290.9519348144531\n",
      "Epoch [102/200] Loss: 288.92498779296875\n",
      "Epoch [103/200] Loss: 286.9110412597656\n",
      "Epoch [104/200] Loss: 284.9100646972656\n",
      "Epoch [105/200] Loss: 282.9219055175781\n",
      "Epoch [106/200] Loss: 280.9465026855469\n",
      "Epoch [107/200] Loss: 278.9836730957031\n",
      "Epoch [108/200] Loss: 277.03338623046875\n",
      "Epoch [109/200] Loss: 275.0955810546875\n",
      "Epoch [110/200] Loss: 273.1700439453125\n",
      "Epoch [111/200] Loss: 271.2567443847656\n",
      "Epoch [112/200] Loss: 269.355712890625\n",
      "Epoch [113/200] Loss: 267.4667663574219\n",
      "Epoch [114/200] Loss: 265.5896911621094\n",
      "Epoch [115/200] Loss: 263.7246398925781\n",
      "Epoch [116/200] Loss: 261.8713684082031\n",
      "Epoch [117/200] Loss: 260.0299072265625\n",
      "Epoch [118/200] Loss: 258.20013427734375\n",
      "Epoch [119/200] Loss: 256.38189697265625\n",
      "Epoch [120/200] Loss: 254.5752410888672\n",
      "Epoch [121/200] Loss: 252.77992248535156\n",
      "Epoch [122/200] Loss: 250.99610900878906\n",
      "Epoch [123/200] Loss: 249.22357177734375\n",
      "Epoch [124/200] Loss: 247.46217346191406\n",
      "Epoch [125/200] Loss: 245.71205139160156\n",
      "Epoch [126/200] Loss: 243.97293090820312\n",
      "Epoch [127/200] Loss: 242.24485778808594\n",
      "Epoch [128/200] Loss: 240.52774047851562\n",
      "Epoch [129/200] Loss: 238.82147216796875\n",
      "Epoch [130/200] Loss: 237.1260528564453\n",
      "Epoch [131/200] Loss: 235.44137573242188\n",
      "Epoch [132/200] Loss: 233.76744079589844\n",
      "Epoch [133/200] Loss: 232.1040496826172\n",
      "Epoch [134/200] Loss: 230.45127868652344\n",
      "Epoch [135/200] Loss: 228.80897521972656\n",
      "Epoch [136/200] Loss: 227.17709350585938\n",
      "Epoch [137/200] Loss: 225.5555877685547\n",
      "Epoch [138/200] Loss: 223.9444122314453\n",
      "Epoch [139/200] Loss: 222.34349060058594\n",
      "Epoch [140/200] Loss: 220.75274658203125\n",
      "Epoch [141/200] Loss: 219.17221069335938\n",
      "Epoch [142/200] Loss: 217.6016845703125\n",
      "Epoch [143/200] Loss: 216.04124450683594\n",
      "Epoch [144/200] Loss: 214.49069213867188\n",
      "Epoch [145/200] Loss: 212.9500732421875\n",
      "Epoch [146/200] Loss: 211.4193572998047\n",
      "Epoch [147/200] Loss: 209.89842224121094\n",
      "Epoch [148/200] Loss: 208.38719177246094\n",
      "Epoch [149/200] Loss: 206.8856964111328\n",
      "Epoch [150/200] Loss: 205.39382934570312\n",
      "Epoch [151/200] Loss: 203.9115753173828\n",
      "Epoch [152/200] Loss: 202.4387969970703\n",
      "Epoch [153/200] Loss: 200.97549438476562\n",
      "Epoch [154/200] Loss: 199.5216827392578\n",
      "Epoch [155/200] Loss: 198.07720947265625\n",
      "Epoch [156/200] Loss: 196.64205932617188\n",
      "Epoch [157/200] Loss: 195.2161865234375\n",
      "Epoch [158/200] Loss: 193.79957580566406\n",
      "Epoch [159/200] Loss: 192.3921356201172\n",
      "Epoch [160/200] Loss: 190.99375915527344\n",
      "Epoch [161/200] Loss: 189.6045379638672\n",
      "Epoch [162/200] Loss: 188.2243194580078\n",
      "Epoch [163/200] Loss: 186.85305786132812\n",
      "Epoch [164/200] Loss: 185.4907684326172\n",
      "Epoch [165/200] Loss: 184.13734436035156\n",
      "Epoch [166/200] Loss: 182.79275512695312\n",
      "Epoch [167/200] Loss: 181.45700073242188\n",
      "Epoch [168/200] Loss: 180.12994384765625\n",
      "Epoch [169/200] Loss: 178.81158447265625\n",
      "Epoch [170/200] Loss: 177.50192260742188\n",
      "Epoch [171/200] Loss: 176.20082092285156\n",
      "Epoch [172/200] Loss: 174.90834045410156\n",
      "Epoch [173/200] Loss: 173.62429809570312\n",
      "Epoch [174/200] Loss: 172.3488006591797\n",
      "Epoch [175/200] Loss: 171.08168029785156\n",
      "Epoch [176/200] Loss: 169.8229522705078\n",
      "Epoch [177/200] Loss: 168.57257080078125\n",
      "Epoch [178/200] Loss: 167.33053588867188\n",
      "Epoch [179/200] Loss: 166.09666442871094\n",
      "Epoch [180/200] Loss: 164.87109375\n",
      "Epoch [181/200] Loss: 163.65357971191406\n",
      "Epoch [182/200] Loss: 162.44424438476562\n",
      "Epoch [183/200] Loss: 161.2429656982422\n",
      "Epoch [184/200] Loss: 160.0497589111328\n",
      "Epoch [185/200] Loss: 158.8645477294922\n",
      "Epoch [186/200] Loss: 157.68728637695312\n",
      "Epoch [187/200] Loss: 156.5179443359375\n",
      "Epoch [188/200] Loss: 155.35643005371094\n",
      "Epoch [189/200] Loss: 154.20277404785156\n",
      "Epoch [190/200] Loss: 153.05691528320312\n",
      "Epoch [191/200] Loss: 151.9188232421875\n",
      "Epoch [192/200] Loss: 150.7883758544922\n",
      "Epoch [193/200] Loss: 149.66563415527344\n",
      "Epoch [194/200] Loss: 148.550537109375\n",
      "Epoch [195/200] Loss: 147.44296264648438\n",
      "Epoch [196/200] Loss: 146.343017578125\n",
      "Epoch [197/200] Loss: 145.25047302246094\n",
      "Epoch [198/200] Loss: 144.1654815673828\n",
      "Epoch [199/200] Loss: 143.08790588378906\n",
      "Epoch [200/200] Loss: 142.0176544189453\n",
      "Predicted days_remaining for parent_id 6: 15.502007484436035\n",
      "Training for parent_id 14...\n",
      "Epoch [1/200] Loss: 767.79248046875\n",
      "Epoch [2/200] Loss: 754.169189453125\n",
      "Epoch [3/200] Loss: 740.7020263671875\n",
      "Epoch [4/200] Loss: 727.4697265625\n",
      "Epoch [5/200] Loss: 714.53662109375\n",
      "Epoch [6/200] Loss: 701.9505004882812\n",
      "Epoch [7/200] Loss: 689.7488403320312\n",
      "Epoch [8/200] Loss: 677.9641723632812\n",
      "Epoch [9/200] Loss: 666.6221923828125\n",
      "Epoch [10/200] Loss: 655.73974609375\n",
      "Epoch [11/200] Loss: 645.32421875\n",
      "Epoch [12/200] Loss: 635.3756713867188\n",
      "Epoch [13/200] Loss: 625.8892211914062\n",
      "Epoch [14/200] Loss: 616.8561401367188\n",
      "Epoch [15/200] Loss: 608.263916015625\n",
      "Epoch [16/200] Loss: 600.0970458984375\n",
      "Epoch [17/200] Loss: 592.338134765625\n",
      "Epoch [18/200] Loss: 584.968017578125\n",
      "Epoch [19/200] Loss: 577.96630859375\n",
      "Epoch [20/200] Loss: 571.3121337890625\n",
      "Epoch [21/200] Loss: 564.9833984375\n",
      "Epoch [22/200] Loss: 558.957763671875\n",
      "Epoch [23/200] Loss: 553.2124633789062\n",
      "Epoch [24/200] Loss: 547.7247314453125\n",
      "Epoch [25/200] Loss: 542.4721069335938\n",
      "Epoch [26/200] Loss: 537.4334106445312\n",
      "Epoch [27/200] Loss: 532.5884399414062\n",
      "Epoch [28/200] Loss: 527.9182739257812\n",
      "Epoch [29/200] Loss: 523.4058837890625\n",
      "Epoch [30/200] Loss: 519.0360107421875\n",
      "Epoch [31/200] Loss: 514.7947998046875\n",
      "Epoch [32/200] Loss: 510.669677734375\n",
      "Epoch [33/200] Loss: 506.64984130859375\n",
      "Epoch [34/200] Loss: 502.72528076171875\n",
      "Epoch [35/200] Loss: 498.8869323730469\n",
      "Epoch [36/200] Loss: 495.126708984375\n",
      "Epoch [37/200] Loss: 491.4374694824219\n",
      "Epoch [38/200] Loss: 487.8123474121094\n",
      "Epoch [39/200] Loss: 484.24554443359375\n",
      "Epoch [40/200] Loss: 480.7318115234375\n",
      "Epoch [41/200] Loss: 477.2665710449219\n",
      "Epoch [42/200] Loss: 473.8455810546875\n",
      "Epoch [43/200] Loss: 470.4656677246094\n",
      "Epoch [44/200] Loss: 467.1235656738281\n",
      "Epoch [45/200] Loss: 463.81707763671875\n",
      "Epoch [46/200] Loss: 460.54388427734375\n",
      "Epoch [47/200] Loss: 457.30230712890625\n",
      "Epoch [48/200] Loss: 454.09075927734375\n",
      "Epoch [49/200] Loss: 450.9081726074219\n",
      "Epoch [50/200] Loss: 447.75335693359375\n",
      "Epoch [51/200] Loss: 444.62542724609375\n",
      "Epoch [52/200] Loss: 441.5237731933594\n",
      "Epoch [53/200] Loss: 438.44757080078125\n",
      "Epoch [54/200] Loss: 435.3964538574219\n",
      "Epoch [55/200] Loss: 432.3699645996094\n",
      "Epoch [56/200] Loss: 429.367431640625\n",
      "Epoch [57/200] Loss: 426.3885803222656\n",
      "Epoch [58/200] Loss: 423.4330749511719\n",
      "Epoch [59/200] Loss: 420.50048828125\n",
      "Epoch [60/200] Loss: 417.59051513671875\n",
      "Epoch [61/200] Loss: 414.7029113769531\n",
      "Epoch [62/200] Loss: 411.8372497558594\n",
      "Epoch [63/200] Loss: 408.9932861328125\n",
      "Epoch [64/200] Loss: 406.1708068847656\n",
      "Epoch [65/200] Loss: 403.3695068359375\n",
      "Epoch [66/200] Loss: 400.5890808105469\n",
      "Epoch [67/200] Loss: 397.829345703125\n",
      "Epoch [68/200] Loss: 395.0898742675781\n",
      "Epoch [69/200] Loss: 392.3707580566406\n",
      "Epoch [70/200] Loss: 389.67144775390625\n",
      "Epoch [71/200] Loss: 386.99188232421875\n",
      "Epoch [72/200] Loss: 384.33184814453125\n",
      "Epoch [73/200] Loss: 381.6910400390625\n",
      "Epoch [74/200] Loss: 379.0692443847656\n",
      "Epoch [75/200] Loss: 376.46630859375\n",
      "Epoch [76/200] Loss: 373.88189697265625\n",
      "Epoch [77/200] Loss: 371.3158874511719\n",
      "Epoch [78/200] Loss: 368.7681884765625\n",
      "Epoch [79/200] Loss: 366.2384948730469\n",
      "Epoch [80/200] Loss: 363.7264404296875\n",
      "Epoch [81/200] Loss: 361.2320251464844\n",
      "Epoch [82/200] Loss: 358.755126953125\n",
      "Epoch [83/200] Loss: 356.2953796386719\n",
      "Epoch [84/200] Loss: 353.8526611328125\n",
      "Epoch [85/200] Loss: 351.4268493652344\n",
      "Epoch [86/200] Loss: 349.0176086425781\n",
      "Epoch [87/200] Loss: 346.62493896484375\n",
      "Epoch [88/200] Loss: 344.2485656738281\n",
      "Epoch [89/200] Loss: 341.88836669921875\n",
      "Epoch [90/200] Loss: 339.5440673828125\n",
      "Epoch [91/200] Loss: 337.21575927734375\n",
      "Epoch [92/200] Loss: 334.90301513671875\n",
      "Epoch [93/200] Loss: 332.6058044433594\n",
      "Epoch [94/200] Loss: 330.323974609375\n",
      "Epoch [95/200] Loss: 328.0572509765625\n",
      "Epoch [96/200] Loss: 325.8057861328125\n",
      "Epoch [97/200] Loss: 323.569091796875\n",
      "Epoch [98/200] Loss: 321.34722900390625\n",
      "Epoch [99/200] Loss: 319.1400451660156\n",
      "Epoch [100/200] Loss: 316.947265625\n",
      "Epoch [101/200] Loss: 314.76898193359375\n",
      "Epoch [102/200] Loss: 312.6048889160156\n",
      "Epoch [103/200] Loss: 310.4549560546875\n",
      "Epoch [104/200] Loss: 308.31903076171875\n",
      "Epoch [105/200] Loss: 306.19696044921875\n",
      "Epoch [106/200] Loss: 304.0887145996094\n",
      "Epoch [107/200] Loss: 301.9939270019531\n",
      "Epoch [108/200] Loss: 299.912841796875\n",
      "Epoch [109/200] Loss: 297.84515380859375\n",
      "Epoch [110/200] Loss: 295.79083251953125\n",
      "Epoch [111/200] Loss: 293.7496032714844\n",
      "Epoch [112/200] Loss: 291.7215270996094\n",
      "Epoch [113/200] Loss: 289.7064208984375\n",
      "Epoch [114/200] Loss: 287.7042236328125\n",
      "Epoch [115/200] Loss: 285.71490478515625\n",
      "Epoch [116/200] Loss: 283.7381591796875\n",
      "Epoch [117/200] Loss: 281.7740478515625\n",
      "Epoch [118/200] Loss: 279.822509765625\n",
      "Epoch [119/200] Loss: 277.8833923339844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/200] Loss: 275.95654296875\n",
      "Epoch [121/200] Loss: 274.0419616699219\n",
      "Epoch [122/200] Loss: 272.1395263671875\n",
      "Epoch [123/200] Loss: 270.24920654296875\n",
      "Epoch [124/200] Loss: 268.37078857421875\n",
      "Epoch [125/200] Loss: 266.5043640136719\n",
      "Epoch [126/200] Loss: 264.649658203125\n",
      "Epoch [127/200] Loss: 262.8067626953125\n",
      "Epoch [128/200] Loss: 260.9754638671875\n",
      "Epoch [129/200] Loss: 259.1557312011719\n",
      "Epoch [130/200] Loss: 257.3475646972656\n",
      "Epoch [131/200] Loss: 255.55079650878906\n",
      "Epoch [132/200] Loss: 253.765380859375\n",
      "Epoch [133/200] Loss: 251.99127197265625\n",
      "Epoch [134/200] Loss: 250.228271484375\n",
      "Epoch [135/200] Loss: 248.47654724121094\n",
      "Epoch [136/200] Loss: 246.7357940673828\n",
      "Epoch [137/200] Loss: 245.00607299804688\n",
      "Epoch [138/200] Loss: 243.2872772216797\n",
      "Epoch [139/200] Loss: 241.57931518554688\n",
      "Epoch [140/200] Loss: 239.8821563720703\n",
      "Epoch [141/200] Loss: 238.19566345214844\n",
      "Epoch [142/200] Loss: 236.51995849609375\n",
      "Epoch [143/200] Loss: 234.854736328125\n",
      "Epoch [144/200] Loss: 233.2001190185547\n",
      "Epoch [145/200] Loss: 231.55592346191406\n",
      "Epoch [146/200] Loss: 229.922119140625\n",
      "Epoch [147/200] Loss: 228.29867553710938\n",
      "Epoch [148/200] Loss: 226.68557739257812\n",
      "Epoch [149/200] Loss: 225.08265686035156\n",
      "Epoch [150/200] Loss: 223.4898681640625\n",
      "Epoch [151/200] Loss: 221.90721130371094\n",
      "Epoch [152/200] Loss: 220.33457946777344\n",
      "Epoch [153/200] Loss: 218.77194213867188\n",
      "Epoch [154/200] Loss: 217.21926879882812\n",
      "Epoch [155/200] Loss: 215.6764373779297\n",
      "Epoch [156/200] Loss: 214.1434783935547\n",
      "Epoch [157/200] Loss: 212.6202392578125\n",
      "Epoch [158/200] Loss: 211.1067352294922\n",
      "Epoch [159/200] Loss: 209.60279846191406\n",
      "Epoch [160/200] Loss: 208.10855102539062\n",
      "Epoch [161/200] Loss: 206.6238250732422\n",
      "Epoch [162/200] Loss: 205.14859008789062\n",
      "Epoch [163/200] Loss: 203.68280029296875\n",
      "Epoch [164/200] Loss: 202.22642517089844\n",
      "Epoch [165/200] Loss: 200.77935791015625\n",
      "Epoch [166/200] Loss: 199.34158325195312\n",
      "Epoch [167/200] Loss: 197.9130401611328\n",
      "Epoch [168/200] Loss: 196.49366760253906\n",
      "Epoch [169/200] Loss: 195.0834503173828\n",
      "Epoch [170/200] Loss: 193.68231201171875\n",
      "Epoch [171/200] Loss: 192.29022216796875\n",
      "Epoch [172/200] Loss: 190.90711975097656\n",
      "Epoch [173/200] Loss: 189.5329132080078\n",
      "Epoch [174/200] Loss: 188.1676025390625\n",
      "Epoch [175/200] Loss: 186.8111572265625\n",
      "Epoch [176/200] Loss: 185.46351623535156\n",
      "Epoch [177/200] Loss: 184.12460327148438\n",
      "Epoch [178/200] Loss: 182.79443359375\n",
      "Epoch [179/200] Loss: 181.472900390625\n",
      "Epoch [180/200] Loss: 180.15992736816406\n",
      "Epoch [181/200] Loss: 178.85552978515625\n",
      "Epoch [182/200] Loss: 177.55967712402344\n",
      "Epoch [183/200] Loss: 176.27227783203125\n",
      "Epoch [184/200] Loss: 174.9933624267578\n",
      "Epoch [185/200] Loss: 173.72274780273438\n",
      "Epoch [186/200] Loss: 172.46044921875\n",
      "Epoch [187/200] Loss: 171.20652770996094\n",
      "Epoch [188/200] Loss: 169.9608612060547\n",
      "Epoch [189/200] Loss: 168.7233123779297\n",
      "Epoch [190/200] Loss: 167.49398803710938\n",
      "Epoch [191/200] Loss: 166.27276611328125\n",
      "Epoch [192/200] Loss: 165.05960083007812\n",
      "Epoch [193/200] Loss: 163.8544921875\n",
      "Epoch [194/200] Loss: 162.65736389160156\n",
      "Epoch [195/200] Loss: 161.4681854248047\n",
      "Epoch [196/200] Loss: 160.28689575195312\n",
      "Epoch [197/200] Loss: 159.11349487304688\n",
      "Epoch [198/200] Loss: 157.94790649414062\n",
      "Epoch [199/200] Loss: 156.7900848388672\n",
      "Epoch [200/200] Loss: 155.64004516601562\n",
      "Predicted days_remaining for parent_id 14: 15.915298461914062\n",
      "Training for parent_id 15...\n",
      "Epoch [1/200] Loss: 844.3501586914062\n",
      "Epoch [2/200] Loss: 831.3956298828125\n",
      "Epoch [3/200] Loss: 818.5361328125\n",
      "Epoch [4/200] Loss: 805.9185791015625\n",
      "Epoch [5/200] Loss: 793.6458129882812\n",
      "Epoch [6/200] Loss: 781.7532348632812\n",
      "Epoch [7/200] Loss: 770.2290649414062\n",
      "Epoch [8/200] Loss: 759.0471801757812\n",
      "Epoch [9/200] Loss: 748.1829223632812\n",
      "Epoch [10/200] Loss: 737.6207275390625\n",
      "Epoch [11/200] Loss: 727.3551025390625\n",
      "Epoch [12/200] Loss: 717.3893432617188\n",
      "Epoch [13/200] Loss: 707.7318115234375\n",
      "Epoch [14/200] Loss: 698.393310546875\n",
      "Epoch [15/200] Loss: 689.3861083984375\n",
      "Epoch [16/200] Loss: 680.7202758789062\n",
      "Epoch [17/200] Loss: 672.403076171875\n",
      "Epoch [18/200] Loss: 664.4370727539062\n",
      "Epoch [19/200] Loss: 656.8200073242188\n",
      "Epoch [20/200] Loss: 649.5449829101562\n",
      "Epoch [21/200] Loss: 642.601318359375\n",
      "Epoch [22/200] Loss: 635.9741821289062\n",
      "Epoch [23/200] Loss: 629.64599609375\n",
      "Epoch [24/200] Loss: 623.5955810546875\n",
      "Epoch [25/200] Loss: 617.8001708984375\n",
      "Epoch [26/200] Loss: 612.2353515625\n",
      "Epoch [27/200] Loss: 606.8772583007812\n",
      "Epoch [28/200] Loss: 601.702880859375\n",
      "Epoch [29/200] Loss: 596.6926879882812\n",
      "Epoch [30/200] Loss: 591.8301391601562\n",
      "Epoch [31/200] Loss: 587.1035766601562\n",
      "Epoch [32/200] Loss: 582.5051879882812\n",
      "Epoch [33/200] Loss: 578.0300903320312\n",
      "Epoch [34/200] Loss: 573.6749267578125\n",
      "Epoch [35/200] Loss: 569.4354858398438\n",
      "Epoch [36/200] Loss: 565.3059692382812\n",
      "Epoch [37/200] Loss: 561.2786254882812\n",
      "Epoch [38/200] Loss: 557.3446655273438\n",
      "Epoch [39/200] Loss: 553.494140625\n",
      "Epoch [40/200] Loss: 549.71826171875\n",
      "Epoch [41/200] Loss: 546.0084838867188\n",
      "Epoch [42/200] Loss: 542.357666015625\n",
      "Epoch [43/200] Loss: 538.7598876953125\n",
      "Epoch [44/200] Loss: 535.2101440429688\n",
      "Epoch [45/200] Loss: 531.704345703125\n",
      "Epoch [46/200] Loss: 528.2393188476562\n",
      "Epoch [47/200] Loss: 524.8121337890625\n",
      "Epoch [48/200] Loss: 521.420654296875\n",
      "Epoch [49/200] Loss: 518.0631103515625\n",
      "Epoch [50/200] Loss: 514.73779296875\n",
      "Epoch [51/200] Loss: 511.44354248046875\n",
      "Epoch [52/200] Loss: 508.1791687011719\n",
      "Epoch [53/200] Loss: 504.94354248046875\n",
      "Epoch [54/200] Loss: 501.7358093261719\n",
      "Epoch [55/200] Loss: 498.5550231933594\n",
      "Epoch [56/200] Loss: 495.400146484375\n",
      "Epoch [57/200] Loss: 492.270263671875\n",
      "Epoch [58/200] Loss: 489.1644287109375\n",
      "Epoch [59/200] Loss: 486.08160400390625\n",
      "Epoch [60/200] Loss: 483.0209045410156\n",
      "Epoch [61/200] Loss: 479.9813537597656\n",
      "Epoch [62/200] Loss: 476.9622802734375\n",
      "Epoch [63/200] Loss: 473.9627685546875\n",
      "Epoch [64/200] Loss: 470.9824523925781\n",
      "Epoch [65/200] Loss: 468.0209045410156\n",
      "Epoch [66/200] Loss: 465.07781982421875\n",
      "Epoch [67/200] Loss: 462.15301513671875\n",
      "Epoch [68/200] Loss: 459.2464599609375\n",
      "Epoch [69/200] Loss: 456.35809326171875\n",
      "Epoch [70/200] Loss: 453.4880065917969\n",
      "Epoch [71/200] Loss: 450.6360168457031\n",
      "Epoch [72/200] Loss: 447.80218505859375\n",
      "Epoch [73/200] Loss: 444.9864807128906\n",
      "Epoch [74/200] Loss: 442.1886901855469\n",
      "Epoch [75/200] Loss: 439.4087219238281\n",
      "Epoch [76/200] Loss: 436.6465148925781\n",
      "Epoch [77/200] Loss: 433.9018859863281\n",
      "Epoch [78/200] Loss: 431.1745300292969\n",
      "Epoch [79/200] Loss: 428.4644470214844\n",
      "Epoch [80/200] Loss: 425.7713623046875\n",
      "Epoch [81/200] Loss: 423.0950012207031\n",
      "Epoch [82/200] Loss: 420.4352722167969\n",
      "Epoch [83/200] Loss: 417.7919616699219\n",
      "Epoch [84/200] Loss: 415.1650085449219\n",
      "Epoch [85/200] Loss: 412.55401611328125\n",
      "Epoch [86/200] Loss: 409.9590148925781\n",
      "Epoch [87/200] Loss: 407.3797302246094\n",
      "Epoch [88/200] Loss: 404.81610107421875\n",
      "Epoch [89/200] Loss: 402.26800537109375\n",
      "Epoch [90/200] Loss: 399.73504638671875\n",
      "Epoch [91/200] Loss: 397.21734619140625\n",
      "Epoch [92/200] Loss: 394.7146911621094\n",
      "Epoch [93/200] Loss: 392.2268981933594\n",
      "Epoch [94/200] Loss: 389.7539367675781\n",
      "Epoch [95/200] Loss: 387.2955322265625\n",
      "Epoch [96/200] Loss: 384.851806640625\n",
      "Epoch [97/200] Loss: 382.42242431640625\n",
      "Epoch [98/200] Loss: 380.0073547363281\n",
      "Epoch [99/200] Loss: 377.6064453125\n",
      "Epoch [100/200] Loss: 375.2196350097656\n",
      "Epoch [101/200] Loss: 372.84674072265625\n",
      "Epoch [102/200] Loss: 370.4877014160156\n",
      "Epoch [103/200] Loss: 368.1424255371094\n",
      "Epoch [104/200] Loss: 365.8108215332031\n",
      "Epoch [105/200] Loss: 363.4927978515625\n",
      "Epoch [106/200] Loss: 361.18817138671875\n",
      "Epoch [107/200] Loss: 358.89697265625\n",
      "Epoch [108/200] Loss: 356.618896484375\n",
      "Epoch [109/200] Loss: 354.3541564941406\n",
      "Epoch [110/200] Loss: 352.1023864746094\n",
      "Epoch [111/200] Loss: 349.8636779785156\n",
      "Epoch [112/200] Loss: 347.6377868652344\n",
      "Epoch [113/200] Loss: 345.42474365234375\n",
      "Epoch [114/200] Loss: 343.2244567871094\n",
      "Epoch [115/200] Loss: 341.03680419921875\n",
      "Epoch [116/200] Loss: 338.8617248535156\n",
      "Epoch [117/200] Loss: 336.69903564453125\n",
      "Epoch [118/200] Loss: 334.5487976074219\n",
      "Epoch [119/200] Loss: 332.41094970703125\n",
      "Epoch [120/200] Loss: 330.2852783203125\n",
      "Epoch [121/200] Loss: 328.17169189453125\n",
      "Epoch [122/200] Loss: 326.0703430175781\n",
      "Epoch [123/200] Loss: 323.98095703125\n",
      "Epoch [124/200] Loss: 321.9035339355469\n",
      "Epoch [125/200] Loss: 319.8380126953125\n",
      "Epoch [126/200] Loss: 317.7841796875\n",
      "Epoch [127/200] Loss: 315.7420959472656\n",
      "Epoch [128/200] Loss: 313.7117614746094\n",
      "Epoch [129/200] Loss: 311.6929931640625\n",
      "Epoch [130/200] Loss: 309.6856994628906\n",
      "Epoch [131/200] Loss: 307.68988037109375\n",
      "Epoch [132/200] Loss: 305.70550537109375\n",
      "Epoch [133/200] Loss: 303.7324523925781\n",
      "Epoch [134/200] Loss: 301.77069091796875\n",
      "Epoch [135/200] Loss: 299.8200378417969\n",
      "Epoch [136/200] Loss: 297.880615234375\n",
      "Epoch [137/200] Loss: 295.95220947265625\n",
      "Epoch [138/200] Loss: 294.03485107421875\n",
      "Epoch [139/200] Loss: 292.1284484863281\n",
      "Epoch [140/200] Loss: 290.2329406738281\n",
      "Epoch [141/200] Loss: 288.3482666015625\n",
      "Epoch [142/200] Loss: 286.4743957519531\n",
      "Epoch [143/200] Loss: 284.6112060546875\n",
      "Epoch [144/200] Loss: 282.7586669921875\n",
      "Epoch [145/200] Loss: 280.9168701171875\n",
      "Epoch [146/200] Loss: 279.08551025390625\n",
      "Epoch [147/200] Loss: 277.2646179199219\n",
      "Epoch [148/200] Loss: 275.45428466796875\n",
      "Epoch [149/200] Loss: 273.6542663574219\n",
      "Epoch [150/200] Loss: 271.8645324707031\n",
      "Epoch [151/200] Loss: 270.085205078125\n",
      "Epoch [152/200] Loss: 268.3160400390625\n",
      "Epoch [153/200] Loss: 266.55706787109375\n",
      "Epoch [154/200] Loss: 264.8081359863281\n",
      "Epoch [155/200] Loss: 263.0694274902344\n",
      "Epoch [156/200] Loss: 261.3407287597656\n",
      "Epoch [157/200] Loss: 259.6218566894531\n",
      "Epoch [158/200] Loss: 257.91302490234375\n",
      "Epoch [159/200] Loss: 256.2140197753906\n",
      "Epoch [160/200] Loss: 254.5248565673828\n",
      "Epoch [161/200] Loss: 252.845458984375\n",
      "Epoch [162/200] Loss: 251.17579650878906\n",
      "Epoch [163/200] Loss: 249.51580810546875\n",
      "Epoch [164/200] Loss: 247.86549377441406\n",
      "Epoch [165/200] Loss: 246.22471618652344\n",
      "Epoch [166/200] Loss: 244.5934600830078\n",
      "Epoch [167/200] Loss: 242.97171020507812\n",
      "Epoch [168/200] Loss: 241.35939025878906\n",
      "Epoch [169/200] Loss: 239.75643920898438\n",
      "Epoch [170/200] Loss: 238.16293334960938\n",
      "Epoch [171/200] Loss: 236.57862854003906\n",
      "Epoch [172/200] Loss: 235.00367736816406\n",
      "Epoch [173/200] Loss: 233.4378662109375\n",
      "Epoch [174/200] Loss: 231.88128662109375\n",
      "Epoch [175/200] Loss: 230.3337860107422\n",
      "Epoch [176/200] Loss: 228.79539489746094\n",
      "Epoch [177/200] Loss: 227.26597595214844\n",
      "Epoch [178/200] Loss: 225.74566650390625\n",
      "Epoch [179/200] Loss: 224.23426818847656\n",
      "Epoch [180/200] Loss: 222.7317657470703\n",
      "Epoch [181/200] Loss: 221.2381134033203\n",
      "Epoch [182/200] Loss: 219.75326538085938\n",
      "Epoch [183/200] Loss: 218.2772979736328\n",
      "Epoch [184/200] Loss: 216.8099822998047\n",
      "Epoch [185/200] Loss: 215.35142517089844\n",
      "Epoch [186/200] Loss: 213.90150451660156\n",
      "Epoch [187/200] Loss: 212.4601593017578\n",
      "Epoch [188/200] Loss: 211.02743530273438\n",
      "Epoch [189/200] Loss: 209.60324096679688\n",
      "Epoch [190/200] Loss: 208.1875\n",
      "Epoch [191/200] Loss: 206.7802276611328\n",
      "Epoch [192/200] Loss: 205.38140869140625\n",
      "Epoch [193/200] Loss: 203.99095153808594\n",
      "Epoch [194/200] Loss: 202.6088409423828\n",
      "Epoch [195/200] Loss: 201.23500061035156\n",
      "Epoch [196/200] Loss: 199.8694305419922\n",
      "Epoch [197/200] Loss: 198.5120849609375\n",
      "Epoch [198/200] Loss: 197.1629180908203\n",
      "Epoch [199/200] Loss: 195.8218231201172\n",
      "Epoch [200/200] Loss: 194.48890686035156\n",
      "Predicted days_remaining for parent_id 15: 15.381135940551758\n",
      "Training for parent_id 25...\n",
      "Epoch [1/200] Loss: 4430.89013671875\n",
      "Epoch [2/200] Loss: 4399.69287109375\n",
      "Epoch [3/200] Loss: 4369.55859375\n",
      "Epoch [4/200] Loss: 4340.46826171875\n",
      "Epoch [5/200] Loss: 4312.41552734375\n",
      "Epoch [6/200] Loss: 4285.39794921875\n",
      "Epoch [7/200] Loss: 4259.42919921875\n",
      "Epoch [8/200] Loss: 4234.5361328125\n",
      "Epoch [9/200] Loss: 4210.73046875\n",
      "Epoch [10/200] Loss: 4187.99169921875\n",
      "Epoch [11/200] Loss: 4166.26708984375\n",
      "Epoch [12/200] Loss: 4145.4765625\n",
      "Epoch [13/200] Loss: 4125.52880859375\n",
      "Epoch [14/200] Loss: 4106.3310546875\n",
      "Epoch [15/200] Loss: 4087.802001953125\n",
      "Epoch [16/200] Loss: 4069.876953125\n",
      "Epoch [17/200] Loss: 4052.5126953125\n",
      "Epoch [18/200] Loss: 4035.68505859375\n",
      "Epoch [19/200] Loss: 4019.384521484375\n",
      "Epoch [20/200] Loss: 4003.612548828125\n",
      "Epoch [21/200] Loss: 3988.374755859375\n",
      "Epoch [22/200] Loss: 3973.67578125\n",
      "Epoch [23/200] Loss: 3959.51513671875\n",
      "Epoch [24/200] Loss: 3945.8837890625\n",
      "Epoch [25/200] Loss: 3932.763671875\n",
      "Epoch [26/200] Loss: 3920.12646484375\n",
      "Epoch [27/200] Loss: 3907.9345703125\n",
      "Epoch [28/200] Loss: 3896.148193359375\n",
      "Epoch [29/200] Loss: 3884.7216796875\n",
      "Epoch [30/200] Loss: 3873.61181640625\n",
      "Epoch [31/200] Loss: 3862.775634765625\n",
      "Epoch [32/200] Loss: 3852.174560546875\n",
      "Epoch [33/200] Loss: 3841.7734375\n",
      "Epoch [34/200] Loss: 3831.54248046875\n",
      "Epoch [35/200] Loss: 3821.453369140625\n",
      "Epoch [36/200] Loss: 3811.484619140625\n",
      "Epoch [37/200] Loss: 3801.61669921875\n",
      "Epoch [38/200] Loss: 3791.832763671875\n",
      "Epoch [39/200] Loss: 3782.118896484375\n",
      "Epoch [40/200] Loss: 3772.46435546875\n",
      "Epoch [41/200] Loss: 3762.859375\n",
      "Epoch [42/200] Loss: 3753.29736328125\n",
      "Epoch [43/200] Loss: 3743.7724609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/200] Loss: 3734.28076171875\n",
      "Epoch [45/200] Loss: 3724.8193359375\n",
      "Epoch [46/200] Loss: 3715.387451171875\n",
      "Epoch [47/200] Loss: 3705.9833984375\n",
      "Epoch [48/200] Loss: 3696.6083984375\n",
      "Epoch [49/200] Loss: 3687.262451171875\n",
      "Epoch [50/200] Loss: 3677.946044921875\n",
      "Epoch [51/200] Loss: 3668.6611328125\n",
      "Epoch [52/200] Loss: 3659.408203125\n",
      "Epoch [53/200] Loss: 3650.189208984375\n",
      "Epoch [54/200] Loss: 3641.005615234375\n",
      "Epoch [55/200] Loss: 3631.8583984375\n",
      "Epoch [56/200] Loss: 3622.748046875\n",
      "Epoch [57/200] Loss: 3613.67578125\n",
      "Epoch [58/200] Loss: 3604.64208984375\n",
      "Epoch [59/200] Loss: 3595.646484375\n",
      "Epoch [60/200] Loss: 3586.690185546875\n",
      "Epoch [61/200] Loss: 3577.77197265625\n",
      "Epoch [62/200] Loss: 3568.8916015625\n",
      "Epoch [63/200] Loss: 3560.048828125\n",
      "Epoch [64/200] Loss: 3551.242919921875\n",
      "Epoch [65/200] Loss: 3542.47412109375\n",
      "Epoch [66/200] Loss: 3533.74072265625\n",
      "Epoch [67/200] Loss: 3525.04248046875\n",
      "Epoch [68/200] Loss: 3516.37890625\n",
      "Epoch [69/200] Loss: 3507.75\n",
      "Epoch [70/200] Loss: 3499.154541015625\n",
      "Epoch [71/200] Loss: 3490.591552734375\n",
      "Epoch [72/200] Loss: 3482.06201171875\n",
      "Epoch [73/200] Loss: 3473.56396484375\n",
      "Epoch [74/200] Loss: 3465.097412109375\n",
      "Epoch [75/200] Loss: 3456.661376953125\n",
      "Epoch [76/200] Loss: 3448.2568359375\n",
      "Epoch [77/200] Loss: 3439.881591796875\n",
      "Epoch [78/200] Loss: 3431.535888671875\n",
      "Epoch [79/200] Loss: 3423.219970703125\n",
      "Epoch [80/200] Loss: 3414.932861328125\n",
      "Epoch [81/200] Loss: 3406.674072265625\n",
      "Epoch [82/200] Loss: 3398.442626953125\n",
      "Epoch [83/200] Loss: 3390.239990234375\n",
      "Epoch [84/200] Loss: 3382.06298828125\n",
      "Epoch [85/200] Loss: 3373.9140625\n",
      "Epoch [86/200] Loss: 3365.790771484375\n",
      "Epoch [87/200] Loss: 3357.69384765625\n",
      "Epoch [88/200] Loss: 3349.62255859375\n",
      "Epoch [89/200] Loss: 3341.57666015625\n",
      "Epoch [90/200] Loss: 3333.556396484375\n",
      "Epoch [91/200] Loss: 3325.560546875\n",
      "Epoch [92/200] Loss: 3317.589111328125\n",
      "Epoch [93/200] Loss: 3309.641845703125\n",
      "Epoch [94/200] Loss: 3301.71875\n",
      "Epoch [95/200] Loss: 3293.81884765625\n",
      "Epoch [96/200] Loss: 3285.9423828125\n",
      "Epoch [97/200] Loss: 3278.088623046875\n",
      "Epoch [98/200] Loss: 3270.258056640625\n",
      "Epoch [99/200] Loss: 3262.4501953125\n",
      "Epoch [100/200] Loss: 3254.6640625\n",
      "Epoch [101/200] Loss: 3246.900146484375\n",
      "Epoch [102/200] Loss: 3239.15869140625\n",
      "Epoch [103/200] Loss: 3231.43798828125\n",
      "Epoch [104/200] Loss: 3223.7392578125\n",
      "Epoch [105/200] Loss: 3216.0615234375\n",
      "Epoch [106/200] Loss: 3208.404296875\n",
      "Epoch [107/200] Loss: 3200.767578125\n",
      "Epoch [108/200] Loss: 3193.15185546875\n",
      "Epoch [109/200] Loss: 3185.556640625\n",
      "Epoch [110/200] Loss: 3177.9814453125\n",
      "Epoch [111/200] Loss: 3170.42578125\n",
      "Epoch [112/200] Loss: 3162.89013671875\n",
      "Epoch [113/200] Loss: 3155.374267578125\n",
      "Epoch [114/200] Loss: 3147.87744140625\n",
      "Epoch [115/200] Loss: 3140.400390625\n",
      "Epoch [116/200] Loss: 3132.94189453125\n",
      "Epoch [117/200] Loss: 3125.502685546875\n",
      "Epoch [118/200] Loss: 3118.081787109375\n",
      "Epoch [119/200] Loss: 3110.679931640625\n",
      "Epoch [120/200] Loss: 3103.29638671875\n",
      "Epoch [121/200] Loss: 3095.931640625\n",
      "Epoch [122/200] Loss: 3088.584716796875\n",
      "Epoch [123/200] Loss: 3081.255615234375\n",
      "Epoch [124/200] Loss: 3073.94482421875\n",
      "Epoch [125/200] Loss: 3066.65185546875\n",
      "Epoch [126/200] Loss: 3059.376708984375\n",
      "Epoch [127/200] Loss: 3052.118408203125\n",
      "Epoch [128/200] Loss: 3044.87841796875\n",
      "Epoch [129/200] Loss: 3037.65478515625\n",
      "Epoch [130/200] Loss: 3030.44873046875\n",
      "Epoch [131/200] Loss: 3023.26025390625\n",
      "Epoch [132/200] Loss: 3016.088134765625\n",
      "Epoch [133/200] Loss: 3008.933349609375\n",
      "Epoch [134/200] Loss: 3001.794921875\n",
      "Epoch [135/200] Loss: 2994.673828125\n",
      "Epoch [136/200] Loss: 2987.56884765625\n",
      "Epoch [137/200] Loss: 2980.480224609375\n",
      "Epoch [138/200] Loss: 2973.408447265625\n",
      "Epoch [139/200] Loss: 2966.352294921875\n",
      "Epoch [140/200] Loss: 2959.3125\n",
      "Epoch [141/200] Loss: 2952.289306640625\n",
      "Epoch [142/200] Loss: 2945.281494140625\n",
      "Epoch [143/200] Loss: 2938.289794921875\n",
      "Epoch [144/200] Loss: 2931.314208984375\n",
      "Epoch [145/200] Loss: 2924.35400390625\n",
      "Epoch [146/200] Loss: 2917.409423828125\n",
      "Epoch [147/200] Loss: 2910.48095703125\n",
      "Epoch [148/200] Loss: 2903.567626953125\n",
      "Epoch [149/200] Loss: 2896.669921875\n",
      "Epoch [150/200] Loss: 2889.787109375\n",
      "Epoch [151/200] Loss: 2882.919921875\n",
      "Epoch [152/200] Loss: 2876.06787109375\n",
      "Epoch [153/200] Loss: 2869.23095703125\n",
      "Epoch [154/200] Loss: 2862.409423828125\n",
      "Epoch [155/200] Loss: 2855.6025390625\n",
      "Epoch [156/200] Loss: 2848.810546875\n",
      "Epoch [157/200] Loss: 2842.033447265625\n",
      "Epoch [158/200] Loss: 2835.271484375\n",
      "Epoch [159/200] Loss: 2828.5234375\n",
      "Epoch [160/200] Loss: 2821.791015625\n",
      "Epoch [161/200] Loss: 2815.07275390625\n",
      "Epoch [162/200] Loss: 2808.369140625\n",
      "Epoch [163/200] Loss: 2801.68017578125\n",
      "Epoch [164/200] Loss: 2795.0048828125\n",
      "Epoch [165/200] Loss: 2788.34423828125\n",
      "Epoch [166/200] Loss: 2781.69873046875\n",
      "Epoch [167/200] Loss: 2775.067138671875\n",
      "Epoch [168/200] Loss: 2768.448974609375\n",
      "Epoch [169/200] Loss: 2761.845458984375\n",
      "Epoch [170/200] Loss: 2755.25634765625\n",
      "Epoch [171/200] Loss: 2748.680419921875\n",
      "Epoch [172/200] Loss: 2742.118896484375\n",
      "Epoch [173/200] Loss: 2735.571533203125\n",
      "Epoch [174/200] Loss: 2729.0380859375\n",
      "Epoch [175/200] Loss: 2722.518310546875\n",
      "Epoch [176/200] Loss: 2716.011962890625\n",
      "Epoch [177/200] Loss: 2709.52001953125\n",
      "Epoch [178/200] Loss: 2703.04150390625\n",
      "Epoch [179/200] Loss: 2696.575927734375\n",
      "Epoch [180/200] Loss: 2690.125\n",
      "Epoch [181/200] Loss: 2683.686767578125\n",
      "Epoch [182/200] Loss: 2677.2626953125\n",
      "Epoch [183/200] Loss: 2670.8515625\n",
      "Epoch [184/200] Loss: 2664.454345703125\n",
      "Epoch [185/200] Loss: 2658.0703125\n",
      "Epoch [186/200] Loss: 2651.698974609375\n",
      "Epoch [187/200] Loss: 2645.34130859375\n",
      "Epoch [188/200] Loss: 2638.997314453125\n",
      "Epoch [189/200] Loss: 2632.665771484375\n",
      "Epoch [190/200] Loss: 2626.34814453125\n",
      "Epoch [191/200] Loss: 2620.04296875\n",
      "Epoch [192/200] Loss: 2613.7509765625\n",
      "Epoch [193/200] Loss: 2607.472412109375\n",
      "Epoch [194/200] Loss: 2601.206787109375\n",
      "Epoch [195/200] Loss: 2594.953369140625\n",
      "Epoch [196/200] Loss: 2588.713623046875\n",
      "Epoch [197/200] Loss: 2582.486328125\n",
      "Epoch [198/200] Loss: 2576.272216796875\n",
      "Epoch [199/200] Loss: 2570.070556640625\n",
      "Epoch [200/200] Loss: 2563.882080078125\n",
      "Predicted days_remaining for parent_id 25: 16.31917381286621\n",
      "Training for parent_id 37...\n",
      "Epoch [1/200] Loss: 159.23934936523438\n",
      "Epoch [2/200] Loss: 153.99655151367188\n",
      "Epoch [3/200] Loss: 148.96597290039062\n",
      "Epoch [4/200] Loss: 144.134033203125\n",
      "Epoch [5/200] Loss: 139.4877471923828\n",
      "Epoch [6/200] Loss: 135.01373291015625\n",
      "Epoch [7/200] Loss: 130.7030487060547\n",
      "Epoch [8/200] Loss: 126.55062103271484\n",
      "Epoch [9/200] Loss: 122.55261993408203\n",
      "Epoch [10/200] Loss: 118.70443725585938\n",
      "Epoch [11/200] Loss: 115.00003814697266\n",
      "Epoch [12/200] Loss: 111.43309020996094\n",
      "Epoch [13/200] Loss: 107.99854278564453\n",
      "Epoch [14/200] Loss: 104.69364166259766\n",
      "Epoch [15/200] Loss: 101.51765441894531\n",
      "Epoch [16/200] Loss: 98.47079467773438\n",
      "Epoch [17/200] Loss: 95.55351257324219\n",
      "Epoch [18/200] Loss: 92.76567840576172\n",
      "Epoch [19/200] Loss: 90.10617065429688\n",
      "Epoch [20/200] Loss: 87.57270050048828\n",
      "Epoch [21/200] Loss: 85.16181182861328\n",
      "Epoch [22/200] Loss: 82.8690185546875\n",
      "Epoch [23/200] Loss: 80.68920135498047\n",
      "Epoch [24/200] Loss: 78.61682891845703\n",
      "Epoch [25/200] Loss: 76.64617919921875\n",
      "Epoch [26/200] Loss: 74.7716293334961\n",
      "Epoch [27/200] Loss: 72.98760986328125\n",
      "Epoch [28/200] Loss: 71.28858947753906\n",
      "Epoch [29/200] Loss: 69.66912841796875\n",
      "Epoch [30/200] Loss: 68.12374114990234\n",
      "Epoch [31/200] Loss: 66.6470947265625\n",
      "Epoch [32/200] Loss: 65.23389434814453\n",
      "Epoch [33/200] Loss: 63.87906265258789\n",
      "Epoch [34/200] Loss: 62.5777702331543\n",
      "Epoch [35/200] Loss: 61.3255729675293\n",
      "Epoch [36/200] Loss: 60.11841583251953\n",
      "Epoch [37/200] Loss: 58.9526252746582\n",
      "Epoch [38/200] Loss: 57.82498550415039\n",
      "Epoch [39/200] Loss: 56.732643127441406\n",
      "Epoch [40/200] Loss: 55.673133850097656\n",
      "Epoch [41/200] Loss: 54.64429473876953\n",
      "Epoch [42/200] Loss: 53.6442756652832\n",
      "Epoch [43/200] Loss: 52.671417236328125\n",
      "Epoch [44/200] Loss: 51.72429275512695\n",
      "Epoch [45/200] Loss: 50.801673889160156\n",
      "Epoch [46/200] Loss: 49.90242004394531\n",
      "Epoch [47/200] Loss: 49.02553176879883\n",
      "Epoch [48/200] Loss: 48.170101165771484\n",
      "Epoch [49/200] Loss: 47.3353271484375\n",
      "Epoch [50/200] Loss: 46.520442962646484\n",
      "Epoch [51/200] Loss: 45.72476577758789\n",
      "Epoch [52/200] Loss: 44.94765090942383\n",
      "Epoch [53/200] Loss: 44.18848419189453\n",
      "Epoch [54/200] Loss: 43.44670104980469\n",
      "Epoch [55/200] Loss: 42.721763610839844\n",
      "Epoch [56/200] Loss: 42.01319885253906\n",
      "Epoch [57/200] Loss: 41.32048416137695\n",
      "Epoch [58/200] Loss: 40.643218994140625\n",
      "Epoch [59/200] Loss: 39.9809455871582\n",
      "Epoch [60/200] Loss: 39.33329391479492\n",
      "Epoch [61/200] Loss: 38.69988250732422\n",
      "Epoch [62/200] Loss: 38.08035659790039\n",
      "Epoch [63/200] Loss: 37.474395751953125\n",
      "Epoch [64/200] Loss: 36.88169860839844\n",
      "Epoch [65/200] Loss: 36.301963806152344\n",
      "Epoch [66/200] Loss: 35.73493194580078\n",
      "Epoch [67/200] Loss: 35.18034362792969\n",
      "Epoch [68/200] Loss: 34.63797378540039\n",
      "Epoch [69/200] Loss: 34.10758590698242\n",
      "Epoch [70/200] Loss: 33.588951110839844\n",
      "Epoch [71/200] Loss: 33.08189392089844\n",
      "Epoch [72/200] Loss: 32.586185455322266\n",
      "Epoch [73/200] Loss: 32.10163497924805\n",
      "Epoch [74/200] Loss: 31.628061294555664\n",
      "Epoch [75/200] Loss: 31.165271759033203\n",
      "Epoch [76/200] Loss: 30.713058471679688\n",
      "Epoch [77/200] Loss: 30.271270751953125\n",
      "Epoch [78/200] Loss: 29.83969497680664\n",
      "Epoch [79/200] Loss: 29.418170928955078\n",
      "Epoch [80/200] Loss: 29.006488800048828\n",
      "Epoch [81/200] Loss: 28.604494094848633\n",
      "Epoch [82/200] Loss: 28.211990356445312\n",
      "Epoch [83/200] Loss: 27.828805923461914\n",
      "Epoch [84/200] Loss: 27.454769134521484\n",
      "Epoch [85/200] Loss: 27.089683532714844\n",
      "Epoch [86/200] Loss: 26.7333984375\n",
      "Epoch [87/200] Loss: 26.38572120666504\n",
      "Epoch [88/200] Loss: 26.046510696411133\n",
      "Epoch [89/200] Loss: 25.715566635131836\n",
      "Epoch [90/200] Loss: 25.392751693725586\n",
      "Epoch [91/200] Loss: 25.077877044677734\n",
      "Epoch [92/200] Loss: 24.770801544189453\n",
      "Epoch [93/200] Loss: 24.471355438232422\n",
      "Epoch [94/200] Loss: 24.179391860961914\n",
      "Epoch [95/200] Loss: 23.894756317138672\n",
      "Epoch [96/200] Loss: 23.617290496826172\n",
      "Epoch [97/200] Loss: 23.346847534179688\n",
      "Epoch [98/200] Loss: 23.08327293395996\n",
      "Epoch [99/200] Loss: 22.82645034790039\n",
      "Epoch [100/200] Loss: 22.576210021972656\n",
      "Epoch [101/200] Loss: 22.33241844177246\n",
      "Epoch [102/200] Loss: 22.094947814941406\n",
      "Epoch [103/200] Loss: 21.863658905029297\n",
      "Epoch [104/200] Loss: 21.638412475585938\n",
      "Epoch [105/200] Loss: 21.419086456298828\n",
      "Epoch [106/200] Loss: 21.205547332763672\n",
      "Epoch [107/200] Loss: 20.997676849365234\n",
      "Epoch [108/200] Loss: 20.79533576965332\n",
      "Epoch [109/200] Loss: 20.59841537475586\n",
      "Epoch [110/200] Loss: 20.40679168701172\n",
      "Epoch [111/200] Loss: 20.22034454345703\n",
      "Epoch [112/200] Loss: 20.038965225219727\n",
      "Epoch [113/200] Loss: 19.86253547668457\n",
      "Epoch [114/200] Loss: 19.69093894958496\n",
      "Epoch [115/200] Loss: 19.524070739746094\n",
      "Epoch [116/200] Loss: 19.361825942993164\n",
      "Epoch [117/200] Loss: 19.204086303710938\n",
      "Epoch [118/200] Loss: 19.05076026916504\n",
      "Epoch [119/200] Loss: 18.901752471923828\n",
      "Epoch [120/200] Loss: 18.756935119628906\n",
      "Epoch [121/200] Loss: 18.616230010986328\n",
      "Epoch [122/200] Loss: 18.479537963867188\n",
      "Epoch [123/200] Loss: 18.34675407409668\n",
      "Epoch [124/200] Loss: 18.217792510986328\n",
      "Epoch [125/200] Loss: 18.092567443847656\n",
      "Epoch [126/200] Loss: 17.970983505249023\n",
      "Epoch [127/200] Loss: 17.852943420410156\n",
      "Epoch [128/200] Loss: 17.73837661743164\n",
      "Epoch [129/200] Loss: 17.627182006835938\n",
      "Epoch [130/200] Loss: 17.519283294677734\n",
      "Epoch [131/200] Loss: 17.414600372314453\n",
      "Epoch [132/200] Loss: 17.313053131103516\n",
      "Epoch [133/200] Loss: 17.21455192565918\n",
      "Epoch [134/200] Loss: 17.119033813476562\n",
      "Epoch [135/200] Loss: 17.026416778564453\n",
      "Epoch [136/200] Loss: 16.936628341674805\n",
      "Epoch [137/200] Loss: 16.849592208862305\n",
      "Epoch [138/200] Loss: 16.76523780822754\n",
      "Epoch [139/200] Loss: 16.683494567871094\n",
      "Epoch [140/200] Loss: 16.604297637939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/200] Loss: 16.527578353881836\n",
      "Epoch [142/200] Loss: 16.453264236450195\n",
      "Epoch [143/200] Loss: 16.38130760192871\n",
      "Epoch [144/200] Loss: 16.311626434326172\n",
      "Epoch [145/200] Loss: 16.24416732788086\n",
      "Epoch [146/200] Loss: 16.178876876831055\n",
      "Epoch [147/200] Loss: 16.115680694580078\n",
      "Epoch [148/200] Loss: 16.054536819458008\n",
      "Epoch [149/200] Loss: 15.995370864868164\n",
      "Epoch [150/200] Loss: 15.938142776489258\n",
      "Epoch [151/200] Loss: 15.882793426513672\n",
      "Epoch [152/200] Loss: 15.829270362854004\n",
      "Epoch [153/200] Loss: 15.777517318725586\n",
      "Epoch [154/200] Loss: 15.72749137878418\n",
      "Epoch [155/200] Loss: 15.679133415222168\n",
      "Epoch [156/200] Loss: 15.632404327392578\n",
      "Epoch [157/200] Loss: 15.587251663208008\n",
      "Epoch [158/200] Loss: 15.543636322021484\n",
      "Epoch [159/200] Loss: 15.501503944396973\n",
      "Epoch [160/200] Loss: 15.460814476013184\n",
      "Epoch [161/200] Loss: 15.421527862548828\n",
      "Epoch [162/200] Loss: 15.38359546661377\n",
      "Epoch [163/200] Loss: 15.346983909606934\n",
      "Epoch [164/200] Loss: 15.311647415161133\n",
      "Epoch [165/200] Loss: 15.277552604675293\n",
      "Epoch [166/200] Loss: 15.244653701782227\n",
      "Epoch [167/200] Loss: 15.212925910949707\n",
      "Epoch [168/200] Loss: 15.18232250213623\n",
      "Epoch [169/200] Loss: 15.152813911437988\n",
      "Epoch [170/200] Loss: 15.124359130859375\n",
      "Epoch [171/200] Loss: 15.096933364868164\n",
      "Epoch [172/200] Loss: 15.070497512817383\n",
      "Epoch [173/200] Loss: 15.045022964477539\n",
      "Epoch [174/200] Loss: 15.020483016967773\n",
      "Epoch [175/200] Loss: 14.996842384338379\n",
      "Epoch [176/200] Loss: 14.974071502685547\n",
      "Epoch [177/200] Loss: 14.952143669128418\n",
      "Epoch [178/200] Loss: 14.931034088134766\n",
      "Epoch [179/200] Loss: 14.910709381103516\n",
      "Epoch [180/200] Loss: 14.891149520874023\n",
      "Epoch [181/200] Loss: 14.87232780456543\n",
      "Epoch [182/200] Loss: 14.854217529296875\n",
      "Epoch [183/200] Loss: 14.83679485321045\n",
      "Epoch [184/200] Loss: 14.820038795471191\n",
      "Epoch [185/200] Loss: 14.803924560546875\n",
      "Epoch [186/200] Loss: 14.788431167602539\n",
      "Epoch [187/200] Loss: 14.773537635803223\n",
      "Epoch [188/200] Loss: 14.759223937988281\n",
      "Epoch [189/200] Loss: 14.745465278625488\n",
      "Epoch [190/200] Loss: 14.732247352600098\n",
      "Epoch [191/200] Loss: 14.719552993774414\n",
      "Epoch [192/200] Loss: 14.707356452941895\n",
      "Epoch [193/200] Loss: 14.695647239685059\n",
      "Epoch [194/200] Loss: 14.684403419494629\n",
      "Epoch [195/200] Loss: 14.673606872558594\n",
      "Epoch [196/200] Loss: 14.663249015808105\n",
      "Epoch [197/200] Loss: 14.653308868408203\n",
      "Epoch [198/200] Loss: 14.643769264221191\n",
      "Epoch [199/200] Loss: 14.634618759155273\n",
      "Epoch [200/200] Loss: 14.625843048095703\n",
      "Predicted days_remaining for parent_id 37: 11.325822830200195\n",
      "Training for parent_id 40...\n",
      "Epoch [1/200] Loss: 237.29693603515625\n",
      "Epoch [2/200] Loss: 230.0119171142578\n",
      "Epoch [3/200] Loss: 222.88221740722656\n",
      "Epoch [4/200] Loss: 216.0045623779297\n",
      "Epoch [5/200] Loss: 209.43955993652344\n",
      "Epoch [6/200] Loss: 203.2093048095703\n",
      "Epoch [7/200] Loss: 197.30862426757812\n",
      "Epoch [8/200] Loss: 191.7138671875\n",
      "Epoch [9/200] Loss: 186.3953857421875\n",
      "Epoch [10/200] Loss: 181.3264617919922\n",
      "Epoch [11/200] Loss: 176.48672485351562\n",
      "Epoch [12/200] Loss: 171.86199951171875\n",
      "Epoch [13/200] Loss: 167.44248962402344\n",
      "Epoch [14/200] Loss: 163.22132873535156\n",
      "Epoch [15/200] Loss: 159.1936492919922\n",
      "Epoch [16/200] Loss: 155.3562774658203\n",
      "Epoch [17/200] Loss: 151.7066192626953\n",
      "Epoch [18/200] Loss: 148.24171447753906\n",
      "Epoch [19/200] Loss: 144.95726013183594\n",
      "Epoch [20/200] Loss: 141.84710693359375\n",
      "Epoch [21/200] Loss: 138.90310668945312\n",
      "Epoch [22/200] Loss: 136.11544799804688\n",
      "Epoch [23/200] Loss: 133.4732666015625\n",
      "Epoch [24/200] Loss: 130.96511840820312\n",
      "Epoch [25/200] Loss: 128.5794677734375\n",
      "Epoch [26/200] Loss: 126.30509948730469\n",
      "Epoch [27/200] Loss: 124.13121032714844\n",
      "Epoch [28/200] Loss: 122.04768371582031\n",
      "Epoch [29/200] Loss: 120.04518127441406\n",
      "Epoch [30/200] Loss: 118.11538696289062\n",
      "Epoch [31/200] Loss: 116.25086212158203\n",
      "Epoch [32/200] Loss: 114.44535827636719\n",
      "Epoch [33/200] Loss: 112.69354248046875\n",
      "Epoch [34/200] Loss: 110.99089813232422\n",
      "Epoch [35/200] Loss: 109.33355712890625\n",
      "Epoch [36/200] Loss: 107.7183837890625\n",
      "Epoch [37/200] Loss: 106.14254760742188\n",
      "Epoch [38/200] Loss: 104.60362243652344\n",
      "Epoch [39/200] Loss: 103.09949493408203\n",
      "Epoch [40/200] Loss: 101.62821197509766\n",
      "Epoch [41/200] Loss: 100.18801879882812\n",
      "Epoch [42/200] Loss: 98.77729034423828\n",
      "Epoch [43/200] Loss: 97.39458465576172\n",
      "Epoch [44/200] Loss: 96.03850555419922\n",
      "Epoch [45/200] Loss: 94.70780944824219\n",
      "Epoch [46/200] Loss: 93.4013900756836\n",
      "Epoch [47/200] Loss: 92.1181640625\n",
      "Epoch [48/200] Loss: 90.85716247558594\n",
      "Epoch [49/200] Loss: 89.61751556396484\n",
      "Epoch [50/200] Loss: 88.39842987060547\n",
      "Epoch [51/200] Loss: 87.19910430908203\n",
      "Epoch [52/200] Loss: 86.01889038085938\n",
      "Epoch [53/200] Loss: 84.85714721679688\n",
      "Epoch [54/200] Loss: 83.71326446533203\n",
      "Epoch [55/200] Loss: 82.5866928100586\n",
      "Epoch [56/200] Loss: 81.47693634033203\n",
      "Epoch [57/200] Loss: 80.38349914550781\n",
      "Epoch [58/200] Loss: 79.30596160888672\n",
      "Epoch [59/200] Loss: 78.243896484375\n",
      "Epoch [60/200] Loss: 77.19689178466797\n",
      "Epoch [61/200] Loss: 76.16464233398438\n",
      "Epoch [62/200] Loss: 75.14678955078125\n",
      "Epoch [63/200] Loss: 74.14302825927734\n",
      "Epoch [64/200] Loss: 73.15303802490234\n",
      "Epoch [65/200] Loss: 72.17658233642578\n",
      "Epoch [66/200] Loss: 71.21334838867188\n",
      "Epoch [67/200] Loss: 70.26316833496094\n",
      "Epoch [68/200] Loss: 69.3258056640625\n",
      "Epoch [69/200] Loss: 68.4010238647461\n",
      "Epoch [70/200] Loss: 67.48868560791016\n",
      "Epoch [71/200] Loss: 66.58859252929688\n",
      "Epoch [72/200] Loss: 65.7005844116211\n",
      "Epoch [73/200] Loss: 64.82453918457031\n",
      "Epoch [74/200] Loss: 63.960323333740234\n",
      "Epoch [75/200] Loss: 63.10784912109375\n",
      "Epoch [76/200] Loss: 62.26699447631836\n",
      "Epoch [77/200] Loss: 61.437644958496094\n",
      "Epoch [78/200] Loss: 60.61975860595703\n",
      "Epoch [79/200] Loss: 59.81322479248047\n",
      "Epoch [80/200] Loss: 59.01797103881836\n",
      "Epoch [81/200] Loss: 58.23391342163086\n",
      "Epoch [82/200] Loss: 57.461021423339844\n",
      "Epoch [83/200] Loss: 56.69916915893555\n",
      "Epoch [84/200] Loss: 55.948307037353516\n",
      "Epoch [85/200] Loss: 55.2083625793457\n",
      "Epoch [86/200] Loss: 54.47924041748047\n",
      "Epoch [87/200] Loss: 53.7608757019043\n",
      "Epoch [88/200] Loss: 53.05315017700195\n",
      "Epoch [89/200] Loss: 52.35601043701172\n",
      "Epoch [90/200] Loss: 51.669334411621094\n",
      "Epoch [91/200] Loss: 50.9930419921875\n",
      "Epoch [92/200] Loss: 50.327022552490234\n",
      "Epoch [93/200] Loss: 49.67116928100586\n",
      "Epoch [94/200] Loss: 49.025394439697266\n",
      "Epoch [95/200] Loss: 48.389583587646484\n",
      "Epoch [96/200] Loss: 47.76362991333008\n",
      "Epoch [97/200] Loss: 47.14740753173828\n",
      "Epoch [98/200] Loss: 46.54082107543945\n",
      "Epoch [99/200] Loss: 45.943748474121094\n",
      "Epoch [100/200] Loss: 45.35609436035156\n",
      "Epoch [101/200] Loss: 44.77772521972656\n",
      "Epoch [102/200] Loss: 44.20854187011719\n",
      "Epoch [103/200] Loss: 43.64842987060547\n",
      "Epoch [104/200] Loss: 43.09726333618164\n",
      "Epoch [105/200] Loss: 42.55495071411133\n",
      "Epoch [106/200] Loss: 42.0213737487793\n",
      "Epoch [107/200] Loss: 41.49641418457031\n",
      "Epoch [108/200] Loss: 40.97998046875\n",
      "Epoch [109/200] Loss: 40.471946716308594\n",
      "Epoch [110/200] Loss: 39.97220230102539\n",
      "Epoch [111/200] Loss: 39.48063659667969\n",
      "Epoch [112/200] Loss: 38.99713897705078\n",
      "Epoch [113/200] Loss: 38.52164077758789\n",
      "Epoch [114/200] Loss: 38.05398941040039\n",
      "Epoch [115/200] Loss: 37.59410095214844\n",
      "Epoch [116/200] Loss: 37.14185333251953\n",
      "Epoch [117/200] Loss: 36.69717025756836\n",
      "Epoch [118/200] Loss: 36.259944915771484\n",
      "Epoch [119/200] Loss: 35.83006286621094\n",
      "Epoch [120/200] Loss: 35.40741729736328\n",
      "Epoch [121/200] Loss: 34.99192428588867\n",
      "Epoch [122/200] Loss: 34.58348846435547\n",
      "Epoch [123/200] Loss: 34.18199920654297\n",
      "Epoch [124/200] Loss: 33.78736877441406\n",
      "Epoch [125/200] Loss: 33.39950942993164\n",
      "Epoch [126/200] Loss: 33.018306732177734\n",
      "Epoch [127/200] Loss: 32.6436767578125\n",
      "Epoch [128/200] Loss: 32.27552795410156\n",
      "Epoch [129/200] Loss: 31.91376304626465\n",
      "Epoch [130/200] Loss: 31.558305740356445\n",
      "Epoch [131/200] Loss: 31.20906639099121\n",
      "Epoch [132/200] Loss: 30.865930557250977\n",
      "Epoch [133/200] Loss: 30.52884864807129\n",
      "Epoch [134/200] Loss: 30.19769859313965\n",
      "Epoch [135/200] Loss: 29.872411727905273\n",
      "Epoch [136/200] Loss: 29.552906036376953\n",
      "Epoch [137/200] Loss: 29.239078521728516\n",
      "Epoch [138/200] Loss: 28.930875778198242\n",
      "Epoch [139/200] Loss: 28.62819480895996\n",
      "Epoch [140/200] Loss: 28.330974578857422\n",
      "Epoch [141/200] Loss: 28.03910255432129\n",
      "Epoch [142/200] Loss: 27.75252914428711\n",
      "Epoch [143/200] Loss: 27.471160888671875\n",
      "Epoch [144/200] Loss: 27.194929122924805\n",
      "Epoch [145/200] Loss: 26.92374610900879\n",
      "Epoch [146/200] Loss: 26.657554626464844\n",
      "Epoch [147/200] Loss: 26.396255493164062\n",
      "Epoch [148/200] Loss: 26.13979721069336\n",
      "Epoch [149/200] Loss: 25.88809585571289\n",
      "Epoch [150/200] Loss: 25.64107894897461\n",
      "Epoch [151/200] Loss: 25.39865493774414\n",
      "Epoch [152/200] Loss: 25.160804748535156\n",
      "Epoch [153/200] Loss: 24.92742347717285\n",
      "Epoch [154/200] Loss: 24.698440551757812\n",
      "Epoch [155/200] Loss: 24.473791122436523\n",
      "Epoch [156/200] Loss: 24.253414154052734\n",
      "Epoch [157/200] Loss: 24.037246704101562\n",
      "Epoch [158/200] Loss: 23.82520294189453\n",
      "Epoch [159/200] Loss: 23.617237091064453\n",
      "Epoch [160/200] Loss: 23.413278579711914\n",
      "Epoch [161/200] Loss: 23.213253021240234\n",
      "Epoch [162/200] Loss: 23.017126083374023\n",
      "Epoch [163/200] Loss: 22.82479476928711\n",
      "Epoch [164/200] Loss: 22.636245727539062\n",
      "Epoch [165/200] Loss: 22.451370239257812\n",
      "Epoch [166/200] Loss: 22.2701473236084\n",
      "Epoch [167/200] Loss: 22.09250259399414\n",
      "Epoch [168/200] Loss: 21.918376922607422\n",
      "Epoch [169/200] Loss: 21.747705459594727\n",
      "Epoch [170/200] Loss: 21.580440521240234\n",
      "Epoch [171/200] Loss: 21.416522979736328\n",
      "Epoch [172/200] Loss: 21.25590705871582\n",
      "Epoch [173/200] Loss: 21.098520278930664\n",
      "Epoch [174/200] Loss: 20.944320678710938\n",
      "Epoch [175/200] Loss: 20.793249130249023\n",
      "Epoch [176/200] Loss: 20.645252227783203\n",
      "Epoch [177/200] Loss: 20.500282287597656\n",
      "Epoch [178/200] Loss: 20.358285903930664\n",
      "Epoch [179/200] Loss: 20.21921157836914\n",
      "Epoch [180/200] Loss: 20.0830135345459\n",
      "Epoch [181/200] Loss: 19.949630737304688\n",
      "Epoch [182/200] Loss: 19.81903076171875\n",
      "Epoch [183/200] Loss: 19.691146850585938\n",
      "Epoch [184/200] Loss: 19.565946578979492\n",
      "Epoch [185/200] Loss: 19.443378448486328\n",
      "Epoch [186/200] Loss: 19.32339096069336\n",
      "Epoch [187/200] Loss: 19.205949783325195\n",
      "Epoch [188/200] Loss: 19.09099578857422\n",
      "Epoch [189/200] Loss: 18.978496551513672\n",
      "Epoch [190/200] Loss: 18.868392944335938\n",
      "Epoch [191/200] Loss: 18.76065444946289\n",
      "Epoch [192/200] Loss: 18.65523910522461\n",
      "Epoch [193/200] Loss: 18.55209732055664\n",
      "Epoch [194/200] Loss: 18.451194763183594\n",
      "Epoch [195/200] Loss: 18.352479934692383\n",
      "Epoch [196/200] Loss: 18.25592803955078\n",
      "Epoch [197/200] Loss: 18.161483764648438\n",
      "Epoch [198/200] Loss: 18.069114685058594\n",
      "Epoch [199/200] Loss: 17.978782653808594\n",
      "Epoch [200/200] Loss: 17.890451431274414\n",
      "Predicted days_remaining for parent_id 40: 12.915176391601562\n",
      "Training for parent_id 41...\n",
      "Epoch [1/200] Loss: 1078.367919921875\n",
      "Epoch [2/200] Loss: 1056.4833984375\n",
      "Epoch [3/200] Loss: 1035.15576171875\n",
      "Epoch [4/200] Loss: 1014.6620483398438\n",
      "Epoch [5/200] Loss: 995.2109375\n",
      "Epoch [6/200] Loss: 976.94140625\n",
      "Epoch [7/200] Loss: 959.911865234375\n",
      "Epoch [8/200] Loss: 944.1109619140625\n",
      "Epoch [9/200] Loss: 929.4734497070312\n",
      "Epoch [10/200] Loss: 915.8974609375\n",
      "Epoch [11/200] Loss: 903.26318359375\n",
      "Epoch [12/200] Loss: 891.4523315429688\n",
      "Epoch [13/200] Loss: 880.3624877929688\n",
      "Epoch [14/200] Loss: 869.9100952148438\n",
      "Epoch [15/200] Loss: 860.0291748046875\n",
      "Epoch [16/200] Loss: 850.6663208007812\n",
      "Epoch [17/200] Loss: 841.7777099609375\n",
      "Epoch [18/200] Loss: 833.3261108398438\n",
      "Epoch [19/200] Loss: 825.27783203125\n",
      "Epoch [20/200] Loss: 817.6024780273438\n",
      "Epoch [21/200] Loss: 810.2705078125\n",
      "Epoch [22/200] Loss: 803.25390625\n",
      "Epoch [23/200] Loss: 796.52490234375\n",
      "Epoch [24/200] Loss: 790.0564575195312\n",
      "Epoch [25/200] Loss: 783.8234252929688\n",
      "Epoch [26/200] Loss: 777.80224609375\n",
      "Epoch [27/200] Loss: 771.970458984375\n",
      "Epoch [28/200] Loss: 766.3089599609375\n",
      "Epoch [29/200] Loss: 760.7996826171875\n",
      "Epoch [30/200] Loss: 755.4274291992188\n",
      "Epoch [31/200] Loss: 750.1786499023438\n",
      "Epoch [32/200] Loss: 745.0418701171875\n",
      "Epoch [33/200] Loss: 740.0068359375\n",
      "Epoch [34/200] Loss: 735.0650634765625\n",
      "Epoch [35/200] Loss: 730.2091064453125\n",
      "Epoch [36/200] Loss: 725.4329223632812\n",
      "Epoch [37/200] Loss: 720.7305908203125\n",
      "Epoch [38/200] Loss: 716.0975952148438\n",
      "Epoch [39/200] Loss: 711.5298461914062\n",
      "Epoch [40/200] Loss: 707.0238037109375\n",
      "Epoch [41/200] Loss: 702.5767211914062\n",
      "Epoch [42/200] Loss: 698.1856079101562\n",
      "Epoch [43/200] Loss: 693.8483276367188\n",
      "Epoch [44/200] Loss: 689.5630493164062\n",
      "Epoch [45/200] Loss: 685.3274536132812\n",
      "Epoch [46/200] Loss: 681.1404418945312\n",
      "Epoch [47/200] Loss: 677.0001220703125\n",
      "Epoch [48/200] Loss: 672.905029296875\n",
      "Epoch [49/200] Loss: 668.8536987304688\n",
      "Epoch [50/200] Loss: 664.8448486328125\n",
      "Epoch [51/200] Loss: 660.877197265625\n",
      "Epoch [52/200] Loss: 656.949462890625\n",
      "Epoch [53/200] Loss: 653.060302734375\n",
      "Epoch [54/200] Loss: 649.20849609375\n",
      "Epoch [55/200] Loss: 645.3929443359375\n",
      "Epoch [56/200] Loss: 641.6124877929688\n",
      "Epoch [57/200] Loss: 637.8659057617188\n",
      "Epoch [58/200] Loss: 634.1522216796875\n",
      "Epoch [59/200] Loss: 630.4705200195312\n",
      "Epoch [60/200] Loss: 626.8197631835938\n",
      "Epoch [61/200] Loss: 623.1991577148438\n",
      "Epoch [62/200] Loss: 619.6076049804688\n",
      "Epoch [63/200] Loss: 616.0443725585938\n",
      "Epoch [64/200] Loss: 612.5086669921875\n",
      "Epoch [65/200] Loss: 608.9999389648438\n",
      "Epoch [66/200] Loss: 605.517333984375\n",
      "Epoch [67/200] Loss: 602.060302734375\n",
      "Epoch [68/200] Loss: 598.6281127929688\n",
      "Epoch [69/200] Loss: 595.22021484375\n",
      "Epoch [70/200] Loss: 591.8361206054688\n",
      "Epoch [71/200] Loss: 588.4752807617188\n",
      "Epoch [72/200] Loss: 585.13720703125\n",
      "Epoch [73/200] Loss: 581.821533203125\n",
      "Epoch [74/200] Loss: 578.5276489257812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/200] Loss: 575.2551879882812\n",
      "Epoch [76/200] Loss: 572.00390625\n",
      "Epoch [77/200] Loss: 568.7733764648438\n",
      "Epoch [78/200] Loss: 565.56298828125\n",
      "Epoch [79/200] Loss: 562.3729248046875\n",
      "Epoch [80/200] Loss: 559.202392578125\n",
      "Epoch [81/200] Loss: 556.0511474609375\n",
      "Epoch [82/200] Loss: 552.919189453125\n",
      "Epoch [83/200] Loss: 549.8059692382812\n",
      "Epoch [84/200] Loss: 546.7113037109375\n",
      "Epoch [85/200] Loss: 543.6350708007812\n",
      "Epoch [86/200] Loss: 540.5767822265625\n",
      "Epoch [87/200] Loss: 537.5362548828125\n",
      "Epoch [88/200] Loss: 534.513427734375\n",
      "Epoch [89/200] Loss: 531.508056640625\n",
      "Epoch [90/200] Loss: 528.5197143554688\n",
      "Epoch [91/200] Loss: 525.54833984375\n",
      "Epoch [92/200] Loss: 522.5938720703125\n",
      "Epoch [93/200] Loss: 519.656005859375\n",
      "Epoch [94/200] Loss: 516.7344970703125\n",
      "Epoch [95/200] Loss: 513.8291625976562\n",
      "Epoch [96/200] Loss: 510.93988037109375\n",
      "Epoch [97/200] Loss: 508.0666198730469\n",
      "Epoch [98/200] Loss: 505.20904541015625\n",
      "Epoch [99/200] Loss: 502.3670959472656\n",
      "Epoch [100/200] Loss: 499.5405578613281\n",
      "Epoch [101/200] Loss: 496.72930908203125\n",
      "Epoch [102/200] Loss: 493.93316650390625\n",
      "Epoch [103/200] Loss: 491.1522216796875\n",
      "Epoch [104/200] Loss: 488.3859558105469\n",
      "Epoch [105/200] Loss: 485.634521484375\n",
      "Epoch [106/200] Loss: 482.8978271484375\n",
      "Epoch [107/200] Loss: 480.17547607421875\n",
      "Epoch [108/200] Loss: 477.4676208496094\n",
      "Epoch [109/200] Loss: 474.7740478515625\n",
      "Epoch [110/200] Loss: 472.0945739746094\n",
      "Epoch [111/200] Loss: 469.4291076660156\n",
      "Epoch [112/200] Loss: 466.7777099609375\n",
      "Epoch [113/200] Loss: 464.1401672363281\n",
      "Epoch [114/200] Loss: 461.5162658691406\n",
      "Epoch [115/200] Loss: 458.9061279296875\n",
      "Epoch [116/200] Loss: 456.3094482421875\n",
      "Epoch [117/200] Loss: 453.7261962890625\n",
      "Epoch [118/200] Loss: 451.1562194824219\n",
      "Epoch [119/200] Loss: 448.5997314453125\n",
      "Epoch [120/200] Loss: 446.0561828613281\n",
      "Epoch [121/200] Loss: 443.5257873535156\n",
      "Epoch [122/200] Loss: 441.00848388671875\n",
      "Epoch [123/200] Loss: 438.50396728515625\n",
      "Epoch [124/200] Loss: 436.0122375488281\n",
      "Epoch [125/200] Loss: 433.5333251953125\n",
      "Epoch [126/200] Loss: 431.067138671875\n",
      "Epoch [127/200] Loss: 428.6134033203125\n",
      "Epoch [128/200] Loss: 426.1722717285156\n",
      "Epoch [129/200] Loss: 423.74365234375\n",
      "Epoch [130/200] Loss: 421.3273010253906\n",
      "Epoch [131/200] Loss: 418.923095703125\n",
      "Epoch [132/200] Loss: 416.53131103515625\n",
      "Epoch [133/200] Loss: 414.1515808105469\n",
      "Epoch [134/200] Loss: 411.7838134765625\n",
      "Epoch [135/200] Loss: 409.4282531738281\n",
      "Epoch [136/200] Loss: 407.08453369140625\n",
      "Epoch [137/200] Loss: 404.7526550292969\n",
      "Epoch [138/200] Loss: 402.43255615234375\n",
      "Epoch [139/200] Loss: 400.12432861328125\n",
      "Epoch [140/200] Loss: 397.82763671875\n",
      "Epoch [141/200] Loss: 395.5426025390625\n",
      "Epoch [142/200] Loss: 393.26910400390625\n",
      "Epoch [143/200] Loss: 391.0072021484375\n",
      "Epoch [144/200] Loss: 388.75653076171875\n",
      "Epoch [145/200] Loss: 386.5173645019531\n",
      "Epoch [146/200] Loss: 384.2894287109375\n",
      "Epoch [147/200] Loss: 382.07275390625\n",
      "Epoch [148/200] Loss: 379.8673095703125\n",
      "Epoch [149/200] Loss: 377.6730041503906\n",
      "Epoch [150/200] Loss: 375.48974609375\n",
      "Epoch [151/200] Loss: 373.31756591796875\n",
      "Epoch [152/200] Loss: 371.15625\n",
      "Epoch [153/200] Loss: 369.0058898925781\n",
      "Epoch [154/200] Loss: 366.8664245605469\n",
      "Epoch [155/200] Loss: 364.73773193359375\n",
      "Epoch [156/200] Loss: 362.619873046875\n",
      "Epoch [157/200] Loss: 360.5126953125\n",
      "Epoch [158/200] Loss: 358.41607666015625\n",
      "Epoch [159/200] Loss: 356.3300476074219\n",
      "Epoch [160/200] Loss: 354.2547302246094\n",
      "Epoch [161/200] Loss: 352.18975830078125\n",
      "Epoch [162/200] Loss: 350.1353759765625\n",
      "Epoch [163/200] Loss: 348.0913391113281\n",
      "Epoch [164/200] Loss: 346.0576477050781\n",
      "Epoch [165/200] Loss: 344.0341491699219\n",
      "Epoch [166/200] Loss: 342.0210876464844\n",
      "Epoch [167/200] Loss: 340.0181884765625\n",
      "Epoch [168/200] Loss: 338.0253601074219\n",
      "Epoch [169/200] Loss: 336.042724609375\n",
      "Epoch [170/200] Loss: 334.0701599121094\n",
      "Epoch [171/200] Loss: 332.10760498046875\n",
      "Epoch [172/200] Loss: 330.15509033203125\n",
      "Epoch [173/200] Loss: 328.21246337890625\n",
      "Epoch [174/200] Loss: 326.2797546386719\n",
      "Epoch [175/200] Loss: 324.35687255859375\n",
      "Epoch [176/200] Loss: 322.4437255859375\n",
      "Epoch [177/200] Loss: 320.54052734375\n",
      "Epoch [178/200] Loss: 318.6468811035156\n",
      "Epoch [179/200] Loss: 316.762939453125\n",
      "Epoch [180/200] Loss: 314.8886413574219\n",
      "Epoch [181/200] Loss: 313.02398681640625\n",
      "Epoch [182/200] Loss: 311.1688232421875\n",
      "Epoch [183/200] Loss: 309.3232727050781\n",
      "Epoch [184/200] Loss: 307.4870300292969\n",
      "Epoch [185/200] Loss: 305.6602478027344\n",
      "Epoch [186/200] Loss: 303.84295654296875\n",
      "Epoch [187/200] Loss: 302.034912109375\n",
      "Epoch [188/200] Loss: 300.2362060546875\n",
      "Epoch [189/200] Loss: 298.44671630859375\n",
      "Epoch [190/200] Loss: 296.6665954589844\n",
      "Epoch [191/200] Loss: 294.8955078125\n",
      "Epoch [192/200] Loss: 293.13360595703125\n",
      "Epoch [193/200] Loss: 291.38079833984375\n",
      "Epoch [194/200] Loss: 289.6370849609375\n",
      "Epoch [195/200] Loss: 287.90240478515625\n",
      "Epoch [196/200] Loss: 286.1766662597656\n",
      "Epoch [197/200] Loss: 284.4599304199219\n",
      "Epoch [198/200] Loss: 282.75213623046875\n",
      "Epoch [199/200] Loss: 281.0531311035156\n",
      "Epoch [200/200] Loss: 279.36309814453125\n",
      "Predicted days_remaining for parent_id 41: 16.525197982788086\n",
      "Training for parent_id 50...\n",
      "Epoch [1/200] Loss: 1077.033935546875\n",
      "Epoch [2/200] Loss: 1061.39306640625\n",
      "Epoch [3/200] Loss: 1046.191162109375\n",
      "Epoch [4/200] Loss: 1031.4403076171875\n",
      "Epoch [5/200] Loss: 1017.151611328125\n",
      "Epoch [6/200] Loss: 1003.3418579101562\n",
      "Epoch [7/200] Loss: 990.0154418945312\n",
      "Epoch [8/200] Loss: 977.1643676757812\n",
      "Epoch [9/200] Loss: 964.7735595703125\n",
      "Epoch [10/200] Loss: 952.8240356445312\n",
      "Epoch [11/200] Loss: 941.2967529296875\n",
      "Epoch [12/200] Loss: 930.174072265625\n",
      "Epoch [13/200] Loss: 919.4408569335938\n",
      "Epoch [14/200] Loss: 909.0842895507812\n",
      "Epoch [15/200] Loss: 899.09326171875\n",
      "Epoch [16/200] Loss: 889.4561767578125\n",
      "Epoch [17/200] Loss: 880.1612548828125\n",
      "Epoch [18/200] Loss: 871.1958618164062\n",
      "Epoch [19/200] Loss: 862.5467529296875\n",
      "Epoch [20/200] Loss: 854.1990966796875\n",
      "Epoch [21/200] Loss: 846.1386108398438\n",
      "Epoch [22/200] Loss: 838.3501586914062\n",
      "Epoch [23/200] Loss: 830.8193359375\n",
      "Epoch [24/200] Loss: 823.5325317382812\n",
      "Epoch [25/200] Loss: 816.4769897460938\n",
      "Epoch [26/200] Loss: 809.6403198242188\n",
      "Epoch [27/200] Loss: 803.0114135742188\n",
      "Epoch [28/200] Loss: 796.5791625976562\n",
      "Epoch [29/200] Loss: 790.33349609375\n",
      "Epoch [30/200] Loss: 784.2645263671875\n",
      "Epoch [31/200] Loss: 778.3627319335938\n",
      "Epoch [32/200] Loss: 772.619140625\n",
      "Epoch [33/200] Loss: 767.0253295898438\n",
      "Epoch [34/200] Loss: 761.5728759765625\n",
      "Epoch [35/200] Loss: 756.2538452148438\n",
      "Epoch [36/200] Loss: 751.0606689453125\n",
      "Epoch [37/200] Loss: 745.986328125\n",
      "Epoch [38/200] Loss: 741.0238037109375\n",
      "Epoch [39/200] Loss: 736.1666259765625\n",
      "Epoch [40/200] Loss: 731.4080810546875\n",
      "Epoch [41/200] Loss: 726.7422485351562\n",
      "Epoch [42/200] Loss: 722.1630249023438\n",
      "Epoch [43/200] Loss: 717.6649780273438\n",
      "Epoch [44/200] Loss: 713.2423706054688\n",
      "Epoch [45/200] Loss: 708.8899536132812\n",
      "Epoch [46/200] Loss: 704.6031494140625\n",
      "Epoch [47/200] Loss: 700.377197265625\n",
      "Epoch [48/200] Loss: 696.2078247070312\n",
      "Epoch [49/200] Loss: 692.0911254882812\n",
      "Epoch [50/200] Loss: 688.0234375\n",
      "Epoch [51/200] Loss: 684.0015869140625\n",
      "Epoch [52/200] Loss: 680.0223999023438\n",
      "Epoch [53/200] Loss: 676.083251953125\n",
      "Epoch [54/200] Loss: 672.181396484375\n",
      "Epoch [55/200] Loss: 668.31494140625\n",
      "Epoch [56/200] Loss: 664.4815673828125\n",
      "Epoch [57/200] Loss: 660.679443359375\n",
      "Epoch [58/200] Loss: 656.906982421875\n",
      "Epoch [59/200] Loss: 653.1624755859375\n",
      "Epoch [60/200] Loss: 649.4447631835938\n",
      "Epoch [61/200] Loss: 645.752685546875\n",
      "Epoch [62/200] Loss: 642.0850830078125\n",
      "Epoch [63/200] Loss: 638.4413452148438\n",
      "Epoch [64/200] Loss: 634.8206176757812\n",
      "Epoch [65/200] Loss: 631.2222290039062\n",
      "Epoch [66/200] Loss: 627.6458740234375\n",
      "Epoch [67/200] Loss: 624.0911254882812\n",
      "Epoch [68/200] Loss: 620.5576171875\n",
      "Epoch [69/200] Loss: 617.0452270507812\n",
      "Epoch [70/200] Loss: 613.5535888671875\n",
      "Epoch [71/200] Loss: 610.0826416015625\n",
      "Epoch [72/200] Loss: 606.6322021484375\n",
      "Epoch [73/200] Loss: 603.2021484375\n",
      "Epoch [74/200] Loss: 599.7923583984375\n",
      "Epoch [75/200] Loss: 596.4027099609375\n",
      "Epoch [76/200] Loss: 593.0330200195312\n",
      "Epoch [77/200] Loss: 589.683349609375\n",
      "Epoch [78/200] Loss: 586.353515625\n",
      "Epoch [79/200] Loss: 583.043212890625\n",
      "Epoch [80/200] Loss: 579.7527465820312\n",
      "Epoch [81/200] Loss: 576.4816284179688\n",
      "Epoch [82/200] Loss: 573.2299194335938\n",
      "Epoch [83/200] Loss: 569.9972534179688\n",
      "Epoch [84/200] Loss: 566.7838134765625\n",
      "Epoch [85/200] Loss: 563.5891723632812\n",
      "Epoch [86/200] Loss: 560.4131469726562\n",
      "Epoch [87/200] Loss: 557.2559814453125\n",
      "Epoch [88/200] Loss: 554.116943359375\n",
      "Epoch [89/200] Loss: 550.996337890625\n",
      "Epoch [90/200] Loss: 547.893798828125\n",
      "Epoch [91/200] Loss: 544.8092041015625\n",
      "Epoch [92/200] Loss: 541.7423095703125\n",
      "Epoch [93/200] Loss: 538.6929321289062\n",
      "Epoch [94/200] Loss: 535.6610107421875\n",
      "Epoch [95/200] Loss: 532.6463012695312\n",
      "Epoch [96/200] Loss: 529.6487426757812\n",
      "Epoch [97/200] Loss: 526.6680908203125\n",
      "Epoch [98/200] Loss: 523.7041015625\n",
      "Epoch [99/200] Loss: 520.7568359375\n",
      "Epoch [100/200] Loss: 517.8259887695312\n",
      "Epoch [101/200] Loss: 514.911376953125\n",
      "Epoch [102/200] Loss: 512.0130004882812\n",
      "Epoch [103/200] Loss: 509.13055419921875\n",
      "Epoch [104/200] Loss: 506.2640075683594\n",
      "Epoch [105/200] Loss: 503.4130859375\n",
      "Epoch [106/200] Loss: 500.5777893066406\n",
      "Epoch [107/200] Loss: 497.7579345703125\n",
      "Epoch [108/200] Loss: 494.9534606933594\n",
      "Epoch [109/200] Loss: 492.1640319824219\n",
      "Epoch [110/200] Loss: 489.3897705078125\n",
      "Epoch [111/200] Loss: 486.6304016113281\n",
      "Epoch [112/200] Loss: 483.8858337402344\n",
      "Epoch [113/200] Loss: 481.156005859375\n",
      "Epoch [114/200] Loss: 478.4405212402344\n",
      "Epoch [115/200] Loss: 475.73968505859375\n",
      "Epoch [116/200] Loss: 473.05316162109375\n",
      "Epoch [117/200] Loss: 470.38079833984375\n",
      "Epoch [118/200] Loss: 467.7225341796875\n",
      "Epoch [119/200] Loss: 465.0784606933594\n",
      "Epoch [120/200] Loss: 462.4480895996094\n",
      "Epoch [121/200] Loss: 459.8316345214844\n",
      "Epoch [122/200] Loss: 457.228759765625\n",
      "Epoch [123/200] Loss: 454.6396484375\n",
      "Epoch [124/200] Loss: 452.06402587890625\n",
      "Epoch [125/200] Loss: 449.50177001953125\n",
      "Epoch [126/200] Loss: 446.952880859375\n",
      "Epoch [127/200] Loss: 444.4172058105469\n",
      "Epoch [128/200] Loss: 441.8946533203125\n",
      "Epoch [129/200] Loss: 439.3852233886719\n",
      "Epoch [130/200] Loss: 436.888671875\n",
      "Epoch [131/200] Loss: 434.4050598144531\n",
      "Epoch [132/200] Loss: 431.9342956542969\n",
      "Epoch [133/200] Loss: 429.4761657714844\n",
      "Epoch [134/200] Loss: 427.03082275390625\n",
      "Epoch [135/200] Loss: 424.5978698730469\n",
      "Epoch [136/200] Loss: 422.1775207519531\n",
      "Epoch [137/200] Loss: 419.76947021484375\n",
      "Epoch [138/200] Loss: 417.3738098144531\n",
      "Epoch [139/200] Loss: 414.99053955078125\n",
      "Epoch [140/200] Loss: 412.619384765625\n",
      "Epoch [141/200] Loss: 410.2602233886719\n",
      "Epoch [142/200] Loss: 407.91326904296875\n",
      "Epoch [143/200] Loss: 405.5782775878906\n",
      "Epoch [144/200] Loss: 403.255126953125\n",
      "Epoch [145/200] Loss: 400.9437561035156\n",
      "Epoch [146/200] Loss: 398.64422607421875\n",
      "Epoch [147/200] Loss: 396.3564758300781\n",
      "Epoch [148/200] Loss: 394.080322265625\n",
      "Epoch [149/200] Loss: 391.81585693359375\n",
      "Epoch [150/200] Loss: 389.562744140625\n",
      "Epoch [151/200] Loss: 387.3211975097656\n",
      "Epoch [152/200] Loss: 385.0909118652344\n",
      "Epoch [153/200] Loss: 382.8721923828125\n",
      "Epoch [154/200] Loss: 380.6646728515625\n",
      "Epoch [155/200] Loss: 378.46826171875\n",
      "Epoch [156/200] Loss: 376.2830505371094\n",
      "Epoch [157/200] Loss: 374.1090393066406\n",
      "Epoch [158/200] Loss: 371.9458923339844\n",
      "Epoch [159/200] Loss: 369.79388427734375\n",
      "Epoch [160/200] Loss: 367.6527404785156\n",
      "Epoch [161/200] Loss: 365.52252197265625\n",
      "Epoch [162/200] Loss: 363.40313720703125\n",
      "Epoch [163/200] Loss: 361.2944641113281\n",
      "Epoch [164/200] Loss: 359.196533203125\n",
      "Epoch [165/200] Loss: 357.1092224121094\n",
      "Epoch [166/200] Loss: 355.0326232910156\n",
      "Epoch [167/200] Loss: 352.96649169921875\n",
      "Epoch [168/200] Loss: 350.9109802246094\n",
      "Epoch [169/200] Loss: 348.8657531738281\n",
      "Epoch [170/200] Loss: 346.8310852050781\n",
      "Epoch [171/200] Loss: 344.80670166015625\n",
      "Epoch [172/200] Loss: 342.79266357421875\n",
      "Epoch [173/200] Loss: 340.7889099121094\n",
      "Epoch [174/200] Loss: 338.79534912109375\n",
      "Epoch [175/200] Loss: 336.8119201660156\n",
      "Epoch [176/200] Loss: 334.83868408203125\n",
      "Epoch [177/200] Loss: 332.87542724609375\n",
      "Epoch [178/200] Loss: 330.92230224609375\n",
      "Epoch [179/200] Loss: 328.9791259765625\n",
      "Epoch [180/200] Loss: 327.0458679199219\n",
      "Epoch [181/200] Loss: 325.1224060058594\n",
      "Epoch [182/200] Loss: 323.20892333984375\n",
      "Epoch [183/200] Loss: 321.30517578125\n",
      "Epoch [184/200] Loss: 319.4112243652344\n",
      "Epoch [185/200] Loss: 317.5269470214844\n",
      "Epoch [186/200] Loss: 315.6523132324219\n",
      "Epoch [187/200] Loss: 313.7873840332031\n",
      "Epoch [188/200] Loss: 311.9319763183594\n",
      "Epoch [189/200] Loss: 310.0860900878906\n",
      "Epoch [190/200] Loss: 308.249755859375\n",
      "Epoch [191/200] Loss: 306.42279052734375\n",
      "Epoch [192/200] Loss: 304.6053466796875\n",
      "Epoch [193/200] Loss: 302.79718017578125\n",
      "Epoch [194/200] Loss: 300.99835205078125\n",
      "Epoch [195/200] Loss: 299.2088928222656\n",
      "Epoch [196/200] Loss: 297.42852783203125\n",
      "Epoch [197/200] Loss: 295.6575927734375\n",
      "Epoch [198/200] Loss: 293.8956604003906\n",
      "Epoch [199/200] Loss: 292.14288330078125\n",
      "Epoch [200/200] Loss: 290.3992004394531\n",
      "Predicted days_remaining for parent_id 50: 16.190196990966797\n",
      "Training for parent_id 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Loss: 175.24856567382812\n",
      "Epoch [2/200] Loss: 169.52005004882812\n",
      "Epoch [3/200] Loss: 164.05357360839844\n",
      "Epoch [4/200] Loss: 158.8621826171875\n",
      "Epoch [5/200] Loss: 153.94371032714844\n",
      "Epoch [6/200] Loss: 149.2857666015625\n",
      "Epoch [7/200] Loss: 144.8702850341797\n",
      "Epoch [8/200] Loss: 140.67813110351562\n",
      "Epoch [9/200] Loss: 136.691162109375\n",
      "Epoch [10/200] Loss: 132.8932647705078\n",
      "Epoch [11/200] Loss: 129.27056884765625\n",
      "Epoch [12/200] Loss: 125.81143951416016\n",
      "Epoch [13/200] Loss: 122.50579833984375\n",
      "Epoch [14/200] Loss: 119.34439849853516\n",
      "Epoch [15/200] Loss: 116.31819915771484\n",
      "Epoch [16/200] Loss: 113.418212890625\n",
      "Epoch [17/200] Loss: 110.63580322265625\n",
      "Epoch [18/200] Loss: 107.96315002441406\n",
      "Epoch [19/200] Loss: 105.39398956298828\n",
      "Epoch [20/200] Loss: 102.92391204833984\n",
      "Epoch [21/200] Loss: 100.55035400390625\n",
      "Epoch [22/200] Loss: 98.27242279052734\n",
      "Epoch [23/200] Loss: 96.08988952636719\n",
      "Epoch [24/200] Loss: 94.00235748291016\n",
      "Epoch [25/200] Loss: 92.00829315185547\n",
      "Epoch [26/200] Loss: 90.10462188720703\n",
      "Epoch [27/200] Loss: 88.28669738769531\n",
      "Epoch [28/200] Loss: 86.54866027832031\n",
      "Epoch [29/200] Loss: 84.88398742675781\n",
      "Epoch [30/200] Loss: 83.28605651855469\n",
      "Epoch [31/200] Loss: 81.74845123291016\n",
      "Epoch [32/200] Loss: 80.26531982421875\n",
      "Epoch [33/200] Loss: 78.83143615722656\n",
      "Epoch [34/200] Loss: 77.44222259521484\n",
      "Epoch [35/200] Loss: 76.09378814697266\n",
      "Epoch [36/200] Loss: 74.78278350830078\n",
      "Epoch [37/200] Loss: 73.50636291503906\n",
      "Epoch [38/200] Loss: 72.2620849609375\n",
      "Epoch [39/200] Loss: 71.04785919189453\n",
      "Epoch [40/200] Loss: 69.86194610595703\n",
      "Epoch [41/200] Loss: 68.70277404785156\n",
      "Epoch [42/200] Loss: 67.56903839111328\n",
      "Epoch [43/200] Loss: 66.4595947265625\n",
      "Epoch [44/200] Loss: 65.3734359741211\n",
      "Epoch [45/200] Loss: 64.30972290039062\n",
      "Epoch [46/200] Loss: 63.26765441894531\n",
      "Epoch [47/200] Loss: 62.246524810791016\n",
      "Epoch [48/200] Loss: 61.24570083618164\n",
      "Epoch [49/200] Loss: 60.26459884643555\n",
      "Epoch [50/200] Loss: 59.30268096923828\n",
      "Epoch [51/200] Loss: 58.35945510864258\n",
      "Epoch [52/200] Loss: 57.4344482421875\n",
      "Epoch [53/200] Loss: 56.52724075317383\n",
      "Epoch [54/200] Loss: 55.63741683959961\n",
      "Epoch [55/200] Loss: 54.764617919921875\n",
      "Epoch [56/200] Loss: 53.908447265625\n",
      "Epoch [57/200] Loss: 53.068565368652344\n",
      "Epoch [58/200] Loss: 52.244632720947266\n",
      "Epoch [59/200] Loss: 51.436241149902344\n",
      "Epoch [60/200] Loss: 50.643123626708984\n",
      "Epoch [61/200] Loss: 49.86488723754883\n",
      "Epoch [62/200] Loss: 49.101173400878906\n",
      "Epoch [63/200] Loss: 48.351627349853516\n",
      "Epoch [64/200] Loss: 47.61592102050781\n",
      "Epoch [65/200] Loss: 46.89365005493164\n",
      "Epoch [66/200] Loss: 46.184513092041016\n",
      "Epoch [67/200] Loss: 45.4881477355957\n",
      "Epoch [68/200] Loss: 44.80426788330078\n",
      "Epoch [69/200] Loss: 44.13258743286133\n",
      "Epoch [70/200] Loss: 43.47284698486328\n",
      "Epoch [71/200] Loss: 42.82489013671875\n",
      "Epoch [72/200] Loss: 42.188507080078125\n",
      "Epoch [73/200] Loss: 41.5635986328125\n",
      "Epoch [74/200] Loss: 40.95006561279297\n",
      "Epoch [75/200] Loss: 40.347816467285156\n",
      "Epoch [76/200] Loss: 39.75679016113281\n",
      "Epoch [77/200] Loss: 39.17691421508789\n",
      "Epoch [78/200] Loss: 38.60812759399414\n",
      "Epoch [79/200] Loss: 38.0503044128418\n",
      "Epoch [80/200] Loss: 37.50336456298828\n",
      "Epoch [81/200] Loss: 36.96718978881836\n",
      "Epoch [82/200] Loss: 36.4416389465332\n",
      "Epoch [83/200] Loss: 35.92658615112305\n",
      "Epoch [84/200] Loss: 35.42188262939453\n",
      "Epoch [85/200] Loss: 34.9273681640625\n",
      "Epoch [86/200] Loss: 34.44290542602539\n",
      "Epoch [87/200] Loss: 33.96833038330078\n",
      "Epoch [88/200] Loss: 33.50344467163086\n",
      "Epoch [89/200] Loss: 33.04814529418945\n",
      "Epoch [90/200] Loss: 32.60225296020508\n",
      "Epoch [91/200] Loss: 32.165584564208984\n",
      "Epoch [92/200] Loss: 31.738014221191406\n",
      "Epoch [93/200] Loss: 31.31936264038086\n",
      "Epoch [94/200] Loss: 30.90949058532715\n",
      "Epoch [95/200] Loss: 30.50822639465332\n",
      "Epoch [96/200] Loss: 30.115453720092773\n",
      "Epoch [97/200] Loss: 29.730974197387695\n",
      "Epoch [98/200] Loss: 29.354677200317383\n",
      "Epoch [99/200] Loss: 28.98639678955078\n",
      "Epoch [100/200] Loss: 28.62600326538086\n",
      "Epoch [101/200] Loss: 28.27334976196289\n",
      "Epoch [102/200] Loss: 27.92828369140625\n",
      "Epoch [103/200] Loss: 27.5906982421875\n",
      "Epoch [104/200] Loss: 27.26042938232422\n",
      "Epoch [105/200] Loss: 26.937358856201172\n",
      "Epoch [106/200] Loss: 26.621334075927734\n",
      "Epoch [107/200] Loss: 26.312255859375\n",
      "Epoch [108/200] Loss: 26.009979248046875\n",
      "Epoch [109/200] Loss: 25.714380264282227\n",
      "Epoch [110/200] Loss: 25.425323486328125\n",
      "Epoch [111/200] Loss: 25.142702102661133\n",
      "Epoch [112/200] Loss: 24.866397857666016\n",
      "Epoch [113/200] Loss: 24.596281051635742\n",
      "Epoch [114/200] Loss: 24.332250595092773\n",
      "Epoch [115/200] Loss: 24.074169158935547\n",
      "Epoch [116/200] Loss: 23.821945190429688\n",
      "Epoch [117/200] Loss: 23.575462341308594\n",
      "Epoch [118/200] Loss: 23.33460235595703\n",
      "Epoch [119/200] Loss: 23.09926414489746\n",
      "Epoch [120/200] Loss: 22.86934471130371\n",
      "Epoch [121/200] Loss: 22.644729614257812\n",
      "Epoch [122/200] Loss: 22.425323486328125\n",
      "Epoch [123/200] Loss: 22.211029052734375\n",
      "Epoch [124/200] Loss: 22.001737594604492\n",
      "Epoch [125/200] Loss: 21.7973575592041\n",
      "Epoch [126/200] Loss: 21.597789764404297\n",
      "Epoch [127/200] Loss: 21.4029541015625\n",
      "Epoch [128/200] Loss: 21.21272850036621\n",
      "Epoch [129/200] Loss: 21.027050018310547\n",
      "Epoch [130/200] Loss: 20.845809936523438\n",
      "Epoch [131/200] Loss: 20.66891860961914\n",
      "Epoch [132/200] Loss: 20.496292114257812\n",
      "Epoch [133/200] Loss: 20.327861785888672\n",
      "Epoch [134/200] Loss: 20.163522720336914\n",
      "Epoch [135/200] Loss: 20.003196716308594\n",
      "Epoch [136/200] Loss: 19.846797943115234\n",
      "Epoch [137/200] Loss: 19.694242477416992\n",
      "Epoch [138/200] Loss: 19.545467376708984\n",
      "Epoch [139/200] Loss: 19.40038299560547\n",
      "Epoch [140/200] Loss: 19.258920669555664\n",
      "Epoch [141/200] Loss: 19.12099266052246\n",
      "Epoch [142/200] Loss: 18.986530303955078\n",
      "Epoch [143/200] Loss: 18.855464935302734\n",
      "Epoch [144/200] Loss: 18.727718353271484\n",
      "Epoch [145/200] Loss: 18.603225708007812\n",
      "Epoch [146/200] Loss: 18.48190689086914\n",
      "Epoch [147/200] Loss: 18.36370086669922\n",
      "Epoch [148/200] Loss: 18.248546600341797\n",
      "Epoch [149/200] Loss: 18.136369705200195\n",
      "Epoch [150/200] Loss: 18.02710723876953\n",
      "Epoch [151/200] Loss: 17.920696258544922\n",
      "Epoch [152/200] Loss: 17.81707000732422\n",
      "Epoch [153/200] Loss: 17.716167449951172\n",
      "Epoch [154/200] Loss: 17.617935180664062\n",
      "Epoch [155/200] Loss: 17.52230453491211\n",
      "Epoch [156/200] Loss: 17.429222106933594\n",
      "Epoch [157/200] Loss: 17.338626861572266\n",
      "Epoch [158/200] Loss: 17.250473022460938\n",
      "Epoch [159/200] Loss: 17.16469383239746\n",
      "Epoch [160/200] Loss: 17.08123207092285\n",
      "Epoch [161/200] Loss: 17.000041961669922\n",
      "Epoch [162/200] Loss: 16.921070098876953\n",
      "Epoch [163/200] Loss: 16.844263076782227\n",
      "Epoch [164/200] Loss: 16.769569396972656\n",
      "Epoch [165/200] Loss: 16.696941375732422\n",
      "Epoch [166/200] Loss: 16.626331329345703\n",
      "Epoch [167/200] Loss: 16.557689666748047\n",
      "Epoch [168/200] Loss: 16.490968704223633\n",
      "Epoch [169/200] Loss: 16.426124572753906\n",
      "Epoch [170/200] Loss: 16.36310577392578\n",
      "Epoch [171/200] Loss: 16.301876068115234\n",
      "Epoch [172/200] Loss: 16.242387771606445\n",
      "Epoch [173/200] Loss: 16.184595108032227\n",
      "Epoch [174/200] Loss: 16.128463745117188\n",
      "Epoch [175/200] Loss: 16.073951721191406\n",
      "Epoch [176/200] Loss: 16.021011352539062\n",
      "Epoch [177/200] Loss: 15.969615936279297\n",
      "Epoch [178/200] Loss: 15.919711112976074\n",
      "Epoch [179/200] Loss: 15.871264457702637\n",
      "Epoch [180/200] Loss: 15.824250221252441\n",
      "Epoch [181/200] Loss: 15.778621673583984\n",
      "Epoch [182/200] Loss: 15.734344482421875\n",
      "Epoch [183/200] Loss: 15.691381454467773\n",
      "Epoch [184/200] Loss: 15.649701118469238\n",
      "Epoch [185/200] Loss: 15.609270095825195\n",
      "Epoch [186/200] Loss: 15.570058822631836\n",
      "Epoch [187/200] Loss: 15.53203010559082\n",
      "Epoch [188/200] Loss: 15.495155334472656\n",
      "Epoch [189/200] Loss: 15.459407806396484\n",
      "Epoch [190/200] Loss: 15.424749374389648\n",
      "Epoch [191/200] Loss: 15.391157150268555\n",
      "Epoch [192/200] Loss: 15.358598709106445\n",
      "Epoch [193/200] Loss: 15.327051162719727\n",
      "Epoch [194/200] Loss: 15.296478271484375\n",
      "Epoch [195/200] Loss: 15.266862869262695\n",
      "Epoch [196/200] Loss: 15.238168716430664\n",
      "Epoch [197/200] Loss: 15.210383415222168\n",
      "Epoch [198/200] Loss: 15.183473587036133\n",
      "Epoch [199/200] Loss: 15.157411575317383\n",
      "Epoch [200/200] Loss: 15.132181167602539\n",
      "Predicted days_remaining for parent_id 51: 11.93130874633789\n",
      "Training for parent_id 61...\n",
      "Epoch [1/200] Loss: 319.17095947265625\n",
      "Epoch [2/200] Loss: 311.26470947265625\n",
      "Epoch [3/200] Loss: 303.5827331542969\n",
      "Epoch [4/200] Loss: 296.1131591796875\n",
      "Epoch [5/200] Loss: 288.84814453125\n",
      "Epoch [6/200] Loss: 281.78900146484375\n",
      "Epoch [7/200] Loss: 274.9447021484375\n",
      "Epoch [8/200] Loss: 268.3269958496094\n",
      "Epoch [9/200] Loss: 261.947021484375\n",
      "Epoch [10/200] Loss: 255.81175231933594\n",
      "Epoch [11/200] Loss: 249.92343139648438\n",
      "Epoch [12/200] Loss: 244.28067016601562\n",
      "Epoch [13/200] Loss: 238.87989807128906\n",
      "Epoch [14/200] Loss: 233.71633911132812\n",
      "Epoch [15/200] Loss: 228.78402709960938\n",
      "Epoch [16/200] Loss: 224.0755157470703\n",
      "Epoch [17/200] Loss: 219.5817413330078\n",
      "Epoch [18/200] Loss: 215.2921905517578\n",
      "Epoch [19/200] Loss: 211.19544982910156\n",
      "Epoch [20/200] Loss: 207.27969360351562\n",
      "Epoch [21/200] Loss: 203.53318786621094\n",
      "Epoch [22/200] Loss: 199.9447784423828\n",
      "Epoch [23/200] Loss: 196.50379943847656\n",
      "Epoch [24/200] Loss: 193.20022583007812\n",
      "Epoch [25/200] Loss: 190.02476501464844\n",
      "Epoch [26/200] Loss: 186.96878051757812\n",
      "Epoch [27/200] Loss: 184.02438354492188\n",
      "Epoch [28/200] Loss: 181.1844482421875\n",
      "Epoch [29/200] Loss: 178.44256591796875\n",
      "Epoch [30/200] Loss: 175.79269409179688\n",
      "Epoch [31/200] Loss: 173.22915649414062\n",
      "Epoch [32/200] Loss: 170.7466278076172\n",
      "Epoch [33/200] Loss: 168.33984375\n",
      "Epoch [34/200] Loss: 166.0038604736328\n",
      "Epoch [35/200] Loss: 163.73394775390625\n",
      "Epoch [36/200] Loss: 161.5254364013672\n",
      "Epoch [37/200] Loss: 159.37408447265625\n",
      "Epoch [38/200] Loss: 157.27584838867188\n",
      "Epoch [39/200] Loss: 155.2271270751953\n",
      "Epoch [40/200] Loss: 153.22438049316406\n",
      "Epoch [41/200] Loss: 151.26467895507812\n",
      "Epoch [42/200] Loss: 149.34523010253906\n",
      "Epoch [43/200] Loss: 147.4635467529297\n",
      "Epoch [44/200] Loss: 145.617431640625\n",
      "Epoch [45/200] Loss: 143.80490112304688\n",
      "Epoch [46/200] Loss: 142.02426147460938\n",
      "Epoch [47/200] Loss: 140.2738800048828\n",
      "Epoch [48/200] Loss: 138.55238342285156\n",
      "Epoch [49/200] Loss: 136.8585662841797\n",
      "Epoch [50/200] Loss: 135.19122314453125\n",
      "Epoch [51/200] Loss: 133.5494842529297\n",
      "Epoch [52/200] Loss: 131.93228149414062\n",
      "Epoch [53/200] Loss: 130.33897399902344\n",
      "Epoch [54/200] Loss: 128.76870727539062\n",
      "Epoch [55/200] Loss: 127.2208251953125\n",
      "Epoch [56/200] Loss: 125.69476318359375\n",
      "Epoch [57/200] Loss: 124.18991088867188\n",
      "Epoch [58/200] Loss: 122.70574951171875\n",
      "Epoch [59/200] Loss: 121.24178314208984\n",
      "Epoch [60/200] Loss: 119.79756927490234\n",
      "Epoch [61/200] Loss: 118.37268829345703\n",
      "Epoch [62/200] Loss: 116.96675872802734\n",
      "Epoch [63/200] Loss: 115.57935333251953\n",
      "Epoch [64/200] Loss: 114.21013641357422\n",
      "Epoch [65/200] Loss: 112.85875701904297\n",
      "Epoch [66/200] Loss: 111.52487182617188\n",
      "Epoch [67/200] Loss: 110.20819854736328\n",
      "Epoch [68/200] Loss: 108.90841674804688\n",
      "Epoch [69/200] Loss: 107.6252212524414\n",
      "Epoch [70/200] Loss: 106.35836029052734\n",
      "Epoch [71/200] Loss: 105.10755157470703\n",
      "Epoch [72/200] Loss: 103.87252807617188\n",
      "Epoch [73/200] Loss: 102.65302276611328\n",
      "Epoch [74/200] Loss: 101.44879913330078\n",
      "Epoch [75/200] Loss: 100.25967407226562\n",
      "Epoch [76/200] Loss: 99.08535766601562\n",
      "Epoch [77/200] Loss: 97.9256362915039\n",
      "Epoch [78/200] Loss: 96.78032684326172\n",
      "Epoch [79/200] Loss: 95.6491928100586\n",
      "Epoch [80/200] Loss: 94.53205871582031\n",
      "Epoch [81/200] Loss: 93.42871856689453\n",
      "Epoch [82/200] Loss: 92.33898162841797\n",
      "Epoch [83/200] Loss: 91.26263427734375\n",
      "Epoch [84/200] Loss: 90.19953918457031\n",
      "Epoch [85/200] Loss: 89.14949035644531\n",
      "Epoch [86/200] Loss: 88.11235046386719\n",
      "Epoch [87/200] Loss: 87.0879135131836\n",
      "Epoch [88/200] Loss: 86.07605743408203\n",
      "Epoch [89/200] Loss: 85.0765609741211\n",
      "Epoch [90/200] Loss: 84.08932495117188\n",
      "Epoch [91/200] Loss: 83.11417388916016\n",
      "Epoch [92/200] Loss: 82.15096282958984\n",
      "Epoch [93/200] Loss: 81.19955444335938\n",
      "Epoch [94/200] Loss: 80.25977325439453\n",
      "Epoch [95/200] Loss: 79.33149719238281\n",
      "Epoch [96/200] Loss: 78.41462707519531\n",
      "Epoch [97/200] Loss: 77.50898742675781\n",
      "Epoch [98/200] Loss: 76.61444091796875\n",
      "Epoch [99/200] Loss: 75.73086547851562\n",
      "Epoch [100/200] Loss: 74.85816955566406\n",
      "Epoch [101/200] Loss: 73.99617004394531\n",
      "Epoch [102/200] Loss: 73.14476013183594\n",
      "Epoch [103/200] Loss: 72.30384063720703\n",
      "Epoch [104/200] Loss: 71.47330474853516\n",
      "Epoch [105/200] Loss: 70.65298461914062\n",
      "Epoch [106/200] Loss: 69.84280395507812\n",
      "Epoch [107/200] Loss: 69.0426254272461\n",
      "Epoch [108/200] Loss: 68.25234985351562\n",
      "Epoch [109/200] Loss: 67.47189331054688\n",
      "Epoch [110/200] Loss: 66.70108795166016\n",
      "Epoch [111/200] Loss: 65.93988800048828\n",
      "Epoch [112/200] Loss: 65.18814849853516\n",
      "Epoch [113/200] Loss: 64.4457778930664\n",
      "Epoch [114/200] Loss: 63.71268081665039\n",
      "Epoch [115/200] Loss: 62.98873519897461\n",
      "Epoch [116/200] Loss: 62.27387619018555\n",
      "Epoch [117/200] Loss: 61.567996978759766\n",
      "Epoch [118/200] Loss: 60.87095642089844\n",
      "Epoch [119/200] Loss: 60.18272399902344\n",
      "Epoch [120/200] Loss: 59.50315475463867\n",
      "Epoch [121/200] Loss: 58.83216857910156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/200] Loss: 58.16969680786133\n",
      "Epoch [123/200] Loss: 57.515625\n",
      "Epoch [124/200] Loss: 56.86985397338867\n",
      "Epoch [125/200] Loss: 56.232322692871094\n",
      "Epoch [126/200] Loss: 55.602928161621094\n",
      "Epoch [127/200] Loss: 54.981590270996094\n",
      "Epoch [128/200] Loss: 54.36819076538086\n",
      "Epoch [129/200] Loss: 53.7627067565918\n",
      "Epoch [130/200] Loss: 53.165008544921875\n",
      "Epoch [131/200] Loss: 52.57502746582031\n",
      "Epoch [132/200] Loss: 51.992671966552734\n",
      "Epoch [133/200] Loss: 51.4178581237793\n",
      "Epoch [134/200] Loss: 50.85055160522461\n",
      "Epoch [135/200] Loss: 50.29058074951172\n",
      "Epoch [136/200] Loss: 49.737945556640625\n",
      "Epoch [137/200] Loss: 49.19254684448242\n",
      "Epoch [138/200] Loss: 48.654293060302734\n",
      "Epoch [139/200] Loss: 48.12311935424805\n",
      "Epoch [140/200] Loss: 47.598941802978516\n",
      "Epoch [141/200] Loss: 47.08171463012695\n",
      "Epoch [142/200] Loss: 46.57130813598633\n",
      "Epoch [143/200] Loss: 46.06770706176758\n",
      "Epoch [144/200] Loss: 45.570796966552734\n",
      "Epoch [145/200] Loss: 45.08052062988281\n",
      "Epoch [146/200] Loss: 44.59681701660156\n",
      "Epoch [147/200] Loss: 44.119606018066406\n",
      "Epoch [148/200] Loss: 43.648799896240234\n",
      "Epoch [149/200] Loss: 43.1843376159668\n",
      "Epoch [150/200] Loss: 42.72617721557617\n",
      "Epoch [151/200] Loss: 42.274227142333984\n",
      "Epoch [152/200] Loss: 41.82840347290039\n",
      "Epoch [153/200] Loss: 41.388675689697266\n",
      "Epoch [154/200] Loss: 40.9549560546875\n",
      "Epoch [155/200] Loss: 40.52717971801758\n",
      "Epoch [156/200] Loss: 40.105262756347656\n",
      "Epoch [157/200] Loss: 39.689186096191406\n",
      "Epoch [158/200] Loss: 39.27885055541992\n",
      "Epoch [159/200] Loss: 38.87419891357422\n",
      "Epoch [160/200] Loss: 38.47517013549805\n",
      "Epoch [161/200] Loss: 38.08170700073242\n",
      "Epoch [162/200] Loss: 37.69373321533203\n",
      "Epoch [163/200] Loss: 37.311195373535156\n",
      "Epoch [164/200] Loss: 36.93402862548828\n",
      "Epoch [165/200] Loss: 36.56217956542969\n",
      "Epoch [166/200] Loss: 36.195587158203125\n",
      "Epoch [167/200] Loss: 35.83418273925781\n",
      "Epoch [168/200] Loss: 35.477928161621094\n",
      "Epoch [169/200] Loss: 35.12673568725586\n",
      "Epoch [170/200] Loss: 34.78057098388672\n",
      "Epoch [171/200] Loss: 34.439353942871094\n",
      "Epoch [172/200] Loss: 34.1030387878418\n",
      "Epoch [173/200] Loss: 33.77158737182617\n",
      "Epoch [174/200] Loss: 33.44491195678711\n",
      "Epoch [175/200] Loss: 33.12299346923828\n",
      "Epoch [176/200] Loss: 32.80573654174805\n",
      "Epoch [177/200] Loss: 32.493099212646484\n",
      "Epoch [178/200] Loss: 32.18503189086914\n",
      "Epoch [179/200] Loss: 31.881498336791992\n",
      "Epoch [180/200] Loss: 31.58241844177246\n",
      "Epoch [181/200] Loss: 31.28774642944336\n",
      "Epoch [182/200] Loss: 30.997426986694336\n",
      "Epoch [183/200] Loss: 30.71142578125\n",
      "Epoch [184/200] Loss: 30.4296817779541\n",
      "Epoch [185/200] Loss: 30.15213394165039\n",
      "Epoch [186/200] Loss: 29.878738403320312\n",
      "Epoch [187/200] Loss: 29.60944175720215\n",
      "Epoch [188/200] Loss: 29.344202041625977\n",
      "Epoch [189/200] Loss: 29.082958221435547\n",
      "Epoch [190/200] Loss: 28.825672149658203\n",
      "Epoch [191/200] Loss: 28.572298049926758\n",
      "Epoch [192/200] Loss: 28.322769165039062\n",
      "Epoch [193/200] Loss: 28.077056884765625\n",
      "Epoch [194/200] Loss: 27.83511734008789\n",
      "Epoch [195/200] Loss: 27.59686851501465\n",
      "Epoch [196/200] Loss: 27.362316131591797\n",
      "Epoch [197/200] Loss: 27.13137435913086\n",
      "Epoch [198/200] Loss: 26.904014587402344\n",
      "Epoch [199/200] Loss: 26.6801815032959\n",
      "Epoch [200/200] Loss: 26.459842681884766\n",
      "Predicted days_remaining for parent_id 61: 14.314095497131348\n",
      "Training for parent_id 70...\n",
      "Epoch [1/200] Loss: 203.85520935058594\n",
      "Epoch [2/200] Loss: 198.7322540283203\n",
      "Epoch [3/200] Loss: 193.7706756591797\n",
      "Epoch [4/200] Loss: 188.97903442382812\n",
      "Epoch [5/200] Loss: 184.34890747070312\n",
      "Epoch [6/200] Loss: 179.86880493164062\n",
      "Epoch [7/200] Loss: 175.53189086914062\n",
      "Epoch [8/200] Loss: 171.33477783203125\n",
      "Epoch [9/200] Loss: 167.27574157714844\n",
      "Epoch [10/200] Loss: 163.35427856445312\n",
      "Epoch [11/200] Loss: 159.57066345214844\n",
      "Epoch [12/200] Loss: 155.92581176757812\n",
      "Epoch [13/200] Loss: 152.42076110839844\n",
      "Epoch [14/200] Loss: 149.05613708496094\n",
      "Epoch [15/200] Loss: 145.83126831054688\n",
      "Epoch [16/200] Loss: 142.744140625\n",
      "Epoch [17/200] Loss: 139.79107666015625\n",
      "Epoch [18/200] Loss: 136.96697998046875\n",
      "Epoch [19/200] Loss: 134.2660675048828\n",
      "Epoch [20/200] Loss: 131.68186950683594\n",
      "Epoch [21/200] Loss: 129.207763671875\n",
      "Epoch [22/200] Loss: 126.83684539794922\n",
      "Epoch [23/200] Loss: 124.56193542480469\n",
      "Epoch [24/200] Loss: 122.37572479248047\n",
      "Epoch [25/200] Loss: 120.27071380615234\n",
      "Epoch [26/200] Loss: 118.23956298828125\n",
      "Epoch [27/200] Loss: 116.27531433105469\n",
      "Epoch [28/200] Loss: 114.37173461914062\n",
      "Epoch [29/200] Loss: 112.52328491210938\n",
      "Epoch [30/200] Loss: 110.7252426147461\n",
      "Epoch [31/200] Loss: 108.97364807128906\n",
      "Epoch [32/200] Loss: 107.26518249511719\n",
      "Epoch [33/200] Loss: 105.59703826904297\n",
      "Epoch [34/200] Loss: 103.96672058105469\n",
      "Epoch [35/200] Loss: 102.37199401855469\n",
      "Epoch [36/200] Loss: 100.81089782714844\n",
      "Epoch [37/200] Loss: 99.28150177001953\n",
      "Epoch [38/200] Loss: 97.78211975097656\n",
      "Epoch [39/200] Loss: 96.31099700927734\n",
      "Epoch [40/200] Loss: 94.86663818359375\n",
      "Epoch [41/200] Loss: 93.44754791259766\n",
      "Epoch [42/200] Loss: 92.05236053466797\n",
      "Epoch [43/200] Loss: 90.67992401123047\n",
      "Epoch [44/200] Loss: 89.32908630371094\n",
      "Epoch [45/200] Loss: 87.99890899658203\n",
      "Epoch [46/200] Loss: 86.68855285644531\n",
      "Epoch [47/200] Loss: 85.39725494384766\n",
      "Epoch [48/200] Loss: 84.12439727783203\n",
      "Epoch [49/200] Loss: 82.8695068359375\n",
      "Epoch [50/200] Loss: 81.63203430175781\n",
      "Epoch [51/200] Loss: 80.4117202758789\n",
      "Epoch [52/200] Loss: 79.20819091796875\n",
      "Epoch [53/200] Loss: 78.02127838134766\n",
      "Epoch [54/200] Loss: 76.8508071899414\n",
      "Epoch [55/200] Loss: 75.69673919677734\n",
      "Epoch [56/200] Loss: 74.55899047851562\n",
      "Epoch [57/200] Loss: 73.43754577636719\n",
      "Epoch [58/200] Loss: 72.3324966430664\n",
      "Epoch [59/200] Loss: 71.2438735961914\n",
      "Epoch [60/200] Loss: 70.17176055908203\n",
      "Epoch [61/200] Loss: 69.11622619628906\n",
      "Epoch [62/200] Loss: 68.07735443115234\n",
      "Epoch [63/200] Loss: 67.05523681640625\n",
      "Epoch [64/200] Loss: 66.04985046386719\n",
      "Epoch [65/200] Loss: 65.06129455566406\n",
      "Epoch [66/200] Loss: 64.08949279785156\n",
      "Epoch [67/200] Loss: 63.1344108581543\n",
      "Epoch [68/200] Loss: 62.19596481323242\n",
      "Epoch [69/200] Loss: 61.2740364074707\n",
      "Epoch [70/200] Loss: 60.36843490600586\n",
      "Epoch [71/200] Loss: 59.479007720947266\n",
      "Epoch [72/200] Loss: 58.605499267578125\n",
      "Epoch [73/200] Loss: 57.74767303466797\n",
      "Epoch [74/200] Loss: 56.90528106689453\n",
      "Epoch [75/200] Loss: 56.07805633544922\n",
      "Epoch [76/200] Loss: 55.265724182128906\n",
      "Epoch [77/200] Loss: 54.467987060546875\n",
      "Epoch [78/200] Loss: 53.684635162353516\n",
      "Epoch [79/200] Loss: 52.91531753540039\n",
      "Epoch [80/200] Loss: 52.159820556640625\n",
      "Epoch [81/200] Loss: 51.4178581237793\n",
      "Epoch [82/200] Loss: 50.68918228149414\n",
      "Epoch [83/200] Loss: 49.97352600097656\n",
      "Epoch [84/200] Loss: 49.27068328857422\n",
      "Epoch [85/200] Loss: 48.580379486083984\n",
      "Epoch [86/200] Loss: 47.902400970458984\n",
      "Epoch [87/200] Loss: 47.23652267456055\n",
      "Epoch [88/200] Loss: 46.582515716552734\n",
      "Epoch [89/200] Loss: 45.9401741027832\n",
      "Epoch [90/200] Loss: 45.30929183959961\n",
      "Epoch [91/200] Loss: 44.689659118652344\n",
      "Epoch [92/200] Loss: 44.08110427856445\n",
      "Epoch [93/200] Loss: 43.48340606689453\n",
      "Epoch [94/200] Loss: 42.89637756347656\n",
      "Epoch [95/200] Loss: 42.31983184814453\n",
      "Epoch [96/200] Loss: 41.75362777709961\n",
      "Epoch [97/200] Loss: 41.19755935668945\n",
      "Epoch [98/200] Loss: 40.65147399902344\n",
      "Epoch [99/200] Loss: 40.11515426635742\n",
      "Epoch [100/200] Loss: 39.588504791259766\n",
      "Epoch [101/200] Loss: 39.07132339477539\n",
      "Epoch [102/200] Loss: 38.563453674316406\n",
      "Epoch [103/200] Loss: 38.06476593017578\n",
      "Epoch [104/200] Loss: 37.57509231567383\n",
      "Epoch [105/200] Loss: 37.094295501708984\n",
      "Epoch [106/200] Loss: 36.62221145629883\n",
      "Epoch [107/200] Loss: 36.15870666503906\n",
      "Epoch [108/200] Loss: 35.703651428222656\n",
      "Epoch [109/200] Loss: 35.25691223144531\n",
      "Epoch [110/200] Loss: 34.818328857421875\n",
      "Epoch [111/200] Loss: 34.38779830932617\n",
      "Epoch [112/200] Loss: 33.96516799926758\n",
      "Epoch [113/200] Loss: 33.550323486328125\n",
      "Epoch [114/200] Loss: 33.14313888549805\n",
      "Epoch [115/200] Loss: 32.74348068237305\n",
      "Epoch [116/200] Loss: 32.35124969482422\n",
      "Epoch [117/200] Loss: 31.966306686401367\n",
      "Epoch [118/200] Loss: 31.588546752929688\n",
      "Epoch [119/200] Loss: 31.217845916748047\n",
      "Epoch [120/200] Loss: 30.854093551635742\n",
      "Epoch [121/200] Loss: 30.497180938720703\n",
      "Epoch [122/200] Loss: 30.146989822387695\n",
      "Epoch [123/200] Loss: 29.803428649902344\n",
      "Epoch [124/200] Loss: 29.466381072998047\n",
      "Epoch [125/200] Loss: 29.135738372802734\n",
      "Epoch [126/200] Loss: 28.811405181884766\n",
      "Epoch [127/200] Loss: 28.493270874023438\n",
      "Epoch [128/200] Loss: 28.181241989135742\n",
      "Epoch [129/200] Loss: 27.875226974487305\n",
      "Epoch [130/200] Loss: 27.575109481811523\n",
      "Epoch [131/200] Loss: 27.280811309814453\n",
      "Epoch [132/200] Loss: 26.992229461669922\n",
      "Epoch [133/200] Loss: 26.709266662597656\n",
      "Epoch [134/200] Loss: 26.431846618652344\n",
      "Epoch [135/200] Loss: 26.15985870361328\n",
      "Epoch [136/200] Loss: 25.89322853088379\n",
      "Epoch [137/200] Loss: 25.631868362426758\n",
      "Epoch [138/200] Loss: 25.375673294067383\n",
      "Epoch [139/200] Loss: 25.124584197998047\n",
      "Epoch [140/200] Loss: 24.878498077392578\n",
      "Epoch [141/200] Loss: 24.637331008911133\n",
      "Epoch [142/200] Loss: 24.401016235351562\n",
      "Epoch [143/200] Loss: 24.169464111328125\n",
      "Epoch [144/200] Loss: 23.942590713500977\n",
      "Epoch [145/200] Loss: 23.720325469970703\n",
      "Epoch [146/200] Loss: 23.502567291259766\n",
      "Epoch [147/200] Loss: 23.28927230834961\n",
      "Epoch [148/200] Loss: 23.080354690551758\n",
      "Epoch [149/200] Loss: 22.875732421875\n",
      "Epoch [150/200] Loss: 22.675329208374023\n",
      "Epoch [151/200] Loss: 22.47908592224121\n",
      "Epoch [152/200] Loss: 22.286922454833984\n",
      "Epoch [153/200] Loss: 22.09876251220703\n",
      "Epoch [154/200] Loss: 21.914546966552734\n",
      "Epoch [155/200] Loss: 21.73420524597168\n",
      "Epoch [156/200] Loss: 21.557662963867188\n",
      "Epoch [157/200] Loss: 21.384857177734375\n",
      "Epoch [158/200] Loss: 21.215728759765625\n",
      "Epoch [159/200] Loss: 21.05020523071289\n",
      "Epoch [160/200] Loss: 20.888221740722656\n",
      "Epoch [161/200] Loss: 20.729719161987305\n",
      "Epoch [162/200] Loss: 20.574630737304688\n",
      "Epoch [163/200] Loss: 20.422895431518555\n",
      "Epoch [164/200] Loss: 20.274456024169922\n",
      "Epoch [165/200] Loss: 20.129247665405273\n",
      "Epoch [166/200] Loss: 19.987220764160156\n",
      "Epoch [167/200] Loss: 19.848304748535156\n",
      "Epoch [168/200] Loss: 19.71245574951172\n",
      "Epoch [169/200] Loss: 19.579599380493164\n",
      "Epoch [170/200] Loss: 19.449705123901367\n",
      "Epoch [171/200] Loss: 19.32268714904785\n",
      "Epoch [172/200] Loss: 19.198516845703125\n",
      "Epoch [173/200] Loss: 19.077131271362305\n",
      "Epoch [174/200] Loss: 18.958477020263672\n",
      "Epoch [175/200] Loss: 18.84250259399414\n",
      "Epoch [176/200] Loss: 18.72915267944336\n",
      "Epoch [177/200] Loss: 18.618385314941406\n",
      "Epoch [178/200] Loss: 18.51015853881836\n",
      "Epoch [179/200] Loss: 18.404399871826172\n",
      "Epoch [180/200] Loss: 18.301076889038086\n",
      "Epoch [181/200] Loss: 18.200143814086914\n",
      "Epoch [182/200] Loss: 18.101547241210938\n",
      "Epoch [183/200] Loss: 18.005237579345703\n",
      "Epoch [184/200] Loss: 17.911182403564453\n",
      "Epoch [185/200] Loss: 17.819320678710938\n",
      "Epoch [186/200] Loss: 17.729633331298828\n",
      "Epoch [187/200] Loss: 17.642053604125977\n",
      "Epoch [188/200] Loss: 17.556549072265625\n",
      "Epoch [189/200] Loss: 17.47307014465332\n",
      "Epoch [190/200] Loss: 17.39158821105957\n",
      "Epoch [191/200] Loss: 17.312049865722656\n",
      "Epoch [192/200] Loss: 17.23442840576172\n",
      "Epoch [193/200] Loss: 17.158674240112305\n",
      "Epoch [194/200] Loss: 17.084753036499023\n",
      "Epoch [195/200] Loss: 17.01262664794922\n",
      "Epoch [196/200] Loss: 16.942256927490234\n",
      "Epoch [197/200] Loss: 16.873607635498047\n",
      "Epoch [198/200] Loss: 16.806640625\n",
      "Epoch [199/200] Loss: 16.74132537841797\n",
      "Epoch [200/200] Loss: 16.67761993408203\n",
      "Predicted days_remaining for parent_id 70: 12.274197578430176\n",
      "Training for parent_id 74...\n",
      "Epoch [1/200] Loss: 111.442626953125\n",
      "Epoch [2/200] Loss: 107.39196014404297\n",
      "Epoch [3/200] Loss: 103.50032043457031\n",
      "Epoch [4/200] Loss: 99.74998474121094\n",
      "Epoch [5/200] Loss: 96.12269592285156\n",
      "Epoch [6/200] Loss: 92.6036605834961\n",
      "Epoch [7/200] Loss: 89.18585968017578\n",
      "Epoch [8/200] Loss: 85.87003326416016\n",
      "Epoch [9/200] Loss: 82.66166687011719\n",
      "Epoch [10/200] Loss: 79.56781768798828\n",
      "Epoch [11/200] Loss: 76.59466552734375\n",
      "Epoch [12/200] Loss: 73.74617004394531\n",
      "Epoch [13/200] Loss: 71.02336883544922\n",
      "Epoch [14/200] Loss: 68.42456817626953\n",
      "Epoch [15/200] Loss: 65.94600677490234\n",
      "Epoch [16/200] Loss: 63.58268356323242\n",
      "Epoch [17/200] Loss: 61.329071044921875\n",
      "Epoch [18/200] Loss: 59.17961502075195\n",
      "Epoch [19/200] Loss: 57.129051208496094\n",
      "Epoch [20/200] Loss: 55.17265701293945\n",
      "Epoch [21/200] Loss: 53.30621337890625\n",
      "Epoch [22/200] Loss: 51.526084899902344\n",
      "Epoch [23/200] Loss: 49.82909393310547\n",
      "Epoch [24/200] Loss: 48.21240234375\n",
      "Epoch [25/200] Loss: 46.6733512878418\n",
      "Epoch [26/200] Loss: 45.209346771240234\n",
      "Epoch [27/200] Loss: 43.81778335571289\n",
      "Epoch [28/200] Loss: 42.495948791503906\n",
      "Epoch [29/200] Loss: 41.24100112915039\n",
      "Epoch [30/200] Loss: 40.050025939941406\n",
      "Epoch [31/200] Loss: 38.91998291015625\n",
      "Epoch [32/200] Loss: 37.84785079956055\n",
      "Epoch [33/200] Loss: 36.83060836791992\n",
      "Epoch [34/200] Loss: 35.86529541015625\n",
      "Epoch [35/200] Loss: 34.949073791503906\n",
      "Epoch [36/200] Loss: 34.079139709472656\n",
      "Epoch [37/200] Loss: 33.25282669067383\n",
      "Epoch [38/200] Loss: 32.467491149902344\n",
      "Epoch [39/200] Loss: 31.720571517944336\n",
      "Epoch [40/200] Loss: 31.009614944458008\n",
      "Epoch [41/200] Loss: 30.332242965698242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/200] Loss: 29.686220169067383\n",
      "Epoch [43/200] Loss: 29.069421768188477\n",
      "Epoch [44/200] Loss: 28.47990608215332\n",
      "Epoch [45/200] Loss: 27.915870666503906\n",
      "Epoch [46/200] Loss: 27.375669479370117\n",
      "Epoch [47/200] Loss: 26.857816696166992\n",
      "Epoch [48/200] Loss: 26.360971450805664\n",
      "Epoch [49/200] Loss: 25.883913040161133\n",
      "Epoch [50/200] Loss: 25.425540924072266\n",
      "Epoch [51/200] Loss: 24.984880447387695\n",
      "Epoch [52/200] Loss: 24.561031341552734\n",
      "Epoch [53/200] Loss: 24.1531925201416\n",
      "Epoch [54/200] Loss: 23.7606201171875\n",
      "Epoch [55/200] Loss: 23.382644653320312\n",
      "Epoch [56/200] Loss: 23.01863670349121\n",
      "Epoch [57/200] Loss: 22.668039321899414\n",
      "Epoch [58/200] Loss: 22.330318450927734\n",
      "Epoch [59/200] Loss: 22.004985809326172\n",
      "Epoch [60/200] Loss: 21.69156837463379\n",
      "Epoch [61/200] Loss: 21.389644622802734\n",
      "Epoch [62/200] Loss: 21.098798751831055\n",
      "Epoch [63/200] Loss: 20.818655014038086\n",
      "Epoch [64/200] Loss: 20.54883575439453\n",
      "Epoch [65/200] Loss: 20.288990020751953\n",
      "Epoch [66/200] Loss: 20.03879165649414\n",
      "Epoch [67/200] Loss: 19.79791259765625\n",
      "Epoch [68/200] Loss: 19.566051483154297\n",
      "Epoch [69/200] Loss: 19.342906951904297\n",
      "Epoch [70/200] Loss: 19.128196716308594\n",
      "Epoch [71/200] Loss: 18.92163848876953\n",
      "Epoch [72/200] Loss: 18.72297477722168\n",
      "Epoch [73/200] Loss: 18.531944274902344\n",
      "Epoch [74/200] Loss: 18.34829330444336\n",
      "Epoch [75/200] Loss: 18.171789169311523\n",
      "Epoch [76/200] Loss: 18.0021915435791\n",
      "Epoch [77/200] Loss: 17.83926773071289\n",
      "Epoch [78/200] Loss: 17.68280792236328\n",
      "Epoch [79/200] Loss: 17.532588958740234\n",
      "Epoch [80/200] Loss: 17.388408660888672\n",
      "Epoch [81/200] Loss: 17.250059127807617\n",
      "Epoch [82/200] Loss: 17.11734390258789\n",
      "Epoch [83/200] Loss: 16.99007225036621\n",
      "Epoch [84/200] Loss: 16.868057250976562\n",
      "Epoch [85/200] Loss: 16.751121520996094\n",
      "Epoch [86/200] Loss: 16.63908576965332\n",
      "Epoch [87/200] Loss: 16.531776428222656\n",
      "Epoch [88/200] Loss: 16.429031372070312\n",
      "Epoch [89/200] Loss: 16.3306827545166\n",
      "Epoch [90/200] Loss: 16.2365779876709\n",
      "Epoch [91/200] Loss: 16.14656639099121\n",
      "Epoch [92/200] Loss: 16.060489654541016\n",
      "Epoch [93/200] Loss: 15.9782133102417\n",
      "Epoch [94/200] Loss: 15.899590492248535\n",
      "Epoch [95/200] Loss: 15.824491500854492\n",
      "Epoch [96/200] Loss: 15.752777099609375\n",
      "Epoch [97/200] Loss: 15.684322357177734\n",
      "Epoch [98/200] Loss: 15.619003295898438\n",
      "Epoch [99/200] Loss: 15.556694030761719\n",
      "Epoch [100/200] Loss: 15.497281074523926\n",
      "Epoch [101/200] Loss: 15.440652847290039\n",
      "Epoch [102/200] Loss: 15.38669204711914\n",
      "Epoch [103/200] Loss: 15.335301399230957\n",
      "Epoch [104/200] Loss: 15.286369323730469\n",
      "Epoch [105/200] Loss: 15.239800453186035\n",
      "Epoch [106/200] Loss: 15.195497512817383\n",
      "Epoch [107/200] Loss: 15.153360366821289\n",
      "Epoch [108/200] Loss: 15.113304138183594\n",
      "Epoch [109/200] Loss: 15.07524299621582\n",
      "Epoch [110/200] Loss: 15.039085388183594\n",
      "Epoch [111/200] Loss: 15.004755020141602\n",
      "Epoch [112/200] Loss: 14.972169876098633\n",
      "Epoch [113/200] Loss: 14.941255569458008\n",
      "Epoch [114/200] Loss: 14.911937713623047\n",
      "Epoch [115/200] Loss: 14.884142875671387\n",
      "Epoch [116/200] Loss: 14.857803344726562\n",
      "Epoch [117/200] Loss: 14.832856178283691\n",
      "Epoch [118/200] Loss: 14.809236526489258\n",
      "Epoch [119/200] Loss: 14.786881446838379\n",
      "Epoch [120/200] Loss: 14.765732765197754\n",
      "Epoch [121/200] Loss: 14.745735168457031\n",
      "Epoch [122/200] Loss: 14.72683048248291\n",
      "Epoch [123/200] Loss: 14.708971977233887\n",
      "Epoch [124/200] Loss: 14.692105293273926\n",
      "Epoch [125/200] Loss: 14.676183700561523\n",
      "Epoch [126/200] Loss: 14.661160469055176\n",
      "Epoch [127/200] Loss: 14.646989822387695\n",
      "Epoch [128/200] Loss: 14.633628845214844\n",
      "Epoch [129/200] Loss: 14.621042251586914\n",
      "Epoch [130/200] Loss: 14.609183311462402\n",
      "Epoch [131/200] Loss: 14.598020553588867\n",
      "Epoch [132/200] Loss: 14.587512969970703\n",
      "Epoch [133/200] Loss: 14.577630996704102\n",
      "Epoch [134/200] Loss: 14.568337440490723\n",
      "Epoch [135/200] Loss: 14.559606552124023\n",
      "Epoch [136/200] Loss: 14.551403045654297\n",
      "Epoch [137/200] Loss: 14.543702125549316\n",
      "Epoch [138/200] Loss: 14.536474227905273\n",
      "Epoch [139/200] Loss: 14.529695510864258\n",
      "Epoch [140/200] Loss: 14.523338317871094\n",
      "Epoch [141/200] Loss: 14.51738166809082\n",
      "Epoch [142/200] Loss: 14.511802673339844\n",
      "Epoch [143/200] Loss: 14.506576538085938\n",
      "Epoch [144/200] Loss: 14.501689910888672\n",
      "Epoch [145/200] Loss: 14.49711799621582\n",
      "Epoch [146/200] Loss: 14.492841720581055\n",
      "Epoch [147/200] Loss: 14.488848686218262\n",
      "Epoch [148/200] Loss: 14.485118865966797\n",
      "Epoch [149/200] Loss: 14.481636047363281\n",
      "Epoch [150/200] Loss: 14.478387832641602\n",
      "Epoch [151/200] Loss: 14.475358963012695\n",
      "Epoch [152/200] Loss: 14.472536087036133\n",
      "Epoch [153/200] Loss: 14.469903945922852\n",
      "Epoch [154/200] Loss: 14.46745491027832\n",
      "Epoch [155/200] Loss: 14.465177536010742\n",
      "Epoch [156/200] Loss: 14.463056564331055\n",
      "Epoch [157/200] Loss: 14.46108627319336\n",
      "Epoch [158/200] Loss: 14.45925521850586\n",
      "Epoch [159/200] Loss: 14.457555770874023\n",
      "Epoch [160/200] Loss: 14.45598030090332\n",
      "Epoch [161/200] Loss: 14.454513549804688\n",
      "Epoch [162/200] Loss: 14.453158378601074\n",
      "Epoch [163/200] Loss: 14.45190143585205\n",
      "Epoch [164/200] Loss: 14.450737953186035\n",
      "Epoch [165/200] Loss: 14.449661254882812\n",
      "Epoch [166/200] Loss: 14.448664665222168\n",
      "Epoch [167/200] Loss: 14.447742462158203\n",
      "Epoch [168/200] Loss: 14.446891784667969\n",
      "Epoch [169/200] Loss: 14.44610595703125\n",
      "Epoch [170/200] Loss: 14.445382118225098\n",
      "Epoch [171/200] Loss: 14.444713592529297\n",
      "Epoch [172/200] Loss: 14.444097518920898\n",
      "Epoch [173/200] Loss: 14.44352912902832\n",
      "Epoch [174/200] Loss: 14.443007469177246\n",
      "Epoch [175/200] Loss: 14.442527770996094\n",
      "Epoch [176/200] Loss: 14.442085266113281\n",
      "Epoch [177/200] Loss: 14.441679000854492\n",
      "Epoch [178/200] Loss: 14.441307067871094\n",
      "Epoch [179/200] Loss: 14.44096565246582\n",
      "Epoch [180/200] Loss: 14.440651893615723\n",
      "Epoch [181/200] Loss: 14.440362930297852\n",
      "Epoch [182/200] Loss: 14.44010066986084\n",
      "Epoch [183/200] Loss: 14.439860343933105\n",
      "Epoch [184/200] Loss: 14.439640045166016\n",
      "Epoch [185/200] Loss: 14.43943977355957\n",
      "Epoch [186/200] Loss: 14.439254760742188\n",
      "Epoch [187/200] Loss: 14.439087867736816\n",
      "Epoch [188/200] Loss: 14.438934326171875\n",
      "Epoch [189/200] Loss: 14.43879508972168\n",
      "Epoch [190/200] Loss: 14.438669204711914\n",
      "Epoch [191/200] Loss: 14.438554763793945\n",
      "Epoch [192/200] Loss: 14.43844985961914\n",
      "Epoch [193/200] Loss: 14.4383544921875\n",
      "Epoch [194/200] Loss: 14.438268661499023\n",
      "Epoch [195/200] Loss: 14.438189506530762\n",
      "Epoch [196/200] Loss: 14.438118934631348\n",
      "Epoch [197/200] Loss: 14.438053131103516\n",
      "Epoch [198/200] Loss: 14.437995910644531\n",
      "Epoch [199/200] Loss: 14.437944412231445\n",
      "Epoch [200/200] Loss: 14.437898635864258\n",
      "Predicted days_remaining for parent_id 74: 9.731175422668457\n",
      "Training for parent_id 76...\n",
      "Epoch [1/200] Loss: 154.63211059570312\n",
      "Epoch [2/200] Loss: 149.4988555908203\n",
      "Epoch [3/200] Loss: 144.5226287841797\n",
      "Epoch [4/200] Loss: 139.72915649414062\n",
      "Epoch [5/200] Loss: 135.13394165039062\n",
      "Epoch [6/200] Loss: 130.7381591796875\n",
      "Epoch [7/200] Loss: 126.537841796875\n",
      "Epoch [8/200] Loss: 122.52657318115234\n",
      "Epoch [9/200] Loss: 118.69500732421875\n",
      "Epoch [10/200] Loss: 115.03179931640625\n",
      "Epoch [11/200] Loss: 111.5259017944336\n",
      "Epoch [12/200] Loss: 108.16791534423828\n",
      "Epoch [13/200] Loss: 104.95064544677734\n",
      "Epoch [14/200] Loss: 101.86925506591797\n",
      "Epoch [15/200] Loss: 98.92066192626953\n",
      "Epoch [16/200] Loss: 96.1025390625\n",
      "Epoch [17/200] Loss: 93.41272735595703\n",
      "Epoch [18/200] Loss: 90.8486328125\n",
      "Epoch [19/200] Loss: 88.40692138671875\n",
      "Epoch [20/200] Loss: 86.08333587646484\n",
      "Epoch [21/200] Loss: 83.87286376953125\n",
      "Epoch [22/200] Loss: 81.76988220214844\n",
      "Epoch [23/200] Loss: 79.76839447021484\n",
      "Epoch [24/200] Loss: 77.86226654052734\n",
      "Epoch [25/200] Loss: 76.0453872680664\n",
      "Epoch [26/200] Loss: 74.31178283691406\n",
      "Epoch [27/200] Loss: 72.6556625366211\n",
      "Epoch [28/200] Loss: 71.07147979736328\n",
      "Epoch [29/200] Loss: 69.55390167236328\n",
      "Epoch [30/200] Loss: 68.09793853759766\n",
      "Epoch [31/200] Loss: 66.69893646240234\n",
      "Epoch [32/200] Loss: 65.35253143310547\n",
      "Epoch [33/200] Loss: 64.05477905273438\n",
      "Epoch [34/200] Loss: 62.80204391479492\n",
      "Epoch [35/200] Loss: 61.59104537963867\n",
      "Epoch [36/200] Loss: 60.41884231567383\n",
      "Epoch [37/200] Loss: 59.28271484375\n",
      "Epoch [38/200] Loss: 58.18030548095703\n",
      "Epoch [39/200] Loss: 57.109413146972656\n",
      "Epoch [40/200] Loss: 56.06812286376953\n",
      "Epoch [41/200] Loss: 55.05463790893555\n",
      "Epoch [42/200] Loss: 54.06740951538086\n",
      "Epoch [43/200] Loss: 53.10499954223633\n",
      "Epoch [44/200] Loss: 52.16609191894531\n",
      "Epoch [45/200] Loss: 51.249549865722656\n",
      "Epoch [46/200] Loss: 50.354270935058594\n",
      "Epoch [47/200] Loss: 49.47928237915039\n",
      "Epoch [48/200] Loss: 48.62370681762695\n",
      "Epoch [49/200] Loss: 47.78670883178711\n",
      "Epoch [50/200] Loss: 46.96757125854492\n",
      "Epoch [51/200] Loss: 46.16561508178711\n",
      "Epoch [52/200] Loss: 45.38025665283203\n",
      "Epoch [53/200] Loss: 44.611019134521484\n",
      "Epoch [54/200] Loss: 43.857425689697266\n",
      "Epoch [55/200] Loss: 43.11913299560547\n",
      "Epoch [56/200] Loss: 42.39585494995117\n",
      "Epoch [57/200] Loss: 41.68735122680664\n",
      "Epoch [58/200] Loss: 40.99339294433594\n",
      "Epoch [59/200] Loss: 40.313819885253906\n",
      "Epoch [60/200] Loss: 39.64847946166992\n",
      "Epoch [61/200] Loss: 38.99720001220703\n",
      "Epoch [62/200] Loss: 38.359832763671875\n",
      "Epoch [63/200] Loss: 37.736175537109375\n",
      "Epoch [64/200] Loss: 37.12605667114258\n",
      "Epoch [65/200] Loss: 36.52927017211914\n",
      "Epoch [66/200] Loss: 35.94562530517578\n",
      "Epoch [67/200] Loss: 35.37488555908203\n",
      "Epoch [68/200] Loss: 34.816829681396484\n",
      "Epoch [69/200] Loss: 34.27123260498047\n",
      "Epoch [70/200] Loss: 33.73787307739258\n",
      "Epoch [71/200] Loss: 33.21653366088867\n",
      "Epoch [72/200] Loss: 32.706974029541016\n",
      "Epoch [73/200] Loss: 32.20899963378906\n",
      "Epoch [74/200] Loss: 31.722402572631836\n",
      "Epoch [75/200] Loss: 31.24696159362793\n",
      "Epoch [76/200] Loss: 30.782495498657227\n",
      "Epoch [77/200] Loss: 30.32879066467285\n",
      "Epoch [78/200] Loss: 29.885671615600586\n",
      "Epoch [79/200] Loss: 29.452943801879883\n",
      "Epoch [80/200] Loss: 29.030437469482422\n",
      "Epoch [81/200] Loss: 28.617963790893555\n",
      "Epoch [82/200] Loss: 28.215347290039062\n",
      "Epoch [83/200] Loss: 27.82241439819336\n",
      "Epoch [84/200] Loss: 27.43899917602539\n",
      "Epoch [85/200] Loss: 27.064910888671875\n",
      "Epoch [86/200] Loss: 26.70000648498535\n",
      "Epoch [87/200] Loss: 26.344099044799805\n",
      "Epoch [88/200] Loss: 25.99701690673828\n",
      "Epoch [89/200] Loss: 25.658613204956055\n",
      "Epoch [90/200] Loss: 25.328702926635742\n",
      "Epoch [91/200] Loss: 25.007129669189453\n",
      "Epoch [92/200] Loss: 24.693716049194336\n",
      "Epoch [93/200] Loss: 24.388317108154297\n",
      "Epoch [94/200] Loss: 24.090768814086914\n",
      "Epoch [95/200] Loss: 23.800901412963867\n",
      "Epoch [96/200] Loss: 23.518556594848633\n",
      "Epoch [97/200] Loss: 23.243587493896484\n",
      "Epoch [98/200] Loss: 22.975826263427734\n",
      "Epoch [99/200] Loss: 22.715133666992188\n",
      "Epoch [100/200] Loss: 22.461347579956055\n",
      "Epoch [101/200] Loss: 22.214326858520508\n",
      "Epoch [102/200] Loss: 21.973915100097656\n",
      "Epoch [103/200] Loss: 21.73998260498047\n",
      "Epoch [104/200] Loss: 21.512353897094727\n",
      "Epoch [105/200] Loss: 21.290918350219727\n",
      "Epoch [106/200] Loss: 21.075523376464844\n",
      "Epoch [107/200] Loss: 20.866039276123047\n",
      "Epoch [108/200] Loss: 20.662330627441406\n",
      "Epoch [109/200] Loss: 20.464248657226562\n",
      "Epoch [110/200] Loss: 20.271682739257812\n",
      "Epoch [111/200] Loss: 20.08449935913086\n",
      "Epoch [112/200] Loss: 19.902576446533203\n",
      "Epoch [113/200] Loss: 19.725788116455078\n",
      "Epoch [114/200] Loss: 19.55400276184082\n",
      "Epoch [115/200] Loss: 19.387117385864258\n",
      "Epoch [116/200] Loss: 19.22500228881836\n",
      "Epoch [117/200] Loss: 19.067550659179688\n",
      "Epoch [118/200] Loss: 18.914653778076172\n",
      "Epoch [119/200] Loss: 18.76618003845215\n",
      "Epoch [120/200] Loss: 18.62204933166504\n",
      "Epoch [121/200] Loss: 18.482133865356445\n",
      "Epoch [122/200] Loss: 18.34634017944336\n",
      "Epoch [123/200] Loss: 18.214561462402344\n",
      "Epoch [124/200] Loss: 18.086700439453125\n",
      "Epoch [125/200] Loss: 17.962650299072266\n",
      "Epoch [126/200] Loss: 17.842330932617188\n",
      "Epoch [127/200] Loss: 17.725631713867188\n",
      "Epoch [128/200] Loss: 17.612468719482422\n",
      "Epoch [129/200] Loss: 17.50274658203125\n",
      "Epoch [130/200] Loss: 17.396379470825195\n",
      "Epoch [131/200] Loss: 17.29328155517578\n",
      "Epoch [132/200] Loss: 17.19336700439453\n",
      "Epoch [133/200] Loss: 17.096546173095703\n",
      "Epoch [134/200] Loss: 17.002742767333984\n",
      "Epoch [135/200] Loss: 16.911880493164062\n",
      "Epoch [136/200] Loss: 16.823871612548828\n",
      "Epoch [137/200] Loss: 16.7386474609375\n",
      "Epoch [138/200] Loss: 16.65612030029297\n",
      "Epoch [139/200] Loss: 16.576234817504883\n",
      "Epoch [140/200] Loss: 16.498910903930664\n",
      "Epoch [141/200] Loss: 16.424072265625\n",
      "Epoch [142/200] Loss: 16.35166358947754\n",
      "Epoch [143/200] Loss: 16.281600952148438\n",
      "Epoch [144/200] Loss: 16.213829040527344\n",
      "Epoch [145/200] Loss: 16.148279190063477\n",
      "Epoch [146/200] Loss: 16.084896087646484\n",
      "Epoch [147/200] Loss: 16.02361297607422\n",
      "Epoch [148/200] Loss: 15.964366912841797\n",
      "Epoch [149/200] Loss: 15.907102584838867\n",
      "Epoch [150/200] Loss: 15.851766586303711\n",
      "Epoch [151/200] Loss: 15.798296928405762\n",
      "Epoch [152/200] Loss: 15.746639251708984\n",
      "Epoch [153/200] Loss: 15.696742057800293\n",
      "Epoch [154/200] Loss: 15.648557662963867\n",
      "Epoch [155/200] Loss: 15.602028846740723\n",
      "Epoch [156/200] Loss: 15.55710506439209\n",
      "Epoch [157/200] Loss: 15.513744354248047\n",
      "Epoch [158/200] Loss: 15.47189712524414\n",
      "Epoch [159/200] Loss: 15.431510925292969\n",
      "Epoch [160/200] Loss: 15.392553329467773\n",
      "Epoch [161/200] Loss: 15.354972839355469\n",
      "Epoch [162/200] Loss: 15.3187255859375\n",
      "Epoch [163/200] Loss: 15.283774375915527\n",
      "Epoch [164/200] Loss: 15.25007438659668\n",
      "Epoch [165/200] Loss: 15.217592239379883\n",
      "Epoch [166/200] Loss: 15.186284065246582\n",
      "Epoch [167/200] Loss: 15.156112670898438\n",
      "Epoch [168/200] Loss: 15.127047538757324\n",
      "Epoch [169/200] Loss: 15.099044799804688\n",
      "Epoch [170/200] Loss: 15.072073936462402\n",
      "Epoch [171/200] Loss: 15.046104431152344\n",
      "Epoch [172/200] Loss: 15.021100044250488\n",
      "Epoch [173/200] Loss: 14.997028350830078\n",
      "Epoch [174/200] Loss: 14.973861694335938\n",
      "Epoch [175/200] Loss: 14.951566696166992\n",
      "Epoch [176/200] Loss: 14.9301176071167\n",
      "Epoch [177/200] Loss: 14.909483909606934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [178/200] Loss: 14.889638900756836\n",
      "Epoch [179/200] Loss: 14.870553970336914\n",
      "Epoch [180/200] Loss: 14.852206230163574\n",
      "Epoch [181/200] Loss: 14.83456802368164\n",
      "Epoch [182/200] Loss: 14.81761646270752\n",
      "Epoch [183/200] Loss: 14.801326751708984\n",
      "Epoch [184/200] Loss: 14.785675048828125\n",
      "Epoch [185/200] Loss: 14.770641326904297\n",
      "Epoch [186/200] Loss: 14.756200790405273\n",
      "Epoch [187/200] Loss: 14.742334365844727\n",
      "Epoch [188/200] Loss: 14.729021072387695\n",
      "Epoch [189/200] Loss: 14.716243743896484\n",
      "Epoch [190/200] Loss: 14.703978538513184\n",
      "Epoch [191/200] Loss: 14.692212104797363\n",
      "Epoch [192/200] Loss: 14.680920600891113\n",
      "Epoch [193/200] Loss: 14.670090675354004\n",
      "Epoch [194/200] Loss: 14.659704208374023\n",
      "Epoch [195/200] Loss: 14.649742126464844\n",
      "Epoch [196/200] Loss: 14.640193939208984\n",
      "Epoch [197/200] Loss: 14.63104248046875\n",
      "Epoch [198/200] Loss: 14.622270584106445\n",
      "Epoch [199/200] Loss: 14.61386489868164\n",
      "Epoch [200/200] Loss: 14.605813980102539\n",
      "Predicted days_remaining for parent_id 76: 11.349247932434082\n",
      "Training for parent_id 80...\n",
      "Epoch [1/200] Loss: 159.7020263671875\n",
      "Epoch [2/200] Loss: 154.42803955078125\n",
      "Epoch [3/200] Loss: 149.1546173095703\n",
      "Epoch [4/200] Loss: 143.889404296875\n",
      "Epoch [5/200] Loss: 138.64688110351562\n",
      "Epoch [6/200] Loss: 133.45535278320312\n",
      "Epoch [7/200] Loss: 128.3557586669922\n",
      "Epoch [8/200] Loss: 123.39592742919922\n",
      "Epoch [9/200] Loss: 118.6248779296875\n",
      "Epoch [10/200] Loss: 114.08494567871094\n",
      "Epoch [11/200] Loss: 109.80413818359375\n",
      "Epoch [12/200] Loss: 105.79377746582031\n",
      "Epoch [13/200] Loss: 102.05204772949219\n",
      "Epoch [14/200] Loss: 98.56906127929688\n",
      "Epoch [15/200] Loss: 95.33082580566406\n",
      "Epoch [16/200] Loss: 92.32152557373047\n",
      "Epoch [17/200] Loss: 89.52458953857422\n",
      "Epoch [18/200] Loss: 86.9231948852539\n",
      "Epoch [19/200] Loss: 84.50054168701172\n",
      "Epoch [20/200] Loss: 82.23988342285156\n",
      "Epoch [21/200] Loss: 80.12503051757812\n",
      "Epoch [22/200] Loss: 78.14063262939453\n",
      "Epoch [23/200] Loss: 76.27252960205078\n",
      "Epoch [24/200] Loss: 74.5079574584961\n",
      "Epoch [25/200] Loss: 72.83562469482422\n",
      "Epoch [26/200] Loss: 71.24581909179688\n",
      "Epoch [27/200] Loss: 69.73013305664062\n",
      "Epoch [28/200] Loss: 68.28144073486328\n",
      "Epoch [29/200] Loss: 66.89356231689453\n",
      "Epoch [30/200] Loss: 65.56124114990234\n",
      "Epoch [31/200] Loss: 64.2799072265625\n",
      "Epoch [32/200] Loss: 63.04556655883789\n",
      "Epoch [33/200] Loss: 61.854679107666016\n",
      "Epoch [34/200] Loss: 60.70403289794922\n",
      "Epoch [35/200] Loss: 59.59076690673828\n",
      "Epoch [36/200] Loss: 58.512245178222656\n",
      "Epoch [37/200] Loss: 57.466087341308594\n",
      "Epoch [38/200] Loss: 56.4500846862793\n",
      "Epoch [39/200] Loss: 55.4622802734375\n",
      "Epoch [40/200] Loss: 54.5008659362793\n",
      "Epoch [41/200] Loss: 53.56420135498047\n",
      "Epoch [42/200] Loss: 52.6508674621582\n",
      "Epoch [43/200] Loss: 51.75958251953125\n",
      "Epoch [44/200] Loss: 50.88918685913086\n",
      "Epoch [45/200] Loss: 50.03868865966797\n",
      "Epoch [46/200] Loss: 49.20722961425781\n",
      "Epoch [47/200] Loss: 48.3940315246582\n",
      "Epoch [48/200] Loss: 47.5984001159668\n",
      "Epoch [49/200] Loss: 46.819759368896484\n",
      "Epoch [50/200] Loss: 46.057586669921875\n",
      "Epoch [51/200] Loss: 45.3114128112793\n",
      "Epoch [52/200] Loss: 44.58081817626953\n",
      "Epoch [53/200] Loss: 43.86543273925781\n",
      "Epoch [54/200] Loss: 43.164886474609375\n",
      "Epoch [55/200] Loss: 42.478885650634766\n",
      "Epoch [56/200] Loss: 41.80710220336914\n",
      "Epoch [57/200] Loss: 41.14926528930664\n",
      "Epoch [58/200] Loss: 40.50510025024414\n",
      "Epoch [59/200] Loss: 39.87432861328125\n",
      "Epoch [60/200] Loss: 39.2567138671875\n",
      "Epoch [61/200] Loss: 38.65199661254883\n",
      "Epoch [62/200] Loss: 38.059940338134766\n",
      "Epoch [63/200] Loss: 37.480316162109375\n",
      "Epoch [64/200] Loss: 36.91288757324219\n",
      "Epoch [65/200] Loss: 36.357421875\n",
      "Epoch [66/200] Loss: 35.8137092590332\n",
      "Epoch [67/200] Loss: 35.28153610229492\n",
      "Epoch [68/200] Loss: 34.76068115234375\n",
      "Epoch [69/200] Loss: 34.25094985961914\n",
      "Epoch [70/200] Loss: 33.75212097167969\n",
      "Epoch [71/200] Loss: 33.26403045654297\n",
      "Epoch [72/200] Loss: 32.786434173583984\n",
      "Epoch [73/200] Loss: 32.319175720214844\n",
      "Epoch [74/200] Loss: 31.862058639526367\n",
      "Epoch [75/200] Loss: 31.414894104003906\n",
      "Epoch [76/200] Loss: 30.97751235961914\n",
      "Epoch [77/200] Loss: 30.549713134765625\n",
      "Epoch [78/200] Loss: 30.131338119506836\n",
      "Epoch [79/200] Loss: 29.722217559814453\n",
      "Epoch [80/200] Loss: 29.322175979614258\n",
      "Epoch [81/200] Loss: 28.931049346923828\n",
      "Epoch [82/200] Loss: 28.548667907714844\n",
      "Epoch [83/200] Loss: 28.174877166748047\n",
      "Epoch [84/200] Loss: 27.809528350830078\n",
      "Epoch [85/200] Loss: 27.452436447143555\n",
      "Epoch [86/200] Loss: 27.103469848632812\n",
      "Epoch [87/200] Loss: 26.762489318847656\n",
      "Epoch [88/200] Loss: 26.429319381713867\n",
      "Epoch [89/200] Loss: 26.10382843017578\n",
      "Epoch [90/200] Loss: 25.785871505737305\n",
      "Epoch [91/200] Loss: 25.47530174255371\n",
      "Epoch [92/200] Loss: 25.171995162963867\n",
      "Epoch [93/200] Loss: 24.875791549682617\n",
      "Epoch [94/200] Loss: 24.586563110351562\n",
      "Epoch [95/200] Loss: 24.3041934967041\n",
      "Epoch [96/200] Loss: 24.028528213500977\n",
      "Epoch [97/200] Loss: 23.75945281982422\n",
      "Epoch [98/200] Loss: 23.496835708618164\n",
      "Epoch [99/200] Loss: 23.24054718017578\n",
      "Epoch [100/200] Loss: 22.99047088623047\n",
      "Epoch [101/200] Loss: 22.746477127075195\n",
      "Epoch [102/200] Loss: 22.508464813232422\n",
      "Epoch [103/200] Loss: 22.27628517150879\n",
      "Epoch [104/200] Loss: 22.049846649169922\n",
      "Epoch [105/200] Loss: 21.829021453857422\n",
      "Epoch [106/200] Loss: 21.613697052001953\n",
      "Epoch [107/200] Loss: 21.403776168823242\n",
      "Epoch [108/200] Loss: 21.199138641357422\n",
      "Epoch [109/200] Loss: 20.999671936035156\n",
      "Epoch [110/200] Loss: 20.80527687072754\n",
      "Epoch [111/200] Loss: 20.615846633911133\n",
      "Epoch [112/200] Loss: 20.43128204345703\n",
      "Epoch [113/200] Loss: 20.251476287841797\n",
      "Epoch [114/200] Loss: 20.076322555541992\n",
      "Epoch [115/200] Loss: 19.905746459960938\n",
      "Epoch [116/200] Loss: 19.739627838134766\n",
      "Epoch [117/200] Loss: 19.577880859375\n",
      "Epoch [118/200] Loss: 19.420408248901367\n",
      "Epoch [119/200] Loss: 19.26711082458496\n",
      "Epoch [120/200] Loss: 19.117921829223633\n",
      "Epoch [121/200] Loss: 18.97272491455078\n",
      "Epoch [122/200] Loss: 18.831449508666992\n",
      "Epoch [123/200] Loss: 18.694000244140625\n",
      "Epoch [124/200] Loss: 18.560291290283203\n",
      "Epoch [125/200] Loss: 18.43024253845215\n",
      "Epoch [126/200] Loss: 18.30377960205078\n",
      "Epoch [127/200] Loss: 18.18079948425293\n",
      "Epoch [128/200] Loss: 18.061243057250977\n",
      "Epoch [129/200] Loss: 17.945022583007812\n",
      "Epoch [130/200] Loss: 17.83206558227539\n",
      "Epoch [131/200] Loss: 17.7222900390625\n",
      "Epoch [132/200] Loss: 17.615623474121094\n",
      "Epoch [133/200] Loss: 17.51199722290039\n",
      "Epoch [134/200] Loss: 17.411333084106445\n",
      "Epoch [135/200] Loss: 17.31357192993164\n",
      "Epoch [136/200] Loss: 17.21862030029297\n",
      "Epoch [137/200] Loss: 17.126441955566406\n",
      "Epoch [138/200] Loss: 17.03693962097168\n",
      "Epoch [139/200] Loss: 16.9500732421875\n",
      "Epoch [140/200] Loss: 16.86576271057129\n",
      "Epoch [141/200] Loss: 16.783946990966797\n",
      "Epoch [142/200] Loss: 16.704570770263672\n",
      "Epoch [143/200] Loss: 16.627561569213867\n",
      "Epoch [144/200] Loss: 16.552865982055664\n",
      "Epoch [145/200] Loss: 16.480430603027344\n",
      "Epoch [146/200] Loss: 16.410194396972656\n",
      "Epoch [147/200] Loss: 16.342092514038086\n",
      "Epoch [148/200] Loss: 16.27608299255371\n",
      "Epoch [149/200] Loss: 16.212099075317383\n",
      "Epoch [150/200] Loss: 16.150096893310547\n",
      "Epoch [151/200] Loss: 16.09002685546875\n",
      "Epoch [152/200] Loss: 16.031827926635742\n",
      "Epoch [153/200] Loss: 15.975452423095703\n",
      "Epoch [154/200] Loss: 15.920860290527344\n",
      "Epoch [155/200] Loss: 15.867993354797363\n",
      "Epoch [156/200] Loss: 15.81680679321289\n",
      "Epoch [157/200] Loss: 15.767261505126953\n",
      "Epoch [158/200] Loss: 15.719303131103516\n",
      "Epoch [159/200] Loss: 15.672897338867188\n",
      "Epoch [160/200] Loss: 15.627991676330566\n",
      "Epoch [161/200] Loss: 15.584552764892578\n",
      "Epoch [162/200] Loss: 15.542530059814453\n",
      "Epoch [163/200] Loss: 15.5018949508667\n",
      "Epoch [164/200] Loss: 15.462601661682129\n",
      "Epoch [165/200] Loss: 15.424609184265137\n",
      "Epoch [166/200] Loss: 15.387883186340332\n",
      "Epoch [167/200] Loss: 15.35239028930664\n",
      "Epoch [168/200] Loss: 15.318086624145508\n",
      "Epoch [169/200] Loss: 15.284948348999023\n",
      "Epoch [170/200] Loss: 15.252931594848633\n",
      "Epoch [171/200] Loss: 15.222005844116211\n",
      "Epoch [172/200] Loss: 15.19214153289795\n",
      "Epoch [173/200] Loss: 15.16330337524414\n",
      "Epoch [174/200] Loss: 15.13546371459961\n",
      "Epoch [175/200] Loss: 15.108591079711914\n",
      "Epoch [176/200] Loss: 15.082651138305664\n",
      "Epoch [177/200] Loss: 15.057622909545898\n",
      "Epoch [178/200] Loss: 15.033477783203125\n",
      "Epoch [179/200] Loss: 15.01018238067627\n",
      "Epoch [180/200] Loss: 14.987714767456055\n",
      "Epoch [181/200] Loss: 14.966049194335938\n",
      "Epoch [182/200] Loss: 14.945157051086426\n",
      "Epoch [183/200] Loss: 14.925018310546875\n",
      "Epoch [184/200] Loss: 14.905607223510742\n",
      "Epoch [185/200] Loss: 14.886899948120117\n",
      "Epoch [186/200] Loss: 14.868873596191406\n",
      "Epoch [187/200] Loss: 14.851507186889648\n",
      "Epoch [188/200] Loss: 14.83477783203125\n",
      "Epoch [189/200] Loss: 14.81866455078125\n",
      "Epoch [190/200] Loss: 14.803152084350586\n",
      "Epoch [191/200] Loss: 14.788214683532715\n",
      "Epoch [192/200] Loss: 14.773836135864258\n",
      "Epoch [193/200] Loss: 14.759997367858887\n",
      "Epoch [194/200] Loss: 14.746679306030273\n",
      "Epoch [195/200] Loss: 14.733867645263672\n",
      "Epoch [196/200] Loss: 14.721538543701172\n",
      "Epoch [197/200] Loss: 14.709686279296875\n",
      "Epoch [198/200] Loss: 14.698280334472656\n",
      "Epoch [199/200] Loss: 14.68731689453125\n",
      "Epoch [200/200] Loss: 14.676778793334961\n",
      "Predicted days_remaining for parent_id 80: 11.271306037902832\n",
      "Training for parent_id 82...\n",
      "Epoch [1/200] Loss: 312.0862121582031\n",
      "Epoch [2/200] Loss: 303.4732971191406\n",
      "Epoch [3/200] Loss: 295.236083984375\n",
      "Epoch [4/200] Loss: 287.351318359375\n",
      "Epoch [5/200] Loss: 279.7962341308594\n",
      "Epoch [6/200] Loss: 272.55877685546875\n",
      "Epoch [7/200] Loss: 265.6289367675781\n",
      "Epoch [8/200] Loss: 258.9942626953125\n",
      "Epoch [9/200] Loss: 252.6405029296875\n",
      "Epoch [10/200] Loss: 246.552734375\n",
      "Epoch [11/200] Loss: 240.71583557128906\n",
      "Epoch [12/200] Loss: 235.1148681640625\n",
      "Epoch [13/200] Loss: 229.73667907714844\n",
      "Epoch [14/200] Loss: 224.57089233398438\n",
      "Epoch [15/200] Loss: 219.6101531982422\n",
      "Epoch [16/200] Loss: 214.84913635253906\n",
      "Epoch [17/200] Loss: 210.28350830078125\n",
      "Epoch [18/200] Loss: 205.9085235595703\n",
      "Epoch [19/200] Loss: 201.71841430664062\n",
      "Epoch [20/200] Loss: 197.7065887451172\n",
      "Epoch [21/200] Loss: 193.8660888671875\n",
      "Epoch [22/200] Loss: 190.19085693359375\n",
      "Epoch [23/200] Loss: 186.67578125\n",
      "Epoch [24/200] Loss: 183.31700134277344\n",
      "Epoch [25/200] Loss: 180.1118927001953\n",
      "Epoch [26/200] Loss: 177.05841064453125\n",
      "Epoch [27/200] Loss: 174.15420532226562\n",
      "Epoch [28/200] Loss: 171.3952178955078\n",
      "Epoch [29/200] Loss: 168.77488708496094\n",
      "Epoch [30/200] Loss: 166.28433227539062\n",
      "Epoch [31/200] Loss: 163.91233825683594\n",
      "Epoch [32/200] Loss: 161.64662170410156\n",
      "Epoch [33/200] Loss: 159.47476196289062\n",
      "Epoch [34/200] Loss: 157.3853302001953\n",
      "Epoch [35/200] Loss: 155.3679962158203\n",
      "Epoch [36/200] Loss: 153.41378784179688\n",
      "Epoch [37/200] Loss: 151.5147247314453\n",
      "Epoch [38/200] Loss: 149.66407775878906\n",
      "Epoch [39/200] Loss: 147.85592651367188\n",
      "Epoch [40/200] Loss: 146.08541870117188\n",
      "Epoch [41/200] Loss: 144.3485565185547\n",
      "Epoch [42/200] Loss: 142.64210510253906\n",
      "Epoch [43/200] Loss: 140.96353149414062\n",
      "Epoch [44/200] Loss: 139.3108367919922\n",
      "Epoch [45/200] Loss: 137.6824493408203\n",
      "Epoch [46/200] Loss: 136.07717895507812\n",
      "Epoch [47/200] Loss: 134.49412536621094\n",
      "Epoch [48/200] Loss: 132.9324951171875\n",
      "Epoch [49/200] Loss: 131.3916778564453\n",
      "Epoch [50/200] Loss: 129.87103271484375\n",
      "Epoch [51/200] Loss: 128.37005615234375\n",
      "Epoch [52/200] Loss: 126.88817596435547\n",
      "Epoch [53/200] Loss: 125.42496490478516\n",
      "Epoch [54/200] Loss: 123.97982025146484\n",
      "Epoch [55/200] Loss: 122.55229949951172\n",
      "Epoch [56/200] Loss: 121.14193725585938\n",
      "Epoch [57/200] Loss: 119.74839782714844\n",
      "Epoch [58/200] Loss: 118.37128448486328\n",
      "Epoch [59/200] Loss: 117.0102767944336\n",
      "Epoch [60/200] Loss: 115.66515350341797\n",
      "Epoch [61/200] Loss: 114.33572387695312\n",
      "Epoch [62/200] Loss: 113.02173614501953\n",
      "Epoch [63/200] Loss: 111.72305297851562\n",
      "Epoch [64/200] Loss: 110.4394760131836\n",
      "Epoch [65/200] Loss: 109.17086029052734\n",
      "Epoch [66/200] Loss: 107.91700744628906\n",
      "Epoch [67/200] Loss: 106.67778015136719\n",
      "Epoch [68/200] Loss: 105.4529800415039\n",
      "Epoch [69/200] Loss: 104.24248504638672\n",
      "Epoch [70/200] Loss: 103.04609680175781\n",
      "Epoch [71/200] Loss: 101.8636474609375\n",
      "Epoch [72/200] Loss: 100.6949462890625\n",
      "Epoch [73/200] Loss: 99.53990936279297\n",
      "Epoch [74/200] Loss: 98.39835357666016\n",
      "Epoch [75/200] Loss: 97.27009582519531\n",
      "Epoch [76/200] Loss: 96.1550064086914\n",
      "Epoch [77/200] Loss: 95.05296325683594\n",
      "Epoch [78/200] Loss: 93.96378326416016\n",
      "Epoch [79/200] Loss: 92.88739776611328\n",
      "Epoch [80/200] Loss: 91.82357788085938\n",
      "Epoch [81/200] Loss: 90.77224731445312\n",
      "Epoch [82/200] Loss: 89.73324584960938\n",
      "Epoch [83/200] Loss: 88.70645141601562\n",
      "Epoch [84/200] Loss: 87.69173431396484\n",
      "Epoch [85/200] Loss: 86.68895721435547\n",
      "Epoch [86/200] Loss: 85.69799041748047\n",
      "Epoch [87/200] Loss: 84.71871185302734\n",
      "Epoch [88/200] Loss: 83.75102233886719\n",
      "Epoch [89/200] Loss: 82.79471588134766\n",
      "Epoch [90/200] Loss: 81.84973907470703\n",
      "Epoch [91/200] Loss: 80.91596984863281\n",
      "Epoch [92/200] Loss: 79.99325561523438\n",
      "Epoch [93/200] Loss: 79.08151245117188\n",
      "Epoch [94/200] Loss: 78.18058013916016\n",
      "Epoch [95/200] Loss: 77.29039764404297\n",
      "Epoch [96/200] Loss: 76.41078186035156\n",
      "Epoch [97/200] Loss: 75.54168701171875\n",
      "Epoch [98/200] Loss: 74.68295288085938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/200] Loss: 73.83451080322266\n",
      "Epoch [100/200] Loss: 72.9961929321289\n",
      "Epoch [101/200] Loss: 72.16797637939453\n",
      "Epoch [102/200] Loss: 71.34970092773438\n",
      "Epoch [103/200] Loss: 70.54125213623047\n",
      "Epoch [104/200] Loss: 69.7425765991211\n",
      "Epoch [105/200] Loss: 68.9535140991211\n",
      "Epoch [106/200] Loss: 68.17398834228516\n",
      "Epoch [107/200] Loss: 67.4039306640625\n",
      "Epoch [108/200] Loss: 66.6431884765625\n",
      "Epoch [109/200] Loss: 65.89170837402344\n",
      "Epoch [110/200] Loss: 65.14936828613281\n",
      "Epoch [111/200] Loss: 64.41609191894531\n",
      "Epoch [112/200] Loss: 63.6917610168457\n",
      "Epoch [113/200] Loss: 62.976314544677734\n",
      "Epoch [114/200] Loss: 62.26964569091797\n",
      "Epoch [115/200] Loss: 61.571651458740234\n",
      "Epoch [116/200] Loss: 60.882266998291016\n",
      "Epoch [117/200] Loss: 60.20138931274414\n",
      "Epoch [118/200] Loss: 59.52894592285156\n",
      "Epoch [119/200] Loss: 58.86482238769531\n",
      "Epoch [120/200] Loss: 58.20895767211914\n",
      "Epoch [121/200] Loss: 57.56126403808594\n",
      "Epoch [122/200] Loss: 56.92164611816406\n",
      "Epoch [123/200] Loss: 56.290008544921875\n",
      "Epoch [124/200] Loss: 55.66633224487305\n",
      "Epoch [125/200] Loss: 55.05046844482422\n",
      "Epoch [126/200] Loss: 54.4423713684082\n",
      "Epoch [127/200] Loss: 53.841941833496094\n",
      "Epoch [128/200] Loss: 53.24912643432617\n",
      "Epoch [129/200] Loss: 52.66382598876953\n",
      "Epoch [130/200] Loss: 52.08595275878906\n",
      "Epoch [131/200] Loss: 51.51546096801758\n",
      "Epoch [132/200] Loss: 50.952247619628906\n",
      "Epoch [133/200] Loss: 50.3962516784668\n",
      "Epoch [134/200] Loss: 49.8474006652832\n",
      "Epoch [135/200] Loss: 49.30562973022461\n",
      "Epoch [136/200] Loss: 48.77085494995117\n",
      "Epoch [137/200] Loss: 48.24298095703125\n",
      "Epoch [138/200] Loss: 47.72197723388672\n",
      "Epoch [139/200] Loss: 47.207733154296875\n",
      "Epoch [140/200] Loss: 46.700218200683594\n",
      "Epoch [141/200] Loss: 46.19933319091797\n",
      "Epoch [142/200] Loss: 45.70499801635742\n",
      "Epoch [143/200] Loss: 45.21718978881836\n",
      "Epoch [144/200] Loss: 44.735816955566406\n",
      "Epoch [145/200] Loss: 44.26080322265625\n",
      "Epoch [146/200] Loss: 43.792091369628906\n",
      "Epoch [147/200] Loss: 43.32960891723633\n",
      "Epoch [148/200] Loss: 42.87327194213867\n",
      "Epoch [149/200] Loss: 42.42304992675781\n",
      "Epoch [150/200] Loss: 41.978878021240234\n",
      "Epoch [151/200] Loss: 41.54066848754883\n",
      "Epoch [152/200] Loss: 41.10835647583008\n",
      "Epoch [153/200] Loss: 40.68190002441406\n",
      "Epoch [154/200] Loss: 40.261234283447266\n",
      "Epoch [155/200] Loss: 39.84627914428711\n",
      "Epoch [156/200] Loss: 39.43696594238281\n",
      "Epoch [157/200] Loss: 39.03326416015625\n",
      "Epoch [158/200] Loss: 38.63508605957031\n",
      "Epoch [159/200] Loss: 38.24242401123047\n",
      "Epoch [160/200] Loss: 37.85513687133789\n",
      "Epoch [161/200] Loss: 37.47321701049805\n",
      "Epoch [162/200] Loss: 37.096588134765625\n",
      "Epoch [163/200] Loss: 36.7252197265625\n",
      "Epoch [164/200] Loss: 36.3590202331543\n",
      "Epoch [165/200] Loss: 35.997947692871094\n",
      "Epoch [166/200] Loss: 35.64194107055664\n",
      "Epoch [167/200] Loss: 35.290931701660156\n",
      "Epoch [168/200] Loss: 34.94489288330078\n",
      "Epoch [169/200] Loss: 34.603755950927734\n",
      "Epoch [170/200] Loss: 34.2674560546875\n",
      "Epoch [171/200] Loss: 33.93594741821289\n",
      "Epoch [172/200] Loss: 33.60918045043945\n",
      "Epoch [173/200] Loss: 33.287078857421875\n",
      "Epoch [174/200] Loss: 32.96962356567383\n",
      "Epoch [175/200] Loss: 32.65672302246094\n",
      "Epoch [176/200] Loss: 32.348350524902344\n",
      "Epoch [177/200] Loss: 32.04445266723633\n",
      "Epoch [178/200] Loss: 31.744977951049805\n",
      "Epoch [179/200] Loss: 31.449867248535156\n",
      "Epoch [180/200] Loss: 31.1590576171875\n",
      "Epoch [181/200] Loss: 30.872529983520508\n",
      "Epoch [182/200] Loss: 30.590211868286133\n",
      "Epoch [183/200] Loss: 30.312061309814453\n",
      "Epoch [184/200] Loss: 30.038015365600586\n",
      "Epoch [185/200] Loss: 29.768054962158203\n",
      "Epoch [186/200] Loss: 29.502099990844727\n",
      "Epoch [187/200] Loss: 29.240123748779297\n",
      "Epoch [188/200] Loss: 28.982057571411133\n",
      "Epoch [189/200] Loss: 28.72787857055664\n",
      "Epoch [190/200] Loss: 28.477527618408203\n",
      "Epoch [191/200] Loss: 28.230968475341797\n",
      "Epoch [192/200] Loss: 27.988128662109375\n",
      "Epoch [193/200] Loss: 27.74898338317871\n",
      "Epoch [194/200] Loss: 27.51349639892578\n",
      "Epoch [195/200] Loss: 27.28159523010254\n",
      "Epoch [196/200] Loss: 27.05326271057129\n",
      "Epoch [197/200] Loss: 26.828426361083984\n",
      "Epoch [198/200] Loss: 26.60706329345703\n",
      "Epoch [199/200] Loss: 26.389131546020508\n",
      "Epoch [200/200] Loss: 26.174583435058594\n",
      "Predicted days_remaining for parent_id 82: 13.355022430419922\n",
      "Training for parent_id 84...\n",
      "Epoch [1/200] Loss: 831.3248901367188\n",
      "Epoch [2/200] Loss: 816.4592895507812\n",
      "Epoch [3/200] Loss: 801.9017944335938\n",
      "Epoch [4/200] Loss: 787.760498046875\n",
      "Epoch [5/200] Loss: 774.1494750976562\n",
      "Epoch [6/200] Loss: 761.1771850585938\n",
      "Epoch [7/200] Loss: 748.9102783203125\n",
      "Epoch [8/200] Loss: 737.3582153320312\n",
      "Epoch [9/200] Loss: 726.4766845703125\n",
      "Epoch [10/200] Loss: 716.1911010742188\n",
      "Epoch [11/200] Loss: 706.4276733398438\n",
      "Epoch [12/200] Loss: 697.1303100585938\n",
      "Epoch [13/200] Loss: 688.2634887695312\n",
      "Epoch [14/200] Loss: 679.8070678710938\n",
      "Epoch [15/200] Loss: 671.74951171875\n",
      "Epoch [16/200] Loss: 664.0836791992188\n",
      "Epoch [17/200] Loss: 656.8018798828125\n",
      "Epoch [18/200] Loss: 649.8933715820312\n",
      "Epoch [19/200] Loss: 643.3428955078125\n",
      "Epoch [20/200] Loss: 637.1310424804688\n",
      "Epoch [21/200] Loss: 631.2339477539062\n",
      "Epoch [22/200] Loss: 625.6258544921875\n",
      "Epoch [23/200] Loss: 620.2791137695312\n",
      "Epoch [24/200] Loss: 615.1660766601562\n",
      "Epoch [25/200] Loss: 610.26025390625\n",
      "Epoch [26/200] Loss: 605.5366821289062\n",
      "Epoch [27/200] Loss: 600.9722290039062\n",
      "Epoch [28/200] Loss: 596.5469970703125\n",
      "Epoch [29/200] Loss: 592.243408203125\n",
      "Epoch [30/200] Loss: 588.046630859375\n",
      "Epoch [31/200] Loss: 583.9442749023438\n",
      "Epoch [32/200] Loss: 579.9262084960938\n",
      "Epoch [33/200] Loss: 575.9840087890625\n",
      "Epoch [34/200] Loss: 572.1112060546875\n",
      "Epoch [35/200] Loss: 568.302734375\n",
      "Epoch [36/200] Loss: 564.554443359375\n",
      "Epoch [37/200] Loss: 560.8630981445312\n",
      "Epoch [38/200] Loss: 557.225830078125\n",
      "Epoch [39/200] Loss: 553.639404296875\n",
      "Epoch [40/200] Loss: 550.1004638671875\n",
      "Epoch [41/200] Loss: 546.6051025390625\n",
      "Epoch [42/200] Loss: 543.1489868164062\n",
      "Epoch [43/200] Loss: 539.7274169921875\n",
      "Epoch [44/200] Loss: 536.3365478515625\n",
      "Epoch [45/200] Loss: 532.9725952148438\n",
      "Epoch [46/200] Loss: 529.6326293945312\n",
      "Epoch [47/200] Loss: 526.3143920898438\n",
      "Epoch [48/200] Loss: 523.0159301757812\n",
      "Epoch [49/200] Loss: 519.7359008789062\n",
      "Epoch [50/200] Loss: 516.4735717773438\n",
      "Epoch [51/200] Loss: 513.228271484375\n",
      "Epoch [52/200] Loss: 509.99981689453125\n",
      "Epoch [53/200] Loss: 506.78814697265625\n",
      "Epoch [54/200] Loss: 503.5934753417969\n",
      "Epoch [55/200] Loss: 500.41595458984375\n",
      "Epoch [56/200] Loss: 497.25567626953125\n",
      "Epoch [57/200] Loss: 494.113037109375\n",
      "Epoch [58/200] Loss: 490.9883117675781\n",
      "Epoch [59/200] Loss: 487.8814697265625\n",
      "Epoch [60/200] Loss: 484.79266357421875\n",
      "Epoch [61/200] Loss: 481.7217102050781\n",
      "Epoch [62/200] Loss: 478.6687316894531\n",
      "Epoch [63/200] Loss: 475.6334228515625\n",
      "Epoch [64/200] Loss: 472.6158142089844\n",
      "Epoch [65/200] Loss: 469.6156005859375\n",
      "Epoch [66/200] Loss: 466.6326904296875\n",
      "Epoch [67/200] Loss: 463.6669921875\n",
      "Epoch [68/200] Loss: 460.7182312011719\n",
      "Epoch [69/200] Loss: 457.7862243652344\n",
      "Epoch [70/200] Loss: 454.8708190917969\n",
      "Epoch [71/200] Loss: 451.97198486328125\n",
      "Epoch [72/200] Loss: 449.0895080566406\n",
      "Epoch [73/200] Loss: 446.22320556640625\n",
      "Epoch [74/200] Loss: 443.3728332519531\n",
      "Epoch [75/200] Loss: 440.5384521484375\n",
      "Epoch [76/200] Loss: 437.7199401855469\n",
      "Epoch [77/200] Loss: 434.9173278808594\n",
      "Epoch [78/200] Loss: 432.13055419921875\n",
      "Epoch [79/200] Loss: 429.3595886230469\n",
      "Epoch [80/200] Loss: 426.6045837402344\n",
      "Epoch [81/200] Loss: 423.86541748046875\n",
      "Epoch [82/200] Loss: 421.1423034667969\n",
      "Epoch [83/200] Loss: 418.4352722167969\n",
      "Epoch [84/200] Loss: 415.7442321777344\n",
      "Epoch [85/200] Loss: 413.06927490234375\n",
      "Epoch [86/200] Loss: 410.4104309082031\n",
      "Epoch [87/200] Loss: 407.7676696777344\n",
      "Epoch [88/200] Loss: 405.1410827636719\n",
      "Epoch [89/200] Loss: 402.53057861328125\n",
      "Epoch [90/200] Loss: 399.9360046386719\n",
      "Epoch [91/200] Loss: 397.357421875\n",
      "Epoch [92/200] Loss: 394.79473876953125\n",
      "Epoch [93/200] Loss: 392.24798583984375\n",
      "Epoch [94/200] Loss: 389.7169494628906\n",
      "Epoch [95/200] Loss: 387.2015686035156\n",
      "Epoch [96/200] Loss: 384.70172119140625\n",
      "Epoch [97/200] Loss: 382.2174072265625\n",
      "Epoch [98/200] Loss: 379.74835205078125\n",
      "Epoch [99/200] Loss: 377.29461669921875\n",
      "Epoch [100/200] Loss: 374.8561096191406\n",
      "Epoch [101/200] Loss: 372.4325866699219\n",
      "Epoch [102/200] Loss: 370.0240478515625\n",
      "Epoch [103/200] Loss: 367.6302490234375\n",
      "Epoch [104/200] Loss: 365.2513122558594\n",
      "Epoch [105/200] Loss: 362.8869323730469\n",
      "Epoch [106/200] Loss: 360.5370788574219\n",
      "Epoch [107/200] Loss: 358.20166015625\n",
      "Epoch [108/200] Loss: 355.8804931640625\n",
      "Epoch [109/200] Loss: 353.57366943359375\n",
      "Epoch [110/200] Loss: 351.28076171875\n",
      "Epoch [111/200] Loss: 349.0019836425781\n",
      "Epoch [112/200] Loss: 346.73699951171875\n",
      "Epoch [113/200] Loss: 344.4858093261719\n",
      "Epoch [114/200] Loss: 342.2484130859375\n",
      "Epoch [115/200] Loss: 340.0244445800781\n",
      "Epoch [116/200] Loss: 337.81396484375\n",
      "Epoch [117/200] Loss: 335.6169128417969\n",
      "Epoch [118/200] Loss: 333.4330749511719\n",
      "Epoch [119/200] Loss: 331.2625427246094\n",
      "Epoch [120/200] Loss: 329.10504150390625\n",
      "Epoch [121/200] Loss: 326.96044921875\n",
      "Epoch [122/200] Loss: 324.828857421875\n",
      "Epoch [123/200] Loss: 322.7099914550781\n",
      "Epoch [124/200] Loss: 320.6039123535156\n",
      "Epoch [125/200] Loss: 318.5104064941406\n",
      "Epoch [126/200] Loss: 316.4294738769531\n",
      "Epoch [127/200] Loss: 314.3609619140625\n",
      "Epoch [128/200] Loss: 312.3048400878906\n",
      "Epoch [129/200] Loss: 310.260986328125\n",
      "Epoch [130/200] Loss: 308.2293701171875\n",
      "Epoch [131/200] Loss: 306.2098388671875\n",
      "Epoch [132/200] Loss: 304.202392578125\n",
      "Epoch [133/200] Loss: 302.2069396972656\n",
      "Epoch [134/200] Loss: 300.22332763671875\n",
      "Epoch [135/200] Loss: 298.2514343261719\n",
      "Epoch [136/200] Loss: 296.29132080078125\n",
      "Epoch [137/200] Loss: 294.3428955078125\n",
      "Epoch [138/200] Loss: 292.40606689453125\n",
      "Epoch [139/200] Loss: 290.4806823730469\n",
      "Epoch [140/200] Loss: 288.5668029785156\n",
      "Epoch [141/200] Loss: 286.66424560546875\n",
      "Epoch [142/200] Loss: 284.77301025390625\n",
      "Epoch [143/200] Loss: 282.8930358886719\n",
      "Epoch [144/200] Loss: 281.0241394042969\n",
      "Epoch [145/200] Loss: 279.1663818359375\n",
      "Epoch [146/200] Loss: 277.3197021484375\n",
      "Epoch [147/200] Loss: 275.48388671875\n",
      "Epoch [148/200] Loss: 273.6590270996094\n",
      "Epoch [149/200] Loss: 271.844970703125\n",
      "Epoch [150/200] Loss: 270.04168701171875\n",
      "Epoch [151/200] Loss: 268.2491760253906\n",
      "Epoch [152/200] Loss: 266.4671936035156\n",
      "Epoch [153/200] Loss: 264.6958923339844\n",
      "Epoch [154/200] Loss: 262.9350280761719\n",
      "Epoch [155/200] Loss: 261.1847229003906\n",
      "Epoch [156/200] Loss: 259.4447326660156\n",
      "Epoch [157/200] Loss: 257.715087890625\n",
      "Epoch [158/200] Loss: 255.9957733154297\n",
      "Epoch [159/200] Loss: 254.2866973876953\n",
      "Epoch [160/200] Loss: 252.5878143310547\n",
      "Epoch [161/200] Loss: 250.8990020751953\n",
      "Epoch [162/200] Loss: 249.2202911376953\n",
      "Epoch [163/200] Loss: 247.55160522460938\n",
      "Epoch [164/200] Loss: 245.89276123046875\n",
      "Epoch [165/200] Loss: 244.24391174316406\n",
      "Epoch [166/200] Loss: 242.6049041748047\n",
      "Epoch [167/200] Loss: 240.97567749023438\n",
      "Epoch [168/200] Loss: 239.35617065429688\n",
      "Epoch [169/200] Loss: 237.7463836669922\n",
      "Epoch [170/200] Loss: 236.146240234375\n",
      "Epoch [171/200] Loss: 234.55564880371094\n",
      "Epoch [172/200] Loss: 232.97462463378906\n",
      "Epoch [173/200] Loss: 231.403076171875\n",
      "Epoch [174/200] Loss: 229.84092712402344\n",
      "Epoch [175/200] Loss: 228.2882080078125\n",
      "Epoch [176/200] Loss: 226.74485778808594\n",
      "Epoch [177/200] Loss: 225.2107391357422\n",
      "Epoch [178/200] Loss: 223.68585205078125\n",
      "Epoch [179/200] Loss: 222.1702117919922\n",
      "Epoch [180/200] Loss: 220.6636505126953\n",
      "Epoch [181/200] Loss: 219.16624450683594\n",
      "Epoch [182/200] Loss: 217.6779022216797\n",
      "Epoch [183/200] Loss: 216.1985321044922\n",
      "Epoch [184/200] Loss: 214.72811889648438\n",
      "Epoch [185/200] Loss: 213.26663208007812\n",
      "Epoch [186/200] Loss: 211.81398010253906\n",
      "Epoch [187/200] Loss: 210.3701934814453\n",
      "Epoch [188/200] Loss: 208.9351806640625\n",
      "Epoch [189/200] Loss: 207.50888061523438\n",
      "Epoch [190/200] Loss: 206.09124755859375\n",
      "Epoch [191/200] Loss: 204.68231201171875\n",
      "Epoch [192/200] Loss: 203.2819366455078\n",
      "Epoch [193/200] Loss: 201.89019775390625\n",
      "Epoch [194/200] Loss: 200.5069122314453\n",
      "Epoch [195/200] Loss: 199.1321258544922\n",
      "Epoch [196/200] Loss: 197.76571655273438\n",
      "Epoch [197/200] Loss: 196.40777587890625\n",
      "Epoch [198/200] Loss: 195.05809020996094\n",
      "Epoch [199/200] Loss: 193.716796875\n",
      "Epoch [200/200] Loss: 192.38368225097656\n",
      "Predicted days_remaining for parent_id 84: 15.460103988647461\n",
      "Training for parent_id 88...\n",
      "Epoch [1/200] Loss: 164.5880889892578\n",
      "Epoch [2/200] Loss: 159.34207153320312\n",
      "Epoch [3/200] Loss: 154.19786071777344\n",
      "Epoch [4/200] Loss: 149.14566040039062\n",
      "Epoch [5/200] Loss: 144.17274475097656\n",
      "Epoch [6/200] Loss: 139.29066467285156\n",
      "Epoch [7/200] Loss: 134.52467346191406\n",
      "Epoch [8/200] Loss: 129.89718627929688\n",
      "Epoch [9/200] Loss: 125.42243957519531\n",
      "Epoch [10/200] Loss: 121.10530090332031\n",
      "Epoch [11/200] Loss: 116.9438247680664\n",
      "Epoch [12/200] Loss: 112.93404388427734\n",
      "Epoch [13/200] Loss: 109.07439422607422\n",
      "Epoch [14/200] Loss: 105.36702728271484\n",
      "Epoch [15/200] Loss: 101.8170394897461\n",
      "Epoch [16/200] Loss: 98.43083953857422\n",
      "Epoch [17/200] Loss: 95.21456146240234\n",
      "Epoch [18/200] Loss: 92.17245483398438\n",
      "Epoch [19/200] Loss: 89.30599212646484\n",
      "Epoch [20/200] Loss: 86.61346435546875\n",
      "Epoch [21/200] Loss: 84.09001159667969\n",
      "Epoch [22/200] Loss: 81.728271484375\n",
      "Epoch [23/200] Loss: 79.518798828125\n",
      "Epoch [24/200] Loss: 77.45085144042969\n",
      "Epoch [25/200] Loss: 75.51303100585938\n",
      "Epoch [26/200] Loss: 73.69361877441406\n",
      "Epoch [27/200] Loss: 71.98110961914062\n",
      "Epoch [28/200] Loss: 70.364501953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/200] Loss: 68.83345794677734\n",
      "Epoch [30/200] Loss: 67.37849426269531\n",
      "Epoch [31/200] Loss: 65.99097442626953\n",
      "Epoch [32/200] Loss: 64.66321563720703\n",
      "Epoch [33/200] Loss: 63.38835144042969\n",
      "Epoch [34/200] Loss: 62.16045379638672\n",
      "Epoch [35/200] Loss: 60.97428512573242\n",
      "Epoch [36/200] Loss: 59.825435638427734\n",
      "Epoch [37/200] Loss: 58.710182189941406\n",
      "Epoch [38/200] Loss: 57.62550735473633\n",
      "Epoch [39/200] Loss: 56.56907653808594\n",
      "Epoch [40/200] Loss: 55.53919982910156\n",
      "Epoch [41/200] Loss: 54.5348014831543\n",
      "Epoch [42/200] Loss: 53.5553092956543\n",
      "Epoch [43/200] Loss: 52.600406646728516\n",
      "Epoch [44/200] Loss: 51.66993713378906\n",
      "Epoch [45/200] Loss: 50.76359176635742\n",
      "Epoch [46/200] Loss: 49.88090515136719\n",
      "Epoch [47/200] Loss: 49.0211296081543\n",
      "Epoch [48/200] Loss: 48.183349609375\n",
      "Epoch [49/200] Loss: 47.366580963134766\n",
      "Epoch [50/200] Loss: 46.5698127746582\n",
      "Epoch [51/200] Loss: 45.792091369628906\n",
      "Epoch [52/200] Loss: 45.032569885253906\n",
      "Epoch [53/200] Loss: 44.29049301147461\n",
      "Epoch [54/200] Loss: 43.5651969909668\n",
      "Epoch [55/200] Loss: 42.8560905456543\n",
      "Epoch [56/200] Loss: 42.16264343261719\n",
      "Epoch [57/200] Loss: 41.48442077636719\n",
      "Epoch [58/200] Loss: 40.82097244262695\n",
      "Epoch [59/200] Loss: 40.171939849853516\n",
      "Epoch [60/200] Loss: 39.53696823120117\n",
      "Epoch [61/200] Loss: 38.91572952270508\n",
      "Epoch [62/200] Loss: 38.30790710449219\n",
      "Epoch [63/200] Loss: 37.713233947753906\n",
      "Epoch [64/200] Loss: 37.13141632080078\n",
      "Epoch [65/200] Loss: 36.562198638916016\n",
      "Epoch [66/200] Loss: 36.00531768798828\n",
      "Epoch [67/200] Loss: 35.460540771484375\n",
      "Epoch [68/200] Loss: 34.92762756347656\n",
      "Epoch [69/200] Loss: 34.406349182128906\n",
      "Epoch [70/200] Loss: 33.89646911621094\n",
      "Epoch [71/200] Loss: 33.39778518676758\n",
      "Epoch [72/200] Loss: 32.91008758544922\n",
      "Epoch [73/200] Loss: 32.43314743041992\n",
      "Epoch [74/200] Loss: 31.966781616210938\n",
      "Epoch [75/200] Loss: 31.510770797729492\n",
      "Epoch [76/200] Loss: 31.0649356842041\n",
      "Epoch [77/200] Loss: 30.629079818725586\n",
      "Epoch [78/200] Loss: 30.20301055908203\n",
      "Epoch [79/200] Loss: 29.786544799804688\n",
      "Epoch [80/200] Loss: 29.37950897216797\n",
      "Epoch [81/200] Loss: 28.981712341308594\n",
      "Epoch [82/200] Loss: 28.59299087524414\n",
      "Epoch [83/200] Loss: 28.213157653808594\n",
      "Epoch [84/200] Loss: 27.84204864501953\n",
      "Epoch [85/200] Loss: 27.479516983032227\n",
      "Epoch [86/200] Loss: 27.1253719329834\n",
      "Epoch [87/200] Loss: 26.779462814331055\n",
      "Epoch [88/200] Loss: 26.441633224487305\n",
      "Epoch [89/200] Loss: 26.111722946166992\n",
      "Epoch [90/200] Loss: 25.78959083557129\n",
      "Epoch [91/200] Loss: 25.475069046020508\n",
      "Epoch [92/200] Loss: 25.168027877807617\n",
      "Epoch [93/200] Loss: 24.868309020996094\n",
      "Epoch [94/200] Loss: 24.575763702392578\n",
      "Epoch [95/200] Loss: 24.290258407592773\n",
      "Epoch [96/200] Loss: 24.01165008544922\n",
      "Epoch [97/200] Loss: 23.739822387695312\n",
      "Epoch [98/200] Loss: 23.474605560302734\n",
      "Epoch [99/200] Loss: 23.215896606445312\n",
      "Epoch [100/200] Loss: 22.963550567626953\n",
      "Epoch [101/200] Loss: 22.717449188232422\n",
      "Epoch [102/200] Loss: 22.477449417114258\n",
      "Epoch [103/200] Loss: 22.243446350097656\n",
      "Epoch [104/200] Loss: 22.015300750732422\n",
      "Epoch [105/200] Loss: 21.792909622192383\n",
      "Epoch [106/200] Loss: 21.576152801513672\n",
      "Epoch [107/200] Loss: 21.364892959594727\n",
      "Epoch [108/200] Loss: 21.159034729003906\n",
      "Epoch [109/200] Loss: 20.958457946777344\n",
      "Epoch [110/200] Loss: 20.763057708740234\n",
      "Epoch [111/200] Loss: 20.572711944580078\n",
      "Epoch [112/200] Loss: 20.387325286865234\n",
      "Epoch [113/200] Loss: 20.2067928314209\n",
      "Epoch [114/200] Loss: 20.031002044677734\n",
      "Epoch [115/200] Loss: 19.859859466552734\n",
      "Epoch [116/200] Loss: 19.693256378173828\n",
      "Epoch [117/200] Loss: 19.53110122680664\n",
      "Epoch [118/200] Loss: 19.37328338623047\n",
      "Epoch [119/200] Loss: 19.21971893310547\n",
      "Epoch [120/200] Loss: 19.0703182220459\n",
      "Epoch [121/200] Loss: 18.92496681213379\n",
      "Epoch [122/200] Loss: 18.783599853515625\n",
      "Epoch [123/200] Loss: 18.64611053466797\n",
      "Epoch [124/200] Loss: 18.51241683959961\n",
      "Epoch [125/200] Loss: 18.382427215576172\n",
      "Epoch [126/200] Loss: 18.256065368652344\n",
      "Epoch [127/200] Loss: 18.13323974609375\n",
      "Epoch [128/200] Loss: 18.013870239257812\n",
      "Epoch [129/200] Loss: 17.89788246154785\n",
      "Epoch [130/200] Loss: 17.785192489624023\n",
      "Epoch [131/200] Loss: 17.675710678100586\n",
      "Epoch [132/200] Loss: 17.56937599182129\n",
      "Epoch [133/200] Loss: 17.466110229492188\n",
      "Epoch [134/200] Loss: 17.3658390045166\n",
      "Epoch [135/200] Loss: 17.268482208251953\n",
      "Epoch [136/200] Loss: 17.173980712890625\n",
      "Epoch [137/200] Loss: 17.082256317138672\n",
      "Epoch [138/200] Loss: 16.993240356445312\n",
      "Epoch [139/200] Loss: 16.9068660736084\n",
      "Epoch [140/200] Loss: 16.823076248168945\n",
      "Epoch [141/200] Loss: 16.741798400878906\n",
      "Epoch [142/200] Loss: 16.662965774536133\n",
      "Epoch [143/200] Loss: 16.586523056030273\n",
      "Epoch [144/200] Loss: 16.512409210205078\n",
      "Epoch [145/200] Loss: 16.4405574798584\n",
      "Epoch [146/200] Loss: 16.37091064453125\n",
      "Epoch [147/200] Loss: 16.303417205810547\n",
      "Epoch [148/200] Loss: 16.23801612854004\n",
      "Epoch [149/200] Loss: 16.174654006958008\n",
      "Epoch [150/200] Loss: 16.113277435302734\n",
      "Epoch [151/200] Loss: 16.053829193115234\n",
      "Epoch [152/200] Loss: 15.99626350402832\n",
      "Epoch [153/200] Loss: 15.940521240234375\n",
      "Epoch [154/200] Loss: 15.886565208435059\n",
      "Epoch [155/200] Loss: 15.83433723449707\n",
      "Epoch [156/200] Loss: 15.783792495727539\n",
      "Epoch [157/200] Loss: 15.734882354736328\n",
      "Epoch [158/200] Loss: 15.687560081481934\n",
      "Epoch [159/200] Loss: 15.641791343688965\n",
      "Epoch [160/200] Loss: 15.597518920898438\n",
      "Epoch [161/200] Loss: 15.554709434509277\n",
      "Epoch [162/200] Loss: 15.513315200805664\n",
      "Epoch [163/200] Loss: 15.473302841186523\n",
      "Epoch [164/200] Loss: 15.434623718261719\n",
      "Epoch [165/200] Loss: 15.397247314453125\n",
      "Epoch [166/200] Loss: 15.361136436462402\n",
      "Epoch [167/200] Loss: 15.326245307922363\n",
      "Epoch [168/200] Loss: 15.292540550231934\n",
      "Epoch [169/200] Loss: 15.259994506835938\n",
      "Epoch [170/200] Loss: 15.22856330871582\n",
      "Epoch [171/200] Loss: 15.198219299316406\n",
      "Epoch [172/200] Loss: 15.168925285339355\n",
      "Epoch [173/200] Loss: 15.140653610229492\n",
      "Epoch [174/200] Loss: 15.113371849060059\n",
      "Epoch [175/200] Loss: 15.087047576904297\n",
      "Epoch [176/200] Loss: 15.061651229858398\n",
      "Epoch [177/200] Loss: 15.037161827087402\n",
      "Epoch [178/200] Loss: 15.013538360595703\n",
      "Epoch [179/200] Loss: 14.990761756896973\n",
      "Epoch [180/200] Loss: 14.968805313110352\n",
      "Epoch [181/200] Loss: 14.947639465332031\n",
      "Epoch [182/200] Loss: 14.927244186401367\n",
      "Epoch [183/200] Loss: 14.907586097717285\n",
      "Epoch [184/200] Loss: 14.888651847839355\n",
      "Epoch [185/200] Loss: 14.870410919189453\n",
      "Epoch [186/200] Loss: 14.85284423828125\n",
      "Epoch [187/200] Loss: 14.835926055908203\n",
      "Epoch [188/200] Loss: 14.819637298583984\n",
      "Epoch [189/200] Loss: 14.803958892822266\n",
      "Epoch [190/200] Loss: 14.788867950439453\n",
      "Epoch [191/200] Loss: 14.774345397949219\n",
      "Epoch [192/200] Loss: 14.760372161865234\n",
      "Epoch [193/200] Loss: 14.746932983398438\n",
      "Epoch [194/200] Loss: 14.734001159667969\n",
      "Epoch [195/200] Loss: 14.721569061279297\n",
      "Epoch [196/200] Loss: 14.709613800048828\n",
      "Epoch [197/200] Loss: 14.698122024536133\n",
      "Epoch [198/200] Loss: 14.68707275390625\n",
      "Epoch [199/200] Loss: 14.676458358764648\n",
      "Epoch [200/200] Loss: 14.666257858276367\n",
      "Predicted days_remaining for parent_id 88: 11.282073020935059\n",
      "Training for parent_id 91...\n",
      "Epoch [1/200] Loss: 1174.10986328125\n",
      "Epoch [2/200] Loss: 1158.447265625\n",
      "Epoch [3/200] Loss: 1142.962890625\n",
      "Epoch [4/200] Loss: 1127.70068359375\n",
      "Epoch [5/200] Loss: 1112.694091796875\n",
      "Epoch [6/200] Loss: 1097.977783203125\n",
      "Epoch [7/200] Loss: 1083.600341796875\n",
      "Epoch [8/200] Loss: 1069.621826171875\n",
      "Epoch [9/200] Loss: 1056.1021728515625\n",
      "Epoch [10/200] Loss: 1043.0863037109375\n",
      "Epoch [11/200] Loss: 1030.6005859375\n",
      "Epoch [12/200] Loss: 1018.6516723632812\n",
      "Epoch [13/200] Loss: 1007.2337646484375\n",
      "Epoch [14/200] Loss: 996.33154296875\n",
      "Epoch [15/200] Loss: 985.92529296875\n",
      "Epoch [16/200] Loss: 975.9920043945312\n",
      "Epoch [17/200] Loss: 966.5066528320312\n",
      "Epoch [18/200] Loss: 957.4425048828125\n",
      "Epoch [19/200] Loss: 948.7702026367188\n",
      "Epoch [20/200] Loss: 940.460693359375\n",
      "Epoch [21/200] Loss: 932.4846801757812\n",
      "Epoch [22/200] Loss: 924.8150024414062\n",
      "Epoch [23/200] Loss: 917.4257202148438\n",
      "Epoch [24/200] Loss: 910.2932739257812\n",
      "Epoch [25/200] Loss: 903.395751953125\n",
      "Epoch [26/200] Loss: 896.71337890625\n",
      "Epoch [27/200] Loss: 890.2276611328125\n",
      "Epoch [28/200] Loss: 883.9224243164062\n",
      "Epoch [29/200] Loss: 877.782958984375\n",
      "Epoch [30/200] Loss: 871.7957763671875\n",
      "Epoch [31/200] Loss: 865.9483642578125\n",
      "Epoch [32/200] Loss: 860.2293090820312\n",
      "Epoch [33/200] Loss: 854.6275024414062\n",
      "Epoch [34/200] Loss: 849.133056640625\n",
      "Epoch [35/200] Loss: 843.736572265625\n",
      "Epoch [36/200] Loss: 838.4296875\n",
      "Epoch [37/200] Loss: 833.2052001953125\n",
      "Epoch [38/200] Loss: 828.0568237304688\n",
      "Epoch [39/200] Loss: 822.9789428710938\n",
      "Epoch [40/200] Loss: 817.967529296875\n",
      "Epoch [41/200] Loss: 813.0186157226562\n",
      "Epoch [42/200] Loss: 808.12939453125\n",
      "Epoch [43/200] Loss: 803.2969970703125\n",
      "Epoch [44/200] Loss: 798.5196533203125\n",
      "Epoch [45/200] Loss: 793.7947387695312\n",
      "Epoch [46/200] Loss: 789.1206665039062\n",
      "Epoch [47/200] Loss: 784.4956665039062\n",
      "Epoch [48/200] Loss: 779.9182739257812\n",
      "Epoch [49/200] Loss: 775.3867797851562\n",
      "Epoch [50/200] Loss: 770.8995361328125\n",
      "Epoch [51/200] Loss: 766.455322265625\n",
      "Epoch [52/200] Loss: 762.0528564453125\n",
      "Epoch [53/200] Loss: 757.6908569335938\n",
      "Epoch [54/200] Loss: 753.3680419921875\n",
      "Epoch [55/200] Loss: 749.0834350585938\n",
      "Epoch [56/200] Loss: 744.8359375\n",
      "Epoch [57/200] Loss: 740.62451171875\n",
      "Epoch [58/200] Loss: 736.4483032226562\n",
      "Epoch [59/200] Loss: 732.3062744140625\n",
      "Epoch [60/200] Loss: 728.1978759765625\n",
      "Epoch [61/200] Loss: 724.1219482421875\n",
      "Epoch [62/200] Loss: 720.0777587890625\n",
      "Epoch [63/200] Loss: 716.0648193359375\n",
      "Epoch [64/200] Loss: 712.0823974609375\n",
      "Epoch [65/200] Loss: 708.1295776367188\n",
      "Epoch [66/200] Loss: 704.2058715820312\n",
      "Epoch [67/200] Loss: 700.3109741210938\n",
      "Epoch [68/200] Loss: 696.44384765625\n",
      "Epoch [69/200] Loss: 692.604248046875\n",
      "Epoch [70/200] Loss: 688.7916259765625\n",
      "Epoch [71/200] Loss: 685.0054321289062\n",
      "Epoch [72/200] Loss: 681.2452392578125\n",
      "Epoch [73/200] Loss: 677.510498046875\n",
      "Epoch [74/200] Loss: 673.80078125\n",
      "Epoch [75/200] Loss: 670.1157836914062\n",
      "Epoch [76/200] Loss: 666.454833984375\n",
      "Epoch [77/200] Loss: 662.8180541992188\n",
      "Epoch [78/200] Loss: 659.2044677734375\n",
      "Epoch [79/200] Loss: 655.614013671875\n",
      "Epoch [80/200] Loss: 652.04638671875\n",
      "Epoch [81/200] Loss: 648.5011596679688\n",
      "Epoch [82/200] Loss: 644.9778442382812\n",
      "Epoch [83/200] Loss: 641.4763793945312\n",
      "Epoch [84/200] Loss: 637.996337890625\n",
      "Epoch [85/200] Loss: 634.5374755859375\n",
      "Epoch [86/200] Loss: 631.0994262695312\n",
      "Epoch [87/200] Loss: 627.681884765625\n",
      "Epoch [88/200] Loss: 624.2847290039062\n",
      "Epoch [89/200] Loss: 620.9075317382812\n",
      "Epoch [90/200] Loss: 617.5499877929688\n",
      "Epoch [91/200] Loss: 614.2122802734375\n",
      "Epoch [92/200] Loss: 610.8936157226562\n",
      "Epoch [93/200] Loss: 607.5941162109375\n",
      "Epoch [94/200] Loss: 604.3134155273438\n",
      "Epoch [95/200] Loss: 601.05126953125\n",
      "Epoch [96/200] Loss: 597.8076782226562\n",
      "Epoch [97/200] Loss: 594.5821533203125\n",
      "Epoch [98/200] Loss: 591.3748168945312\n",
      "Epoch [99/200] Loss: 588.1851806640625\n",
      "Epoch [100/200] Loss: 585.0130004882812\n",
      "Epoch [101/200] Loss: 581.8583984375\n",
      "Epoch [102/200] Loss: 578.7210693359375\n",
      "Epoch [103/200] Loss: 575.6007080078125\n",
      "Epoch [104/200] Loss: 572.4972534179688\n",
      "Epoch [105/200] Loss: 569.4105834960938\n",
      "Epoch [106/200] Loss: 566.3404541015625\n",
      "Epoch [107/200] Loss: 563.2868041992188\n",
      "Epoch [108/200] Loss: 560.2493896484375\n",
      "Epoch [109/200] Loss: 557.2280883789062\n",
      "Epoch [110/200] Loss: 554.22265625\n",
      "Epoch [111/200] Loss: 551.233154296875\n",
      "Epoch [112/200] Loss: 548.25927734375\n",
      "Epoch [113/200] Loss: 545.3009643554688\n",
      "Epoch [114/200] Loss: 542.3580932617188\n",
      "Epoch [115/200] Loss: 539.430419921875\n",
      "Epoch [116/200] Loss: 536.51806640625\n",
      "Epoch [117/200] Loss: 533.6206665039062\n",
      "Epoch [118/200] Loss: 530.7382202148438\n",
      "Epoch [119/200] Loss: 527.8704833984375\n",
      "Epoch [120/200] Loss: 525.0174560546875\n",
      "Epoch [121/200] Loss: 522.1790771484375\n",
      "Epoch [122/200] Loss: 519.3551025390625\n",
      "Epoch [123/200] Loss: 516.54541015625\n",
      "Epoch [124/200] Loss: 513.7498779296875\n",
      "Epoch [125/200] Loss: 510.9687194824219\n",
      "Epoch [126/200] Loss: 508.2014465332031\n",
      "Epoch [127/200] Loss: 505.4482421875\n",
      "Epoch [128/200] Loss: 502.708740234375\n",
      "Epoch [129/200] Loss: 499.9831237792969\n",
      "Epoch [130/200] Loss: 497.2709655761719\n",
      "Epoch [131/200] Loss: 494.5725402832031\n",
      "Epoch [132/200] Loss: 491.887451171875\n",
      "Epoch [133/200] Loss: 489.2156982421875\n",
      "Epoch [134/200] Loss: 486.5572509765625\n",
      "Epoch [135/200] Loss: 483.912109375\n",
      "Epoch [136/200] Loss: 481.2800598144531\n",
      "Epoch [137/200] Loss: 478.6610107421875\n",
      "Epoch [138/200] Loss: 476.0549011230469\n",
      "Epoch [139/200] Loss: 473.46173095703125\n",
      "Epoch [140/200] Loss: 470.8812561035156\n",
      "Epoch [141/200] Loss: 468.31353759765625\n",
      "Epoch [142/200] Loss: 465.7583923339844\n",
      "Epoch [143/200] Loss: 463.2159423828125\n",
      "Epoch [144/200] Loss: 460.6859130859375\n",
      "Epoch [145/200] Loss: 458.168212890625\n",
      "Epoch [146/200] Loss: 455.6629333496094\n",
      "Epoch [147/200] Loss: 453.16998291015625\n",
      "Epoch [148/200] Loss: 450.6890869140625\n",
      "Epoch [149/200] Loss: 448.22039794921875\n",
      "Epoch [150/200] Loss: 445.7637634277344\n",
      "Epoch [151/200] Loss: 443.319091796875\n",
      "Epoch [152/200] Loss: 440.8865661621094\n",
      "Epoch [153/200] Loss: 438.4657287597656\n",
      "Epoch [154/200] Loss: 436.0566711425781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/200] Loss: 433.65948486328125\n",
      "Epoch [156/200] Loss: 431.27386474609375\n",
      "Epoch [157/200] Loss: 428.8998718261719\n",
      "Epoch [158/200] Loss: 426.5374755859375\n",
      "Epoch [159/200] Loss: 424.1865234375\n",
      "Epoch [160/200] Loss: 421.8470764160156\n",
      "Epoch [161/200] Loss: 419.5189208984375\n",
      "Epoch [162/200] Loss: 417.2021179199219\n",
      "Epoch [163/200] Loss: 414.8966064453125\n",
      "Epoch [164/200] Loss: 412.6022644042969\n",
      "Epoch [165/200] Loss: 410.3191223144531\n",
      "Epoch [166/200] Loss: 408.04705810546875\n",
      "Epoch [167/200] Loss: 405.78607177734375\n",
      "Epoch [168/200] Loss: 403.5361022949219\n",
      "Epoch [169/200] Loss: 401.2969665527344\n",
      "Epoch [170/200] Loss: 399.06878662109375\n",
      "Epoch [171/200] Loss: 396.85137939453125\n",
      "Epoch [172/200] Loss: 394.6446838378906\n",
      "Epoch [173/200] Loss: 392.4487609863281\n",
      "Epoch [174/200] Loss: 390.2635803222656\n",
      "Epoch [175/200] Loss: 388.0889892578125\n",
      "Epoch [176/200] Loss: 385.9249572753906\n",
      "Epoch [177/200] Loss: 383.771484375\n",
      "Epoch [178/200] Loss: 381.6285705566406\n",
      "Epoch [179/200] Loss: 379.4959411621094\n",
      "Epoch [180/200] Loss: 377.37371826171875\n",
      "Epoch [181/200] Loss: 375.2619934082031\n",
      "Epoch [182/200] Loss: 373.16033935546875\n",
      "Epoch [183/200] Loss: 371.0689697265625\n",
      "Epoch [184/200] Loss: 368.9878845214844\n",
      "Epoch [185/200] Loss: 366.9169006347656\n",
      "Epoch [186/200] Loss: 364.8559875488281\n",
      "Epoch [187/200] Loss: 362.80523681640625\n",
      "Epoch [188/200] Loss: 360.7643737792969\n",
      "Epoch [189/200] Loss: 358.7335510253906\n",
      "Epoch [190/200] Loss: 356.7126159667969\n",
      "Epoch [191/200] Loss: 354.70159912109375\n",
      "Epoch [192/200] Loss: 352.700439453125\n",
      "Epoch [193/200] Loss: 350.7090148925781\n",
      "Epoch [194/200] Loss: 348.7273864746094\n",
      "Epoch [195/200] Loss: 346.7554931640625\n",
      "Epoch [196/200] Loss: 344.793212890625\n",
      "Epoch [197/200] Loss: 342.840576171875\n",
      "Epoch [198/200] Loss: 340.8974609375\n",
      "Epoch [199/200] Loss: 338.96405029296875\n",
      "Epoch [200/200] Loss: 337.0400390625\n",
      "Predicted days_remaining for parent_id 91: 15.842236518859863\n",
      "Training for parent_id 96...\n",
      "Epoch [1/200] Loss: 1084.8399658203125\n",
      "Epoch [2/200] Loss: 1069.627197265625\n",
      "Epoch [3/200] Loss: 1054.8359375\n",
      "Epoch [4/200] Loss: 1040.5501708984375\n",
      "Epoch [5/200] Loss: 1026.818115234375\n",
      "Epoch [6/200] Loss: 1013.6638793945312\n",
      "Epoch [7/200] Loss: 1001.0735473632812\n",
      "Epoch [8/200] Loss: 989.0007934570312\n",
      "Epoch [9/200] Loss: 977.3837890625\n",
      "Epoch [10/200] Loss: 966.158935546875\n",
      "Epoch [11/200] Loss: 955.269287109375\n",
      "Epoch [12/200] Loss: 944.6690063476562\n",
      "Epoch [13/200] Loss: 934.32763671875\n",
      "Epoch [14/200] Loss: 924.2276000976562\n",
      "Epoch [15/200] Loss: 914.3626098632812\n",
      "Epoch [16/200] Loss: 904.731201171875\n",
      "Epoch [17/200] Loss: 895.333740234375\n",
      "Epoch [18/200] Loss: 886.1688842773438\n",
      "Epoch [19/200] Loss: 877.2337646484375\n",
      "Epoch [20/200] Loss: 868.5247192382812\n",
      "Epoch [21/200] Loss: 860.0380859375\n",
      "Epoch [22/200] Loss: 851.7718505859375\n",
      "Epoch [23/200] Loss: 843.7257080078125\n",
      "Epoch [24/200] Loss: 835.9000244140625\n",
      "Epoch [25/200] Loss: 828.2960205078125\n",
      "Epoch [26/200] Loss: 820.9141235351562\n",
      "Epoch [27/200] Loss: 813.7538452148438\n",
      "Epoch [28/200] Loss: 806.8125\n",
      "Epoch [29/200] Loss: 800.0856323242188\n",
      "Epoch [30/200] Loss: 793.5662231445312\n",
      "Epoch [31/200] Loss: 787.24609375\n",
      "Epoch [32/200] Loss: 781.1155395507812\n",
      "Epoch [33/200] Loss: 775.164306640625\n",
      "Epoch [34/200] Loss: 769.3818359375\n",
      "Epoch [35/200] Loss: 763.759033203125\n",
      "Epoch [36/200] Loss: 758.2862548828125\n",
      "Epoch [37/200] Loss: 752.955810546875\n",
      "Epoch [38/200] Loss: 747.7598876953125\n",
      "Epoch [39/200] Loss: 742.6913452148438\n",
      "Epoch [40/200] Loss: 737.742919921875\n",
      "Epoch [41/200] Loss: 732.9072265625\n",
      "Epoch [42/200] Loss: 728.1764526367188\n",
      "Epoch [43/200] Loss: 723.5419921875\n",
      "Epoch [44/200] Loss: 718.9959106445312\n",
      "Epoch [45/200] Loss: 714.5293579101562\n",
      "Epoch [46/200] Loss: 710.1348876953125\n",
      "Epoch [47/200] Loss: 705.8047485351562\n",
      "Epoch [48/200] Loss: 701.5325317382812\n",
      "Epoch [49/200] Loss: 697.3123168945312\n",
      "Epoch [50/200] Loss: 693.1392822265625\n",
      "Epoch [51/200] Loss: 689.0087890625\n",
      "Epoch [52/200] Loss: 684.9177856445312\n",
      "Epoch [53/200] Loss: 680.8629760742188\n",
      "Epoch [54/200] Loss: 676.8421020507812\n",
      "Epoch [55/200] Loss: 672.8533325195312\n",
      "Epoch [56/200] Loss: 668.8948974609375\n",
      "Epoch [57/200] Loss: 664.965576171875\n",
      "Epoch [58/200] Loss: 661.0648193359375\n",
      "Epoch [59/200] Loss: 657.1914672851562\n",
      "Epoch [60/200] Loss: 653.3450927734375\n",
      "Epoch [61/200] Loss: 649.5254516601562\n",
      "Epoch [62/200] Loss: 645.7318725585938\n",
      "Epoch [63/200] Loss: 641.9645385742188\n",
      "Epoch [64/200] Loss: 638.2229614257812\n",
      "Epoch [65/200] Loss: 634.507080078125\n",
      "Epoch [66/200] Loss: 630.816650390625\n",
      "Epoch [67/200] Loss: 627.1513061523438\n",
      "Epoch [68/200] Loss: 623.5112915039062\n",
      "Epoch [69/200] Loss: 619.8958740234375\n",
      "Epoch [70/200] Loss: 616.3050537109375\n",
      "Epoch [71/200] Loss: 612.7385864257812\n",
      "Epoch [72/200] Loss: 609.1961059570312\n",
      "Epoch [73/200] Loss: 605.677490234375\n",
      "Epoch [74/200] Loss: 602.1822509765625\n",
      "Epoch [75/200] Loss: 598.7102661132812\n",
      "Epoch [76/200] Loss: 595.2610473632812\n",
      "Epoch [77/200] Loss: 591.8345947265625\n",
      "Epoch [78/200] Loss: 588.4303588867188\n",
      "Epoch [79/200] Loss: 585.0481567382812\n",
      "Epoch [80/200] Loss: 581.6878662109375\n",
      "Epoch [81/200] Loss: 578.3490600585938\n",
      "Epoch [82/200] Loss: 575.031494140625\n",
      "Epoch [83/200] Loss: 571.73486328125\n",
      "Epoch [84/200] Loss: 568.4588623046875\n",
      "Epoch [85/200] Loss: 565.20361328125\n",
      "Epoch [86/200] Loss: 561.968505859375\n",
      "Epoch [87/200] Loss: 558.7532348632812\n",
      "Epoch [88/200] Loss: 555.5579833984375\n",
      "Epoch [89/200] Loss: 552.3822021484375\n",
      "Epoch [90/200] Loss: 549.2256469726562\n",
      "Epoch [91/200] Loss: 546.0882568359375\n",
      "Epoch [92/200] Loss: 542.9697875976562\n",
      "Epoch [93/200] Loss: 539.8699951171875\n",
      "Epoch [94/200] Loss: 536.7886962890625\n",
      "Epoch [95/200] Loss: 533.7257080078125\n",
      "Epoch [96/200] Loss: 530.6806640625\n",
      "Epoch [97/200] Loss: 527.6537475585938\n",
      "Epoch [98/200] Loss: 524.6443481445312\n",
      "Epoch [99/200] Loss: 521.652587890625\n",
      "Epoch [100/200] Loss: 518.6781616210938\n",
      "Epoch [101/200] Loss: 515.720947265625\n",
      "Epoch [102/200] Loss: 512.78076171875\n",
      "Epoch [103/200] Loss: 509.8573913574219\n",
      "Epoch [104/200] Loss: 506.95074462890625\n",
      "Epoch [105/200] Loss: 504.0605163574219\n",
      "Epoch [106/200] Loss: 501.1866760253906\n",
      "Epoch [107/200] Loss: 498.3290710449219\n",
      "Epoch [108/200] Loss: 495.48760986328125\n",
      "Epoch [109/200] Loss: 492.6620788574219\n",
      "Epoch [110/200] Loss: 489.852294921875\n",
      "Epoch [111/200] Loss: 487.0580139160156\n",
      "Epoch [112/200] Loss: 484.27935791015625\n",
      "Epoch [113/200] Loss: 481.51611328125\n",
      "Epoch [114/200] Loss: 478.76800537109375\n",
      "Epoch [115/200] Loss: 476.0350036621094\n",
      "Epoch [116/200] Loss: 473.3170471191406\n",
      "Epoch [117/200] Loss: 470.6139831542969\n",
      "Epoch [118/200] Loss: 467.92547607421875\n",
      "Epoch [119/200] Loss: 465.251708984375\n",
      "Epoch [120/200] Loss: 462.5924377441406\n",
      "Epoch [121/200] Loss: 459.9475402832031\n",
      "Epoch [122/200] Loss: 457.3169250488281\n",
      "Epoch [123/200] Loss: 454.70050048828125\n",
      "Epoch [124/200] Loss: 452.0980529785156\n",
      "Epoch [125/200] Loss: 449.509521484375\n",
      "Epoch [126/200] Loss: 446.9349670410156\n",
      "Epoch [127/200] Loss: 444.3741455078125\n",
      "Epoch [128/200] Loss: 441.82684326171875\n",
      "Epoch [129/200] Loss: 439.293212890625\n",
      "Epoch [130/200] Loss: 436.7730407714844\n",
      "Epoch [131/200] Loss: 434.2660827636719\n",
      "Epoch [132/200] Loss: 431.77252197265625\n",
      "Epoch [133/200] Loss: 429.2921447753906\n",
      "Epoch [134/200] Loss: 426.82476806640625\n",
      "Epoch [135/200] Loss: 424.37054443359375\n",
      "Epoch [136/200] Loss: 421.9290771484375\n",
      "Epoch [137/200] Loss: 419.5004577636719\n",
      "Epoch [138/200] Loss: 417.084716796875\n",
      "Epoch [139/200] Loss: 414.681396484375\n",
      "Epoch [140/200] Loss: 412.2908630371094\n",
      "Epoch [141/200] Loss: 409.9126892089844\n",
      "Epoch [142/200] Loss: 407.5470886230469\n",
      "Epoch [143/200] Loss: 405.1937561035156\n",
      "Epoch [144/200] Loss: 402.8526916503906\n",
      "Epoch [145/200] Loss: 400.52386474609375\n",
      "Epoch [146/200] Loss: 398.20709228515625\n",
      "Epoch [147/200] Loss: 395.9024353027344\n",
      "Epoch [148/200] Loss: 393.60980224609375\n",
      "Epoch [149/200] Loss: 391.32904052734375\n",
      "Epoch [150/200] Loss: 389.0600891113281\n",
      "Epoch [151/200] Loss: 386.8028869628906\n",
      "Epoch [152/200] Loss: 384.5574951171875\n",
      "Epoch [153/200] Loss: 382.32366943359375\n",
      "Epoch [154/200] Loss: 380.1014404296875\n",
      "Epoch [155/200] Loss: 377.8907165527344\n",
      "Epoch [156/200] Loss: 375.69140625\n",
      "Epoch [157/200] Loss: 373.50360107421875\n",
      "Epoch [158/200] Loss: 371.3269348144531\n",
      "Epoch [159/200] Loss: 369.16162109375\n",
      "Epoch [160/200] Loss: 367.00750732421875\n",
      "Epoch [161/200] Loss: 364.8645324707031\n",
      "Epoch [162/200] Loss: 362.73272705078125\n",
      "Epoch [163/200] Loss: 360.61181640625\n",
      "Epoch [164/200] Loss: 358.50189208984375\n",
      "Epoch [165/200] Loss: 356.40301513671875\n",
      "Epoch [166/200] Loss: 354.3149108886719\n",
      "Epoch [167/200] Loss: 352.237548828125\n",
      "Epoch [168/200] Loss: 350.1708679199219\n",
      "Epoch [169/200] Loss: 348.11505126953125\n",
      "Epoch [170/200] Loss: 346.0697937011719\n",
      "Epoch [171/200] Loss: 344.0351257324219\n",
      "Epoch [172/200] Loss: 342.0109558105469\n",
      "Epoch [173/200] Loss: 339.9972229003906\n",
      "Epoch [174/200] Loss: 337.99395751953125\n",
      "Epoch [175/200] Loss: 336.0010681152344\n",
      "Epoch [176/200] Loss: 334.0185852050781\n",
      "Epoch [177/200] Loss: 332.0462646484375\n",
      "Epoch [178/200] Loss: 330.0841979980469\n",
      "Epoch [179/200] Loss: 328.13232421875\n",
      "Epoch [180/200] Loss: 326.19049072265625\n",
      "Epoch [181/200] Loss: 324.2587890625\n",
      "Epoch [182/200] Loss: 322.337158203125\n",
      "Epoch [183/200] Loss: 320.42535400390625\n",
      "Epoch [184/200] Loss: 318.5235900878906\n",
      "Epoch [185/200] Loss: 316.63177490234375\n",
      "Epoch [186/200] Loss: 314.7497253417969\n",
      "Epoch [187/200] Loss: 312.8774108886719\n",
      "Epoch [188/200] Loss: 311.01495361328125\n",
      "Epoch [189/200] Loss: 309.1620788574219\n",
      "Epoch [190/200] Loss: 307.3190002441406\n",
      "Epoch [191/200] Loss: 305.48553466796875\n",
      "Epoch [192/200] Loss: 303.6614990234375\n",
      "Epoch [193/200] Loss: 301.84710693359375\n",
      "Epoch [194/200] Loss: 300.0421142578125\n",
      "Epoch [195/200] Loss: 298.24658203125\n",
      "Epoch [196/200] Loss: 296.4604187011719\n",
      "Epoch [197/200] Loss: 294.6836853027344\n",
      "Epoch [198/200] Loss: 292.91619873046875\n",
      "Epoch [199/200] Loss: 291.15802001953125\n",
      "Epoch [200/200] Loss: 289.40899658203125\n",
      "Predicted days_remaining for parent_id 96: 16.22027587890625\n",
      "Training for parent_id 99...\n",
      "Epoch [1/200] Loss: 126.18824005126953\n",
      "Epoch [2/200] Loss: 121.63845825195312\n",
      "Epoch [3/200] Loss: 117.2190170288086\n",
      "Epoch [4/200] Loss: 112.9324951171875\n",
      "Epoch [5/200] Loss: 108.77680206298828\n",
      "Epoch [6/200] Loss: 104.74674224853516\n",
      "Epoch [7/200] Loss: 100.840576171875\n",
      "Epoch [8/200] Loss: 97.05980682373047\n",
      "Epoch [9/200] Loss: 93.40767669677734\n",
      "Epoch [10/200] Loss: 89.88835144042969\n",
      "Epoch [11/200] Loss: 86.50701141357422\n",
      "Epoch [12/200] Loss: 83.26982116699219\n",
      "Epoch [13/200] Loss: 80.18345642089844\n",
      "Epoch [14/200] Loss: 77.25338745117188\n",
      "Epoch [15/200] Loss: 74.48263549804688\n",
      "Epoch [16/200] Loss: 71.87103271484375\n",
      "Epoch [17/200] Loss: 69.41539001464844\n",
      "Epoch [18/200] Loss: 67.11018371582031\n",
      "Epoch [19/200] Loss: 64.9483413696289\n",
      "Epoch [20/200] Loss: 62.921875\n",
      "Epoch [21/200] Loss: 61.02226257324219\n",
      "Epoch [22/200] Loss: 59.240882873535156\n",
      "Epoch [23/200] Loss: 57.56903076171875\n",
      "Epoch [24/200] Loss: 55.99819564819336\n",
      "Epoch [25/200] Loss: 54.520111083984375\n",
      "Epoch [26/200] Loss: 53.126888275146484\n",
      "Epoch [27/200] Loss: 51.81108474731445\n",
      "Epoch [28/200] Loss: 50.565696716308594\n",
      "Epoch [29/200] Loss: 49.38428497314453\n",
      "Epoch [30/200] Loss: 48.26083755493164\n",
      "Epoch [31/200] Loss: 47.18992614746094\n",
      "Epoch [32/200] Loss: 46.16664123535156\n",
      "Epoch [33/200] Loss: 45.18658447265625\n",
      "Epoch [34/200] Loss: 44.24589920043945\n",
      "Epoch [35/200] Loss: 43.3411865234375\n",
      "Epoch [36/200] Loss: 42.46953201293945\n",
      "Epoch [37/200] Loss: 41.62835693359375\n",
      "Epoch [38/200] Loss: 40.81549835205078\n",
      "Epoch [39/200] Loss: 40.0290412902832\n",
      "Epoch [40/200] Loss: 39.267356872558594\n",
      "Epoch [41/200] Loss: 38.52903366088867\n",
      "Epoch [42/200] Loss: 37.812801361083984\n",
      "Epoch [43/200] Loss: 37.1175537109375\n",
      "Epoch [44/200] Loss: 36.442317962646484\n",
      "Epoch [45/200] Loss: 35.78618621826172\n",
      "Epoch [46/200] Loss: 35.14832305908203\n",
      "Epoch [47/200] Loss: 34.52796173095703\n",
      "Epoch [48/200] Loss: 33.92438888549805\n",
      "Epoch [49/200] Loss: 33.336917877197266\n",
      "Epoch [50/200] Loss: 32.764923095703125\n",
      "Epoch [51/200] Loss: 32.20779800415039\n",
      "Epoch [52/200] Loss: 31.664968490600586\n",
      "Epoch [53/200] Loss: 31.135910034179688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/200] Loss: 30.620115280151367\n",
      "Epoch [55/200] Loss: 30.117145538330078\n",
      "Epoch [56/200] Loss: 29.626554489135742\n",
      "Epoch [57/200] Loss: 29.147972106933594\n",
      "Epoch [58/200] Loss: 28.681058883666992\n",
      "Epoch [59/200] Loss: 28.22551727294922\n",
      "Epoch [60/200] Loss: 27.781082153320312\n",
      "Epoch [61/200] Loss: 27.3475341796875\n",
      "Epoch [62/200] Loss: 26.924671173095703\n",
      "Epoch [63/200] Loss: 26.512346267700195\n",
      "Epoch [64/200] Loss: 26.110397338867188\n",
      "Epoch [65/200] Loss: 25.718711853027344\n",
      "Epoch [66/200] Loss: 25.337162017822266\n",
      "Epoch [67/200] Loss: 24.96563148498535\n",
      "Epoch [68/200] Loss: 24.604005813598633\n",
      "Epoch [69/200] Loss: 24.25215721130371\n",
      "Epoch [70/200] Loss: 23.90995216369629\n",
      "Epoch [71/200] Loss: 23.577259063720703\n",
      "Epoch [72/200] Loss: 23.253925323486328\n",
      "Epoch [73/200] Loss: 22.939800262451172\n",
      "Epoch [74/200] Loss: 22.634735107421875\n",
      "Epoch [75/200] Loss: 22.338550567626953\n",
      "Epoch [76/200] Loss: 22.051082611083984\n",
      "Epoch [77/200] Loss: 21.772174835205078\n",
      "Epoch [78/200] Loss: 21.501644134521484\n",
      "Epoch [79/200] Loss: 21.239328384399414\n",
      "Epoch [80/200] Loss: 20.985044479370117\n",
      "Epoch [81/200] Loss: 20.738636016845703\n",
      "Epoch [82/200] Loss: 20.499919891357422\n",
      "Epoch [83/200] Loss: 20.268735885620117\n",
      "Epoch [84/200] Loss: 20.044902801513672\n",
      "Epoch [85/200] Loss: 19.828266143798828\n",
      "Epoch [86/200] Loss: 19.618640899658203\n",
      "Epoch [87/200] Loss: 19.415878295898438\n",
      "Epoch [88/200] Loss: 19.219799041748047\n",
      "Epoch [89/200] Loss: 19.030242919921875\n",
      "Epoch [90/200] Loss: 18.847049713134766\n",
      "Epoch [91/200] Loss: 18.67005157470703\n",
      "Epoch [92/200] Loss: 18.499101638793945\n",
      "Epoch [93/200] Loss: 18.33403778076172\n",
      "Epoch [94/200] Loss: 18.174697875976562\n",
      "Epoch [95/200] Loss: 18.020936965942383\n",
      "Epoch [96/200] Loss: 17.87259864807129\n",
      "Epoch [97/200] Loss: 17.729537963867188\n",
      "Epoch [98/200] Loss: 17.591602325439453\n",
      "Epoch [99/200] Loss: 17.458650588989258\n",
      "Epoch [100/200] Loss: 17.330541610717773\n",
      "Epoch [101/200] Loss: 17.20712661743164\n",
      "Epoch [102/200] Loss: 17.088289260864258\n",
      "Epoch [103/200] Loss: 16.9738712310791\n",
      "Epoch [104/200] Loss: 16.86374855041504\n",
      "Epoch [105/200] Loss: 16.75779151916504\n",
      "Epoch [106/200] Loss: 16.6558780670166\n",
      "Epoch [107/200] Loss: 16.557865142822266\n",
      "Epoch [108/200] Loss: 16.463647842407227\n",
      "Epoch [109/200] Loss: 16.373096466064453\n",
      "Epoch [110/200] Loss: 16.28609848022461\n",
      "Epoch [111/200] Loss: 16.20253562927246\n",
      "Epoch [112/200] Loss: 16.122297286987305\n",
      "Epoch [113/200] Loss: 16.045272827148438\n",
      "Epoch [114/200] Loss: 15.971351623535156\n",
      "Epoch [115/200] Loss: 15.900435447692871\n",
      "Epoch [116/200] Loss: 15.832414627075195\n",
      "Epoch [117/200] Loss: 15.767200469970703\n",
      "Epoch [118/200] Loss: 15.704684257507324\n",
      "Epoch [119/200] Loss: 15.644773483276367\n",
      "Epoch [120/200] Loss: 15.587377548217773\n",
      "Epoch [121/200] Loss: 15.532413482666016\n",
      "Epoch [122/200] Loss: 15.47978401184082\n",
      "Epoch [123/200] Loss: 15.429412841796875\n",
      "Epoch [124/200] Loss: 15.381211280822754\n",
      "Epoch [125/200] Loss: 15.335102081298828\n",
      "Epoch [126/200] Loss: 15.291004180908203\n",
      "Epoch [127/200] Loss: 15.248844146728516\n",
      "Epoch [128/200] Loss: 15.208551406860352\n",
      "Epoch [129/200] Loss: 15.170055389404297\n",
      "Epoch [130/200] Loss: 15.133275985717773\n",
      "Epoch [131/200] Loss: 15.098159790039062\n",
      "Epoch [132/200] Loss: 15.064634323120117\n",
      "Epoch [133/200] Loss: 15.032642364501953\n",
      "Epoch [134/200] Loss: 15.002115249633789\n",
      "Epoch [135/200] Loss: 14.973003387451172\n",
      "Epoch [136/200] Loss: 14.945241928100586\n",
      "Epoch [137/200] Loss: 14.918782234191895\n",
      "Epoch [138/200] Loss: 14.893566131591797\n",
      "Epoch [139/200] Loss: 14.869544982910156\n",
      "Epoch [140/200] Loss: 14.846668243408203\n",
      "Epoch [141/200] Loss: 14.824888229370117\n",
      "Epoch [142/200] Loss: 14.804161071777344\n",
      "Epoch [143/200] Loss: 14.784438133239746\n",
      "Epoch [144/200] Loss: 14.765678405761719\n",
      "Epoch [145/200] Loss: 14.747838973999023\n",
      "Epoch [146/200] Loss: 14.730881690979004\n",
      "Epoch [147/200] Loss: 14.714765548706055\n",
      "Epoch [148/200] Loss: 14.699459075927734\n",
      "Epoch [149/200] Loss: 14.684919357299805\n",
      "Epoch [150/200] Loss: 14.67111587524414\n",
      "Epoch [151/200] Loss: 14.658012390136719\n",
      "Epoch [152/200] Loss: 14.645584106445312\n",
      "Epoch [153/200] Loss: 14.633793830871582\n",
      "Epoch [154/200] Loss: 14.622613906860352\n",
      "Epoch [155/200] Loss: 14.612015724182129\n",
      "Epoch [156/200] Loss: 14.601972579956055\n",
      "Epoch [157/200] Loss: 14.592458724975586\n",
      "Epoch [158/200] Loss: 14.583450317382812\n",
      "Epoch [159/200] Loss: 14.574921607971191\n",
      "Epoch [160/200] Loss: 14.566850662231445\n",
      "Epoch [161/200] Loss: 14.559215545654297\n",
      "Epoch [162/200] Loss: 14.551992416381836\n",
      "Epoch [163/200] Loss: 14.545164108276367\n",
      "Epoch [164/200] Loss: 14.53870964050293\n",
      "Epoch [165/200] Loss: 14.532611846923828\n",
      "Epoch [166/200] Loss: 14.52685260772705\n",
      "Epoch [167/200] Loss: 14.521413803100586\n",
      "Epoch [168/200] Loss: 14.516281127929688\n",
      "Epoch [169/200] Loss: 14.511436462402344\n",
      "Epoch [170/200] Loss: 14.506868362426758\n",
      "Epoch [171/200] Loss: 14.502558708190918\n",
      "Epoch [172/200] Loss: 14.498497009277344\n",
      "Epoch [173/200] Loss: 14.494670867919922\n",
      "Epoch [174/200] Loss: 14.491065979003906\n",
      "Epoch [175/200] Loss: 14.487668991088867\n",
      "Epoch [176/200] Loss: 14.484476089477539\n",
      "Epoch [177/200] Loss: 14.481468200683594\n",
      "Epoch [178/200] Loss: 14.478639602661133\n",
      "Epoch [179/200] Loss: 14.47597885131836\n",
      "Epoch [180/200] Loss: 14.473478317260742\n",
      "Epoch [181/200] Loss: 14.471126556396484\n",
      "Epoch [182/200] Loss: 14.46891975402832\n",
      "Epoch [183/200] Loss: 14.46684741973877\n",
      "Epoch [184/200] Loss: 14.464900016784668\n",
      "Epoch [185/200] Loss: 14.46307373046875\n",
      "Epoch [186/200] Loss: 14.461360931396484\n",
      "Epoch [187/200] Loss: 14.459755897521973\n",
      "Epoch [188/200] Loss: 14.458248138427734\n",
      "Epoch [189/200] Loss: 14.456836700439453\n",
      "Epoch [190/200] Loss: 14.455516815185547\n",
      "Epoch [191/200] Loss: 14.454279899597168\n",
      "Epoch [192/200] Loss: 14.453121185302734\n",
      "Epoch [193/200] Loss: 14.452037811279297\n",
      "Epoch [194/200] Loss: 14.45102310180664\n",
      "Epoch [195/200] Loss: 14.450075149536133\n",
      "Epoch [196/200] Loss: 14.449188232421875\n",
      "Epoch [197/200] Loss: 14.448360443115234\n",
      "Epoch [198/200] Loss: 14.447588920593262\n",
      "Epoch [199/200] Loss: 14.446866989135742\n",
      "Epoch [200/200] Loss: 14.44619369506836\n",
      "Predicted days_remaining for parent_id 99: 10.66019344329834\n",
      "Training for parent_id 103...\n",
      "Epoch [1/200] Loss: 178.30604553222656\n",
      "Epoch [2/200] Loss: 172.05337524414062\n",
      "Epoch [3/200] Loss: 166.10003662109375\n",
      "Epoch [4/200] Loss: 160.42507934570312\n",
      "Epoch [5/200] Loss: 154.9915313720703\n",
      "Epoch [6/200] Loss: 149.77133178710938\n",
      "Epoch [7/200] Loss: 144.75\n",
      "Epoch [8/200] Loss: 139.92156982421875\n",
      "Epoch [9/200] Loss: 135.2843475341797\n",
      "Epoch [10/200] Loss: 130.8383331298828\n",
      "Epoch [11/200] Loss: 126.58271026611328\n",
      "Epoch [12/200] Loss: 122.51463317871094\n",
      "Epoch [13/200] Loss: 118.62896728515625\n",
      "Epoch [14/200] Loss: 114.91930389404297\n",
      "Epoch [15/200] Loss: 111.3789291381836\n",
      "Epoch [16/200] Loss: 108.00157165527344\n",
      "Epoch [17/200] Loss: 104.78181457519531\n",
      "Epoch [18/200] Loss: 101.71509552001953\n",
      "Epoch [19/200] Loss: 98.7973403930664\n",
      "Epoch [20/200] Loss: 96.02462005615234\n",
      "Epoch [21/200] Loss: 93.39285278320312\n",
      "Epoch [22/200] Loss: 90.8973617553711\n",
      "Epoch [23/200] Loss: 88.53269958496094\n",
      "Epoch [24/200] Loss: 86.29273223876953\n",
      "Epoch [25/200] Loss: 84.17054748535156\n",
      "Epoch [26/200] Loss: 82.15882110595703\n",
      "Epoch [27/200] Loss: 80.25003814697266\n",
      "Epoch [28/200] Loss: 78.43663024902344\n",
      "Epoch [29/200] Loss: 76.71125793457031\n",
      "Epoch [30/200] Loss: 75.06688690185547\n",
      "Epoch [31/200] Loss: 73.49696350097656\n",
      "Epoch [32/200] Loss: 71.99531555175781\n",
      "Epoch [33/200] Loss: 70.55642700195312\n",
      "Epoch [34/200] Loss: 69.17514038085938\n",
      "Epoch [35/200] Loss: 67.84680938720703\n",
      "Epoch [36/200] Loss: 66.56723022460938\n",
      "Epoch [37/200] Loss: 65.3324966430664\n",
      "Epoch [38/200] Loss: 64.1391372680664\n",
      "Epoch [39/200] Loss: 62.983924865722656\n",
      "Epoch [40/200] Loss: 61.863975524902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/200] Loss: 60.7767333984375\n",
      "Epoch [42/200] Loss: 59.71992492675781\n",
      "Epoch [43/200] Loss: 58.6915283203125\n",
      "Epoch [44/200] Loss: 57.689781188964844\n",
      "Epoch [45/200] Loss: 56.713138580322266\n",
      "Epoch [46/200] Loss: 55.76023483276367\n",
      "Epoch [47/200] Loss: 54.82984924316406\n",
      "Epoch [48/200] Loss: 53.92092514038086\n",
      "Epoch [49/200] Loss: 53.032508850097656\n",
      "Epoch [50/200] Loss: 52.16372299194336\n",
      "Epoch [51/200] Loss: 51.31383514404297\n",
      "Epoch [52/200] Loss: 50.48214340209961\n",
      "Epoch [53/200] Loss: 49.66799545288086\n",
      "Epoch [54/200] Loss: 48.870845794677734\n",
      "Epoch [55/200] Loss: 48.09016799926758\n",
      "Epoch [56/200] Loss: 47.32550811767578\n",
      "Epoch [57/200] Loss: 46.5764274597168\n",
      "Epoch [58/200] Loss: 45.842529296875\n",
      "Epoch [59/200] Loss: 45.12348175048828\n",
      "Epoch [60/200] Loss: 44.41891860961914\n",
      "Epoch [61/200] Loss: 43.728538513183594\n",
      "Epoch [62/200] Loss: 43.052059173583984\n",
      "Epoch [63/200] Loss: 42.38924026489258\n",
      "Epoch [64/200] Loss: 41.73978042602539\n",
      "Epoch [65/200] Loss: 41.10344696044922\n",
      "Epoch [66/200] Loss: 40.480003356933594\n",
      "Epoch [67/200] Loss: 39.86919021606445\n",
      "Epoch [68/200] Loss: 39.27081298828125\n",
      "Epoch [69/200] Loss: 38.68461608886719\n",
      "Epoch [70/200] Loss: 38.11042022705078\n",
      "Epoch [71/200] Loss: 37.5479736328125\n",
      "Epoch [72/200] Loss: 36.99708557128906\n",
      "Epoch [73/200] Loss: 36.457523345947266\n",
      "Epoch [74/200] Loss: 35.92913055419922\n",
      "Epoch [75/200] Loss: 35.411659240722656\n",
      "Epoch [76/200] Loss: 34.90495300292969\n",
      "Epoch [77/200] Loss: 34.40876388549805\n",
      "Epoch [78/200] Loss: 33.92294692993164\n",
      "Epoch [79/200] Loss: 33.44730758666992\n",
      "Epoch [80/200] Loss: 32.98166275024414\n",
      "Epoch [81/200] Loss: 32.52580261230469\n",
      "Epoch [82/200] Loss: 32.07959747314453\n",
      "Epoch [83/200] Loss: 31.642845153808594\n",
      "Epoch [84/200] Loss: 31.21538543701172\n",
      "Epoch [85/200] Loss: 30.79705047607422\n",
      "Epoch [86/200] Loss: 30.387657165527344\n",
      "Epoch [87/200] Loss: 29.987071990966797\n",
      "Epoch [88/200] Loss: 29.595115661621094\n",
      "Epoch [89/200] Loss: 29.21163558959961\n",
      "Epoch [90/200] Loss: 28.83649253845215\n",
      "Epoch [91/200] Loss: 28.46951675415039\n",
      "Epoch [92/200] Loss: 28.11057472229004\n",
      "Epoch [93/200] Loss: 27.759504318237305\n",
      "Epoch [94/200] Loss: 27.41617202758789\n",
      "Epoch [95/200] Loss: 27.080429077148438\n",
      "Epoch [96/200] Loss: 26.752145767211914\n",
      "Epoch [97/200] Loss: 26.43117904663086\n",
      "Epoch [98/200] Loss: 26.11740493774414\n",
      "Epoch [99/200] Loss: 25.810665130615234\n",
      "Epoch [100/200] Loss: 25.5108642578125\n",
      "Epoch [101/200] Loss: 25.21783447265625\n",
      "Epoch [102/200] Loss: 24.931486129760742\n",
      "Epoch [103/200] Loss: 24.651670455932617\n",
      "Epoch [104/200] Loss: 24.378271102905273\n",
      "Epoch [105/200] Loss: 24.111167907714844\n",
      "Epoch [106/200] Loss: 23.85024642944336\n",
      "Epoch [107/200] Loss: 23.595382690429688\n",
      "Epoch [108/200] Loss: 23.34645652770996\n",
      "Epoch [109/200] Loss: 23.103370666503906\n",
      "Epoch [110/200] Loss: 22.86599349975586\n",
      "Epoch [111/200] Loss: 22.634225845336914\n",
      "Epoch [112/200] Loss: 22.40795135498047\n",
      "Epoch [113/200] Loss: 22.18708038330078\n",
      "Epoch [114/200] Loss: 21.971481323242188\n",
      "Epoch [115/200] Loss: 21.76108741760254\n",
      "Epoch [116/200] Loss: 21.555763244628906\n",
      "Epoch [117/200] Loss: 21.355417251586914\n",
      "Epoch [118/200] Loss: 21.159957885742188\n",
      "Epoch [119/200] Loss: 20.969282150268555\n",
      "Epoch [120/200] Loss: 20.783288955688477\n",
      "Epoch [121/200] Loss: 20.601884841918945\n",
      "Epoch [122/200] Loss: 20.424989700317383\n",
      "Epoch [123/200] Loss: 20.252500534057617\n",
      "Epoch [124/200] Loss: 20.084320068359375\n",
      "Epoch [125/200] Loss: 19.920373916625977\n",
      "Epoch [126/200] Loss: 19.760568618774414\n",
      "Epoch [127/200] Loss: 19.604816436767578\n",
      "Epoch [128/200] Loss: 19.453031539916992\n",
      "Epoch [129/200] Loss: 19.30513572692871\n",
      "Epoch [130/200] Loss: 19.161046981811523\n",
      "Epoch [131/200] Loss: 19.020679473876953\n",
      "Epoch [132/200] Loss: 18.883955001831055\n",
      "Epoch [133/200] Loss: 18.75078582763672\n",
      "Epoch [134/200] Loss: 18.621116638183594\n",
      "Epoch [135/200] Loss: 18.49485206604004\n",
      "Epoch [136/200] Loss: 18.371931076049805\n",
      "Epoch [137/200] Loss: 18.25227165222168\n",
      "Epoch [138/200] Loss: 18.13580322265625\n",
      "Epoch [139/200] Loss: 18.0224552154541\n",
      "Epoch [140/200] Loss: 17.91216468811035\n",
      "Epoch [141/200] Loss: 17.80484390258789\n",
      "Epoch [142/200] Loss: 17.70044708251953\n",
      "Epoch [143/200] Loss: 17.59889793395996\n",
      "Epoch [144/200] Loss: 17.500133514404297\n",
      "Epoch [145/200] Loss: 17.404085159301758\n",
      "Epoch [146/200] Loss: 17.310701370239258\n",
      "Epoch [147/200] Loss: 17.21990203857422\n",
      "Epoch [148/200] Loss: 17.131649017333984\n",
      "Epoch [149/200] Loss: 17.045862197875977\n",
      "Epoch [150/200] Loss: 16.96249771118164\n",
      "Epoch [151/200] Loss: 16.881488800048828\n",
      "Epoch [152/200] Loss: 16.802783966064453\n",
      "Epoch [153/200] Loss: 16.72632598876953\n",
      "Epoch [154/200] Loss: 16.652057647705078\n",
      "Epoch [155/200] Loss: 16.57993507385254\n",
      "Epoch [156/200] Loss: 16.509899139404297\n",
      "Epoch [157/200] Loss: 16.44189453125\n",
      "Epoch [158/200] Loss: 16.375873565673828\n",
      "Epoch [159/200] Loss: 16.311796188354492\n",
      "Epoch [160/200] Loss: 16.249603271484375\n",
      "Epoch [161/200] Loss: 16.18924331665039\n",
      "Epoch [162/200] Loss: 16.130685806274414\n",
      "Epoch [163/200] Loss: 16.073877334594727\n",
      "Epoch [164/200] Loss: 16.018766403198242\n",
      "Epoch [165/200] Loss: 15.96531867980957\n",
      "Epoch [166/200] Loss: 15.91348934173584\n",
      "Epoch [167/200] Loss: 15.863229751586914\n",
      "Epoch [168/200] Loss: 15.814508438110352\n",
      "Epoch [169/200] Loss: 15.767277717590332\n",
      "Epoch [170/200] Loss: 15.721500396728516\n",
      "Epoch [171/200] Loss: 15.67713737487793\n",
      "Epoch [172/200] Loss: 15.634154319763184\n",
      "Epoch [173/200] Loss: 15.592509269714355\n",
      "Epoch [174/200] Loss: 15.552167892456055\n",
      "Epoch [175/200] Loss: 15.513093948364258\n",
      "Epoch [176/200] Loss: 15.475255966186523\n",
      "Epoch [177/200] Loss: 15.438619613647461\n",
      "Epoch [178/200] Loss: 15.403148651123047\n",
      "Epoch [179/200] Loss: 15.36881160736084\n",
      "Epoch [180/200] Loss: 15.335578918457031\n",
      "Epoch [181/200] Loss: 15.303415298461914\n",
      "Epoch [182/200] Loss: 15.272294998168945\n",
      "Epoch [183/200] Loss: 15.242188453674316\n",
      "Epoch [184/200] Loss: 15.213066101074219\n",
      "Epoch [185/200] Loss: 15.184893608093262\n",
      "Epoch [186/200] Loss: 15.157655715942383\n",
      "Epoch [187/200] Loss: 15.131314277648926\n",
      "Epoch [188/200] Loss: 15.105847358703613\n",
      "Epoch [189/200] Loss: 15.081228256225586\n",
      "Epoch [190/200] Loss: 15.057437896728516\n",
      "Epoch [191/200] Loss: 15.034444808959961\n",
      "Epoch [192/200] Loss: 15.012226104736328\n",
      "Epoch [193/200] Loss: 14.990764617919922\n",
      "Epoch [194/200] Loss: 14.970032691955566\n",
      "Epoch [195/200] Loss: 14.950004577636719\n",
      "Epoch [196/200] Loss: 14.930665969848633\n",
      "Epoch [197/200] Loss: 14.911994934082031\n",
      "Epoch [198/200] Loss: 14.89396858215332\n",
      "Epoch [199/200] Loss: 14.876567840576172\n",
      "Epoch [200/200] Loss: 14.859774589538574\n",
      "Predicted days_remaining for parent_id 103: 12.11276626586914\n",
      "Training for parent_id 105...\n",
      "Epoch [1/200] Loss: 294.0753479003906\n",
      "Epoch [2/200] Loss: 285.66259765625\n",
      "Epoch [3/200] Loss: 277.4764404296875\n",
      "Epoch [4/200] Loss: 269.5838928222656\n",
      "Epoch [5/200] Loss: 262.0202331542969\n",
      "Epoch [6/200] Loss: 254.7999725341797\n",
      "Epoch [7/200] Loss: 247.91912841796875\n",
      "Epoch [8/200] Loss: 241.36044311523438\n",
      "Epoch [9/200] Loss: 235.10379028320312\n",
      "Epoch [10/200] Loss: 229.13455200195312\n",
      "Epoch [11/200] Loss: 223.4445037841797\n",
      "Epoch [12/200] Loss: 218.0281219482422\n",
      "Epoch [13/200] Loss: 212.87884521484375\n",
      "Epoch [14/200] Loss: 207.98886108398438\n",
      "Epoch [15/200] Loss: 203.34942626953125\n",
      "Epoch [16/200] Loss: 198.95086669921875\n",
      "Epoch [17/200] Loss: 194.7828369140625\n",
      "Epoch [18/200] Loss: 190.8342742919922\n",
      "Epoch [19/200] Loss: 187.09378051757812\n",
      "Epoch [20/200] Loss: 183.549560546875\n",
      "Epoch [21/200] Loss: 180.18954467773438\n",
      "Epoch [22/200] Loss: 177.00123596191406\n",
      "Epoch [23/200] Loss: 173.9715576171875\n",
      "Epoch [24/200] Loss: 171.087158203125\n",
      "Epoch [25/200] Loss: 168.33445739746094\n",
      "Epoch [26/200] Loss: 165.70005798339844\n",
      "Epoch [27/200] Loss: 163.1710205078125\n",
      "Epoch [28/200] Loss: 160.7352752685547\n",
      "Epoch [29/200] Loss: 158.38214111328125\n",
      "Epoch [30/200] Loss: 156.10235595703125\n",
      "Epoch [31/200] Loss: 153.88853454589844\n",
      "Epoch [32/200] Loss: 151.7349395751953\n",
      "Epoch [33/200] Loss: 149.6374969482422\n",
      "Epoch [34/200] Loss: 147.5934295654297\n",
      "Epoch [35/200] Loss: 145.60064697265625\n",
      "Epoch [36/200] Loss: 143.65744018554688\n",
      "Epoch [37/200] Loss: 141.76205444335938\n",
      "Epoch [38/200] Loss: 139.91238403320312\n",
      "Epoch [39/200] Loss: 138.10586547851562\n",
      "Epoch [40/200] Loss: 136.3397979736328\n",
      "Epoch [41/200] Loss: 134.611083984375\n",
      "Epoch [42/200] Loss: 132.9169464111328\n",
      "Epoch [43/200] Loss: 131.25454711914062\n",
      "Epoch [44/200] Loss: 129.62152099609375\n",
      "Epoch [45/200] Loss: 128.01585388183594\n",
      "Epoch [46/200] Loss: 126.43577575683594\n",
      "Epoch [47/200] Loss: 124.87996673583984\n",
      "Epoch [48/200] Loss: 123.3472900390625\n",
      "Epoch [49/200] Loss: 121.83687591552734\n",
      "Epoch [50/200] Loss: 120.34796905517578\n",
      "Epoch [51/200] Loss: 118.87992095947266\n",
      "Epoch [52/200] Loss: 117.43215942382812\n",
      "Epoch [53/200] Loss: 116.00410461425781\n",
      "Epoch [54/200] Loss: 114.59518432617188\n",
      "Epoch [55/200] Loss: 113.20489501953125\n",
      "Epoch [56/200] Loss: 111.83273315429688\n",
      "Epoch [57/200] Loss: 110.47821807861328\n",
      "Epoch [58/200] Loss: 109.14091491699219\n",
      "Epoch [59/200] Loss: 107.82040405273438\n",
      "Epoch [60/200] Loss: 106.516357421875\n",
      "Epoch [61/200] Loss: 105.22846221923828\n",
      "Epoch [62/200] Loss: 103.9564208984375\n",
      "Epoch [63/200] Loss: 102.70001983642578\n",
      "Epoch [64/200] Loss: 101.458984375\n",
      "Epoch [65/200] Loss: 100.23310852050781\n",
      "Epoch [66/200] Loss: 99.02225494384766\n",
      "Epoch [67/200] Loss: 97.8261947631836\n",
      "Epoch [68/200] Loss: 96.64471435546875\n",
      "Epoch [69/200] Loss: 95.47770690917969\n",
      "Epoch [70/200] Loss: 94.32496643066406\n",
      "Epoch [71/200] Loss: 93.18638610839844\n",
      "Epoch [72/200] Loss: 92.06175994873047\n",
      "Epoch [73/200] Loss: 90.95097351074219\n",
      "Epoch [74/200] Loss: 89.8538589477539\n",
      "Epoch [75/200] Loss: 88.77029418945312\n",
      "Epoch [76/200] Loss: 87.70011901855469\n",
      "Epoch [77/200] Loss: 86.6432113647461\n",
      "Epoch [78/200] Loss: 85.59941101074219\n",
      "Epoch [79/200] Loss: 84.56859588623047\n",
      "Epoch [80/200] Loss: 83.55064392089844\n",
      "Epoch [81/200] Loss: 82.54540252685547\n",
      "Epoch [82/200] Loss: 81.55276489257812\n",
      "Epoch [83/200] Loss: 80.57254791259766\n",
      "Epoch [84/200] Loss: 79.60467529296875\n",
      "Epoch [85/200] Loss: 78.64897155761719\n",
      "Epoch [86/200] Loss: 77.70536804199219\n",
      "Epoch [87/200] Loss: 76.7737045288086\n",
      "Epoch [88/200] Loss: 75.85381317138672\n",
      "Epoch [89/200] Loss: 74.94559478759766\n",
      "Epoch [90/200] Loss: 74.0489730834961\n",
      "Epoch [91/200] Loss: 73.16374969482422\n",
      "Epoch [92/200] Loss: 72.28982543945312\n",
      "Epoch [93/200] Loss: 71.42708587646484\n",
      "Epoch [94/200] Loss: 70.57537078857422\n",
      "Epoch [95/200] Loss: 69.73458099365234\n",
      "Epoch [96/200] Loss: 68.90463256835938\n",
      "Epoch [97/200] Loss: 68.08534240722656\n",
      "Epoch [98/200] Loss: 67.27661895751953\n",
      "Epoch [99/200] Loss: 66.4783706665039\n",
      "Epoch [100/200] Loss: 65.69041442871094\n",
      "Epoch [101/200] Loss: 64.91268157958984\n",
      "Epoch [102/200] Loss: 64.1450424194336\n",
      "Epoch [103/200] Loss: 63.38737487792969\n",
      "Epoch [104/200] Loss: 62.63959884643555\n",
      "Epoch [105/200] Loss: 61.901573181152344\n",
      "Epoch [106/200] Loss: 61.173194885253906\n",
      "Epoch [107/200] Loss: 60.45436096191406\n",
      "Epoch [108/200] Loss: 59.74495315551758\n",
      "Epoch [109/200] Loss: 59.04486846923828\n",
      "Epoch [110/200] Loss: 58.35401916503906\n",
      "Epoch [111/200] Loss: 57.672271728515625\n",
      "Epoch [112/200] Loss: 56.999568939208984\n",
      "Epoch [113/200] Loss: 56.33573532104492\n",
      "Epoch [114/200] Loss: 55.68074417114258\n",
      "Epoch [115/200] Loss: 55.03445053100586\n",
      "Epoch [116/200] Loss: 54.396793365478516\n",
      "Epoch [117/200] Loss: 53.76763916015625\n",
      "Epoch [118/200] Loss: 53.146907806396484\n",
      "Epoch [119/200] Loss: 52.53451156616211\n",
      "Epoch [120/200] Loss: 51.93034362792969\n",
      "Epoch [121/200] Loss: 51.334327697753906\n",
      "Epoch [122/200] Loss: 50.746376037597656\n",
      "Epoch [123/200] Loss: 50.16637420654297\n",
      "Epoch [124/200] Loss: 49.5942268371582\n",
      "Epoch [125/200] Loss: 49.02986145019531\n",
      "Epoch [126/200] Loss: 48.47320556640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127/200] Loss: 47.9241828918457\n",
      "Epoch [128/200] Loss: 47.38264846801758\n",
      "Epoch [129/200] Loss: 46.848575592041016\n",
      "Epoch [130/200] Loss: 46.321834564208984\n",
      "Epoch [131/200] Loss: 45.80238342285156\n",
      "Epoch [132/200] Loss: 45.290122985839844\n",
      "Epoch [133/200] Loss: 44.784976959228516\n",
      "Epoch [134/200] Loss: 44.28683090209961\n",
      "Epoch [135/200] Loss: 43.795654296875\n",
      "Epoch [136/200] Loss: 43.311344146728516\n",
      "Epoch [137/200] Loss: 42.833824157714844\n",
      "Epoch [138/200] Loss: 42.3630256652832\n",
      "Epoch [139/200] Loss: 41.89885711669922\n",
      "Epoch [140/200] Loss: 41.44124221801758\n",
      "Epoch [141/200] Loss: 40.99013137817383\n",
      "Epoch [142/200] Loss: 40.54541778564453\n",
      "Epoch [143/200] Loss: 40.107059478759766\n",
      "Epoch [144/200] Loss: 39.674949645996094\n",
      "Epoch [145/200] Loss: 39.24903869628906\n",
      "Epoch [146/200] Loss: 38.829254150390625\n",
      "Epoch [147/200] Loss: 38.415531158447266\n",
      "Epoch [148/200] Loss: 38.00775909423828\n",
      "Epoch [149/200] Loss: 37.605918884277344\n",
      "Epoch [150/200] Loss: 37.20991516113281\n",
      "Epoch [151/200] Loss: 36.81968307495117\n",
      "Epoch [152/200] Loss: 36.43515396118164\n",
      "Epoch [153/200] Loss: 36.0562744140625\n",
      "Epoch [154/200] Loss: 35.68295669555664\n",
      "Epoch [155/200] Loss: 35.315155029296875\n",
      "Epoch [156/200] Loss: 34.95277786254883\n",
      "Epoch [157/200] Loss: 34.59579086303711\n",
      "Epoch [158/200] Loss: 34.24411392211914\n",
      "Epoch [159/200] Loss: 33.89767837524414\n",
      "Epoch [160/200] Loss: 33.55644607543945\n",
      "Epoch [161/200] Loss: 33.22031021118164\n",
      "Epoch [162/200] Loss: 32.88925552368164\n",
      "Epoch [163/200] Loss: 32.56318283081055\n",
      "Epoch [164/200] Loss: 32.242061614990234\n",
      "Epoch [165/200] Loss: 31.925811767578125\n",
      "Epoch [166/200] Loss: 31.6143798828125\n",
      "Epoch [167/200] Loss: 31.307720184326172\n",
      "Epoch [168/200] Loss: 31.005760192871094\n",
      "Epoch [169/200] Loss: 30.70842742919922\n",
      "Epoch [170/200] Loss: 30.415712356567383\n",
      "Epoch [171/200] Loss: 30.12748908996582\n",
      "Epoch [172/200] Loss: 29.8437557220459\n",
      "Epoch [173/200] Loss: 29.564428329467773\n",
      "Epoch [174/200] Loss: 29.289478302001953\n",
      "Epoch [175/200] Loss: 29.01882553100586\n",
      "Epoch [176/200] Loss: 28.75242042541504\n",
      "Epoch [177/200] Loss: 28.490201950073242\n",
      "Epoch [178/200] Loss: 28.23214340209961\n",
      "Epoch [179/200] Loss: 27.97817039489746\n",
      "Epoch [180/200] Loss: 27.72823143005371\n",
      "Epoch [181/200] Loss: 27.48227310180664\n",
      "Epoch [182/200] Loss: 27.240272521972656\n",
      "Epoch [183/200] Loss: 27.002140045166016\n",
      "Epoch [184/200] Loss: 26.76784324645996\n",
      "Epoch [185/200] Loss: 26.53731918334961\n",
      "Epoch [186/200] Loss: 26.310537338256836\n",
      "Epoch [187/200] Loss: 26.08743667602539\n",
      "Epoch [188/200] Loss: 25.867963790893555\n",
      "Epoch [189/200] Loss: 25.6520938873291\n",
      "Epoch [190/200] Loss: 25.439741134643555\n",
      "Epoch [191/200] Loss: 25.230899810791016\n",
      "Epoch [192/200] Loss: 25.02549934387207\n",
      "Epoch [193/200] Loss: 24.823486328125\n",
      "Epoch [194/200] Loss: 24.624826431274414\n",
      "Epoch [195/200] Loss: 24.42948341369629\n",
      "Epoch [196/200] Loss: 24.237390518188477\n",
      "Epoch [197/200] Loss: 24.04851722717285\n",
      "Epoch [198/200] Loss: 23.862821578979492\n",
      "Epoch [199/200] Loss: 23.680252075195312\n",
      "Epoch [200/200] Loss: 23.500761032104492\n",
      "Predicted days_remaining for parent_id 105: 13.768922805786133\n",
      "Training for parent_id 107...\n",
      "Epoch [1/200] Loss: 124.67933654785156\n",
      "Epoch [2/200] Loss: 120.33061218261719\n",
      "Epoch [3/200] Loss: 116.07633209228516\n",
      "Epoch [4/200] Loss: 111.9292221069336\n",
      "Epoch [5/200] Loss: 107.90567779541016\n",
      "Epoch [6/200] Loss: 104.02217864990234\n",
      "Epoch [7/200] Loss: 100.29080200195312\n",
      "Epoch [8/200] Loss: 96.71759033203125\n",
      "Epoch [9/200] Loss: 93.30226135253906\n",
      "Epoch [10/200] Loss: 90.03976440429688\n",
      "Epoch [11/200] Loss: 86.92315673828125\n",
      "Epoch [12/200] Loss: 83.9460678100586\n",
      "Epoch [13/200] Loss: 81.10367584228516\n",
      "Epoch [14/200] Loss: 78.3926773071289\n",
      "Epoch [15/200] Loss: 75.81072998046875\n",
      "Epoch [16/200] Loss: 73.35589599609375\n",
      "Epoch [17/200] Loss: 71.02615356445312\n",
      "Epoch [18/200] Loss: 68.81878662109375\n",
      "Epoch [19/200] Loss: 66.73014068603516\n",
      "Epoch [20/200] Loss: 64.75552368164062\n",
      "Epoch [21/200] Loss: 62.88903045654297\n",
      "Epoch [22/200] Loss: 61.12403106689453\n",
      "Epoch [23/200] Loss: 59.45326232910156\n",
      "Epoch [24/200] Loss: 57.869258880615234\n",
      "Epoch [25/200] Loss: 56.36468505859375\n",
      "Epoch [26/200] Loss: 54.9326057434082\n",
      "Epoch [27/200] Loss: 53.566715240478516\n",
      "Epoch [28/200] Loss: 52.26142883300781\n",
      "Epoch [29/200] Loss: 51.01198959350586\n",
      "Epoch [30/200] Loss: 49.81439208984375\n",
      "Epoch [31/200] Loss: 48.6652946472168\n",
      "Epoch [32/200] Loss: 47.56184387207031\n",
      "Epoch [33/200] Loss: 46.5015983581543\n",
      "Epoch [34/200] Loss: 45.4822883605957\n",
      "Epoch [35/200] Loss: 44.50180435180664\n",
      "Epoch [36/200] Loss: 43.55805969238281\n",
      "Epoch [37/200] Loss: 42.64903259277344\n",
      "Epoch [38/200] Loss: 41.772682189941406\n",
      "Epoch [39/200] Loss: 40.927040100097656\n",
      "Epoch [40/200] Loss: 40.11024475097656\n",
      "Epoch [41/200] Loss: 39.320526123046875\n",
      "Epoch [42/200] Loss: 38.55622863769531\n",
      "Epoch [43/200] Loss: 37.815895080566406\n",
      "Epoch [44/200] Loss: 37.0981559753418\n",
      "Epoch [45/200] Loss: 36.401824951171875\n",
      "Epoch [46/200] Loss: 35.72584915161133\n",
      "Epoch [47/200] Loss: 35.06929016113281\n",
      "Epoch [48/200] Loss: 34.43133544921875\n",
      "Epoch [49/200] Loss: 33.811256408691406\n",
      "Epoch [50/200] Loss: 33.20842361450195\n",
      "Epoch [51/200] Loss: 32.62227249145508\n",
      "Epoch [52/200] Loss: 32.05228805541992\n",
      "Epoch [53/200] Loss: 31.498016357421875\n",
      "Epoch [54/200] Loss: 30.95903968811035\n",
      "Epoch [55/200] Loss: 30.434961318969727\n",
      "Epoch [56/200] Loss: 29.925437927246094\n",
      "Epoch [57/200] Loss: 29.430089950561523\n",
      "Epoch [58/200] Loss: 28.948596954345703\n",
      "Epoch [59/200] Loss: 28.48064422607422\n",
      "Epoch [60/200] Loss: 28.025894165039062\n",
      "Epoch [61/200] Loss: 27.584054946899414\n",
      "Epoch [62/200] Loss: 27.15480613708496\n",
      "Epoch [63/200] Loss: 26.737844467163086\n",
      "Epoch [64/200] Loss: 26.3328857421875\n",
      "Epoch [65/200] Loss: 25.939624786376953\n",
      "Epoch [66/200] Loss: 25.557777404785156\n",
      "Epoch [67/200] Loss: 25.187063217163086\n",
      "Epoch [68/200] Loss: 24.827190399169922\n",
      "Epoch [69/200] Loss: 24.4778995513916\n",
      "Epoch [70/200] Loss: 24.13890838623047\n",
      "Epoch [71/200] Loss: 23.809953689575195\n",
      "Epoch [72/200] Loss: 23.490785598754883\n",
      "Epoch [73/200] Loss: 23.181142807006836\n",
      "Epoch [74/200] Loss: 22.88077735900879\n",
      "Epoch [75/200] Loss: 22.589458465576172\n",
      "Epoch [76/200] Loss: 22.306936264038086\n",
      "Epoch [77/200] Loss: 22.03297233581543\n",
      "Epoch [78/200] Loss: 21.767356872558594\n",
      "Epoch [79/200] Loss: 21.509868621826172\n",
      "Epoch [80/200] Loss: 21.260278701782227\n",
      "Epoch [81/200] Loss: 21.018402099609375\n",
      "Epoch [82/200] Loss: 20.783998489379883\n",
      "Epoch [83/200] Loss: 20.55689811706543\n",
      "Epoch [84/200] Loss: 20.33689308166504\n",
      "Epoch [85/200] Loss: 20.123788833618164\n",
      "Epoch [86/200] Loss: 19.91741371154785\n",
      "Epoch [87/200] Loss: 19.717573165893555\n",
      "Epoch [88/200] Loss: 19.52409553527832\n",
      "Epoch [89/200] Loss: 19.336803436279297\n",
      "Epoch [90/200] Loss: 19.155534744262695\n",
      "Epoch [91/200] Loss: 18.98012351989746\n",
      "Epoch [92/200] Loss: 18.810409545898438\n",
      "Epoch [93/200] Loss: 18.64623260498047\n",
      "Epoch [94/200] Loss: 18.487436294555664\n",
      "Epoch [95/200] Loss: 18.333881378173828\n",
      "Epoch [96/200] Loss: 18.18541717529297\n",
      "Epoch [97/200] Loss: 18.04189682006836\n",
      "Epoch [98/200] Loss: 17.903186798095703\n",
      "Epoch [99/200] Loss: 17.769145965576172\n",
      "Epoch [100/200] Loss: 17.6396541595459\n",
      "Epoch [101/200] Loss: 17.51456069946289\n",
      "Epoch [102/200] Loss: 17.393756866455078\n",
      "Epoch [103/200] Loss: 17.2771053314209\n",
      "Epoch [104/200] Loss: 17.164493560791016\n",
      "Epoch [105/200] Loss: 17.055809020996094\n",
      "Epoch [106/200] Loss: 16.950925827026367\n",
      "Epoch [107/200] Loss: 16.8497314453125\n",
      "Epoch [108/200] Loss: 16.752120971679688\n",
      "Epoch [109/200] Loss: 16.657989501953125\n",
      "Epoch [110/200] Loss: 16.567232131958008\n",
      "Epoch [111/200] Loss: 16.479740142822266\n",
      "Epoch [112/200] Loss: 16.395416259765625\n",
      "Epoch [113/200] Loss: 16.314163208007812\n",
      "Epoch [114/200] Loss: 16.235891342163086\n",
      "Epoch [115/200] Loss: 16.160511016845703\n",
      "Epoch [116/200] Loss: 16.08791160583496\n",
      "Epoch [117/200] Loss: 16.018028259277344\n",
      "Epoch [118/200] Loss: 15.950763702392578\n",
      "Epoch [119/200] Loss: 15.886035919189453\n",
      "Epoch [120/200] Loss: 15.823760986328125\n",
      "Epoch [121/200] Loss: 15.763866424560547\n",
      "Epoch [122/200] Loss: 15.706270217895508\n",
      "Epoch [123/200] Loss: 15.650897026062012\n",
      "Epoch [124/200] Loss: 15.597674369812012\n",
      "Epoch [125/200] Loss: 15.546531677246094\n",
      "Epoch [126/200] Loss: 15.497396469116211\n",
      "Epoch [127/200] Loss: 15.450202941894531\n",
      "Epoch [128/200] Loss: 15.404885292053223\n",
      "Epoch [129/200] Loss: 15.361376762390137\n",
      "Epoch [130/200] Loss: 15.319622039794922\n",
      "Epoch [131/200] Loss: 15.279552459716797\n",
      "Epoch [132/200] Loss: 15.241113662719727\n",
      "Epoch [133/200] Loss: 15.204248428344727\n",
      "Epoch [134/200] Loss: 15.16889762878418\n",
      "Epoch [135/200] Loss: 15.135008811950684\n",
      "Epoch [136/200] Loss: 15.102530479431152\n",
      "Epoch [137/200] Loss: 15.071412086486816\n",
      "Epoch [138/200] Loss: 15.041604995727539\n",
      "Epoch [139/200] Loss: 15.013059616088867\n",
      "Epoch [140/200] Loss: 14.98572826385498\n",
      "Epoch [141/200] Loss: 14.959566116333008\n",
      "Epoch [142/200] Loss: 14.934531211853027\n",
      "Epoch [143/200] Loss: 14.910581588745117\n",
      "Epoch [144/200] Loss: 14.887672424316406\n",
      "Epoch [145/200] Loss: 14.865767478942871\n",
      "Epoch [146/200] Loss: 14.844826698303223\n",
      "Epoch [147/200] Loss: 14.824813842773438\n",
      "Epoch [148/200] Loss: 14.80569076538086\n",
      "Epoch [149/200] Loss: 14.787425994873047\n",
      "Epoch [150/200] Loss: 14.769981384277344\n",
      "Epoch [151/200] Loss: 14.753328323364258\n",
      "Epoch [152/200] Loss: 14.737432479858398\n",
      "Epoch [153/200] Loss: 14.722265243530273\n",
      "Epoch [154/200] Loss: 14.707796096801758\n",
      "Epoch [155/200] Loss: 14.69399642944336\n",
      "Epoch [156/200] Loss: 14.680834770202637\n",
      "Epoch [157/200] Loss: 14.668294906616211\n",
      "Epoch [158/200] Loss: 14.656341552734375\n",
      "Epoch [159/200] Loss: 14.644952774047852\n",
      "Epoch [160/200] Loss: 14.634106636047363\n",
      "Epoch [161/200] Loss: 14.623779296875\n",
      "Epoch [162/200] Loss: 14.613948822021484\n",
      "Epoch [163/200] Loss: 14.604591369628906\n",
      "Epoch [164/200] Loss: 14.59568977355957\n",
      "Epoch [165/200] Loss: 14.5872220993042\n",
      "Epoch [166/200] Loss: 14.579172134399414\n",
      "Epoch [167/200] Loss: 14.571516036987305\n",
      "Epoch [168/200] Loss: 14.564241409301758\n",
      "Epoch [169/200] Loss: 14.557331085205078\n",
      "Epoch [170/200] Loss: 14.550765991210938\n",
      "Epoch [171/200] Loss: 14.544530868530273\n",
      "Epoch [172/200] Loss: 14.538612365722656\n",
      "Epoch [173/200] Loss: 14.532995223999023\n",
      "Epoch [174/200] Loss: 14.527666091918945\n",
      "Epoch [175/200] Loss: 14.522611618041992\n",
      "Epoch [176/200] Loss: 14.517816543579102\n",
      "Epoch [177/200] Loss: 14.513272285461426\n",
      "Epoch [178/200] Loss: 14.508966445922852\n",
      "Epoch [179/200] Loss: 14.5048828125\n",
      "Epoch [180/200] Loss: 14.501018524169922\n",
      "Epoch [181/200] Loss: 14.497357368469238\n",
      "Epoch [182/200] Loss: 14.493892669677734\n",
      "Epoch [183/200] Loss: 14.490612030029297\n",
      "Epoch [184/200] Loss: 14.487509727478027\n",
      "Epoch [185/200] Loss: 14.484575271606445\n",
      "Epoch [186/200] Loss: 14.481799125671387\n",
      "Epoch [187/200] Loss: 14.479175567626953\n",
      "Epoch [188/200] Loss: 14.476696968078613\n",
      "Epoch [189/200] Loss: 14.474353790283203\n",
      "Epoch [190/200] Loss: 14.47214126586914\n",
      "Epoch [191/200] Loss: 14.470052719116211\n",
      "Epoch [192/200] Loss: 14.468080520629883\n",
      "Epoch [193/200] Loss: 14.466218948364258\n",
      "Epoch [194/200] Loss: 14.464463233947754\n",
      "Epoch [195/200] Loss: 14.462808609008789\n",
      "Epoch [196/200] Loss: 14.461247444152832\n",
      "Epoch [197/200] Loss: 14.459775924682617\n",
      "Epoch [198/200] Loss: 14.458389282226562\n",
      "Epoch [199/200] Loss: 14.457083702087402\n",
      "Epoch [200/200] Loss: 14.455854415893555\n",
      "Predicted days_remaining for parent_id 107: 10.618864059448242\n",
      "Training for parent_id 113...\n",
      "Epoch [1/200] Loss: 620.9312133789062\n",
      "Epoch [2/200] Loss: 609.5828247070312\n",
      "Epoch [3/200] Loss: 598.6100463867188\n",
      "Epoch [4/200] Loss: 588.0322265625\n",
      "Epoch [5/200] Loss: 577.8248291015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200] Loss: 567.9608154296875\n",
      "Epoch [7/200] Loss: 558.4288330078125\n",
      "Epoch [8/200] Loss: 549.2296142578125\n",
      "Epoch [9/200] Loss: 540.3655395507812\n",
      "Epoch [10/200] Loss: 531.8348999023438\n",
      "Epoch [11/200] Loss: 523.631103515625\n",
      "Epoch [12/200] Loss: 515.7449951171875\n",
      "Epoch [13/200] Loss: 508.1662902832031\n",
      "Epoch [14/200] Loss: 500.88433837890625\n",
      "Epoch [15/200] Loss: 493.8895263671875\n",
      "Epoch [16/200] Loss: 487.173095703125\n",
      "Epoch [17/200] Loss: 480.7275695800781\n",
      "Epoch [18/200] Loss: 474.5459289550781\n",
      "Epoch [19/200] Loss: 468.6207275390625\n",
      "Epoch [20/200] Loss: 462.9441833496094\n",
      "Epoch [21/200] Loss: 457.5076904296875\n",
      "Epoch [22/200] Loss: 452.3011779785156\n",
      "Epoch [23/200] Loss: 447.31329345703125\n",
      "Epoch [24/200] Loss: 442.53118896484375\n",
      "Epoch [25/200] Loss: 437.9404602050781\n",
      "Epoch [26/200] Loss: 433.52630615234375\n",
      "Epoch [27/200] Loss: 429.273681640625\n",
      "Epoch [28/200] Loss: 425.1680908203125\n",
      "Epoch [29/200] Loss: 421.19598388671875\n",
      "Epoch [30/200] Loss: 417.34490966796875\n",
      "Epoch [31/200] Loss: 413.6034851074219\n",
      "Epoch [32/200] Loss: 409.9617004394531\n",
      "Epoch [33/200] Loss: 406.4103088378906\n",
      "Epoch [34/200] Loss: 402.9408874511719\n",
      "Epoch [35/200] Loss: 399.54595947265625\n",
      "Epoch [36/200] Loss: 396.2189636230469\n",
      "Epoch [37/200] Loss: 392.9541931152344\n",
      "Epoch [38/200] Loss: 389.7467346191406\n",
      "Epoch [39/200] Loss: 386.5921630859375\n",
      "Epoch [40/200] Loss: 383.48663330078125\n",
      "Epoch [41/200] Loss: 380.42669677734375\n",
      "Epoch [42/200] Loss: 377.4090881347656\n",
      "Epoch [43/200] Loss: 374.4309387207031\n",
      "Epoch [44/200] Loss: 371.4897766113281\n",
      "Epoch [45/200] Loss: 368.58319091796875\n",
      "Epoch [46/200] Loss: 365.7093811035156\n",
      "Epoch [47/200] Loss: 362.8665466308594\n",
      "Epoch [48/200] Loss: 360.0530700683594\n",
      "Epoch [49/200] Loss: 357.2677001953125\n",
      "Epoch [50/200] Loss: 354.5090026855469\n",
      "Epoch [51/200] Loss: 351.77581787109375\n",
      "Epoch [52/200] Loss: 349.0667724609375\n",
      "Epoch [53/200] Loss: 346.3805847167969\n",
      "Epoch [54/200] Loss: 343.7159118652344\n",
      "Epoch [55/200] Loss: 341.07159423828125\n",
      "Epoch [56/200] Loss: 338.4464416503906\n",
      "Epoch [57/200] Loss: 335.839599609375\n",
      "Epoch [58/200] Loss: 333.2502136230469\n",
      "Epoch [59/200] Loss: 330.6780700683594\n",
      "Epoch [60/200] Loss: 328.1227111816406\n",
      "Epoch [61/200] Loss: 325.5841369628906\n",
      "Epoch [62/200] Loss: 323.0623779296875\n",
      "Epoch [63/200] Loss: 320.5576477050781\n",
      "Epoch [64/200] Loss: 318.0701599121094\n",
      "Epoch [65/200] Loss: 315.5999755859375\n",
      "Epoch [66/200] Loss: 313.14739990234375\n",
      "Epoch [67/200] Loss: 310.7124328613281\n",
      "Epoch [68/200] Loss: 308.2953796386719\n",
      "Epoch [69/200] Loss: 305.8960266113281\n",
      "Epoch [70/200] Loss: 303.5146789550781\n",
      "Epoch [71/200] Loss: 301.1512756347656\n",
      "Epoch [72/200] Loss: 298.8059387207031\n",
      "Epoch [73/200] Loss: 296.47857666015625\n",
      "Epoch [74/200] Loss: 294.1690673828125\n",
      "Epoch [75/200] Loss: 291.8775634765625\n",
      "Epoch [76/200] Loss: 289.60406494140625\n",
      "Epoch [77/200] Loss: 287.3481750488281\n",
      "Epoch [78/200] Loss: 285.11004638671875\n",
      "Epoch [79/200] Loss: 282.88958740234375\n",
      "Epoch [80/200] Loss: 280.6866455078125\n",
      "Epoch [81/200] Loss: 278.50103759765625\n",
      "Epoch [82/200] Loss: 276.332763671875\n",
      "Epoch [83/200] Loss: 274.1815490722656\n",
      "Epoch [84/200] Loss: 272.0473327636719\n",
      "Epoch [85/200] Loss: 269.9299621582031\n",
      "Epoch [86/200] Loss: 267.8292541503906\n",
      "Epoch [87/200] Loss: 265.7450256347656\n",
      "Epoch [88/200] Loss: 263.67718505859375\n",
      "Epoch [89/200] Loss: 261.6255798339844\n",
      "Epoch [90/200] Loss: 259.58990478515625\n",
      "Epoch [91/200] Loss: 257.5701904296875\n",
      "Epoch [92/200] Loss: 255.566162109375\n",
      "Epoch [93/200] Loss: 253.57774353027344\n",
      "Epoch [94/200] Loss: 251.60459899902344\n",
      "Epoch [95/200] Loss: 249.64683532714844\n",
      "Epoch [96/200] Loss: 247.70404052734375\n",
      "Epoch [97/200] Loss: 245.77615356445312\n",
      "Epoch [98/200] Loss: 243.8631134033203\n",
      "Epoch [99/200] Loss: 241.96466064453125\n",
      "Epoch [100/200] Loss: 240.0806884765625\n",
      "Epoch [101/200] Loss: 238.21096801757812\n",
      "Epoch [102/200] Loss: 236.3555145263672\n",
      "Epoch [103/200] Loss: 234.5140838623047\n",
      "Epoch [104/200] Loss: 232.68658447265625\n",
      "Epoch [105/200] Loss: 230.8727569580078\n",
      "Epoch [106/200] Loss: 229.07261657714844\n",
      "Epoch [107/200] Loss: 227.28604125976562\n",
      "Epoch [108/200] Loss: 225.51280212402344\n",
      "Epoch [109/200] Loss: 223.75282287597656\n",
      "Epoch [110/200] Loss: 222.0059051513672\n",
      "Epoch [111/200] Loss: 220.2721405029297\n",
      "Epoch [112/200] Loss: 218.55120849609375\n",
      "Epoch [113/200] Loss: 216.84307861328125\n",
      "Epoch [114/200] Loss: 215.14755249023438\n",
      "Epoch [115/200] Loss: 213.46461486816406\n",
      "Epoch [116/200] Loss: 211.79420471191406\n",
      "Epoch [117/200] Loss: 210.13601684570312\n",
      "Epoch [118/200] Loss: 208.4901580810547\n",
      "Epoch [119/200] Loss: 206.8563690185547\n",
      "Epoch [120/200] Loss: 205.2346649169922\n",
      "Epoch [121/200] Loss: 203.62489318847656\n",
      "Epoch [122/200] Loss: 202.02696228027344\n",
      "Epoch [123/200] Loss: 200.44076538085938\n",
      "Epoch [124/200] Loss: 198.86627197265625\n",
      "Epoch [125/200] Loss: 197.30328369140625\n",
      "Epoch [126/200] Loss: 195.7517547607422\n",
      "Epoch [127/200] Loss: 194.21160888671875\n",
      "Epoch [128/200] Loss: 192.68280029296875\n",
      "Epoch [129/200] Loss: 191.1651611328125\n",
      "Epoch [130/200] Loss: 189.65866088867188\n",
      "Epoch [131/200] Loss: 188.16319274902344\n",
      "Epoch [132/200] Loss: 186.67864990234375\n",
      "Epoch [133/200] Loss: 185.2050018310547\n",
      "Epoch [134/200] Loss: 183.74215698242188\n",
      "Epoch [135/200] Loss: 182.29010009765625\n",
      "Epoch [136/200] Loss: 180.84857177734375\n",
      "Epoch [137/200] Loss: 179.41766357421875\n",
      "Epoch [138/200] Loss: 177.99720764160156\n",
      "Epoch [139/200] Loss: 176.58721923828125\n",
      "Epoch [140/200] Loss: 175.18753051757812\n",
      "Epoch [141/200] Loss: 173.7981414794922\n",
      "Epoch [142/200] Loss: 172.4189453125\n",
      "Epoch [143/200] Loss: 171.04981994628906\n",
      "Epoch [144/200] Loss: 169.6908416748047\n",
      "Epoch [145/200] Loss: 168.34176635742188\n",
      "Epoch [146/200] Loss: 167.00267028808594\n",
      "Epoch [147/200] Loss: 165.67340087890625\n",
      "Epoch [148/200] Loss: 164.3539276123047\n",
      "Epoch [149/200] Loss: 163.04417419433594\n",
      "Epoch [150/200] Loss: 161.7440643310547\n",
      "Epoch [151/200] Loss: 160.4535675048828\n",
      "Epoch [152/200] Loss: 159.17262268066406\n",
      "Epoch [153/200] Loss: 157.90109252929688\n",
      "Epoch [154/200] Loss: 156.63900756835938\n",
      "Epoch [155/200] Loss: 155.38626098632812\n",
      "Epoch [156/200] Loss: 154.1427764892578\n",
      "Epoch [157/200] Loss: 152.90855407714844\n",
      "Epoch [158/200] Loss: 151.68344116210938\n",
      "Epoch [159/200] Loss: 150.46749877929688\n",
      "Epoch [160/200] Loss: 149.2605438232422\n",
      "Epoch [161/200] Loss: 148.06260681152344\n",
      "Epoch [162/200] Loss: 146.87356567382812\n",
      "Epoch [163/200] Loss: 145.6934356689453\n",
      "Epoch [164/200] Loss: 144.52210998535156\n",
      "Epoch [165/200] Loss: 143.35952758789062\n",
      "Epoch [166/200] Loss: 142.20565795898438\n",
      "Epoch [167/200] Loss: 141.0604705810547\n",
      "Epoch [168/200] Loss: 139.92384338378906\n",
      "Epoch [169/200] Loss: 138.79574584960938\n",
      "Epoch [170/200] Loss: 137.67616271972656\n",
      "Epoch [171/200] Loss: 136.5649871826172\n",
      "Epoch [172/200] Loss: 135.4622039794922\n",
      "Epoch [173/200] Loss: 134.3677978515625\n",
      "Epoch [174/200] Loss: 133.2816162109375\n",
      "Epoch [175/200] Loss: 132.20367431640625\n",
      "Epoch [176/200] Loss: 131.13389587402344\n",
      "Epoch [177/200] Loss: 130.07223510742188\n",
      "Epoch [178/200] Loss: 129.0186767578125\n",
      "Epoch [179/200] Loss: 127.9731216430664\n",
      "Epoch [180/200] Loss: 126.93553161621094\n",
      "Epoch [181/200] Loss: 125.90589141845703\n",
      "Epoch [182/200] Loss: 124.88411712646484\n",
      "Epoch [183/200] Loss: 123.87015533447266\n",
      "Epoch [184/200] Loss: 122.86395263671875\n",
      "Epoch [185/200] Loss: 121.86549377441406\n",
      "Epoch [186/200] Loss: 120.87471008300781\n",
      "Epoch [187/200] Loss: 119.89156341552734\n",
      "Epoch [188/200] Loss: 118.91603088378906\n",
      "Epoch [189/200] Loss: 117.94801330566406\n",
      "Epoch [190/200] Loss: 116.98748779296875\n",
      "Epoch [191/200] Loss: 116.03438568115234\n",
      "Epoch [192/200] Loss: 115.08869934082031\n",
      "Epoch [193/200] Loss: 114.150390625\n",
      "Epoch [194/200] Loss: 113.2193374633789\n",
      "Epoch [195/200] Loss: 112.29558563232422\n",
      "Epoch [196/200] Loss: 111.37904357910156\n",
      "Epoch [197/200] Loss: 110.46963500976562\n",
      "Epoch [198/200] Loss: 109.5674057006836\n",
      "Epoch [199/200] Loss: 108.6722183227539\n",
      "Epoch [200/200] Loss: 107.78413391113281\n",
      "Predicted days_remaining for parent_id 113: 15.134101867675781\n",
      "Training for parent_id 123...\n",
      "Epoch [1/200] Loss: 772.29541015625\n",
      "Epoch [2/200] Loss: 759.8760375976562\n",
      "Epoch [3/200] Loss: 747.7938232421875\n",
      "Epoch [4/200] Loss: 736.1378784179688\n",
      "Epoch [5/200] Loss: 724.9517822265625\n",
      "Epoch [6/200] Loss: 714.2357177734375\n",
      "Epoch [7/200] Loss: 703.9661865234375\n",
      "Epoch [8/200] Loss: 694.10888671875\n",
      "Epoch [9/200] Loss: 684.6273193359375\n",
      "Epoch [10/200] Loss: 675.4898071289062\n",
      "Epoch [11/200] Loss: 666.673095703125\n",
      "Epoch [12/200] Loss: 658.16259765625\n",
      "Epoch [13/200] Loss: 649.951171875\n",
      "Epoch [14/200] Loss: 642.0375366210938\n",
      "Epoch [15/200] Loss: 634.422607421875\n",
      "Epoch [16/200] Loss: 627.1085815429688\n",
      "Epoch [17/200] Loss: 620.0946044921875\n",
      "Epoch [18/200] Loss: 613.3761596679688\n",
      "Epoch [19/200] Loss: 606.9442138671875\n",
      "Epoch [20/200] Loss: 600.7847900390625\n",
      "Epoch [21/200] Loss: 594.880859375\n",
      "Epoch [22/200] Loss: 589.2137451171875\n",
      "Epoch [23/200] Loss: 583.7640380859375\n",
      "Epoch [24/200] Loss: 578.5128784179688\n",
      "Epoch [25/200] Loss: 573.4419555664062\n",
      "Epoch [26/200] Loss: 568.5348510742188\n",
      "Epoch [27/200] Loss: 563.7757568359375\n",
      "Epoch [28/200] Loss: 559.1510620117188\n",
      "Epoch [29/200] Loss: 554.6478881835938\n",
      "Epoch [30/200] Loss: 550.2549438476562\n",
      "Epoch [31/200] Loss: 545.9620361328125\n",
      "Epoch [32/200] Loss: 541.7601928710938\n",
      "Epoch [33/200] Loss: 537.641357421875\n",
      "Epoch [34/200] Loss: 533.5986328125\n",
      "Epoch [35/200] Loss: 529.6256713867188\n",
      "Epoch [36/200] Loss: 525.7171630859375\n",
      "Epoch [37/200] Loss: 521.8683471679688\n",
      "Epoch [38/200] Loss: 518.0748901367188\n",
      "Epoch [39/200] Loss: 514.3329467773438\n",
      "Epoch [40/200] Loss: 510.6390380859375\n",
      "Epoch [41/200] Loss: 506.99005126953125\n",
      "Epoch [42/200] Loss: 503.3834228515625\n",
      "Epoch [43/200] Loss: 499.8166198730469\n",
      "Epoch [44/200] Loss: 496.2875061035156\n",
      "Epoch [45/200] Loss: 492.7941589355469\n",
      "Epoch [46/200] Loss: 489.3349609375\n",
      "Epoch [47/200] Loss: 485.9084777832031\n",
      "Epoch [48/200] Loss: 482.5134582519531\n",
      "Epoch [49/200] Loss: 479.1488037109375\n",
      "Epoch [50/200] Loss: 475.8135986328125\n",
      "Epoch [51/200] Loss: 472.5072021484375\n",
      "Epoch [52/200] Loss: 469.2288513183594\n",
      "Epoch [53/200] Loss: 465.9781799316406\n",
      "Epoch [54/200] Loss: 462.7546081542969\n",
      "Epoch [55/200] Loss: 459.5578308105469\n",
      "Epoch [56/200] Loss: 456.3875732421875\n",
      "Epoch [57/200] Loss: 453.24334716796875\n",
      "Epoch [58/200] Loss: 450.1250915527344\n",
      "Epoch [59/200] Loss: 447.0321960449219\n",
      "Epoch [60/200] Loss: 443.9645690917969\n",
      "Epoch [61/200] Loss: 440.92181396484375\n",
      "Epoch [62/200] Loss: 437.903564453125\n",
      "Epoch [63/200] Loss: 434.90948486328125\n",
      "Epoch [64/200] Loss: 431.93927001953125\n",
      "Epoch [65/200] Loss: 428.99261474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/200] Loss: 426.0689697265625\n",
      "Epoch [67/200] Loss: 423.168212890625\n",
      "Epoch [68/200] Loss: 420.2898254394531\n",
      "Epoch [69/200] Loss: 417.4336242675781\n",
      "Epoch [70/200] Loss: 414.5992736816406\n",
      "Epoch [71/200] Loss: 411.7864074707031\n",
      "Epoch [72/200] Loss: 408.9947814941406\n",
      "Epoch [73/200] Loss: 406.22406005859375\n",
      "Epoch [74/200] Loss: 403.47393798828125\n",
      "Epoch [75/200] Loss: 400.7441711425781\n",
      "Epoch [76/200] Loss: 398.03448486328125\n",
      "Epoch [77/200] Loss: 395.34454345703125\n",
      "Epoch [78/200] Loss: 392.67413330078125\n",
      "Epoch [79/200] Loss: 390.0229797363281\n",
      "Epoch [80/200] Loss: 387.390869140625\n",
      "Epoch [81/200] Loss: 384.77752685546875\n",
      "Epoch [82/200] Loss: 382.1827087402344\n",
      "Epoch [83/200] Loss: 379.6061706542969\n",
      "Epoch [84/200] Loss: 377.0477294921875\n",
      "Epoch [85/200] Loss: 374.5071716308594\n",
      "Epoch [86/200] Loss: 371.98419189453125\n",
      "Epoch [87/200] Loss: 369.47869873046875\n",
      "Epoch [88/200] Loss: 366.990478515625\n",
      "Epoch [89/200] Loss: 364.51910400390625\n",
      "Epoch [90/200] Loss: 362.0647277832031\n",
      "Epoch [91/200] Loss: 359.6269226074219\n",
      "Epoch [92/200] Loss: 357.2055969238281\n",
      "Epoch [93/200] Loss: 354.80059814453125\n",
      "Epoch [94/200] Loss: 352.4117126464844\n",
      "Epoch [95/200] Loss: 350.0386962890625\n",
      "Epoch [96/200] Loss: 347.6815185546875\n",
      "Epoch [97/200] Loss: 345.3398742675781\n",
      "Epoch [98/200] Loss: 343.01373291015625\n",
      "Epoch [99/200] Loss: 340.7029724121094\n",
      "Epoch [100/200] Loss: 338.4072265625\n",
      "Epoch [101/200] Loss: 336.12652587890625\n",
      "Epoch [102/200] Loss: 333.8606872558594\n",
      "Epoch [103/200] Loss: 331.6094665527344\n",
      "Epoch [104/200] Loss: 329.3729248046875\n",
      "Epoch [105/200] Loss: 327.1508483886719\n",
      "Epoch [106/200] Loss: 324.94305419921875\n",
      "Epoch [107/200] Loss: 322.7493591308594\n",
      "Epoch [108/200] Loss: 320.56982421875\n",
      "Epoch [109/200] Loss: 318.40423583984375\n",
      "Epoch [110/200] Loss: 316.25238037109375\n",
      "Epoch [111/200] Loss: 314.11431884765625\n",
      "Epoch [112/200] Loss: 311.9897155761719\n",
      "Epoch [113/200] Loss: 309.87872314453125\n",
      "Epoch [114/200] Loss: 307.781005859375\n",
      "Epoch [115/200] Loss: 305.6966247558594\n",
      "Epoch [116/200] Loss: 303.62530517578125\n",
      "Epoch [117/200] Loss: 301.56707763671875\n",
      "Epoch [118/200] Loss: 299.5218811035156\n",
      "Epoch [119/200] Loss: 297.48944091796875\n",
      "Epoch [120/200] Loss: 295.4696960449219\n",
      "Epoch [121/200] Loss: 293.4627380371094\n",
      "Epoch [122/200] Loss: 291.46826171875\n",
      "Epoch [123/200] Loss: 289.4862365722656\n",
      "Epoch [124/200] Loss: 287.51666259765625\n",
      "Epoch [125/200] Loss: 285.55926513671875\n",
      "Epoch [126/200] Loss: 283.6141662597656\n",
      "Epoch [127/200] Loss: 281.6811218261719\n",
      "Epoch [128/200] Loss: 279.76019287109375\n",
      "Epoch [129/200] Loss: 277.85113525390625\n",
      "Epoch [130/200] Loss: 275.9539489746094\n",
      "Epoch [131/200] Loss: 274.0685729980469\n",
      "Epoch [132/200] Loss: 272.1949157714844\n",
      "Epoch [133/200] Loss: 270.3328857421875\n",
      "Epoch [134/200] Loss: 268.4823913574219\n",
      "Epoch [135/200] Loss: 266.64337158203125\n",
      "Epoch [136/200] Loss: 264.8157958984375\n",
      "Epoch [137/200] Loss: 262.99951171875\n",
      "Epoch [138/200] Loss: 261.1944580078125\n",
      "Epoch [139/200] Loss: 259.4006652832031\n",
      "Epoch [140/200] Loss: 257.6179504394531\n",
      "Epoch [141/200] Loss: 255.84625244140625\n",
      "Epoch [142/200] Loss: 254.08554077148438\n",
      "Epoch [143/200] Loss: 252.33575439453125\n",
      "Epoch [144/200] Loss: 250.5968475341797\n",
      "Epoch [145/200] Loss: 248.86866760253906\n",
      "Epoch [146/200] Loss: 247.15126037597656\n",
      "Epoch [147/200] Loss: 245.44448852539062\n",
      "Epoch [148/200] Loss: 243.7482147216797\n",
      "Epoch [149/200] Loss: 242.06256103515625\n",
      "Epoch [150/200] Loss: 240.38731384277344\n",
      "Epoch [151/200] Loss: 238.7224884033203\n",
      "Epoch [152/200] Loss: 237.0679931640625\n",
      "Epoch [153/200] Loss: 235.4237823486328\n",
      "Epoch [154/200] Loss: 233.789794921875\n",
      "Epoch [155/200] Loss: 232.1659698486328\n",
      "Epoch [156/200] Loss: 230.55226135253906\n",
      "Epoch [157/200] Loss: 228.9485626220703\n",
      "Epoch [158/200] Loss: 227.3549041748047\n",
      "Epoch [159/200] Loss: 225.7711181640625\n",
      "Epoch [160/200] Loss: 224.197265625\n",
      "Epoch [161/200] Loss: 222.6331787109375\n",
      "Epoch [162/200] Loss: 221.07887268066406\n",
      "Epoch [163/200] Loss: 219.53428649902344\n",
      "Epoch [164/200] Loss: 217.9993438720703\n",
      "Epoch [165/200] Loss: 216.47402954101562\n",
      "Epoch [166/200] Loss: 214.95826721191406\n",
      "Epoch [167/200] Loss: 213.45201110839844\n",
      "Epoch [168/200] Loss: 211.9551239013672\n",
      "Epoch [169/200] Loss: 210.46775817871094\n",
      "Epoch [170/200] Loss: 208.9896697998047\n",
      "Epoch [171/200] Loss: 207.5208740234375\n",
      "Epoch [172/200] Loss: 206.0613555908203\n",
      "Epoch [173/200] Loss: 204.6110076904297\n",
      "Epoch [174/200] Loss: 203.16978454589844\n",
      "Epoch [175/200] Loss: 201.73765563964844\n",
      "Epoch [176/200] Loss: 200.3146514892578\n",
      "Epoch [177/200] Loss: 198.90057373046875\n",
      "Epoch [178/200] Loss: 197.49545288085938\n",
      "Epoch [179/200] Loss: 196.09925842285156\n",
      "Epoch [180/200] Loss: 194.71194458007812\n",
      "Epoch [181/200] Loss: 193.3334197998047\n",
      "Epoch [182/200] Loss: 191.96363830566406\n",
      "Epoch [183/200] Loss: 190.60260009765625\n",
      "Epoch [184/200] Loss: 189.25021362304688\n",
      "Epoch [185/200] Loss: 187.9064483642578\n",
      "Epoch [186/200] Loss: 186.5712890625\n",
      "Epoch [187/200] Loss: 185.24464416503906\n",
      "Epoch [188/200] Loss: 183.92652893066406\n",
      "Epoch [189/200] Loss: 182.61683654785156\n",
      "Epoch [190/200] Loss: 181.31553649902344\n",
      "Epoch [191/200] Loss: 180.02259826660156\n",
      "Epoch [192/200] Loss: 178.73797607421875\n",
      "Epoch [193/200] Loss: 177.46160888671875\n",
      "Epoch [194/200] Loss: 176.19354248046875\n",
      "Epoch [195/200] Loss: 174.93359375\n",
      "Epoch [196/200] Loss: 173.6817626953125\n",
      "Epoch [197/200] Loss: 172.43809509277344\n",
      "Epoch [198/200] Loss: 171.20245361328125\n",
      "Epoch [199/200] Loss: 169.9748077392578\n",
      "Epoch [200/200] Loss: 168.7552032470703\n",
      "Predicted days_remaining for parent_id 123: 15.376399993896484\n",
      "Training for parent_id 124...\n",
      "Epoch [1/200] Loss: 224.79454040527344\n",
      "Epoch [2/200] Loss: 216.98255920410156\n",
      "Epoch [3/200] Loss: 209.5589141845703\n",
      "Epoch [4/200] Loss: 202.59947204589844\n",
      "Epoch [5/200] Loss: 196.10897827148438\n",
      "Epoch [6/200] Loss: 190.06076049804688\n",
      "Epoch [7/200] Loss: 184.41607666015625\n",
      "Epoch [8/200] Loss: 179.132568359375\n",
      "Epoch [9/200] Loss: 174.1697998046875\n",
      "Epoch [10/200] Loss: 169.49110412597656\n",
      "Epoch [11/200] Loss: 165.06417846679688\n",
      "Epoch [12/200] Loss: 160.86180114746094\n",
      "Epoch [13/200] Loss: 156.8622283935547\n",
      "Epoch [14/200] Loss: 153.04864501953125\n",
      "Epoch [15/200] Loss: 149.4083251953125\n",
      "Epoch [16/200] Loss: 145.9315948486328\n",
      "Epoch [17/200] Loss: 142.6107177734375\n",
      "Epoch [18/200] Loss: 139.43902587890625\n",
      "Epoch [19/200] Loss: 136.41064453125\n",
      "Epoch [20/200] Loss: 133.51995849609375\n",
      "Epoch [21/200] Loss: 130.7615203857422\n",
      "Epoch [22/200] Loss: 128.12969970703125\n",
      "Epoch [23/200] Loss: 125.6186294555664\n",
      "Epoch [24/200] Loss: 123.2219467163086\n",
      "Epoch [25/200] Loss: 120.93273162841797\n",
      "Epoch [26/200] Loss: 118.74373626708984\n",
      "Epoch [27/200] Loss: 116.64729309082031\n",
      "Epoch [28/200] Loss: 114.63581085205078\n",
      "Epoch [29/200] Loss: 112.70187377929688\n",
      "Epoch [30/200] Loss: 110.83843231201172\n",
      "Epoch [31/200] Loss: 109.03903198242188\n",
      "Epoch [32/200] Loss: 107.29774475097656\n",
      "Epoch [33/200] Loss: 105.60941314697266\n",
      "Epoch [34/200] Loss: 103.96939086914062\n",
      "Epoch [35/200] Loss: 102.37364196777344\n",
      "Epoch [36/200] Loss: 100.81861877441406\n",
      "Epoch [37/200] Loss: 99.3012466430664\n",
      "Epoch [38/200] Loss: 97.81887817382812\n",
      "Epoch [39/200] Loss: 96.3691177368164\n",
      "Epoch [40/200] Loss: 94.95013427734375\n",
      "Epoch [41/200] Loss: 93.56007385253906\n",
      "Epoch [42/200] Loss: 92.19756317138672\n",
      "Epoch [43/200] Loss: 90.86127471923828\n",
      "Epoch [44/200] Loss: 89.55014038085938\n",
      "Epoch [45/200] Loss: 88.26315307617188\n",
      "Epoch [46/200] Loss: 86.99946594238281\n",
      "Epoch [47/200] Loss: 85.75825500488281\n",
      "Epoch [48/200] Loss: 84.53881072998047\n",
      "Epoch [49/200] Loss: 83.34046936035156\n",
      "Epoch [50/200] Loss: 82.16259765625\n",
      "Epoch [51/200] Loss: 81.00466918945312\n",
      "Epoch [52/200] Loss: 79.86610412597656\n",
      "Epoch [53/200] Loss: 78.74637603759766\n",
      "Epoch [54/200] Loss: 77.64506530761719\n",
      "Epoch [55/200] Loss: 76.5616683959961\n",
      "Epoch [56/200] Loss: 75.4958267211914\n",
      "Epoch [57/200] Loss: 74.4471206665039\n",
      "Epoch [58/200] Loss: 73.4151611328125\n",
      "Epoch [59/200] Loss: 72.3996353149414\n",
      "Epoch [60/200] Loss: 71.40015411376953\n",
      "Epoch [61/200] Loss: 70.41645812988281\n",
      "Epoch [62/200] Loss: 69.4482192993164\n",
      "Epoch [63/200] Loss: 68.49514770507812\n",
      "Epoch [64/200] Loss: 67.55695343017578\n",
      "Epoch [65/200] Loss: 66.63336944580078\n",
      "Epoch [66/200] Loss: 65.72418975830078\n",
      "Epoch [67/200] Loss: 64.82911682128906\n",
      "Epoch [68/200] Loss: 63.94794464111328\n",
      "Epoch [69/200] Loss: 63.08041000366211\n",
      "Epoch [70/200] Loss: 62.22632598876953\n",
      "Epoch [71/200] Loss: 61.38545608520508\n",
      "Epoch [72/200] Loss: 60.55760955810547\n",
      "Epoch [73/200] Loss: 59.74253845214844\n",
      "Epoch [74/200] Loss: 58.94011306762695\n",
      "Epoch [75/200] Loss: 58.15008544921875\n",
      "Epoch [76/200] Loss: 57.37230682373047\n",
      "Epoch [77/200] Loss: 56.606544494628906\n",
      "Epoch [78/200] Loss: 55.8526725769043\n",
      "Epoch [79/200] Loss: 55.1104850769043\n",
      "Epoch [80/200] Loss: 54.379798889160156\n",
      "Epoch [81/200] Loss: 53.660491943359375\n",
      "Epoch [82/200] Loss: 52.952354431152344\n",
      "Epoch [83/200] Loss: 52.255245208740234\n",
      "Epoch [84/200] Loss: 51.56901550292969\n",
      "Epoch [85/200] Loss: 50.89348220825195\n",
      "Epoch [86/200] Loss: 50.228538513183594\n",
      "Epoch [87/200] Loss: 49.57398223876953\n",
      "Epoch [88/200] Loss: 48.929718017578125\n",
      "Epoch [89/200] Loss: 48.295555114746094\n",
      "Epoch [90/200] Loss: 47.67138671875\n",
      "Epoch [91/200] Loss: 47.05707550048828\n",
      "Epoch [92/200] Loss: 46.45246124267578\n",
      "Epoch [93/200] Loss: 45.85742950439453\n",
      "Epoch [94/200] Loss: 45.27182388305664\n",
      "Epoch [95/200] Loss: 44.69554901123047\n",
      "Epoch [96/200] Loss: 44.12845230102539\n",
      "Epoch [97/200] Loss: 43.5704231262207\n",
      "Epoch [98/200] Loss: 43.02134323120117\n",
      "Epoch [99/200] Loss: 42.48107147216797\n",
      "Epoch [100/200] Loss: 41.949485778808594\n",
      "Epoch [101/200] Loss: 41.426483154296875\n",
      "Epoch [102/200] Loss: 40.911956787109375\n",
      "Epoch [103/200] Loss: 40.40575408935547\n",
      "Epoch [104/200] Loss: 39.90779113769531\n",
      "Epoch [105/200] Loss: 39.41796112060547\n",
      "Epoch [106/200] Loss: 38.93613815307617\n",
      "Epoch [107/200] Loss: 38.46220779418945\n",
      "Epoch [108/200] Loss: 37.99609375\n",
      "Epoch [109/200] Loss: 37.53767395019531\n",
      "Epoch [110/200] Loss: 37.086822509765625\n",
      "Epoch [111/200] Loss: 36.64344787597656\n",
      "Epoch [112/200] Loss: 36.20746612548828\n",
      "Epoch [113/200] Loss: 35.778770446777344\n",
      "Epoch [114/200] Loss: 35.35724639892578\n",
      "Epoch [115/200] Loss: 34.942806243896484\n",
      "Epoch [116/200] Loss: 34.53535461425781\n",
      "Epoch [117/200] Loss: 34.13478088378906\n",
      "Epoch [118/200] Loss: 33.74101638793945\n",
      "Epoch [119/200] Loss: 33.353946685791016\n",
      "Epoch [120/200] Loss: 32.97349166870117\n",
      "Epoch [121/200] Loss: 32.599544525146484\n",
      "Epoch [122/200] Loss: 32.23203659057617\n",
      "Epoch [123/200] Loss: 31.870859146118164\n",
      "Epoch [124/200] Loss: 31.515939712524414\n",
      "Epoch [125/200] Loss: 31.16716957092285\n",
      "Epoch [126/200] Loss: 30.824487686157227\n",
      "Epoch [127/200] Loss: 30.48779296875\n",
      "Epoch [128/200] Loss: 30.157012939453125\n",
      "Epoch [129/200] Loss: 29.832040786743164\n",
      "Epoch [130/200] Loss: 29.512813568115234\n",
      "Epoch [131/200] Loss: 29.199241638183594\n",
      "Epoch [132/200] Loss: 28.891254425048828\n",
      "Epoch [133/200] Loss: 28.58875846862793\n",
      "Epoch [134/200] Loss: 28.29168701171875\n",
      "Epoch [135/200] Loss: 27.999948501586914\n",
      "Epoch [136/200] Loss: 27.713478088378906\n",
      "Epoch [137/200] Loss: 27.432188034057617\n",
      "Epoch [138/200] Loss: 27.156024932861328\n",
      "Epoch [139/200] Loss: 26.884872436523438\n",
      "Epoch [140/200] Loss: 26.618703842163086\n",
      "Epoch [141/200] Loss: 26.357418060302734\n",
      "Epoch [142/200] Loss: 26.100942611694336\n",
      "Epoch [143/200] Loss: 25.84921646118164\n",
      "Epoch [144/200] Loss: 25.602161407470703\n",
      "Epoch [145/200] Loss: 25.359704971313477\n",
      "Epoch [146/200] Loss: 25.121793746948242\n",
      "Epoch [147/200] Loss: 24.88834571838379\n",
      "Epoch [148/200] Loss: 24.65928840637207\n",
      "Epoch [149/200] Loss: 24.434568405151367\n",
      "Epoch [150/200] Loss: 24.214113235473633\n",
      "Epoch [151/200] Loss: 23.99785804748535\n",
      "Epoch [152/200] Loss: 23.785722732543945\n",
      "Epoch [153/200] Loss: 23.57767105102539\n",
      "Epoch [154/200] Loss: 23.373619079589844\n",
      "Epoch [155/200] Loss: 23.17351722717285\n",
      "Epoch [156/200] Loss: 22.977294921875\n",
      "Epoch [157/200] Loss: 22.78488540649414\n",
      "Epoch [158/200] Loss: 22.596235275268555\n",
      "Epoch [159/200] Loss: 22.411285400390625\n",
      "Epoch [160/200] Loss: 22.229995727539062\n",
      "Epoch [161/200] Loss: 22.052257537841797\n",
      "Epoch [162/200] Loss: 21.8780574798584\n",
      "Epoch [163/200] Loss: 21.707324981689453\n",
      "Epoch [164/200] Loss: 21.54000473022461\n",
      "Epoch [165/200] Loss: 21.37603187561035\n",
      "Epoch [166/200] Loss: 21.215360641479492\n",
      "Epoch [167/200] Loss: 21.057937622070312\n",
      "Epoch [168/200] Loss: 20.903697967529297\n",
      "Epoch [169/200] Loss: 20.752605438232422\n",
      "Epoch [170/200] Loss: 20.604591369628906\n",
      "Epoch [171/200] Loss: 20.459613800048828\n",
      "Epoch [172/200] Loss: 20.317611694335938\n",
      "Epoch [173/200] Loss: 20.178539276123047\n",
      "Epoch [174/200] Loss: 20.042354583740234\n",
      "Epoch [175/200] Loss: 19.909000396728516\n",
      "Epoch [176/200] Loss: 19.778423309326172\n",
      "Epoch [177/200] Loss: 19.65058708190918\n",
      "Epoch [178/200] Loss: 19.525436401367188\n",
      "Epoch [179/200] Loss: 19.40292739868164\n",
      "Epoch [180/200] Loss: 19.28300666809082\n",
      "Epoch [181/200] Loss: 19.165632247924805\n",
      "Epoch [182/200] Loss: 19.050764083862305\n",
      "Epoch [183/200] Loss: 18.9383544921875\n",
      "Epoch [184/200] Loss: 18.828359603881836\n",
      "Epoch [185/200] Loss: 18.720733642578125\n",
      "Epoch [186/200] Loss: 18.615434646606445\n",
      "Epoch [187/200] Loss: 18.51242446899414\n",
      "Epoch [188/200] Loss: 18.411659240722656\n",
      "Epoch [189/200] Loss: 18.313091278076172\n",
      "Epoch [190/200] Loss: 18.216693878173828\n",
      "Epoch [191/200] Loss: 18.122413635253906\n",
      "Epoch [192/200] Loss: 18.030216217041016\n",
      "Epoch [193/200] Loss: 17.9400691986084\n",
      "Epoch [194/200] Loss: 17.85192108154297\n",
      "Epoch [195/200] Loss: 17.76573944091797\n",
      "Epoch [196/200] Loss: 17.681493759155273\n",
      "Epoch [197/200] Loss: 17.59914779663086\n",
      "Epoch [198/200] Loss: 17.518653869628906\n",
      "Epoch [199/200] Loss: 17.43998146057129\n",
      "Epoch [200/200] Loss: 17.363100051879883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted days_remaining for parent_id 124: 13.061666488647461\n",
      "Training for parent_id 138...\n",
      "Epoch [1/200] Loss: 217.076416015625\n",
      "Epoch [2/200] Loss: 210.70782470703125\n",
      "Epoch [3/200] Loss: 204.4838104248047\n",
      "Epoch [4/200] Loss: 198.48150634765625\n",
      "Epoch [5/200] Loss: 192.7320098876953\n",
      "Epoch [6/200] Loss: 187.23687744140625\n",
      "Epoch [7/200] Loss: 181.98190307617188\n",
      "Epoch [8/200] Loss: 176.95274353027344\n",
      "Epoch [9/200] Loss: 172.14068603515625\n",
      "Epoch [10/200] Loss: 167.54071044921875\n",
      "Epoch [11/200] Loss: 163.1486053466797\n",
      "Epoch [12/200] Loss: 158.95904541015625\n",
      "Epoch [13/200] Loss: 154.96530151367188\n",
      "Epoch [14/200] Loss: 151.1595458984375\n",
      "Epoch [15/200] Loss: 147.5331573486328\n",
      "Epoch [16/200] Loss: 144.07728576660156\n",
      "Epoch [17/200] Loss: 140.7831573486328\n",
      "Epoch [18/200] Loss: 137.64208984375\n",
      "Epoch [19/200] Loss: 134.64572143554688\n",
      "Epoch [20/200] Loss: 131.78590393066406\n",
      "Epoch [21/200] Loss: 129.05484008789062\n",
      "Epoch [22/200] Loss: 126.44518280029297\n",
      "Epoch [23/200] Loss: 123.95014953613281\n",
      "Epoch [24/200] Loss: 121.56353759765625\n",
      "Epoch [25/200] Loss: 119.27971649169922\n",
      "Epoch [26/200] Loss: 117.09346008300781\n",
      "Epoch [27/200] Loss: 114.99958038330078\n",
      "Epoch [28/200] Loss: 112.9927749633789\n",
      "Epoch [29/200] Loss: 111.06729125976562\n",
      "Epoch [30/200] Loss: 109.21711730957031\n",
      "Epoch [31/200] Loss: 107.43592071533203\n",
      "Epoch [32/200] Loss: 105.71744537353516\n",
      "Epoch [33/200] Loss: 104.05571746826172\n",
      "Epoch [34/200] Loss: 102.44525146484375\n",
      "Epoch [35/200] Loss: 100.88117218017578\n",
      "Epoch [36/200] Loss: 99.35920715332031\n",
      "Epoch [37/200] Loss: 97.87578582763672\n",
      "Epoch [38/200] Loss: 96.42778778076172\n",
      "Epoch [39/200] Loss: 95.01263427734375\n",
      "Epoch [40/200] Loss: 93.62810516357422\n",
      "Epoch [41/200] Loss: 92.27234649658203\n",
      "Epoch [42/200] Loss: 90.9437484741211\n",
      "Epoch [43/200] Loss: 89.64089965820312\n",
      "Epoch [44/200] Loss: 88.36260223388672\n",
      "Epoch [45/200] Loss: 87.10774230957031\n",
      "Epoch [46/200] Loss: 85.87541198730469\n",
      "Epoch [47/200] Loss: 84.66468811035156\n",
      "Epoch [48/200] Loss: 83.47478485107422\n",
      "Epoch [49/200] Loss: 82.30504608154297\n",
      "Epoch [50/200] Loss: 81.15473175048828\n",
      "Epoch [51/200] Loss: 80.02323150634766\n",
      "Epoch [52/200] Loss: 78.90997314453125\n",
      "Epoch [53/200] Loss: 77.81441497802734\n",
      "Epoch [54/200] Loss: 76.73603057861328\n",
      "Epoch [55/200] Loss: 75.67431640625\n",
      "Epoch [56/200] Loss: 74.62879180908203\n",
      "Epoch [57/200] Loss: 73.59906005859375\n",
      "Epoch [58/200] Loss: 72.58463287353516\n",
      "Epoch [59/200] Loss: 71.58515167236328\n",
      "Epoch [60/200] Loss: 70.60018157958984\n",
      "Epoch [61/200] Loss: 69.62937927246094\n",
      "Epoch [62/200] Loss: 68.67242431640625\n",
      "Epoch [63/200] Loss: 67.72896575927734\n",
      "Epoch [64/200] Loss: 66.79877471923828\n",
      "Epoch [65/200] Loss: 65.88160705566406\n",
      "Epoch [66/200] Loss: 64.97722625732422\n",
      "Epoch [67/200] Loss: 64.08551788330078\n",
      "Epoch [68/200] Loss: 63.20634078979492\n",
      "Epoch [69/200] Loss: 62.33958435058594\n",
      "Epoch [70/200] Loss: 61.48519515991211\n",
      "Epoch [71/200] Loss: 60.64307403564453\n",
      "Epoch [72/200] Loss: 59.81322479248047\n",
      "Epoch [73/200] Loss: 58.995540618896484\n",
      "Epoch [74/200] Loss: 58.19001388549805\n",
      "Epoch [75/200] Loss: 57.39657211303711\n",
      "Epoch [76/200] Loss: 56.61516189575195\n",
      "Epoch [77/200] Loss: 55.84571075439453\n",
      "Epoch [78/200] Loss: 55.08811950683594\n",
      "Epoch [79/200] Loss: 54.34230041503906\n",
      "Epoch [80/200] Loss: 53.608150482177734\n",
      "Epoch [81/200] Loss: 52.88554000854492\n",
      "Epoch [82/200] Loss: 52.17436218261719\n",
      "Epoch [83/200] Loss: 51.47449493408203\n",
      "Epoch [84/200] Loss: 50.785770416259766\n",
      "Epoch [85/200] Loss: 50.10807800292969\n",
      "Epoch [86/200] Loss: 49.44127655029297\n",
      "Epoch [87/200] Loss: 48.785213470458984\n",
      "Epoch [88/200] Loss: 48.13974380493164\n",
      "Epoch [89/200] Loss: 47.504730224609375\n",
      "Epoch [90/200] Loss: 46.88002014160156\n",
      "Epoch [91/200] Loss: 46.265472412109375\n",
      "Epoch [92/200] Loss: 45.66095733642578\n",
      "Epoch [93/200] Loss: 45.06631088256836\n",
      "Epoch [94/200] Loss: 44.481407165527344\n",
      "Epoch [95/200] Loss: 43.9061279296875\n",
      "Epoch [96/200] Loss: 43.340301513671875\n",
      "Epoch [97/200] Loss: 42.78382110595703\n",
      "Epoch [98/200] Loss: 42.236534118652344\n",
      "Epoch [99/200] Loss: 41.69832992553711\n",
      "Epoch [100/200] Loss: 41.169044494628906\n",
      "Epoch [101/200] Loss: 40.648597717285156\n",
      "Epoch [102/200] Loss: 40.13682556152344\n",
      "Epoch [103/200] Loss: 39.63362121582031\n",
      "Epoch [104/200] Loss: 39.13887405395508\n",
      "Epoch [105/200] Loss: 38.65244674682617\n",
      "Epoch [106/200] Loss: 38.174232482910156\n",
      "Epoch [107/200] Loss: 37.7041015625\n",
      "Epoch [108/200] Loss: 37.24193572998047\n",
      "Epoch [109/200] Loss: 36.78765869140625\n",
      "Epoch [110/200] Loss: 36.34110641479492\n",
      "Epoch [111/200] Loss: 35.9022102355957\n",
      "Epoch [112/200] Loss: 35.47083282470703\n",
      "Epoch [113/200] Loss: 35.04688262939453\n",
      "Epoch [114/200] Loss: 34.63024139404297\n",
      "Epoch [115/200] Loss: 34.220829010009766\n",
      "Epoch [116/200] Loss: 33.81850814819336\n",
      "Epoch [117/200] Loss: 33.423194885253906\n",
      "Epoch [118/200] Loss: 33.034793853759766\n",
      "Epoch [119/200] Loss: 32.6531982421875\n",
      "Epoch [120/200] Loss: 32.2783088684082\n",
      "Epoch [121/200] Loss: 31.910037994384766\n",
      "Epoch [122/200] Loss: 31.548261642456055\n",
      "Epoch [123/200] Loss: 31.19292449951172\n",
      "Epoch [124/200] Loss: 30.84389305114746\n",
      "Epoch [125/200] Loss: 30.501123428344727\n",
      "Epoch [126/200] Loss: 30.16448211669922\n",
      "Epoch [127/200] Loss: 29.833890914916992\n",
      "Epoch [128/200] Loss: 29.509258270263672\n",
      "Epoch [129/200] Loss: 29.190509796142578\n",
      "Epoch [130/200] Loss: 28.877546310424805\n",
      "Epoch [131/200] Loss: 28.570287704467773\n",
      "Epoch [132/200] Loss: 28.268646240234375\n",
      "Epoch [133/200] Loss: 27.9725399017334\n",
      "Epoch [134/200] Loss: 27.68187141418457\n",
      "Epoch [135/200] Loss: 27.396595001220703\n",
      "Epoch [136/200] Loss: 27.11659812927246\n",
      "Epoch [137/200] Loss: 26.84180450439453\n",
      "Epoch [138/200] Loss: 26.572145462036133\n",
      "Epoch [139/200] Loss: 26.307533264160156\n",
      "Epoch [140/200] Loss: 26.047883987426758\n",
      "Epoch [141/200] Loss: 25.79315948486328\n",
      "Epoch [142/200] Loss: 25.543237686157227\n",
      "Epoch [143/200] Loss: 25.298059463500977\n",
      "Epoch [144/200] Loss: 25.057567596435547\n",
      "Epoch [145/200] Loss: 24.821674346923828\n",
      "Epoch [146/200] Loss: 24.590299606323242\n",
      "Epoch [147/200] Loss: 24.363399505615234\n",
      "Epoch [148/200] Loss: 24.140884399414062\n",
      "Epoch [149/200] Loss: 23.922677993774414\n",
      "Epoch [150/200] Loss: 23.708728790283203\n",
      "Epoch [151/200] Loss: 23.49896240234375\n",
      "Epoch [152/200] Loss: 23.293304443359375\n",
      "Epoch [153/200] Loss: 23.091707229614258\n",
      "Epoch [154/200] Loss: 22.894092559814453\n",
      "Epoch [155/200] Loss: 22.70038414001465\n",
      "Epoch [156/200] Loss: 22.51054573059082\n",
      "Epoch [157/200] Loss: 22.324487686157227\n",
      "Epoch [158/200] Loss: 22.142168045043945\n",
      "Epoch [159/200] Loss: 21.963516235351562\n",
      "Epoch [160/200] Loss: 21.78847312927246\n",
      "Epoch [161/200] Loss: 21.616968154907227\n",
      "Epoch [162/200] Loss: 21.4489688873291\n",
      "Epoch [163/200] Loss: 21.28438949584961\n",
      "Epoch [164/200] Loss: 21.123188018798828\n",
      "Epoch [165/200] Loss: 20.96529197692871\n",
      "Epoch [166/200] Loss: 20.8106632232666\n",
      "Epoch [167/200] Loss: 20.659236907958984\n",
      "Epoch [168/200] Loss: 20.51095199584961\n",
      "Epoch [169/200] Loss: 20.36577033996582\n",
      "Epoch [170/200] Loss: 20.223628997802734\n",
      "Epoch [171/200] Loss: 20.08446502685547\n",
      "Epoch [172/200] Loss: 19.9482421875\n",
      "Epoch [173/200] Loss: 19.814908981323242\n",
      "Epoch [174/200] Loss: 19.684398651123047\n",
      "Epoch [175/200] Loss: 19.556684494018555\n",
      "Epoch [176/200] Loss: 19.43169593811035\n",
      "Epoch [177/200] Loss: 19.30938720703125\n",
      "Epoch [178/200] Loss: 19.189722061157227\n",
      "Epoch [179/200] Loss: 19.072650909423828\n",
      "Epoch [180/200] Loss: 18.958114624023438\n",
      "Epoch [181/200] Loss: 18.846071243286133\n",
      "Epoch [182/200] Loss: 18.73648452758789\n",
      "Epoch [183/200] Loss: 18.629302978515625\n",
      "Epoch [184/200] Loss: 18.524478912353516\n",
      "Epoch [185/200] Loss: 18.421974182128906\n",
      "Epoch [186/200] Loss: 18.32174301147461\n",
      "Epoch [187/200] Loss: 18.223739624023438\n",
      "Epoch [188/200] Loss: 18.12793731689453\n",
      "Epoch [189/200] Loss: 18.034273147583008\n",
      "Epoch [190/200] Loss: 17.942718505859375\n",
      "Epoch [191/200] Loss: 17.853229522705078\n",
      "Epoch [192/200] Loss: 17.765775680541992\n",
      "Epoch [193/200] Loss: 17.68030548095703\n",
      "Epoch [194/200] Loss: 17.596786499023438\n",
      "Epoch [195/200] Loss: 17.51518440246582\n",
      "Epoch [196/200] Loss: 17.43545150756836\n",
      "Epoch [197/200] Loss: 17.357559204101562\n",
      "Epoch [198/200] Loss: 17.281475067138672\n",
      "Epoch [199/200] Loss: 17.2071533203125\n",
      "Epoch [200/200] Loss: 17.134565353393555\n",
      "Predicted days_remaining for parent_id 138: 12.129451751708984\n",
      "Training for parent_id 144...\n",
      "Epoch [1/200] Loss: 130.98927307128906\n",
      "Epoch [2/200] Loss: 125.7568588256836\n",
      "Epoch [3/200] Loss: 120.79064178466797\n",
      "Epoch [4/200] Loss: 116.09925079345703\n",
      "Epoch [5/200] Loss: 111.68317413330078\n",
      "Epoch [6/200] Loss: 107.54780578613281\n",
      "Epoch [7/200] Loss: 103.69213104248047\n",
      "Epoch [8/200] Loss: 100.1058349609375\n",
      "Epoch [9/200] Loss: 96.7718734741211\n",
      "Epoch [10/200] Loss: 93.6694107055664\n",
      "Epoch [11/200] Loss: 90.77604675292969\n",
      "Epoch [12/200] Loss: 88.06946563720703\n",
      "Epoch [13/200] Loss: 85.52896118164062\n",
      "Epoch [14/200] Loss: 83.13654327392578\n",
      "Epoch [15/200] Loss: 80.87713623046875\n",
      "Epoch [16/200] Loss: 78.73825073242188\n",
      "Epoch [17/200] Loss: 76.70919036865234\n",
      "Epoch [18/200] Loss: 74.7805404663086\n",
      "Epoch [19/200] Loss: 72.94380187988281\n",
      "Epoch [20/200] Loss: 71.1911849975586\n",
      "Epoch [21/200] Loss: 69.51541137695312\n",
      "Epoch [22/200] Loss: 67.90985107421875\n",
      "Epoch [23/200] Loss: 66.36843872070312\n",
      "Epoch [24/200] Loss: 64.88578796386719\n",
      "Epoch [25/200] Loss: 63.457183837890625\n",
      "Epoch [26/200] Loss: 62.078678131103516\n",
      "Epoch [27/200] Loss: 60.74695587158203\n",
      "Epoch [28/200] Loss: 59.45925521850586\n",
      "Epoch [29/200] Loss: 58.213287353515625\n",
      "Epoch [30/200] Loss: 57.00713348388672\n",
      "Epoch [31/200] Loss: 55.839088439941406\n",
      "Epoch [32/200] Loss: 54.70761489868164\n",
      "Epoch [33/200] Loss: 53.61128234863281\n",
      "Epoch [34/200] Loss: 52.548728942871094\n",
      "Epoch [35/200] Loss: 51.518592834472656\n",
      "Epoch [36/200] Loss: 50.519588470458984\n",
      "Epoch [37/200] Loss: 49.55044174194336\n",
      "Epoch [38/200] Loss: 48.60988998413086\n",
      "Epoch [39/200] Loss: 47.696781158447266\n",
      "Epoch [40/200] Loss: 46.810020446777344\n",
      "Epoch [41/200] Loss: 45.948585510253906\n",
      "Epoch [42/200] Loss: 45.11155700683594\n",
      "Epoch [43/200] Loss: 44.29803466796875\n",
      "Epoch [44/200] Loss: 43.507144927978516\n",
      "Epoch [45/200] Loss: 42.738006591796875\n",
      "Epoch [46/200] Loss: 41.98973846435547\n",
      "Epoch [47/200] Loss: 41.26138687133789\n",
      "Epoch [48/200] Loss: 40.55206298828125\n",
      "Epoch [49/200] Loss: 39.86091613769531\n",
      "Epoch [50/200] Loss: 39.187156677246094\n",
      "Epoch [51/200] Loss: 38.53009033203125\n",
      "Epoch [52/200] Loss: 37.889095306396484\n",
      "Epoch [53/200] Loss: 37.263607025146484\n",
      "Epoch [54/200] Loss: 36.65313720703125\n",
      "Epoch [55/200] Loss: 36.0572509765625\n",
      "Epoch [56/200] Loss: 35.47554016113281\n",
      "Epoch [57/200] Loss: 34.90765380859375\n",
      "Epoch [58/200] Loss: 34.353248596191406\n",
      "Epoch [59/200] Loss: 33.811988830566406\n",
      "Epoch [60/200] Loss: 33.2835693359375\n",
      "Epoch [61/200] Loss: 32.76769256591797\n",
      "Epoch [62/200] Loss: 32.26407241821289\n",
      "Epoch [63/200] Loss: 31.772409439086914\n",
      "Epoch [64/200] Loss: 31.29243278503418\n",
      "Epoch [65/200] Loss: 30.823869705200195\n",
      "Epoch [66/200] Loss: 30.366424560546875\n",
      "Epoch [67/200] Loss: 29.919841766357422\n",
      "Epoch [68/200] Loss: 29.483867645263672\n",
      "Epoch [69/200] Loss: 29.058250427246094\n",
      "Epoch [70/200] Loss: 28.64275550842285\n",
      "Epoch [71/200] Loss: 28.23716926574707\n",
      "Epoch [72/200] Loss: 27.841272354125977\n",
      "Epoch [73/200] Loss: 27.45487403869629\n",
      "Epoch [74/200] Loss: 27.077789306640625\n",
      "Epoch [75/200] Loss: 26.7098445892334\n",
      "Epoch [76/200] Loss: 26.350887298583984\n",
      "Epoch [77/200] Loss: 26.000734329223633\n",
      "Epoch [78/200] Loss: 25.659231185913086\n",
      "Epoch [79/200] Loss: 25.326242446899414\n",
      "Epoch [80/200] Loss: 25.001596450805664\n",
      "Epoch [81/200] Loss: 24.685144424438477\n",
      "Epoch [82/200] Loss: 24.376728057861328\n",
      "Epoch [83/200] Loss: 24.076189041137695\n",
      "Epoch [84/200] Loss: 23.78338623046875\n",
      "Epoch [85/200] Loss: 23.498146057128906\n",
      "Epoch [86/200] Loss: 23.220325469970703\n",
      "Epoch [87/200] Loss: 22.949777603149414\n",
      "Epoch [88/200] Loss: 22.68633460998535\n",
      "Epoch [89/200] Loss: 22.42986297607422\n",
      "Epoch [90/200] Loss: 22.18020248413086\n",
      "Epoch [91/200] Loss: 21.937213897705078\n",
      "Epoch [92/200] Loss: 21.700754165649414\n",
      "Epoch [93/200] Loss: 21.47067642211914\n",
      "Epoch [94/200] Loss: 21.246854782104492\n",
      "Epoch [95/200] Loss: 21.02913475036621\n",
      "Epoch [96/200] Loss: 20.817392349243164\n",
      "Epoch [97/200] Loss: 20.611495971679688\n",
      "Epoch [98/200] Loss: 20.411304473876953\n",
      "Epoch [99/200] Loss: 20.216703414916992\n",
      "Epoch [100/200] Loss: 20.02755355834961\n",
      "Epoch [101/200] Loss: 19.843746185302734\n",
      "Epoch [102/200] Loss: 19.665138244628906\n",
      "Epoch [103/200] Loss: 19.49163055419922\n",
      "Epoch [104/200] Loss: 19.32309341430664\n",
      "Epoch [105/200] Loss: 19.159408569335938\n",
      "Epoch [106/200] Loss: 19.000476837158203\n",
      "Epoch [107/200] Loss: 18.846176147460938\n",
      "Epoch [108/200] Loss: 18.696392059326172\n",
      "Epoch [109/200] Loss: 18.551021575927734\n",
      "Epoch [110/200] Loss: 18.409963607788086\n",
      "Epoch [111/200] Loss: 18.273101806640625\n",
      "Epoch [112/200] Loss: 18.140342712402344\n",
      "Epoch [113/200] Loss: 18.011581420898438\n",
      "Epoch [114/200] Loss: 17.88671875\n",
      "Epoch [115/200] Loss: 17.76565933227539\n",
      "Epoch [116/200] Loss: 17.6483097076416\n",
      "Epoch [117/200] Loss: 17.534570693969727\n",
      "Epoch [118/200] Loss: 17.42435073852539\n",
      "Epoch [119/200] Loss: 17.317562103271484\n",
      "Epoch [120/200] Loss: 17.21411895751953\n",
      "Epoch [121/200] Loss: 17.113929748535156\n",
      "Epoch [122/200] Loss: 17.01691436767578\n",
      "Epoch [123/200] Loss: 16.92298126220703\n",
      "Epoch [124/200] Loss: 16.832056045532227\n",
      "Epoch [125/200] Loss: 16.744054794311523\n",
      "Epoch [126/200] Loss: 16.658899307250977\n",
      "Epoch [127/200] Loss: 16.576513290405273\n",
      "Epoch [128/200] Loss: 16.496822357177734\n",
      "Epoch [129/200] Loss: 16.419750213623047\n",
      "Epoch [130/200] Loss: 16.34522247314453\n",
      "Epoch [131/200] Loss: 16.273178100585938\n",
      "Epoch [132/200] Loss: 16.203535079956055\n",
      "Epoch [133/200] Loss: 16.136232376098633\n",
      "Epoch [134/200] Loss: 16.07120132446289\n",
      "Epoch [135/200] Loss: 16.008380889892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [136/200] Loss: 15.947702407836914\n",
      "Epoch [137/200] Loss: 15.889108657836914\n",
      "Epoch [138/200] Loss: 15.832530975341797\n",
      "Epoch [139/200] Loss: 15.77791976928711\n",
      "Epoch [140/200] Loss: 15.725212097167969\n",
      "Epoch [141/200] Loss: 15.674347877502441\n",
      "Epoch [142/200] Loss: 15.625280380249023\n",
      "Epoch [143/200] Loss: 15.57795238494873\n",
      "Epoch [144/200] Loss: 15.532304763793945\n",
      "Epoch [145/200] Loss: 15.488289833068848\n",
      "Epoch [146/200] Loss: 15.44586181640625\n",
      "Epoch [147/200] Loss: 15.40496826171875\n",
      "Epoch [148/200] Loss: 15.365560531616211\n",
      "Epoch [149/200] Loss: 15.327592849731445\n",
      "Epoch [150/200] Loss: 15.291020393371582\n",
      "Epoch [151/200] Loss: 15.255793571472168\n",
      "Epoch [152/200] Loss: 15.221878051757812\n",
      "Epoch [153/200] Loss: 15.189228057861328\n",
      "Epoch [154/200] Loss: 15.157798767089844\n",
      "Epoch [155/200] Loss: 15.127555847167969\n",
      "Epoch [156/200] Loss: 15.098457336425781\n",
      "Epoch [157/200] Loss: 15.070466041564941\n",
      "Epoch [158/200] Loss: 15.04354476928711\n",
      "Epoch [159/200] Loss: 15.017658233642578\n",
      "Epoch [160/200] Loss: 14.992774963378906\n",
      "Epoch [161/200] Loss: 14.968852996826172\n",
      "Epoch [162/200] Loss: 14.945868492126465\n",
      "Epoch [163/200] Loss: 14.923784255981445\n",
      "Epoch [164/200] Loss: 14.902569770812988\n",
      "Epoch [165/200] Loss: 14.882194519042969\n",
      "Epoch [166/200] Loss: 14.862630844116211\n",
      "Epoch [167/200] Loss: 14.843849182128906\n",
      "Epoch [168/200] Loss: 14.825820922851562\n",
      "Epoch [169/200] Loss: 14.808525085449219\n",
      "Epoch [170/200] Loss: 14.7919282913208\n",
      "Epoch [171/200] Loss: 14.776008605957031\n",
      "Epoch [172/200] Loss: 14.7607421875\n",
      "Epoch [173/200] Loss: 14.746099472045898\n",
      "Epoch [174/200] Loss: 14.732065200805664\n",
      "Epoch [175/200] Loss: 14.71861457824707\n",
      "Epoch [176/200] Loss: 14.705724716186523\n",
      "Epoch [177/200] Loss: 14.693375587463379\n",
      "Epoch [178/200] Loss: 14.68154525756836\n",
      "Epoch [179/200] Loss: 14.670221328735352\n",
      "Epoch [180/200] Loss: 14.659372329711914\n",
      "Epoch [181/200] Loss: 14.648991584777832\n",
      "Epoch [182/200] Loss: 14.639055252075195\n",
      "Epoch [183/200] Loss: 14.629548072814941\n",
      "Epoch [184/200] Loss: 14.620452880859375\n",
      "Epoch [185/200] Loss: 14.611753463745117\n",
      "Epoch [186/200] Loss: 14.603433609008789\n",
      "Epoch [187/200] Loss: 14.595481872558594\n",
      "Epoch [188/200] Loss: 14.587879180908203\n",
      "Epoch [189/200] Loss: 14.580612182617188\n",
      "Epoch [190/200] Loss: 14.57366943359375\n",
      "Epoch [191/200] Loss: 14.567039489746094\n",
      "Epoch [192/200] Loss: 14.56070327758789\n",
      "Epoch [193/200] Loss: 14.554656982421875\n",
      "Epoch [194/200] Loss: 14.548885345458984\n",
      "Epoch [195/200] Loss: 14.543374061584473\n",
      "Epoch [196/200] Loss: 14.538116455078125\n",
      "Epoch [197/200] Loss: 14.533100128173828\n",
      "Epoch [198/200] Loss: 14.528313636779785\n",
      "Epoch [199/200] Loss: 14.523750305175781\n",
      "Epoch [200/200] Loss: 14.519399642944336\n",
      "Predicted days_remaining for parent_id 144: 10.471158981323242\n",
      "Training for parent_id 150...\n",
      "Epoch [1/200] Loss: 222.3502960205078\n",
      "Epoch [2/200] Loss: 215.67221069335938\n",
      "Epoch [3/200] Loss: 209.15081787109375\n",
      "Epoch [4/200] Loss: 202.82101440429688\n",
      "Epoch [5/200] Loss: 196.6986083984375\n",
      "Epoch [6/200] Loss: 190.78981018066406\n",
      "Epoch [7/200] Loss: 185.09860229492188\n",
      "Epoch [8/200] Loss: 179.62713623046875\n",
      "Epoch [9/200] Loss: 174.3753662109375\n",
      "Epoch [10/200] Loss: 169.3407440185547\n",
      "Epoch [11/200] Loss: 164.51869201660156\n",
      "Epoch [12/200] Loss: 159.902587890625\n",
      "Epoch [13/200] Loss: 155.4844512939453\n",
      "Epoch [14/200] Loss: 151.25582885742188\n",
      "Epoch [15/200] Loss: 147.20858764648438\n",
      "Epoch [16/200] Loss: 143.33555603027344\n",
      "Epoch [17/200] Loss: 139.63058471679688\n",
      "Epoch [18/200] Loss: 136.08827209472656\n",
      "Epoch [19/200] Loss: 132.70350646972656\n",
      "Epoch [20/200] Loss: 129.47105407714844\n",
      "Epoch [21/200] Loss: 126.38538360595703\n",
      "Epoch [22/200] Loss: 123.44047546386719\n",
      "Epoch [23/200] Loss: 120.62981414794922\n",
      "Epoch [24/200] Loss: 117.946533203125\n",
      "Epoch [25/200] Loss: 115.38347625732422\n",
      "Epoch [26/200] Loss: 112.93348693847656\n",
      "Epoch [27/200] Loss: 110.5893325805664\n",
      "Epoch [28/200] Loss: 108.34397888183594\n",
      "Epoch [29/200] Loss: 106.19060516357422\n",
      "Epoch [30/200] Loss: 104.122802734375\n",
      "Epoch [31/200] Loss: 102.13444519042969\n",
      "Epoch [32/200] Loss: 100.22002410888672\n",
      "Epoch [33/200] Loss: 98.37440490722656\n",
      "Epoch [34/200] Loss: 96.59295654296875\n",
      "Epoch [35/200] Loss: 94.8714828491211\n",
      "Epoch [36/200] Loss: 93.20606994628906\n",
      "Epoch [37/200] Loss: 91.59327697753906\n",
      "Epoch [38/200] Loss: 90.02983856201172\n",
      "Epoch [39/200] Loss: 88.5127944946289\n",
      "Epoch [40/200] Loss: 87.03942108154297\n",
      "Epoch [41/200] Loss: 85.60719299316406\n",
      "Epoch [42/200] Loss: 84.21379089355469\n",
      "Epoch [43/200] Loss: 82.85704040527344\n",
      "Epoch [44/200] Loss: 81.53491973876953\n",
      "Epoch [45/200] Loss: 80.24555969238281\n",
      "Epoch [46/200] Loss: 78.98722839355469\n",
      "Epoch [47/200] Loss: 77.75828552246094\n",
      "Epoch [48/200] Loss: 76.5572280883789\n",
      "Epoch [49/200] Loss: 75.38268280029297\n",
      "Epoch [50/200] Loss: 74.23334503173828\n",
      "Epoch [51/200] Loss: 73.10807800292969\n",
      "Epoch [52/200] Loss: 72.00579833984375\n",
      "Epoch [53/200] Loss: 70.92552947998047\n",
      "Epoch [54/200] Loss: 69.86640167236328\n",
      "Epoch [55/200] Loss: 68.82759094238281\n",
      "Epoch [56/200] Loss: 67.80840301513672\n",
      "Epoch [57/200] Loss: 66.80814361572266\n",
      "Epoch [58/200] Loss: 65.82624816894531\n",
      "Epoch [59/200] Loss: 64.86212921142578\n",
      "Epoch [60/200] Loss: 63.915306091308594\n",
      "Epoch [61/200] Loss: 62.985294342041016\n",
      "Epoch [62/200] Loss: 62.07171630859375\n",
      "Epoch [63/200] Loss: 61.17411422729492\n",
      "Epoch [64/200] Loss: 60.29212951660156\n",
      "Epoch [65/200] Loss: 59.425453186035156\n",
      "Epoch [66/200] Loss: 58.57368850708008\n",
      "Epoch [67/200] Loss: 57.73658752441406\n",
      "Epoch [68/200] Loss: 56.913822174072266\n",
      "Epoch [69/200] Loss: 56.10511779785156\n",
      "Epoch [70/200] Loss: 55.310211181640625\n",
      "Epoch [71/200] Loss: 54.528839111328125\n",
      "Epoch [72/200] Loss: 53.76076126098633\n",
      "Epoch [73/200] Loss: 53.00572204589844\n",
      "Epoch [74/200] Loss: 52.263511657714844\n",
      "Epoch [75/200] Loss: 51.53390884399414\n",
      "Epoch [76/200] Loss: 50.816673278808594\n",
      "Epoch [77/200] Loss: 50.11162567138672\n",
      "Epoch [78/200] Loss: 49.418540954589844\n",
      "Epoch [79/200] Loss: 48.73723220825195\n",
      "Epoch [80/200] Loss: 48.067501068115234\n",
      "Epoch [81/200] Loss: 47.409156799316406\n",
      "Epoch [82/200] Loss: 46.76203918457031\n",
      "Epoch [83/200] Loss: 46.125938415527344\n",
      "Epoch [84/200] Loss: 45.500709533691406\n",
      "Epoch [85/200] Loss: 44.88615417480469\n",
      "Epoch [86/200] Loss: 44.28212356567383\n",
      "Epoch [87/200] Loss: 43.688438415527344\n",
      "Epoch [88/200] Loss: 43.10496520996094\n",
      "Epoch [89/200] Loss: 42.531524658203125\n",
      "Epoch [90/200] Loss: 41.96795654296875\n",
      "Epoch [91/200] Loss: 41.41413879394531\n",
      "Epoch [92/200] Loss: 40.86989212036133\n",
      "Epoch [93/200] Loss: 40.33507537841797\n",
      "Epoch [94/200] Loss: 39.809566497802734\n",
      "Epoch [95/200] Loss: 39.293212890625\n",
      "Epoch [96/200] Loss: 38.78587341308594\n",
      "Epoch [97/200] Loss: 38.28740310668945\n",
      "Epoch [98/200] Loss: 37.797698974609375\n",
      "Epoch [99/200] Loss: 37.31660461425781\n",
      "Epoch [100/200] Loss: 36.8439826965332\n",
      "Epoch [101/200] Loss: 36.37974166870117\n",
      "Epoch [102/200] Loss: 35.92373275756836\n",
      "Epoch [103/200] Loss: 35.4758186340332\n",
      "Epoch [104/200] Loss: 35.03590393066406\n",
      "Epoch [105/200] Loss: 34.603858947753906\n",
      "Epoch [106/200] Loss: 34.17955780029297\n",
      "Epoch [107/200] Loss: 33.76291275024414\n",
      "Epoch [108/200] Loss: 33.3537712097168\n",
      "Epoch [109/200] Loss: 32.952049255371094\n",
      "Epoch [110/200] Loss: 32.5576171875\n",
      "Epoch [111/200] Loss: 32.170379638671875\n",
      "Epoch [112/200] Loss: 31.790205001831055\n",
      "Epoch [113/200] Loss: 31.417020797729492\n",
      "Epoch [114/200] Loss: 31.050691604614258\n",
      "Epoch [115/200] Loss: 30.69112777709961\n",
      "Epoch [116/200] Loss: 30.338245391845703\n",
      "Epoch [117/200] Loss: 29.991899490356445\n",
      "Epoch [118/200] Loss: 29.652019500732422\n",
      "Epoch [119/200] Loss: 29.318511962890625\n",
      "Epoch [120/200] Loss: 28.991247177124023\n",
      "Epoch [121/200] Loss: 28.67017364501953\n",
      "Epoch [122/200] Loss: 28.355154037475586\n",
      "Epoch [123/200] Loss: 28.04610824584961\n",
      "Epoch [124/200] Loss: 27.742969512939453\n",
      "Epoch [125/200] Loss: 27.445613861083984\n",
      "Epoch [126/200] Loss: 27.15395736694336\n",
      "Epoch [127/200] Loss: 26.867935180664062\n",
      "Epoch [128/200] Loss: 26.58742332458496\n",
      "Epoch [129/200] Loss: 26.312358856201172\n",
      "Epoch [130/200] Loss: 26.04265785217285\n",
      "Epoch [131/200] Loss: 25.778213500976562\n",
      "Epoch [132/200] Loss: 25.518957138061523\n",
      "Epoch [133/200] Loss: 25.264799118041992\n",
      "Epoch [134/200] Loss: 25.015670776367188\n",
      "Epoch [135/200] Loss: 24.771488189697266\n",
      "Epoch [136/200] Loss: 24.532155990600586\n",
      "Epoch [137/200] Loss: 24.297616958618164\n",
      "Epoch [138/200] Loss: 24.06777000427246\n",
      "Epoch [139/200] Loss: 23.84255599975586\n",
      "Epoch [140/200] Loss: 23.62190055847168\n",
      "Epoch [141/200] Loss: 23.405710220336914\n",
      "Epoch [142/200] Loss: 23.193923950195312\n",
      "Epoch [143/200] Loss: 22.98647689819336\n",
      "Epoch [144/200] Loss: 22.783279418945312\n",
      "Epoch [145/200] Loss: 22.584264755249023\n",
      "Epoch [146/200] Loss: 22.389371871948242\n",
      "Epoch [147/200] Loss: 22.198503494262695\n",
      "Epoch [148/200] Loss: 22.011640548706055\n",
      "Epoch [149/200] Loss: 21.828664779663086\n",
      "Epoch [150/200] Loss: 21.6495361328125\n",
      "Epoch [151/200] Loss: 21.474193572998047\n",
      "Epoch [152/200] Loss: 21.302547454833984\n",
      "Epoch [153/200] Loss: 21.13455581665039\n",
      "Epoch [154/200] Loss: 20.97013282775879\n",
      "Epoch [155/200] Loss: 20.809246063232422\n",
      "Epoch [156/200] Loss: 20.651805877685547\n",
      "Epoch [157/200] Loss: 20.497770309448242\n",
      "Epoch [158/200] Loss: 20.347064971923828\n",
      "Epoch [159/200] Loss: 20.19963264465332\n",
      "Epoch [160/200] Loss: 20.05541229248047\n",
      "Epoch [161/200] Loss: 19.91436004638672\n",
      "Epoch [162/200] Loss: 19.776405334472656\n",
      "Epoch [163/200] Loss: 19.641502380371094\n",
      "Epoch [164/200] Loss: 19.509578704833984\n",
      "Epoch [165/200] Loss: 19.38058853149414\n",
      "Epoch [166/200] Loss: 19.254478454589844\n",
      "Epoch [167/200] Loss: 19.13119888305664\n",
      "Epoch [168/200] Loss: 19.010690689086914\n",
      "Epoch [169/200] Loss: 18.89290428161621\n",
      "Epoch [170/200] Loss: 18.77779197692871\n",
      "Epoch [171/200] Loss: 18.66529083251953\n",
      "Epoch [172/200] Loss: 18.55535888671875\n",
      "Epoch [173/200] Loss: 18.447956085205078\n",
      "Epoch [174/200] Loss: 18.343021392822266\n",
      "Epoch [175/200] Loss: 18.240509033203125\n",
      "Epoch [176/200] Loss: 18.140378952026367\n",
      "Epoch [177/200] Loss: 18.042572021484375\n",
      "Epoch [178/200] Loss: 17.947057723999023\n",
      "Epoch [179/200] Loss: 17.853775024414062\n",
      "Epoch [180/200] Loss: 17.762699127197266\n",
      "Epoch [181/200] Loss: 17.67377281188965\n",
      "Epoch [182/200] Loss: 17.586950302124023\n",
      "Epoch [183/200] Loss: 17.502197265625\n",
      "Epoch [184/200] Loss: 17.419469833374023\n",
      "Epoch [185/200] Loss: 17.338729858398438\n",
      "Epoch [186/200] Loss: 17.259925842285156\n",
      "Epoch [187/200] Loss: 17.183029174804688\n",
      "Epoch [188/200] Loss: 17.10799789428711\n",
      "Epoch [189/200] Loss: 17.034793853759766\n",
      "Epoch [190/200] Loss: 16.96337890625\n",
      "Epoch [191/200] Loss: 16.893718719482422\n",
      "Epoch [192/200] Loss: 16.825769424438477\n",
      "Epoch [193/200] Loss: 16.759498596191406\n",
      "Epoch [194/200] Loss: 16.694869995117188\n",
      "Epoch [195/200] Loss: 16.63184928894043\n",
      "Epoch [196/200] Loss: 16.570404052734375\n",
      "Epoch [197/200] Loss: 16.510498046875\n",
      "Epoch [198/200] Loss: 16.452098846435547\n",
      "Epoch [199/200] Loss: 16.395177841186523\n",
      "Epoch [200/200] Loss: 16.339691162109375\n",
      "Predicted days_remaining for parent_id 150: 12.390543937683105\n",
      "Training for parent_id 166...\n",
      "Epoch [1/200] Loss: 2664.422119140625\n",
      "Epoch [2/200] Loss: 2637.419189453125\n",
      "Epoch [3/200] Loss: 2611.259033203125\n",
      "Epoch [4/200] Loss: 2586.13623046875\n",
      "Epoch [5/200] Loss: 2562.104736328125\n",
      "Epoch [6/200] Loss: 2539.113525390625\n",
      "Epoch [7/200] Loss: 2517.07275390625\n",
      "Epoch [8/200] Loss: 2495.909423828125\n",
      "Epoch [9/200] Loss: 2475.57763671875\n",
      "Epoch [10/200] Loss: 2456.055419921875\n",
      "Epoch [11/200] Loss: 2437.328125\n",
      "Epoch [12/200] Loss: 2419.378662109375\n",
      "Epoch [13/200] Loss: 2402.1845703125\n",
      "Epoch [14/200] Loss: 2385.714599609375\n",
      "Epoch [15/200] Loss: 2369.934814453125\n",
      "Epoch [16/200] Loss: 2354.809326171875\n",
      "Epoch [17/200] Loss: 2340.306640625\n",
      "Epoch [18/200] Loss: 2326.396728515625\n",
      "Epoch [19/200] Loss: 2313.052978515625\n",
      "Epoch [20/200] Loss: 2300.24853515625\n",
      "Epoch [21/200] Loss: 2287.95654296875\n",
      "Epoch [22/200] Loss: 2276.14990234375\n",
      "Epoch [23/200] Loss: 2264.7998046875\n",
      "Epoch [24/200] Loss: 2253.876708984375\n",
      "Epoch [25/200] Loss: 2243.35205078125\n",
      "Epoch [26/200] Loss: 2233.19580078125\n",
      "Epoch [27/200] Loss: 2223.376708984375\n",
      "Epoch [28/200] Loss: 2213.8642578125\n",
      "Epoch [29/200] Loss: 2204.6279296875\n",
      "Epoch [30/200] Loss: 2195.637939453125\n",
      "Epoch [31/200] Loss: 2186.866455078125\n",
      "Epoch [32/200] Loss: 2178.287109375\n",
      "Epoch [33/200] Loss: 2169.876708984375\n",
      "Epoch [34/200] Loss: 2161.614990234375\n",
      "Epoch [35/200] Loss: 2153.48388671875\n",
      "Epoch [36/200] Loss: 2145.46875\n",
      "Epoch [37/200] Loss: 2137.55859375\n",
      "Epoch [38/200] Loss: 2129.743408203125\n",
      "Epoch [39/200] Loss: 2122.015869140625\n",
      "Epoch [40/200] Loss: 2114.369873046875\n",
      "Epoch [41/200] Loss: 2106.801025390625\n",
      "Epoch [42/200] Loss: 2099.304443359375\n",
      "Epoch [43/200] Loss: 2091.876708984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/200] Loss: 2084.5146484375\n",
      "Epoch [45/200] Loss: 2077.21533203125\n",
      "Epoch [46/200] Loss: 2069.975341796875\n",
      "Epoch [47/200] Loss: 2062.792236328125\n",
      "Epoch [48/200] Loss: 2055.6630859375\n",
      "Epoch [49/200] Loss: 2048.5859375\n",
      "Epoch [50/200] Loss: 2041.558837890625\n",
      "Epoch [51/200] Loss: 2034.579345703125\n",
      "Epoch [52/200] Loss: 2027.646240234375\n",
      "Epoch [53/200] Loss: 2020.758056640625\n",
      "Epoch [54/200] Loss: 2013.9127197265625\n",
      "Epoch [55/200] Loss: 2007.1097412109375\n",
      "Epoch [56/200] Loss: 2000.3472900390625\n",
      "Epoch [57/200] Loss: 1993.625\n",
      "Epoch [58/200] Loss: 1986.94140625\n",
      "Epoch [59/200] Loss: 1980.2958984375\n",
      "Epoch [60/200] Loss: 1973.686767578125\n",
      "Epoch [61/200] Loss: 1967.1138916015625\n",
      "Epoch [62/200] Loss: 1960.575927734375\n",
      "Epoch [63/200] Loss: 1954.0721435546875\n",
      "Epoch [64/200] Loss: 1947.6019287109375\n",
      "Epoch [65/200] Loss: 1941.164794921875\n",
      "Epoch [66/200] Loss: 1934.75927734375\n",
      "Epoch [67/200] Loss: 1928.3853759765625\n",
      "Epoch [68/200] Loss: 1922.0421142578125\n",
      "Epoch [69/200] Loss: 1915.72900390625\n",
      "Epoch [70/200] Loss: 1909.4453125\n",
      "Epoch [71/200] Loss: 1903.190673828125\n",
      "Epoch [72/200] Loss: 1896.964599609375\n",
      "Epoch [73/200] Loss: 1890.7666015625\n",
      "Epoch [74/200] Loss: 1884.5960693359375\n",
      "Epoch [75/200] Loss: 1878.4525146484375\n",
      "Epoch [76/200] Loss: 1872.33544921875\n",
      "Epoch [77/200] Loss: 1866.244873046875\n",
      "Epoch [78/200] Loss: 1860.1795654296875\n",
      "Epoch [79/200] Loss: 1854.1400146484375\n",
      "Epoch [80/200] Loss: 1848.1251220703125\n",
      "Epoch [81/200] Loss: 1842.1351318359375\n",
      "Epoch [82/200] Loss: 1836.1690673828125\n",
      "Epoch [83/200] Loss: 1830.2271728515625\n",
      "Epoch [84/200] Loss: 1824.308349609375\n",
      "Epoch [85/200] Loss: 1818.4132080078125\n",
      "Epoch [86/200] Loss: 1812.5408935546875\n",
      "Epoch [87/200] Loss: 1806.691162109375\n",
      "Epoch [88/200] Loss: 1800.8638916015625\n",
      "Epoch [89/200] Loss: 1795.058349609375\n",
      "Epoch [90/200] Loss: 1789.27490234375\n",
      "Epoch [91/200] Loss: 1783.5125732421875\n",
      "Epoch [92/200] Loss: 1777.77197265625\n",
      "Epoch [93/200] Loss: 1772.0521240234375\n",
      "Epoch [94/200] Loss: 1766.3531494140625\n",
      "Epoch [95/200] Loss: 1760.674560546875\n",
      "Epoch [96/200] Loss: 1755.0162353515625\n",
      "Epoch [97/200] Loss: 1749.3780517578125\n",
      "Epoch [98/200] Loss: 1743.7598876953125\n",
      "Epoch [99/200] Loss: 1738.161376953125\n",
      "Epoch [100/200] Loss: 1732.5821533203125\n",
      "Epoch [101/200] Loss: 1727.0224609375\n",
      "Epoch [102/200] Loss: 1721.4814453125\n",
      "Epoch [103/200] Loss: 1715.959716796875\n",
      "Epoch [104/200] Loss: 1710.4560546875\n",
      "Epoch [105/200] Loss: 1704.9715576171875\n",
      "Epoch [106/200] Loss: 1699.5052490234375\n",
      "Epoch [107/200] Loss: 1694.0570068359375\n",
      "Epoch [108/200] Loss: 1688.627197265625\n",
      "Epoch [109/200] Loss: 1683.2147216796875\n",
      "Epoch [110/200] Loss: 1677.8203125\n",
      "Epoch [111/200] Loss: 1672.443115234375\n",
      "Epoch [112/200] Loss: 1667.0841064453125\n",
      "Epoch [113/200] Loss: 1661.741943359375\n",
      "Epoch [114/200] Loss: 1656.4163818359375\n",
      "Epoch [115/200] Loss: 1651.10888671875\n",
      "Epoch [116/200] Loss: 1645.817626953125\n",
      "Epoch [117/200] Loss: 1640.543212890625\n",
      "Epoch [118/200] Loss: 1635.28564453125\n",
      "Epoch [119/200] Loss: 1630.044189453125\n",
      "Epoch [120/200] Loss: 1624.819580078125\n",
      "Epoch [121/200] Loss: 1619.611083984375\n",
      "Epoch [122/200] Loss: 1614.4188232421875\n",
      "Epoch [123/200] Loss: 1609.242431640625\n",
      "Epoch [124/200] Loss: 1604.082275390625\n",
      "Epoch [125/200] Loss: 1598.9378662109375\n",
      "Epoch [126/200] Loss: 1593.8092041015625\n",
      "Epoch [127/200] Loss: 1588.6962890625\n",
      "Epoch [128/200] Loss: 1583.5987548828125\n",
      "Epoch [129/200] Loss: 1578.5166015625\n",
      "Epoch [130/200] Loss: 1573.4500732421875\n",
      "Epoch [131/200] Loss: 1568.3988037109375\n",
      "Epoch [132/200] Loss: 1563.362548828125\n",
      "Epoch [133/200] Loss: 1558.34130859375\n",
      "Epoch [134/200] Loss: 1553.33544921875\n",
      "Epoch [135/200] Loss: 1548.344482421875\n",
      "Epoch [136/200] Loss: 1543.367919921875\n",
      "Epoch [137/200] Loss: 1538.4063720703125\n",
      "Epoch [138/200] Loss: 1533.45947265625\n",
      "Epoch [139/200] Loss: 1528.527099609375\n",
      "Epoch [140/200] Loss: 1523.6092529296875\n",
      "Epoch [141/200] Loss: 1518.7064208984375\n",
      "Epoch [142/200] Loss: 1513.8172607421875\n",
      "Epoch [143/200] Loss: 1508.942626953125\n",
      "Epoch [144/200] Loss: 1504.0821533203125\n",
      "Epoch [145/200] Loss: 1499.236083984375\n",
      "Epoch [146/200] Loss: 1494.404052734375\n",
      "Epoch [147/200] Loss: 1489.5859375\n",
      "Epoch [148/200] Loss: 1484.7818603515625\n",
      "Epoch [149/200] Loss: 1479.991455078125\n",
      "Epoch [150/200] Loss: 1475.215087890625\n",
      "Epoch [151/200] Loss: 1470.4525146484375\n",
      "Epoch [152/200] Loss: 1465.7032470703125\n",
      "Epoch [153/200] Loss: 1460.9681396484375\n",
      "Epoch [154/200] Loss: 1456.24658203125\n",
      "Epoch [155/200] Loss: 1451.5382080078125\n",
      "Epoch [156/200] Loss: 1446.8436279296875\n",
      "Epoch [157/200] Loss: 1442.162109375\n",
      "Epoch [158/200] Loss: 1437.494384765625\n",
      "Epoch [159/200] Loss: 1432.83984375\n",
      "Epoch [160/200] Loss: 1428.1982421875\n",
      "Epoch [161/200] Loss: 1423.5699462890625\n",
      "Epoch [162/200] Loss: 1418.9549560546875\n",
      "Epoch [163/200] Loss: 1414.352783203125\n",
      "Epoch [164/200] Loss: 1409.763671875\n",
      "Epoch [165/200] Loss: 1405.18798828125\n",
      "Epoch [166/200] Loss: 1400.6248779296875\n",
      "Epoch [167/200] Loss: 1396.0745849609375\n",
      "Epoch [168/200] Loss: 1391.537353515625\n",
      "Epoch [169/200] Loss: 1387.0126953125\n",
      "Epoch [170/200] Loss: 1382.5009765625\n",
      "Epoch [171/200] Loss: 1378.00146484375\n",
      "Epoch [172/200] Loss: 1373.51513671875\n",
      "Epoch [173/200] Loss: 1369.041015625\n",
      "Epoch [174/200] Loss: 1364.5799560546875\n",
      "Epoch [175/200] Loss: 1360.130859375\n",
      "Epoch [176/200] Loss: 1355.694580078125\n",
      "Epoch [177/200] Loss: 1351.2706298828125\n",
      "Epoch [178/200] Loss: 1346.85888671875\n",
      "Epoch [179/200] Loss: 1342.4595947265625\n",
      "Epoch [180/200] Loss: 1338.07275390625\n",
      "Epoch [181/200] Loss: 1333.6978759765625\n",
      "Epoch [182/200] Loss: 1329.3353271484375\n",
      "Epoch [183/200] Loss: 1324.98486328125\n",
      "Epoch [184/200] Loss: 1320.646728515625\n",
      "Epoch [185/200] Loss: 1316.320556640625\n",
      "Epoch [186/200] Loss: 1312.006103515625\n",
      "Epoch [187/200] Loss: 1307.70458984375\n",
      "Epoch [188/200] Loss: 1303.4140625\n",
      "Epoch [189/200] Loss: 1299.1358642578125\n",
      "Epoch [190/200] Loss: 1294.8697509765625\n",
      "Epoch [191/200] Loss: 1290.614990234375\n",
      "Epoch [192/200] Loss: 1286.3724365234375\n",
      "Epoch [193/200] Loss: 1282.141357421875\n",
      "Epoch [194/200] Loss: 1277.92236328125\n",
      "Epoch [195/200] Loss: 1273.7149658203125\n",
      "Epoch [196/200] Loss: 1269.51904296875\n",
      "Epoch [197/200] Loss: 1265.3353271484375\n",
      "Epoch [198/200] Loss: 1261.1624755859375\n",
      "Epoch [199/200] Loss: 1257.0015869140625\n",
      "Epoch [200/200] Loss: 1252.852294921875\n",
      "Predicted days_remaining for parent_id 166: 16.617725372314453\n",
      "Training for parent_id 168...\n",
      "Epoch [1/200] Loss: 404.34588623046875\n",
      "Epoch [2/200] Loss: 394.3899230957031\n",
      "Epoch [3/200] Loss: 384.6529235839844\n",
      "Epoch [4/200] Loss: 375.2071228027344\n",
      "Epoch [5/200] Loss: 366.1052551269531\n",
      "Epoch [6/200] Loss: 357.3782653808594\n",
      "Epoch [7/200] Loss: 349.0284729003906\n",
      "Epoch [8/200] Loss: 341.0392150878906\n",
      "Epoch [9/200] Loss: 333.38983154296875\n",
      "Epoch [10/200] Loss: 326.06524658203125\n",
      "Epoch [11/200] Loss: 319.056884765625\n",
      "Epoch [12/200] Loss: 312.36029052734375\n",
      "Epoch [13/200] Loss: 305.9732971191406\n",
      "Epoch [14/200] Loss: 299.89349365234375\n",
      "Epoch [15/200] Loss: 294.1172790527344\n",
      "Epoch [16/200] Loss: 288.6383056640625\n",
      "Epoch [17/200] Loss: 283.44732666015625\n",
      "Epoch [18/200] Loss: 278.5318603515625\n",
      "Epoch [19/200] Loss: 273.8763732910156\n",
      "Epoch [20/200] Loss: 269.46337890625\n",
      "Epoch [21/200] Loss: 265.2742919921875\n",
      "Epoch [22/200] Loss: 261.29046630859375\n",
      "Epoch [23/200] Loss: 257.4942932128906\n",
      "Epoch [24/200] Loss: 253.86903381347656\n",
      "Epoch [25/200] Loss: 250.3994598388672\n",
      "Epoch [26/200] Loss: 247.07150268554688\n",
      "Epoch [27/200] Loss: 243.872314453125\n",
      "Epoch [28/200] Loss: 240.7901153564453\n",
      "Epoch [29/200] Loss: 237.8141326904297\n",
      "Epoch [30/200] Loss: 234.93463134765625\n",
      "Epoch [31/200] Loss: 232.14260864257812\n",
      "Epoch [32/200] Loss: 229.429931640625\n",
      "Epoch [33/200] Loss: 226.78919982910156\n",
      "Epoch [34/200] Loss: 224.21372985839844\n",
      "Epoch [35/200] Loss: 221.69757080078125\n",
      "Epoch [36/200] Loss: 219.23541259765625\n",
      "Epoch [37/200] Loss: 216.8226776123047\n",
      "Epoch [38/200] Loss: 214.45535278320312\n",
      "Epoch [39/200] Loss: 212.13003540039062\n",
      "Epoch [40/200] Loss: 209.84396362304688\n",
      "Epoch [41/200] Loss: 207.5946807861328\n",
      "Epoch [42/200] Loss: 205.38021850585938\n",
      "Epoch [43/200] Loss: 203.19886779785156\n",
      "Epoch [44/200] Loss: 201.04904174804688\n",
      "Epoch [45/200] Loss: 198.92938232421875\n",
      "Epoch [46/200] Loss: 196.83868408203125\n",
      "Epoch [47/200] Loss: 194.7757110595703\n",
      "Epoch [48/200] Loss: 192.73951721191406\n",
      "Epoch [49/200] Loss: 190.72915649414062\n",
      "Epoch [50/200] Loss: 188.7437744140625\n",
      "Epoch [51/200] Loss: 186.7825927734375\n",
      "Epoch [52/200] Loss: 184.84494018554688\n",
      "Epoch [53/200] Loss: 182.93026733398438\n",
      "Epoch [54/200] Loss: 181.03793334960938\n",
      "Epoch [55/200] Loss: 179.16749572753906\n",
      "Epoch [56/200] Loss: 177.31854248046875\n",
      "Epoch [57/200] Loss: 175.4905548095703\n",
      "Epoch [58/200] Loss: 173.6831817626953\n",
      "Epoch [59/200] Loss: 171.89614868164062\n",
      "Epoch [60/200] Loss: 170.1289520263672\n",
      "Epoch [61/200] Loss: 168.38137817382812\n",
      "Epoch [62/200] Loss: 166.653076171875\n",
      "Epoch [63/200] Loss: 164.9437713623047\n",
      "Epoch [64/200] Loss: 163.2531280517578\n",
      "Epoch [65/200] Loss: 161.58094787597656\n",
      "Epoch [66/200] Loss: 159.92689514160156\n",
      "Epoch [67/200] Loss: 158.29071044921875\n",
      "Epoch [68/200] Loss: 156.67218017578125\n",
      "Epoch [69/200] Loss: 155.07101440429688\n",
      "Epoch [70/200] Loss: 153.4869842529297\n",
      "Epoch [71/200] Loss: 151.91981506347656\n",
      "Epoch [72/200] Loss: 150.36935424804688\n",
      "Epoch [73/200] Loss: 148.83538818359375\n",
      "Epoch [74/200] Loss: 147.31759643554688\n",
      "Epoch [75/200] Loss: 145.81578063964844\n",
      "Epoch [76/200] Loss: 144.32977294921875\n",
      "Epoch [77/200] Loss: 142.85935974121094\n",
      "Epoch [78/200] Loss: 141.404296875\n",
      "Epoch [79/200] Loss: 139.9644317626953\n",
      "Epoch [80/200] Loss: 138.53956604003906\n",
      "Epoch [81/200] Loss: 137.12948608398438\n",
      "Epoch [82/200] Loss: 135.7340087890625\n",
      "Epoch [83/200] Loss: 134.3529510498047\n",
      "Epoch [84/200] Loss: 132.9861602783203\n",
      "Epoch [85/200] Loss: 131.6334228515625\n",
      "Epoch [86/200] Loss: 130.29461669921875\n",
      "Epoch [87/200] Loss: 128.9695587158203\n",
      "Epoch [88/200] Loss: 127.65804290771484\n",
      "Epoch [89/200] Loss: 126.35997772216797\n",
      "Epoch [90/200] Loss: 125.07516479492188\n",
      "Epoch [91/200] Loss: 123.80345916748047\n",
      "Epoch [92/200] Loss: 122.54472351074219\n",
      "Epoch [93/200] Loss: 121.29878997802734\n",
      "Epoch [94/200] Loss: 120.0655517578125\n",
      "Epoch [95/200] Loss: 118.84481048583984\n",
      "Epoch [96/200] Loss: 117.63652038574219\n",
      "Epoch [97/200] Loss: 116.44050598144531\n",
      "Epoch [98/200] Loss: 115.25656127929688\n",
      "Epoch [99/200] Loss: 114.08466339111328\n",
      "Epoch [100/200] Loss: 112.92462158203125\n",
      "Epoch [101/200] Loss: 111.7763442993164\n",
      "Epoch [102/200] Loss: 110.63970184326172\n",
      "Epoch [103/200] Loss: 109.51455688476562\n",
      "Epoch [104/200] Loss: 108.40084075927734\n",
      "Epoch [105/200] Loss: 107.29839324951172\n",
      "Epoch [106/200] Loss: 106.20711517333984\n",
      "Epoch [107/200] Loss: 105.12692260742188\n",
      "Epoch [108/200] Loss: 104.05763244628906\n",
      "Epoch [109/200] Loss: 102.99917602539062\n",
      "Epoch [110/200] Loss: 101.95152282714844\n",
      "Epoch [111/200] Loss: 100.91446685791016\n",
      "Epoch [112/200] Loss: 99.88793182373047\n",
      "Epoch [113/200] Loss: 98.87184143066406\n",
      "Epoch [114/200] Loss: 97.86609649658203\n",
      "Epoch [115/200] Loss: 96.87055969238281\n",
      "Epoch [116/200] Loss: 95.88519287109375\n",
      "Epoch [117/200] Loss: 94.90985107421875\n",
      "Epoch [118/200] Loss: 93.94447326660156\n",
      "Epoch [119/200] Loss: 92.98892974853516\n",
      "Epoch [120/200] Loss: 92.04315185546875\n",
      "Epoch [121/200] Loss: 91.10710144042969\n",
      "Epoch [122/200] Loss: 90.18061828613281\n",
      "Epoch [123/200] Loss: 89.26363372802734\n",
      "Epoch [124/200] Loss: 88.3560562133789\n",
      "Epoch [125/200] Loss: 87.45783233642578\n",
      "Epoch [126/200] Loss: 86.56884765625\n",
      "Epoch [127/200] Loss: 85.68903350830078\n",
      "Epoch [128/200] Loss: 84.81831359863281\n",
      "Epoch [129/200] Loss: 83.95655059814453\n",
      "Epoch [130/200] Loss: 83.10376739501953\n",
      "Epoch [131/200] Loss: 82.25980377197266\n",
      "Epoch [132/200] Loss: 81.42460632324219\n",
      "Epoch [133/200] Loss: 80.59810638427734\n",
      "Epoch [134/200] Loss: 79.78020477294922\n",
      "Epoch [135/200] Loss: 78.97086334228516\n",
      "Epoch [136/200] Loss: 78.16995239257812\n",
      "Epoch [137/200] Loss: 77.37748718261719\n",
      "Epoch [138/200] Loss: 76.59327697753906\n",
      "Epoch [139/200] Loss: 75.81731414794922\n",
      "Epoch [140/200] Loss: 75.04952239990234\n",
      "Epoch [141/200] Loss: 74.28984832763672\n",
      "Epoch [142/200] Loss: 73.5381851196289\n",
      "Epoch [143/200] Loss: 72.79450225830078\n",
      "Epoch [144/200] Loss: 72.05870056152344\n",
      "Epoch [145/200] Loss: 71.33069610595703\n",
      "Epoch [146/200] Loss: 70.61044311523438\n",
      "Epoch [147/200] Loss: 69.89789581298828\n",
      "Epoch [148/200] Loss: 69.19294738769531\n",
      "Epoch [149/200] Loss: 68.49556732177734\n",
      "Epoch [150/200] Loss: 67.8056640625\n",
      "Epoch [151/200] Loss: 67.1231689453125\n",
      "Epoch [152/200] Loss: 66.44805145263672\n",
      "Epoch [153/200] Loss: 65.78021240234375\n",
      "Epoch [154/200] Loss: 65.11958312988281\n",
      "Epoch [155/200] Loss: 64.46614837646484\n",
      "Epoch [156/200] Loss: 63.81978225708008\n",
      "Epoch [157/200] Loss: 63.18048858642578\n",
      "Epoch [158/200] Loss: 62.548160552978516\n",
      "Epoch [159/200] Loss: 61.922760009765625\n",
      "Epoch [160/200] Loss: 61.3041877746582\n",
      "Epoch [161/200] Loss: 60.69244384765625\n",
      "Epoch [162/200] Loss: 60.087406158447266\n",
      "Epoch [163/200] Loss: 59.489070892333984\n",
      "Epoch [164/200] Loss: 58.8973274230957\n",
      "Epoch [165/200] Loss: 58.312164306640625\n",
      "Epoch [166/200] Loss: 57.7335090637207\n",
      "Epoch [167/200] Loss: 57.161285400390625\n",
      "Epoch [168/200] Loss: 56.595436096191406\n",
      "Epoch [169/200] Loss: 56.03592300415039\n",
      "Epoch [170/200] Loss: 55.482696533203125\n",
      "Epoch [171/200] Loss: 54.9356803894043\n",
      "Epoch [172/200] Loss: 54.39482498168945\n",
      "Epoch [173/200] Loss: 53.86008834838867\n",
      "Epoch [174/200] Loss: 53.331382751464844\n",
      "Epoch [175/200] Loss: 52.80870819091797\n",
      "Epoch [176/200] Loss: 52.291954040527344\n",
      "Epoch [177/200] Loss: 51.781070709228516\n",
      "Epoch [178/200] Loss: 51.27606964111328\n",
      "Epoch [179/200] Loss: 50.776824951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/200] Loss: 50.28333282470703\n",
      "Epoch [181/200] Loss: 49.79549026489258\n",
      "Epoch [182/200] Loss: 49.31329345703125\n",
      "Epoch [183/200] Loss: 48.8366584777832\n",
      "Epoch [184/200] Loss: 48.36556625366211\n",
      "Epoch [185/200] Loss: 47.89994812011719\n",
      "Epoch [186/200] Loss: 47.43975067138672\n",
      "Epoch [187/200] Loss: 46.984928131103516\n",
      "Epoch [188/200] Loss: 46.53541946411133\n",
      "Epoch [189/200] Loss: 46.091190338134766\n",
      "Epoch [190/200] Loss: 45.65218734741211\n",
      "Epoch [191/200] Loss: 45.2183723449707\n",
      "Epoch [192/200] Loss: 44.78968048095703\n",
      "Epoch [193/200] Loss: 44.366058349609375\n",
      "Epoch [194/200] Loss: 43.94748306274414\n",
      "Epoch [195/200] Loss: 43.533878326416016\n",
      "Epoch [196/200] Loss: 43.12520217895508\n",
      "Epoch [197/200] Loss: 42.72145080566406\n",
      "Epoch [198/200] Loss: 42.322532653808594\n",
      "Epoch [199/200] Loss: 41.92840576171875\n",
      "Epoch [200/200] Loss: 41.53902816772461\n",
      "Predicted days_remaining for parent_id 168: 14.581165313720703\n",
      "Training for parent_id 177...\n",
      "Epoch [1/200] Loss: 2895.53125\n",
      "Epoch [2/200] Loss: 2868.75390625\n",
      "Epoch [3/200] Loss: 2842.557373046875\n",
      "Epoch [4/200] Loss: 2817.25146484375\n",
      "Epoch [5/200] Loss: 2793.053955078125\n",
      "Epoch [6/200] Loss: 2770.09228515625\n",
      "Epoch [7/200] Loss: 2748.400146484375\n",
      "Epoch [8/200] Loss: 2727.951171875\n",
      "Epoch [9/200] Loss: 2708.68359375\n",
      "Epoch [10/200] Loss: 2690.509521484375\n",
      "Epoch [11/200] Loss: 2673.326904296875\n",
      "Epoch [12/200] Loss: 2657.02880859375\n",
      "Epoch [13/200] Loss: 2641.5126953125\n",
      "Epoch [14/200] Loss: 2626.68359375\n",
      "Epoch [15/200] Loss: 2612.457763671875\n",
      "Epoch [16/200] Loss: 2598.7646484375\n",
      "Epoch [17/200] Loss: 2585.547607421875\n",
      "Epoch [18/200] Loss: 2572.765625\n",
      "Epoch [19/200] Loss: 2560.38818359375\n",
      "Epoch [20/200] Loss: 2548.395263671875\n",
      "Epoch [21/200] Loss: 2536.772705078125\n",
      "Epoch [22/200] Loss: 2525.507080078125\n",
      "Epoch [23/200] Loss: 2514.582763671875\n",
      "Epoch [24/200] Loss: 2503.98046875\n",
      "Epoch [25/200] Loss: 2493.67919921875\n",
      "Epoch [26/200] Loss: 2483.654052734375\n",
      "Epoch [27/200] Loss: 2473.881103515625\n",
      "Epoch [28/200] Loss: 2464.33544921875\n",
      "Epoch [29/200] Loss: 2454.99365234375\n",
      "Epoch [30/200] Loss: 2445.834716796875\n",
      "Epoch [31/200] Loss: 2436.84130859375\n",
      "Epoch [32/200] Loss: 2427.998291015625\n",
      "Epoch [33/200] Loss: 2419.29345703125\n",
      "Epoch [34/200] Loss: 2410.718994140625\n",
      "Epoch [35/200] Loss: 2402.268310546875\n",
      "Epoch [36/200] Loss: 2393.93701171875\n",
      "Epoch [37/200] Loss: 2385.719970703125\n",
      "Epoch [38/200] Loss: 2377.6123046875\n",
      "Epoch [39/200] Loss: 2369.60498046875\n",
      "Epoch [40/200] Loss: 2361.690185546875\n",
      "Epoch [41/200] Loss: 2353.8583984375\n",
      "Epoch [42/200] Loss: 2346.10009765625\n",
      "Epoch [43/200] Loss: 2338.408935546875\n",
      "Epoch [44/200] Loss: 2330.779296875\n",
      "Epoch [45/200] Loss: 2323.206298828125\n",
      "Epoch [46/200] Loss: 2315.6875\n",
      "Epoch [47/200] Loss: 2308.219482421875\n",
      "Epoch [48/200] Loss: 2300.80126953125\n",
      "Epoch [49/200] Loss: 2293.4296875\n",
      "Epoch [50/200] Loss: 2286.103759765625\n",
      "Epoch [51/200] Loss: 2278.820556640625\n",
      "Epoch [52/200] Loss: 2271.578369140625\n",
      "Epoch [53/200] Loss: 2264.37646484375\n",
      "Epoch [54/200] Loss: 2257.213134765625\n",
      "Epoch [55/200] Loss: 2250.08740234375\n",
      "Epoch [56/200] Loss: 2242.99853515625\n",
      "Epoch [57/200] Loss: 2235.94580078125\n",
      "Epoch [58/200] Loss: 2228.9287109375\n",
      "Epoch [59/200] Loss: 2221.94677734375\n",
      "Epoch [60/200] Loss: 2214.999755859375\n",
      "Epoch [61/200] Loss: 2208.086181640625\n",
      "Epoch [62/200] Loss: 2201.20654296875\n",
      "Epoch [63/200] Loss: 2194.359130859375\n",
      "Epoch [64/200] Loss: 2187.5439453125\n",
      "Epoch [65/200] Loss: 2180.7607421875\n",
      "Epoch [66/200] Loss: 2174.007568359375\n",
      "Epoch [67/200] Loss: 2167.285400390625\n",
      "Epoch [68/200] Loss: 2160.5927734375\n",
      "Epoch [69/200] Loss: 2153.9287109375\n",
      "Epoch [70/200] Loss: 2147.2939453125\n",
      "Epoch [71/200] Loss: 2140.6865234375\n",
      "Epoch [72/200] Loss: 2134.107177734375\n",
      "Epoch [73/200] Loss: 2127.5546875\n",
      "Epoch [74/200] Loss: 2121.029052734375\n",
      "Epoch [75/200] Loss: 2114.52978515625\n",
      "Epoch [76/200] Loss: 2108.056884765625\n",
      "Epoch [77/200] Loss: 2101.608642578125\n",
      "Epoch [78/200] Loss: 2095.1865234375\n",
      "Epoch [79/200] Loss: 2088.7890625\n",
      "Epoch [80/200] Loss: 2082.415771484375\n",
      "Epoch [81/200] Loss: 2076.067138671875\n",
      "Epoch [82/200] Loss: 2069.742431640625\n",
      "Epoch [83/200] Loss: 2063.44091796875\n",
      "Epoch [84/200] Loss: 2057.1630859375\n",
      "Epoch [85/200] Loss: 2050.908447265625\n",
      "Epoch [86/200] Loss: 2044.676513671875\n",
      "Epoch [87/200] Loss: 2038.466552734375\n",
      "Epoch [88/200] Loss: 2032.279296875\n",
      "Epoch [89/200] Loss: 2026.1141357421875\n",
      "Epoch [90/200] Loss: 2019.970703125\n",
      "Epoch [91/200] Loss: 2013.8486328125\n",
      "Epoch [92/200] Loss: 2007.7481689453125\n",
      "Epoch [93/200] Loss: 2001.6680908203125\n",
      "Epoch [94/200] Loss: 1995.609375\n",
      "Epoch [95/200] Loss: 1989.5711669921875\n",
      "Epoch [96/200] Loss: 1983.552978515625\n",
      "Epoch [97/200] Loss: 1977.55517578125\n",
      "Epoch [98/200] Loss: 1971.5771484375\n",
      "Epoch [99/200] Loss: 1965.6190185546875\n",
      "Epoch [100/200] Loss: 1959.6807861328125\n",
      "Epoch [101/200] Loss: 1953.7615966796875\n",
      "Epoch [102/200] Loss: 1947.8614501953125\n",
      "Epoch [103/200] Loss: 1941.9805908203125\n",
      "Epoch [104/200] Loss: 1936.11865234375\n",
      "Epoch [105/200] Loss: 1930.275390625\n",
      "Epoch [106/200] Loss: 1924.45068359375\n",
      "Epoch [107/200] Loss: 1918.6441650390625\n",
      "Epoch [108/200] Loss: 1912.8560791015625\n",
      "Epoch [109/200] Loss: 1907.08544921875\n",
      "Epoch [110/200] Loss: 1901.33349609375\n",
      "Epoch [111/200] Loss: 1895.598876953125\n",
      "Epoch [112/200] Loss: 1889.8817138671875\n",
      "Epoch [113/200] Loss: 1884.1824951171875\n",
      "Epoch [114/200] Loss: 1878.5003662109375\n",
      "Epoch [115/200] Loss: 1872.8353271484375\n",
      "Epoch [116/200] Loss: 1867.1873779296875\n",
      "Epoch [117/200] Loss: 1861.5562744140625\n",
      "Epoch [118/200] Loss: 1855.9423828125\n",
      "Epoch [119/200] Loss: 1850.3448486328125\n",
      "Epoch [120/200] Loss: 1844.764404296875\n",
      "Epoch [121/200] Loss: 1839.1998291015625\n",
      "Epoch [122/200] Loss: 1833.6517333984375\n",
      "Epoch [123/200] Loss: 1828.1202392578125\n",
      "Epoch [124/200] Loss: 1822.6043701171875\n",
      "Epoch [125/200] Loss: 1817.10498046875\n",
      "Epoch [126/200] Loss: 1811.6212158203125\n",
      "Epoch [127/200] Loss: 1806.153564453125\n",
      "Epoch [128/200] Loss: 1800.701171875\n",
      "Epoch [129/200] Loss: 1795.2650146484375\n",
      "Epoch [130/200] Loss: 1789.8438720703125\n",
      "Epoch [131/200] Loss: 1784.4385986328125\n",
      "Epoch [132/200] Loss: 1779.0482177734375\n",
      "Epoch [133/200] Loss: 1773.673583984375\n",
      "Epoch [134/200] Loss: 1768.3138427734375\n",
      "Epoch [135/200] Loss: 1762.96923828125\n",
      "Epoch [136/200] Loss: 1757.6397705078125\n",
      "Epoch [137/200] Loss: 1752.3250732421875\n",
      "Epoch [138/200] Loss: 1747.0252685546875\n",
      "Epoch [139/200] Loss: 1741.7406005859375\n",
      "Epoch [140/200] Loss: 1736.470458984375\n",
      "Epoch [141/200] Loss: 1731.214599609375\n",
      "Epoch [142/200] Loss: 1725.97314453125\n",
      "Epoch [143/200] Loss: 1720.74658203125\n",
      "Epoch [144/200] Loss: 1715.53466796875\n",
      "Epoch [145/200] Loss: 1710.336669921875\n",
      "Epoch [146/200] Loss: 1705.1531982421875\n",
      "Epoch [147/200] Loss: 1699.983642578125\n",
      "Epoch [148/200] Loss: 1694.828125\n",
      "Epoch [149/200] Loss: 1689.68701171875\n",
      "Epoch [150/200] Loss: 1684.559814453125\n",
      "Epoch [151/200] Loss: 1679.4466552734375\n",
      "Epoch [152/200] Loss: 1674.34716796875\n",
      "Epoch [153/200] Loss: 1669.261474609375\n",
      "Epoch [154/200] Loss: 1664.1895751953125\n",
      "Epoch [155/200] Loss: 1659.13134765625\n",
      "Epoch [156/200] Loss: 1654.0869140625\n",
      "Epoch [157/200] Loss: 1649.0556640625\n",
      "Epoch [158/200] Loss: 1644.0384521484375\n",
      "Epoch [159/200] Loss: 1639.03466796875\n",
      "Epoch [160/200] Loss: 1634.0435791015625\n",
      "Epoch [161/200] Loss: 1629.0667724609375\n",
      "Epoch [162/200] Loss: 1624.1026611328125\n",
      "Epoch [163/200] Loss: 1619.15234375\n",
      "Epoch [164/200] Loss: 1614.21484375\n",
      "Epoch [165/200] Loss: 1609.290283203125\n",
      "Epoch [166/200] Loss: 1604.3787841796875\n",
      "Epoch [167/200] Loss: 1599.4808349609375\n",
      "Epoch [168/200] Loss: 1594.595947265625\n",
      "Epoch [169/200] Loss: 1589.723388671875\n",
      "Epoch [170/200] Loss: 1584.864013671875\n",
      "Epoch [171/200] Loss: 1580.017822265625\n",
      "Epoch [172/200] Loss: 1575.1842041015625\n",
      "Epoch [173/200] Loss: 1570.36328125\n",
      "Epoch [174/200] Loss: 1565.554931640625\n",
      "Epoch [175/200] Loss: 1560.7593994140625\n",
      "Epoch [176/200] Loss: 1555.9766845703125\n",
      "Epoch [177/200] Loss: 1551.20654296875\n",
      "Epoch [178/200] Loss: 1546.4483642578125\n",
      "Epoch [179/200] Loss: 1541.703125\n",
      "Epoch [180/200] Loss: 1536.9705810546875\n",
      "Epoch [181/200] Loss: 1532.2501220703125\n",
      "Epoch [182/200] Loss: 1527.5419921875\n",
      "Epoch [183/200] Loss: 1522.8465576171875\n",
      "Epoch [184/200] Loss: 1518.1630859375\n",
      "Epoch [185/200] Loss: 1513.491943359375\n",
      "Epoch [186/200] Loss: 1508.8330078125\n",
      "Epoch [187/200] Loss: 1504.1865234375\n",
      "Epoch [188/200] Loss: 1499.552001953125\n",
      "Epoch [189/200] Loss: 1494.929443359375\n",
      "Epoch [190/200] Loss: 1490.3192138671875\n",
      "Epoch [191/200] Loss: 1485.7208251953125\n",
      "Epoch [192/200] Loss: 1481.13427734375\n",
      "Epoch [193/200] Loss: 1476.5599365234375\n",
      "Epoch [194/200] Loss: 1471.9971923828125\n",
      "Epoch [195/200] Loss: 1467.44677734375\n",
      "Epoch [196/200] Loss: 1462.907958984375\n",
      "Epoch [197/200] Loss: 1458.3809814453125\n",
      "Epoch [198/200] Loss: 1453.865966796875\n",
      "Epoch [199/200] Loss: 1449.3624267578125\n",
      "Epoch [200/200] Loss: 1444.8707275390625\n",
      "Predicted days_remaining for parent_id 177: 15.98820686340332\n",
      "Training for parent_id 179...\n",
      "Epoch [1/200] Loss: 206.52569580078125\n",
      "Epoch [2/200] Loss: 200.638671875\n",
      "Epoch [3/200] Loss: 194.99513244628906\n",
      "Epoch [4/200] Loss: 189.605712890625\n",
      "Epoch [5/200] Loss: 184.45877075195312\n",
      "Epoch [6/200] Loss: 179.52854919433594\n",
      "Epoch [7/200] Loss: 174.78619384765625\n",
      "Epoch [8/200] Loss: 170.2083740234375\n",
      "Epoch [9/200] Loss: 165.77989196777344\n",
      "Epoch [10/200] Loss: 161.4921417236328\n",
      "Epoch [11/200] Loss: 157.3400421142578\n",
      "Epoch [12/200] Loss: 153.3201446533203\n",
      "Epoch [13/200] Loss: 149.42922973632812\n",
      "Epoch [14/200] Loss: 145.66397094726562\n",
      "Epoch [15/200] Loss: 142.02120971679688\n",
      "Epoch [16/200] Loss: 138.49803161621094\n",
      "Epoch [17/200] Loss: 135.09194946289062\n",
      "Epoch [18/200] Loss: 131.80087280273438\n",
      "Epoch [19/200] Loss: 128.62301635742188\n",
      "Epoch [20/200] Loss: 125.55663299560547\n",
      "Epoch [21/200] Loss: 122.59994506835938\n",
      "Epoch [22/200] Loss: 119.75086975097656\n",
      "Epoch [23/200] Loss: 117.00674438476562\n",
      "Epoch [24/200] Loss: 114.364501953125\n",
      "Epoch [25/200] Loss: 111.8204574584961\n",
      "Epoch [26/200] Loss: 109.37049865722656\n",
      "Epoch [27/200] Loss: 107.0102310180664\n",
      "Epoch [28/200] Loss: 104.73517608642578\n",
      "Epoch [29/200] Loss: 102.54098510742188\n",
      "Epoch [30/200] Loss: 100.42345428466797\n",
      "Epoch [31/200] Loss: 98.378662109375\n",
      "Epoch [32/200] Loss: 96.40278625488281\n",
      "Epoch [33/200] Loss: 94.49225616455078\n",
      "Epoch [34/200] Loss: 92.6435317993164\n",
      "Epoch [35/200] Loss: 90.8532485961914\n",
      "Epoch [36/200] Loss: 89.1181411743164\n",
      "Epoch [37/200] Loss: 87.43510437011719\n",
      "Epoch [38/200] Loss: 85.80123901367188\n",
      "Epoch [39/200] Loss: 84.21379089355469\n",
      "Epoch [40/200] Loss: 82.6702880859375\n",
      "Epoch [41/200] Loss: 81.16842651367188\n",
      "Epoch [42/200] Loss: 79.70604705810547\n",
      "Epoch [43/200] Loss: 78.28109741210938\n",
      "Epoch [44/200] Loss: 76.8917465209961\n",
      "Epoch [45/200] Loss: 75.53614807128906\n",
      "Epoch [46/200] Loss: 74.2126235961914\n",
      "Epoch [47/200] Loss: 72.9195556640625\n",
      "Epoch [48/200] Loss: 71.65548706054688\n",
      "Epoch [49/200] Loss: 70.41905212402344\n",
      "Epoch [50/200] Loss: 69.208984375\n",
      "Epoch [51/200] Loss: 68.024169921875\n",
      "Epoch [52/200] Loss: 66.86357879638672\n",
      "Epoch [53/200] Loss: 65.726318359375\n",
      "Epoch [54/200] Loss: 64.61157989501953\n",
      "Epoch [55/200] Loss: 63.51860427856445\n",
      "Epoch [56/200] Loss: 62.44675064086914\n",
      "Epoch [57/200] Loss: 61.39543914794922\n",
      "Epoch [58/200] Loss: 60.36415100097656\n",
      "Epoch [59/200] Loss: 59.352455139160156\n",
      "Epoch [60/200] Loss: 58.359920501708984\n",
      "Epoch [61/200] Loss: 57.386199951171875\n",
      "Epoch [62/200] Loss: 56.43104553222656\n",
      "Epoch [63/200] Loss: 55.49415969848633\n",
      "Epoch [64/200] Loss: 54.57532501220703\n",
      "Epoch [65/200] Loss: 53.67433547973633\n",
      "Epoch [66/200] Loss: 52.79099655151367\n",
      "Epoch [67/200] Loss: 51.92512512207031\n",
      "Epoch [68/200] Loss: 51.0765495300293\n",
      "Epoch [69/200] Loss: 50.245052337646484\n",
      "Epoch [70/200] Loss: 49.43044662475586\n",
      "Epoch [71/200] Loss: 48.63252258300781\n",
      "Epoch [72/200] Loss: 47.851043701171875\n",
      "Epoch [73/200] Loss: 47.08576965332031\n",
      "Epoch [74/200] Loss: 46.33646774291992\n",
      "Epoch [75/200] Loss: 45.60284423828125\n",
      "Epoch [76/200] Loss: 44.884639739990234\n",
      "Epoch [77/200] Loss: 44.18160629272461\n",
      "Epoch [78/200] Loss: 43.49342346191406\n",
      "Epoch [79/200] Loss: 42.81979751586914\n",
      "Epoch [80/200] Loss: 42.16049575805664\n",
      "Epoch [81/200] Loss: 41.51517105102539\n",
      "Epoch [82/200] Loss: 40.88356018066406\n",
      "Epoch [83/200] Loss: 40.2653923034668\n",
      "Epoch [84/200] Loss: 39.66039276123047\n",
      "Epoch [85/200] Loss: 39.06825256347656\n",
      "Epoch [86/200] Loss: 38.4887580871582\n",
      "Epoch [87/200] Loss: 37.92158889770508\n",
      "Epoch [88/200] Loss: 37.366546630859375\n",
      "Epoch [89/200] Loss: 36.823333740234375\n",
      "Epoch [90/200] Loss: 36.2917366027832\n",
      "Epoch [91/200] Loss: 35.77149200439453\n",
      "Epoch [92/200] Loss: 35.26237106323242\n",
      "Epoch [93/200] Loss: 34.76416778564453\n",
      "Epoch [94/200] Loss: 34.27663040161133\n",
      "Epoch [95/200] Loss: 33.79957962036133\n",
      "Epoch [96/200] Loss: 33.33275604248047\n",
      "Epoch [97/200] Loss: 32.87599182128906\n",
      "Epoch [98/200] Loss: 32.4290657043457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/200] Loss: 31.991796493530273\n",
      "Epoch [100/200] Loss: 31.56396484375\n",
      "Epoch [101/200] Loss: 31.145414352416992\n",
      "Epoch [102/200] Loss: 30.735942840576172\n",
      "Epoch [103/200] Loss: 30.33537483215332\n",
      "Epoch [104/200] Loss: 29.943523406982422\n",
      "Epoch [105/200] Loss: 29.560232162475586\n",
      "Epoch [106/200] Loss: 29.185314178466797\n",
      "Epoch [107/200] Loss: 28.81863021850586\n",
      "Epoch [108/200] Loss: 28.459978103637695\n",
      "Epoch [109/200] Loss: 28.109243392944336\n",
      "Epoch [110/200] Loss: 27.766258239746094\n",
      "Epoch [111/200] Loss: 27.43084716796875\n",
      "Epoch [112/200] Loss: 27.10288429260254\n",
      "Epoch [113/200] Loss: 26.782197952270508\n",
      "Epoch [114/200] Loss: 26.46868133544922\n",
      "Epoch [115/200] Loss: 26.16217803955078\n",
      "Epoch [116/200] Loss: 25.86254119873047\n",
      "Epoch [117/200] Loss: 25.569639205932617\n",
      "Epoch [118/200] Loss: 25.283348083496094\n",
      "Epoch [119/200] Loss: 25.0035343170166\n",
      "Epoch [120/200] Loss: 24.730056762695312\n",
      "Epoch [121/200] Loss: 24.462814331054688\n",
      "Epoch [122/200] Loss: 24.201671600341797\n",
      "Epoch [123/200] Loss: 23.94650650024414\n",
      "Epoch [124/200] Loss: 23.69719886779785\n",
      "Epoch [125/200] Loss: 23.453643798828125\n",
      "Epoch [126/200] Loss: 23.215717315673828\n",
      "Epoch [127/200] Loss: 22.983299255371094\n",
      "Epoch [128/200] Loss: 22.756301879882812\n",
      "Epoch [129/200] Loss: 22.534603118896484\n",
      "Epoch [130/200] Loss: 22.31809425354004\n",
      "Epoch [131/200] Loss: 22.106674194335938\n",
      "Epoch [132/200] Loss: 21.900259017944336\n",
      "Epoch [133/200] Loss: 21.698715209960938\n",
      "Epoch [134/200] Loss: 21.501962661743164\n",
      "Epoch [135/200] Loss: 21.309913635253906\n",
      "Epoch [136/200] Loss: 21.122438430786133\n",
      "Epoch [137/200] Loss: 20.93947982788086\n",
      "Epoch [138/200] Loss: 20.760923385620117\n",
      "Epoch [139/200] Loss: 20.586702346801758\n",
      "Epoch [140/200] Loss: 20.41670036315918\n",
      "Epoch [141/200] Loss: 20.250844955444336\n",
      "Epoch [142/200] Loss: 20.089038848876953\n",
      "Epoch [143/200] Loss: 19.93120574951172\n",
      "Epoch [144/200] Loss: 19.777273178100586\n",
      "Epoch [145/200] Loss: 19.627140045166016\n",
      "Epoch [146/200] Loss: 19.480737686157227\n",
      "Epoch [147/200] Loss: 19.33798599243164\n",
      "Epoch [148/200] Loss: 19.19880485534668\n",
      "Epoch [149/200] Loss: 19.06311798095703\n",
      "Epoch [150/200] Loss: 18.93085289001465\n",
      "Epoch [151/200] Loss: 18.80193519592285\n",
      "Epoch [152/200] Loss: 18.676288604736328\n",
      "Epoch [153/200] Loss: 18.553852081298828\n",
      "Epoch [154/200] Loss: 18.43454360961914\n",
      "Epoch [155/200] Loss: 18.31830596923828\n",
      "Epoch [156/200] Loss: 18.20507049560547\n",
      "Epoch [157/200] Loss: 18.09476089477539\n",
      "Epoch [158/200] Loss: 17.98732566833496\n",
      "Epoch [159/200] Loss: 17.8826847076416\n",
      "Epoch [160/200] Loss: 17.780799865722656\n",
      "Epoch [161/200] Loss: 17.681583404541016\n",
      "Epoch [162/200] Loss: 17.584991455078125\n",
      "Epoch [163/200] Loss: 17.49095916748047\n",
      "Epoch [164/200] Loss: 17.399433135986328\n",
      "Epoch [165/200] Loss: 17.31034278869629\n",
      "Epoch [166/200] Loss: 17.22365379333496\n",
      "Epoch [167/200] Loss: 17.139293670654297\n",
      "Epoch [168/200] Loss: 17.057214736938477\n",
      "Epoch [169/200] Loss: 16.97736358642578\n",
      "Epoch [170/200] Loss: 16.899686813354492\n",
      "Epoch [171/200] Loss: 16.82413673400879\n",
      "Epoch [172/200] Loss: 16.750656127929688\n",
      "Epoch [173/200] Loss: 16.6792049407959\n",
      "Epoch [174/200] Loss: 16.609725952148438\n",
      "Epoch [175/200] Loss: 16.542179107666016\n",
      "Epoch [176/200] Loss: 16.476518630981445\n",
      "Epoch [177/200] Loss: 16.41269302368164\n",
      "Epoch [178/200] Loss: 16.350664138793945\n",
      "Epoch [179/200] Loss: 16.290382385253906\n",
      "Epoch [180/200] Loss: 16.231807708740234\n",
      "Epoch [181/200] Loss: 16.174896240234375\n",
      "Epoch [182/200] Loss: 16.119613647460938\n",
      "Epoch [183/200] Loss: 16.06591033935547\n",
      "Epoch [184/200] Loss: 16.013750076293945\n",
      "Epoch [185/200] Loss: 15.963101387023926\n",
      "Epoch [186/200] Loss: 15.913915634155273\n",
      "Epoch [187/200] Loss: 15.866166114807129\n",
      "Epoch [188/200] Loss: 15.819801330566406\n",
      "Epoch [189/200] Loss: 15.774805068969727\n",
      "Epoch [190/200] Loss: 15.731125831604004\n",
      "Epoch [191/200] Loss: 15.688742637634277\n",
      "Epoch [192/200] Loss: 15.64760971069336\n",
      "Epoch [193/200] Loss: 15.607704162597656\n",
      "Epoch [194/200] Loss: 15.568991661071777\n",
      "Epoch [195/200] Loss: 15.531436920166016\n",
      "Epoch [196/200] Loss: 15.495012283325195\n",
      "Epoch [197/200] Loss: 15.459689140319824\n",
      "Epoch [198/200] Loss: 15.425440788269043\n",
      "Epoch [199/200] Loss: 15.392228126525879\n",
      "Epoch [200/200] Loss: 15.360034942626953\n",
      "Predicted days_remaining for parent_id 179: 12.805899620056152\n",
      "Training for parent_id 199...\n",
      "Epoch [1/200] Loss: 325.5508117675781\n",
      "Epoch [2/200] Loss: 316.8611145019531\n",
      "Epoch [3/200] Loss: 308.335693359375\n",
      "Epoch [4/200] Loss: 300.0299987792969\n",
      "Epoch [5/200] Loss: 292.0095520019531\n",
      "Epoch [6/200] Loss: 284.32403564453125\n",
      "Epoch [7/200] Loss: 277.0078125\n",
      "Epoch [8/200] Loss: 270.07928466796875\n",
      "Epoch [9/200] Loss: 263.5410461425781\n",
      "Epoch [10/200] Loss: 257.3829345703125\n",
      "Epoch [11/200] Loss: 251.58522033691406\n",
      "Epoch [12/200] Loss: 246.12217712402344\n",
      "Epoch [13/200] Loss: 240.9649658203125\n",
      "Epoch [14/200] Loss: 236.085205078125\n",
      "Epoch [15/200] Loss: 231.45703125\n",
      "Epoch [16/200] Loss: 227.05844116210938\n",
      "Epoch [17/200] Loss: 222.8716278076172\n",
      "Epoch [18/200] Loss: 218.88226318359375\n",
      "Epoch [19/200] Loss: 215.07847595214844\n",
      "Epoch [20/200] Loss: 211.44964599609375\n",
      "Epoch [21/200] Loss: 207.98550415039062\n",
      "Epoch [22/200] Loss: 204.67518615722656\n",
      "Epoch [23/200] Loss: 201.50706481933594\n",
      "Epoch [24/200] Loss: 198.46888732910156\n",
      "Epoch [25/200] Loss: 195.54820251464844\n",
      "Epoch [26/200] Loss: 192.73304748535156\n",
      "Epoch [27/200] Loss: 190.0126190185547\n",
      "Epoch [28/200] Loss: 187.3776092529297\n",
      "Epoch [29/200] Loss: 184.8205108642578\n",
      "Epoch [30/200] Loss: 182.33506774902344\n",
      "Epoch [31/200] Loss: 179.91627502441406\n",
      "Epoch [32/200] Loss: 177.559814453125\n",
      "Epoch [33/200] Loss: 175.2618865966797\n",
      "Epoch [34/200] Loss: 173.01895141601562\n",
      "Epoch [35/200] Loss: 170.82748413085938\n",
      "Epoch [36/200] Loss: 168.68414306640625\n",
      "Epoch [37/200] Loss: 166.58575439453125\n",
      "Epoch [38/200] Loss: 164.52915954589844\n",
      "Epoch [39/200] Loss: 162.51150512695312\n",
      "Epoch [40/200] Loss: 160.5301971435547\n",
      "Epoch [41/200] Loss: 158.58290100097656\n",
      "Epoch [42/200] Loss: 156.66766357421875\n",
      "Epoch [43/200] Loss: 154.7826690673828\n",
      "Epoch [44/200] Loss: 152.926513671875\n",
      "Epoch [45/200] Loss: 151.0978546142578\n",
      "Epoch [46/200] Loss: 149.29566955566406\n",
      "Epoch [47/200] Loss: 147.51895141601562\n",
      "Epoch [48/200] Loss: 145.76698303222656\n",
      "Epoch [49/200] Loss: 144.03890991210938\n",
      "Epoch [50/200] Loss: 142.3341827392578\n",
      "Epoch [51/200] Loss: 140.6521453857422\n",
      "Epoch [52/200] Loss: 138.99234008789062\n",
      "Epoch [53/200] Loss: 137.35418701171875\n",
      "Epoch [54/200] Loss: 135.73727416992188\n",
      "Epoch [55/200] Loss: 134.14117431640625\n",
      "Epoch [56/200] Loss: 132.56544494628906\n",
      "Epoch [57/200] Loss: 131.009765625\n",
      "Epoch [58/200] Loss: 129.47369384765625\n",
      "Epoch [59/200] Loss: 127.95694732666016\n",
      "Epoch [60/200] Loss: 126.45916748046875\n",
      "Epoch [61/200] Loss: 124.98008728027344\n",
      "Epoch [62/200] Loss: 123.51934051513672\n",
      "Epoch [63/200] Loss: 122.07669830322266\n",
      "Epoch [64/200] Loss: 120.65189361572266\n",
      "Epoch [65/200] Loss: 119.24463653564453\n",
      "Epoch [66/200] Loss: 117.85468292236328\n",
      "Epoch [67/200] Loss: 116.4817886352539\n",
      "Epoch [68/200] Loss: 115.12570190429688\n",
      "Epoch [69/200] Loss: 113.78620910644531\n",
      "Epoch [70/200] Loss: 112.46312713623047\n",
      "Epoch [71/200] Loss: 111.15616607666016\n",
      "Epoch [72/200] Loss: 109.86517333984375\n",
      "Epoch [73/200] Loss: 108.58993530273438\n",
      "Epoch [74/200] Loss: 107.33025360107422\n",
      "Epoch [75/200] Loss: 106.08594512939453\n",
      "Epoch [76/200] Loss: 104.85674285888672\n",
      "Epoch [77/200] Loss: 103.642578125\n",
      "Epoch [78/200] Loss: 102.44316864013672\n",
      "Epoch [79/200] Loss: 101.25835418701172\n",
      "Epoch [80/200] Loss: 100.08797454833984\n",
      "Epoch [81/200] Loss: 98.9318618774414\n",
      "Epoch [82/200] Loss: 97.789794921875\n",
      "Epoch [83/200] Loss: 96.66157531738281\n",
      "Epoch [84/200] Loss: 95.54714965820312\n",
      "Epoch [85/200] Loss: 94.44624328613281\n",
      "Epoch [86/200] Loss: 93.35871124267578\n",
      "Epoch [87/200] Loss: 92.28438568115234\n",
      "Epoch [88/200] Loss: 91.22315216064453\n",
      "Epoch [89/200] Loss: 90.1747817993164\n",
      "Epoch [90/200] Loss: 89.13916015625\n",
      "Epoch [91/200] Loss: 88.11610412597656\n",
      "Epoch [92/200] Loss: 87.1054458618164\n",
      "Epoch [93/200] Loss: 86.10708618164062\n",
      "Epoch [94/200] Loss: 85.12085723876953\n",
      "Epoch [95/200] Loss: 84.14656829833984\n",
      "Epoch [96/200] Loss: 83.18412780761719\n",
      "Epoch [97/200] Loss: 82.23340606689453\n",
      "Epoch [98/200] Loss: 81.294189453125\n",
      "Epoch [99/200] Loss: 80.36637115478516\n",
      "Epoch [100/200] Loss: 79.44984436035156\n",
      "Epoch [101/200] Loss: 78.54444885253906\n",
      "Epoch [102/200] Loss: 77.6500473022461\n",
      "Epoch [103/200] Loss: 76.7665786743164\n",
      "Epoch [104/200] Loss: 75.89379119873047\n",
      "Epoch [105/200] Loss: 75.03166961669922\n",
      "Epoch [106/200] Loss: 74.1800308227539\n",
      "Epoch [107/200] Loss: 73.33878326416016\n",
      "Epoch [108/200] Loss: 72.50779724121094\n",
      "Epoch [109/200] Loss: 71.68695068359375\n",
      "Epoch [110/200] Loss: 70.87613677978516\n",
      "Epoch [111/200] Loss: 70.07521057128906\n",
      "Epoch [112/200] Loss: 69.28412628173828\n",
      "Epoch [113/200] Loss: 68.50270080566406\n",
      "Epoch [114/200] Loss: 67.73088836669922\n",
      "Epoch [115/200] Loss: 66.96854400634766\n",
      "Epoch [116/200] Loss: 66.21556854248047\n",
      "Epoch [117/200] Loss: 65.47187042236328\n",
      "Epoch [118/200] Loss: 64.73731994628906\n",
      "Epoch [119/200] Loss: 64.0118179321289\n",
      "Epoch [120/200] Loss: 63.29533386230469\n",
      "Epoch [121/200] Loss: 62.587684631347656\n",
      "Epoch [122/200] Loss: 61.888797760009766\n",
      "Epoch [123/200] Loss: 61.19860076904297\n",
      "Epoch [124/200] Loss: 60.51700973510742\n",
      "Epoch [125/200] Loss: 59.8438835144043\n",
      "Epoch [126/200] Loss: 59.17915725708008\n",
      "Epoch [127/200] Loss: 58.52273178100586\n",
      "Epoch [128/200] Loss: 57.87453842163086\n",
      "Epoch [129/200] Loss: 57.23446273803711\n",
      "Epoch [130/200] Loss: 56.602420806884766\n",
      "Epoch [131/200] Loss: 55.978363037109375\n",
      "Epoch [132/200] Loss: 55.36216354370117\n",
      "Epoch [133/200] Loss: 54.75373077392578\n",
      "Epoch [134/200] Loss: 54.15302658081055\n",
      "Epoch [135/200] Loss: 53.5599479675293\n",
      "Epoch [136/200] Loss: 52.97438430786133\n",
      "Epoch [137/200] Loss: 52.396297454833984\n",
      "Epoch [138/200] Loss: 51.825584411621094\n",
      "Epoch [139/200] Loss: 51.262210845947266\n",
      "Epoch [140/200] Loss: 50.706016540527344\n",
      "Epoch [141/200] Loss: 50.1569938659668\n",
      "Epoch [142/200] Loss: 49.61503982543945\n",
      "Epoch [143/200] Loss: 49.080078125\n",
      "Epoch [144/200] Loss: 48.55203628540039\n",
      "Epoch [145/200] Loss: 48.030845642089844\n",
      "Epoch [146/200] Loss: 47.51643371582031\n",
      "Epoch [147/200] Loss: 47.00871658325195\n",
      "Epoch [148/200] Loss: 46.50765609741211\n",
      "Epoch [149/200] Loss: 46.013145446777344\n",
      "Epoch [150/200] Loss: 45.52511978149414\n",
      "Epoch [151/200] Loss: 45.043521881103516\n",
      "Epoch [152/200] Loss: 44.56826400756836\n",
      "Epoch [153/200] Loss: 44.09929656982422\n",
      "Epoch [154/200] Loss: 43.63655471801758\n",
      "Epoch [155/200] Loss: 43.17994689941406\n",
      "Epoch [156/200] Loss: 42.72944259643555\n",
      "Epoch [157/200] Loss: 42.284950256347656\n",
      "Epoch [158/200] Loss: 41.846397399902344\n",
      "Epoch [159/200] Loss: 41.413753509521484\n",
      "Epoch [160/200] Loss: 40.986915588378906\n",
      "Epoch [161/200] Loss: 40.56585693359375\n",
      "Epoch [162/200] Loss: 40.15047073364258\n",
      "Epoch [163/200] Loss: 39.74074935913086\n",
      "Epoch [164/200] Loss: 39.33659744262695\n",
      "Epoch [165/200] Loss: 38.937950134277344\n",
      "Epoch [166/200] Loss: 38.54473876953125\n",
      "Epoch [167/200] Loss: 38.156944274902344\n",
      "Epoch [168/200] Loss: 37.77447509765625\n",
      "Epoch [169/200] Loss: 37.39726638793945\n",
      "Epoch [170/200] Loss: 37.025291442871094\n",
      "Epoch [171/200] Loss: 36.65845489501953\n",
      "Epoch [172/200] Loss: 36.29673767089844\n",
      "Epoch [173/200] Loss: 35.9400520324707\n",
      "Epoch [174/200] Loss: 35.588348388671875\n",
      "Epoch [175/200] Loss: 35.241573333740234\n",
      "Epoch [176/200] Loss: 34.89968490600586\n",
      "Epoch [177/200] Loss: 34.56260299682617\n",
      "Epoch [178/200] Loss: 34.230281829833984\n",
      "Epoch [179/200] Loss: 33.902679443359375\n",
      "Epoch [180/200] Loss: 33.579734802246094\n",
      "Epoch [181/200] Loss: 33.261383056640625\n",
      "Epoch [182/200] Loss: 32.94758605957031\n",
      "Epoch [183/200] Loss: 32.638282775878906\n",
      "Epoch [184/200] Loss: 32.333412170410156\n",
      "Epoch [185/200] Loss: 32.032936096191406\n",
      "Epoch [186/200] Loss: 31.73680877685547\n",
      "Epoch [187/200] Loss: 31.444974899291992\n",
      "Epoch [188/200] Loss: 31.157379150390625\n",
      "Epoch [189/200] Loss: 30.873992919921875\n",
      "Epoch [190/200] Loss: 30.594722747802734\n",
      "Epoch [191/200] Loss: 30.319551467895508\n",
      "Epoch [192/200] Loss: 30.048431396484375\n",
      "Epoch [193/200] Loss: 29.781299591064453\n",
      "Epoch [194/200] Loss: 29.51811981201172\n",
      "Epoch [195/200] Loss: 29.258834838867188\n",
      "Epoch [196/200] Loss: 29.00341033935547\n",
      "Epoch [197/200] Loss: 28.751785278320312\n",
      "Epoch [198/200] Loss: 28.503925323486328\n",
      "Epoch [199/200] Loss: 28.259784698486328\n",
      "Epoch [200/200] Loss: 28.019309997558594\n",
      "Predicted days_remaining for parent_id 199: 14.096924781799316\n",
      "Training for parent_id 211...\n",
      "Epoch [1/200] Loss: 264.3025207519531\n",
      "Epoch [2/200] Loss: 258.2651672363281\n",
      "Epoch [3/200] Loss: 252.2320098876953\n",
      "Epoch [4/200] Loss: 246.21014404296875\n",
      "Epoch [5/200] Loss: 240.21408081054688\n",
      "Epoch [6/200] Loss: 234.26260375976562\n",
      "Epoch [7/200] Loss: 228.37701416015625\n",
      "Epoch [8/200] Loss: 222.580810546875\n",
      "Epoch [9/200] Loss: 216.89639282226562\n",
      "Epoch [10/200] Loss: 211.341796875\n",
      "Epoch [11/200] Loss: 205.930419921875\n",
      "Epoch [12/200] Loss: 200.67344665527344\n",
      "Epoch [13/200] Loss: 195.581787109375\n",
      "Epoch [14/200] Loss: 190.66700744628906\n",
      "Epoch [15/200] Loss: 185.94078063964844\n",
      "Epoch [16/200] Loss: 181.41314697265625\n",
      "Epoch [17/200] Loss: 177.09132385253906\n",
      "Epoch [18/200] Loss: 172.978759765625\n",
      "Epoch [19/200] Loss: 169.07479858398438\n",
      "Epoch [20/200] Loss: 165.37527465820312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200] Loss: 161.8731689453125\n",
      "Epoch [22/200] Loss: 158.5594024658203\n",
      "Epoch [23/200] Loss: 155.4235382080078\n",
      "Epoch [24/200] Loss: 152.45436096191406\n",
      "Epoch [25/200] Loss: 149.64007568359375\n",
      "Epoch [26/200] Loss: 146.96875\n",
      "Epoch [27/200] Loss: 144.42837524414062\n",
      "Epoch [28/200] Loss: 142.00726318359375\n",
      "Epoch [29/200] Loss: 139.69419860839844\n",
      "Epoch [30/200] Loss: 137.47874450683594\n",
      "Epoch [31/200] Loss: 135.35118103027344\n",
      "Epoch [32/200] Loss: 133.30291748046875\n",
      "Epoch [33/200] Loss: 131.32620239257812\n",
      "Epoch [34/200] Loss: 129.4141387939453\n",
      "Epoch [35/200] Loss: 127.56063079833984\n",
      "Epoch [36/200] Loss: 125.7603988647461\n",
      "Epoch [37/200] Loss: 124.0086898803711\n",
      "Epoch [38/200] Loss: 122.30143737792969\n",
      "Epoch [39/200] Loss: 120.63504791259766\n",
      "Epoch [40/200] Loss: 119.00638580322266\n",
      "Epoch [41/200] Loss: 117.41268920898438\n",
      "Epoch [42/200] Loss: 115.85160064697266\n",
      "Epoch [43/200] Loss: 114.32107543945312\n",
      "Epoch [44/200] Loss: 112.81930541992188\n",
      "Epoch [45/200] Loss: 111.34465026855469\n",
      "Epoch [46/200] Loss: 109.8957748413086\n",
      "Epoch [47/200] Loss: 108.47142791748047\n",
      "Epoch [48/200] Loss: 107.07051849365234\n",
      "Epoch [49/200] Loss: 105.6920166015625\n",
      "Epoch [50/200] Loss: 104.3350830078125\n",
      "Epoch [51/200] Loss: 102.9988784790039\n",
      "Epoch [52/200] Loss: 101.68264770507812\n",
      "Epoch [53/200] Loss: 100.3857421875\n",
      "Epoch [54/200] Loss: 99.10750579833984\n",
      "Epoch [55/200] Loss: 97.84736633300781\n",
      "Epoch [56/200] Loss: 96.60478210449219\n",
      "Epoch [57/200] Loss: 95.37931823730469\n",
      "Epoch [58/200] Loss: 94.17048645019531\n",
      "Epoch [59/200] Loss: 92.97791290283203\n",
      "Epoch [60/200] Loss: 91.80120086669922\n",
      "Epoch [61/200] Loss: 90.64005279541016\n",
      "Epoch [62/200] Loss: 89.49415588378906\n",
      "Epoch [63/200] Loss: 88.36327362060547\n",
      "Epoch [64/200] Loss: 87.24714660644531\n",
      "Epoch [65/200] Loss: 86.14555358886719\n",
      "Epoch [66/200] Loss: 85.05831909179688\n",
      "Epoch [67/200] Loss: 83.98526000976562\n",
      "Epoch [68/200] Loss: 82.92618560791016\n",
      "Epoch [69/200] Loss: 81.8809585571289\n",
      "Epoch [70/200] Loss: 80.84945678710938\n",
      "Epoch [71/200] Loss: 79.83146667480469\n",
      "Epoch [72/200] Loss: 78.8268814086914\n",
      "Epoch [73/200] Loss: 77.83556365966797\n",
      "Epoch [74/200] Loss: 76.85737609863281\n",
      "Epoch [75/200] Loss: 75.89215087890625\n",
      "Epoch [76/200] Loss: 74.93976593017578\n",
      "Epoch [77/200] Loss: 74.00006866455078\n",
      "Epoch [78/200] Loss: 73.07294464111328\n",
      "Epoch [79/200] Loss: 72.15821075439453\n",
      "Epoch [80/200] Loss: 71.25572967529297\n",
      "Epoch [81/200] Loss: 70.36536407470703\n",
      "Epoch [82/200] Loss: 69.48699188232422\n",
      "Epoch [83/200] Loss: 68.62044525146484\n",
      "Epoch [84/200] Loss: 67.76559448242188\n",
      "Epoch [85/200] Loss: 66.92231750488281\n",
      "Epoch [86/200] Loss: 66.0904541015625\n",
      "Epoch [87/200] Loss: 65.26984405517578\n",
      "Epoch [88/200] Loss: 64.46038818359375\n",
      "Epoch [89/200] Loss: 63.66194152832031\n",
      "Epoch [90/200] Loss: 62.87437438964844\n",
      "Epoch [91/200] Loss: 62.09756088256836\n",
      "Epoch [92/200] Loss: 61.33138656616211\n",
      "Epoch [93/200] Loss: 60.57567596435547\n",
      "Epoch [94/200] Loss: 59.83035659790039\n",
      "Epoch [95/200] Loss: 59.09526443481445\n",
      "Epoch [96/200] Loss: 58.370338439941406\n",
      "Epoch [97/200] Loss: 57.655391693115234\n",
      "Epoch [98/200] Loss: 56.95035171508789\n",
      "Epoch [99/200] Loss: 56.25504684448242\n",
      "Epoch [100/200] Loss: 55.56943130493164\n",
      "Epoch [101/200] Loss: 54.89333724975586\n",
      "Epoch [102/200] Loss: 54.2266845703125\n",
      "Epoch [103/200] Loss: 53.56935119628906\n",
      "Epoch [104/200] Loss: 52.92123031616211\n",
      "Epoch [105/200] Loss: 52.28219223022461\n",
      "Epoch [106/200] Loss: 51.652183532714844\n",
      "Epoch [107/200] Loss: 51.03105163574219\n",
      "Epoch [108/200] Loss: 50.41869354248047\n",
      "Epoch [109/200] Loss: 49.815025329589844\n",
      "Epoch [110/200] Loss: 49.21994400024414\n",
      "Epoch [111/200] Loss: 48.633331298828125\n",
      "Epoch [112/200] Loss: 48.05510711669922\n",
      "Epoch [113/200] Loss: 47.485164642333984\n",
      "Epoch [114/200] Loss: 46.92340087890625\n",
      "Epoch [115/200] Loss: 46.369728088378906\n",
      "Epoch [116/200] Loss: 45.82405090332031\n",
      "Epoch [117/200] Loss: 45.2862663269043\n",
      "Epoch [118/200] Loss: 44.75629425048828\n",
      "Epoch [119/200] Loss: 44.234046936035156\n",
      "Epoch [120/200] Loss: 43.71938705444336\n",
      "Epoch [121/200] Loss: 43.21228790283203\n",
      "Epoch [122/200] Loss: 42.71262741088867\n",
      "Epoch [123/200] Loss: 42.2203254699707\n",
      "Epoch [124/200] Loss: 41.73527908325195\n",
      "Epoch [125/200] Loss: 41.25740432739258\n",
      "Epoch [126/200] Loss: 40.78664016723633\n",
      "Epoch [127/200] Loss: 40.32288360595703\n",
      "Epoch [128/200] Loss: 39.86606216430664\n",
      "Epoch [129/200] Loss: 39.41606903076172\n",
      "Epoch [130/200] Loss: 38.97283935546875\n",
      "Epoch [131/200] Loss: 38.53628921508789\n",
      "Epoch [132/200] Loss: 38.106327056884766\n",
      "Epoch [133/200] Loss: 37.6828727722168\n",
      "Epoch [134/200] Loss: 37.265869140625\n",
      "Epoch [135/200] Loss: 36.855228424072266\n",
      "Epoch [136/200] Loss: 36.45087432861328\n",
      "Epoch [137/200] Loss: 36.05271530151367\n",
      "Epoch [138/200] Loss: 35.66068649291992\n",
      "Epoch [139/200] Loss: 35.27470779418945\n",
      "Epoch [140/200] Loss: 34.894718170166016\n",
      "Epoch [141/200] Loss: 34.52062225341797\n",
      "Epoch [142/200] Loss: 34.15235137939453\n",
      "Epoch [143/200] Loss: 33.78984451293945\n",
      "Epoch [144/200] Loss: 33.43302917480469\n",
      "Epoch [145/200] Loss: 33.081809997558594\n",
      "Epoch [146/200] Loss: 32.73613739013672\n",
      "Epoch [147/200] Loss: 32.39593505859375\n",
      "Epoch [148/200] Loss: 32.0611686706543\n",
      "Epoch [149/200] Loss: 31.731704711914062\n",
      "Epoch [150/200] Loss: 31.407514572143555\n",
      "Epoch [151/200] Loss: 31.088531494140625\n",
      "Epoch [152/200] Loss: 30.7746639251709\n",
      "Epoch [153/200] Loss: 30.465885162353516\n",
      "Epoch [154/200] Loss: 30.162094116210938\n",
      "Epoch [155/200] Loss: 29.86322784423828\n",
      "Epoch [156/200] Loss: 29.569246292114258\n",
      "Epoch [157/200] Loss: 29.280071258544922\n",
      "Epoch [158/200] Loss: 28.995637893676758\n",
      "Epoch [159/200] Loss: 28.715879440307617\n",
      "Epoch [160/200] Loss: 28.440746307373047\n",
      "Epoch [161/200] Loss: 28.170181274414062\n",
      "Epoch [162/200] Loss: 27.904102325439453\n",
      "Epoch [163/200] Loss: 27.642452239990234\n",
      "Epoch [164/200] Loss: 27.38520050048828\n",
      "Epoch [165/200] Loss: 27.13224983215332\n",
      "Epoch [166/200] Loss: 26.883563995361328\n",
      "Epoch [167/200] Loss: 26.639080047607422\n",
      "Epoch [168/200] Loss: 26.398738861083984\n",
      "Epoch [169/200] Loss: 26.16248321533203\n",
      "Epoch [170/200] Loss: 25.930269241333008\n",
      "Epoch [171/200] Loss: 25.702028274536133\n",
      "Epoch [172/200] Loss: 25.477699279785156\n",
      "Epoch [173/200] Loss: 25.257232666015625\n",
      "Epoch [174/200] Loss: 25.040578842163086\n",
      "Epoch [175/200] Loss: 24.827688217163086\n",
      "Epoch [176/200] Loss: 24.618501663208008\n",
      "Epoch [177/200] Loss: 24.4129695892334\n",
      "Epoch [178/200] Loss: 24.21103858947754\n",
      "Epoch [179/200] Loss: 24.012649536132812\n",
      "Epoch [180/200] Loss: 23.817760467529297\n",
      "Epoch [181/200] Loss: 23.626323699951172\n",
      "Epoch [182/200] Loss: 23.43828582763672\n",
      "Epoch [183/200] Loss: 23.25358772277832\n",
      "Epoch [184/200] Loss: 23.07219696044922\n",
      "Epoch [185/200] Loss: 22.894052505493164\n",
      "Epoch [186/200] Loss: 22.719112396240234\n",
      "Epoch [187/200] Loss: 22.547334671020508\n",
      "Epoch [188/200] Loss: 22.37865447998047\n",
      "Epoch [189/200] Loss: 22.21304702758789\n",
      "Epoch [190/200] Loss: 22.05044937133789\n",
      "Epoch [191/200] Loss: 21.89083480834961\n",
      "Epoch [192/200] Loss: 21.73413848876953\n",
      "Epoch [193/200] Loss: 21.580324172973633\n",
      "Epoch [194/200] Loss: 21.429357528686523\n",
      "Epoch [195/200] Loss: 21.281177520751953\n",
      "Epoch [196/200] Loss: 21.1357479095459\n",
      "Epoch [197/200] Loss: 20.9930419921875\n",
      "Epoch [198/200] Loss: 20.852994918823242\n",
      "Epoch [199/200] Loss: 20.7155704498291\n",
      "Epoch [200/200] Loss: 20.580738067626953\n",
      "Predicted days_remaining for parent_id 211: 13.298274993896484\n",
      "Training for parent_id 215...\n",
      "Epoch [1/200] Loss: 349.69525146484375\n",
      "Epoch [2/200] Loss: 341.6529541015625\n",
      "Epoch [3/200] Loss: 333.9087829589844\n",
      "Epoch [4/200] Loss: 326.3907470703125\n",
      "Epoch [5/200] Loss: 319.06866455078125\n",
      "Epoch [6/200] Loss: 311.932373046875\n",
      "Epoch [7/200] Loss: 304.9773864746094\n",
      "Epoch [8/200] Loss: 298.20361328125\n",
      "Epoch [9/200] Loss: 291.6179504394531\n",
      "Epoch [10/200] Loss: 285.23492431640625\n",
      "Epoch [11/200] Loss: 279.07403564453125\n",
      "Epoch [12/200] Loss: 273.1546936035156\n",
      "Epoch [13/200] Loss: 267.4906005859375\n",
      "Epoch [14/200] Loss: 262.087646484375\n",
      "Epoch [15/200] Loss: 256.9450988769531\n",
      "Epoch [16/200] Loss: 252.05833435058594\n",
      "Epoch [17/200] Loss: 247.42037963867188\n",
      "Epoch [18/200] Loss: 243.02230834960938\n",
      "Epoch [19/200] Loss: 238.8534393310547\n",
      "Epoch [20/200] Loss: 234.90109252929688\n",
      "Epoch [21/200] Loss: 231.15069580078125\n",
      "Epoch [22/200] Loss: 227.58627319335938\n",
      "Epoch [23/200] Loss: 224.19107055664062\n",
      "Epoch [24/200] Loss: 220.9485626220703\n",
      "Epoch [25/200] Loss: 217.8428192138672\n",
      "Epoch [26/200] Loss: 214.8592529296875\n",
      "Epoch [27/200] Loss: 211.98516845703125\n",
      "Epoch [28/200] Loss: 209.209228515625\n",
      "Epoch [29/200] Loss: 206.52198791503906\n",
      "Epoch [30/200] Loss: 203.9151153564453\n",
      "Epoch [31/200] Loss: 201.38160705566406\n",
      "Epoch [32/200] Loss: 198.91539001464844\n",
      "Epoch [33/200] Loss: 196.51101684570312\n",
      "Epoch [34/200] Loss: 194.1636962890625\n",
      "Epoch [35/200] Loss: 191.86927795410156\n",
      "Epoch [36/200] Loss: 189.6239471435547\n",
      "Epoch [37/200] Loss: 187.42430114746094\n",
      "Epoch [38/200] Loss: 185.2673797607422\n",
      "Epoch [39/200] Loss: 183.1504364013672\n",
      "Epoch [40/200] Loss: 181.0711669921875\n",
      "Epoch [41/200] Loss: 179.02737426757812\n",
      "Epoch [42/200] Loss: 177.01715087890625\n",
      "Epoch [43/200] Loss: 175.03883361816406\n",
      "Epoch [44/200] Loss: 173.09083557128906\n",
      "Epoch [45/200] Loss: 171.17181396484375\n",
      "Epoch [46/200] Loss: 169.2805938720703\n",
      "Epoch [47/200] Loss: 167.41600036621094\n",
      "Epoch [48/200] Loss: 165.5771484375\n",
      "Epoch [49/200] Loss: 163.76300048828125\n",
      "Epoch [50/200] Loss: 161.97288513183594\n",
      "Epoch [51/200] Loss: 160.20594787597656\n",
      "Epoch [52/200] Loss: 158.46163940429688\n",
      "Epoch [53/200] Loss: 156.73927307128906\n",
      "Epoch [54/200] Loss: 155.038330078125\n",
      "Epoch [55/200] Loss: 153.3582000732422\n",
      "Epoch [56/200] Loss: 151.6985321044922\n",
      "Epoch [57/200] Loss: 150.05877685546875\n",
      "Epoch [58/200] Loss: 148.43858337402344\n",
      "Epoch [59/200] Loss: 146.83749389648438\n",
      "Epoch [60/200] Loss: 145.2551727294922\n",
      "Epoch [61/200] Loss: 143.69125366210938\n",
      "Epoch [62/200] Loss: 142.14541625976562\n",
      "Epoch [63/200] Loss: 140.61734008789062\n",
      "Epoch [64/200] Loss: 139.10675048828125\n",
      "Epoch [65/200] Loss: 137.61329650878906\n",
      "Epoch [66/200] Loss: 136.13671875\n",
      "Epoch [67/200] Loss: 134.6768035888672\n",
      "Epoch [68/200] Loss: 133.2332305908203\n",
      "Epoch [69/200] Loss: 131.8057861328125\n",
      "Epoch [70/200] Loss: 130.39422607421875\n",
      "Epoch [71/200] Loss: 128.99830627441406\n",
      "Epoch [72/200] Loss: 127.6177978515625\n",
      "Epoch [73/200] Loss: 126.25252532958984\n",
      "Epoch [74/200] Loss: 124.90228271484375\n",
      "Epoch [75/200] Loss: 123.5667953491211\n",
      "Epoch [76/200] Loss: 122.24591827392578\n",
      "Epoch [77/200] Loss: 120.93948364257812\n",
      "Epoch [78/200] Loss: 119.64724731445312\n",
      "Epoch [79/200] Loss: 118.36907958984375\n",
      "Epoch [80/200] Loss: 117.10475158691406\n",
      "Epoch [81/200] Loss: 115.8541259765625\n",
      "Epoch [82/200] Loss: 114.61705780029297\n",
      "Epoch [83/200] Loss: 113.39331817626953\n",
      "Epoch [84/200] Loss: 112.18280029296875\n",
      "Epoch [85/200] Loss: 110.9853286743164\n",
      "Epoch [86/200] Loss: 109.80076599121094\n",
      "Epoch [87/200] Loss: 108.6289291381836\n",
      "Epoch [88/200] Loss: 107.4697036743164\n",
      "Epoch [89/200] Loss: 106.32293701171875\n",
      "Epoch [90/200] Loss: 105.18852233886719\n",
      "Epoch [91/200] Loss: 104.06624603271484\n",
      "Epoch [92/200] Loss: 102.95602416992188\n",
      "Epoch [93/200] Loss: 101.85772705078125\n",
      "Epoch [94/200] Loss: 100.77124786376953\n",
      "Epoch [95/200] Loss: 99.69640350341797\n",
      "Epoch [96/200] Loss: 98.63309478759766\n",
      "Epoch [97/200] Loss: 97.58123016357422\n",
      "Epoch [98/200] Loss: 96.5406494140625\n",
      "Epoch [99/200] Loss: 95.51128387451172\n",
      "Epoch [100/200] Loss: 94.492919921875\n",
      "Epoch [101/200] Loss: 93.48558044433594\n",
      "Epoch [102/200] Loss: 92.48904418945312\n",
      "Epoch [103/200] Loss: 91.5032730102539\n",
      "Epoch [104/200] Loss: 90.5280990600586\n",
      "Epoch [105/200] Loss: 89.56346893310547\n",
      "Epoch [106/200] Loss: 88.60924530029297\n",
      "Epoch [107/200] Loss: 87.66535186767578\n",
      "Epoch [108/200] Loss: 86.73167419433594\n",
      "Epoch [109/200] Loss: 85.8080825805664\n",
      "Epoch [110/200] Loss: 84.8945541381836\n",
      "Epoch [111/200] Loss: 83.99090576171875\n",
      "Epoch [112/200] Loss: 83.09710693359375\n",
      "Epoch [113/200] Loss: 82.21304321289062\n",
      "Epoch [114/200] Loss: 81.33857727050781\n",
      "Epoch [115/200] Loss: 80.47370910644531\n",
      "Epoch [116/200] Loss: 79.6182632446289\n",
      "Epoch [117/200] Loss: 78.77220153808594\n",
      "Epoch [118/200] Loss: 77.9354248046875\n",
      "Epoch [119/200] Loss: 77.10781860351562\n",
      "Epoch [120/200] Loss: 76.2893295288086\n",
      "Epoch [121/200] Loss: 75.4798355102539\n",
      "Epoch [122/200] Loss: 74.6792984008789\n",
      "Epoch [123/200] Loss: 73.88762664794922\n",
      "Epoch [124/200] Loss: 73.1047134399414\n",
      "Epoch [125/200] Loss: 72.33049011230469\n",
      "Epoch [126/200] Loss: 71.56489562988281\n",
      "Epoch [127/200] Loss: 70.80782318115234\n",
      "Epoch [128/200] Loss: 70.05917358398438\n",
      "Epoch [129/200] Loss: 69.31890106201172\n",
      "Epoch [130/200] Loss: 68.58692169189453\n",
      "Epoch [131/200] Loss: 67.86318969726562\n",
      "Epoch [132/200] Loss: 67.14757537841797\n",
      "Epoch [133/200] Loss: 66.44003295898438\n",
      "Epoch [134/200] Loss: 65.740478515625\n",
      "Epoch [135/200] Loss: 65.04886627197266\n",
      "Epoch [136/200] Loss: 64.36507415771484\n",
      "Epoch [137/200] Loss: 63.68907165527344\n",
      "Epoch [138/200] Loss: 63.020751953125\n",
      "Epoch [139/200] Loss: 62.3600959777832\n",
      "Epoch [140/200] Loss: 61.70697021484375\n",
      "Epoch [141/200] Loss: 61.06135559082031\n",
      "Epoch [142/200] Loss: 60.42316436767578\n",
      "Epoch [143/200] Loss: 59.79231262207031\n",
      "Epoch [144/200] Loss: 59.16875076293945\n",
      "Epoch [145/200] Loss: 58.552425384521484\n",
      "Epoch [146/200] Loss: 57.94321060180664\n",
      "Epoch [147/200] Loss: 57.341102600097656\n",
      "Epoch [148/200] Loss: 56.74602127075195\n",
      "Epoch [149/200] Loss: 56.15789031982422\n",
      "Epoch [150/200] Loss: 55.57664489746094\n",
      "Epoch [151/200] Loss: 55.00222396850586\n",
      "Epoch [152/200] Loss: 54.434574127197266\n",
      "Epoch [153/200] Loss: 53.87361145019531\n",
      "Epoch [154/200] Loss: 53.31928253173828\n",
      "Epoch [155/200] Loss: 52.77153015136719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [156/200] Loss: 52.230281829833984\n",
      "Epoch [157/200] Loss: 51.69548034667969\n",
      "Epoch [158/200] Loss: 51.16707229614258\n",
      "Epoch [159/200] Loss: 50.64499282836914\n",
      "Epoch [160/200] Loss: 50.12918472290039\n",
      "Epoch [161/200] Loss: 49.61957550048828\n",
      "Epoch [162/200] Loss: 49.11613082885742\n",
      "Epoch [163/200] Loss: 48.6187629699707\n",
      "Epoch [164/200] Loss: 48.127437591552734\n",
      "Epoch [165/200] Loss: 47.6420783996582\n",
      "Epoch [166/200] Loss: 47.16262435913086\n",
      "Epoch [167/200] Loss: 46.689064025878906\n",
      "Epoch [168/200] Loss: 46.22126770019531\n",
      "Epoch [169/200] Loss: 45.75922775268555\n",
      "Epoch [170/200] Loss: 45.30289077758789\n",
      "Epoch [171/200] Loss: 44.85218048095703\n",
      "Epoch [172/200] Loss: 44.40705108642578\n",
      "Epoch [173/200] Loss: 43.967464447021484\n",
      "Epoch [174/200] Loss: 43.53334426879883\n",
      "Epoch [175/200] Loss: 43.1046142578125\n",
      "Epoch [176/200] Loss: 42.68126678466797\n",
      "Epoch [177/200] Loss: 42.26324462890625\n",
      "Epoch [178/200] Loss: 41.8504524230957\n",
      "Epoch [179/200] Loss: 41.44289016723633\n",
      "Epoch [180/200] Loss: 41.04047393798828\n",
      "Epoch [181/200] Loss: 40.64313888549805\n",
      "Epoch [182/200] Loss: 40.250885009765625\n",
      "Epoch [183/200] Loss: 39.863616943359375\n",
      "Epoch [184/200] Loss: 39.48130416870117\n",
      "Epoch [185/200] Loss: 39.10388946533203\n",
      "Epoch [186/200] Loss: 38.731327056884766\n",
      "Epoch [187/200] Loss: 38.36355972290039\n",
      "Epoch [188/200] Loss: 38.00053787231445\n",
      "Epoch [189/200] Loss: 37.64222717285156\n",
      "Epoch [190/200] Loss: 37.28857421875\n",
      "Epoch [191/200] Loss: 36.93951416015625\n",
      "Epoch [192/200] Loss: 36.595008850097656\n",
      "Epoch [193/200] Loss: 36.25503921508789\n",
      "Epoch [194/200] Loss: 35.919525146484375\n",
      "Epoch [195/200] Loss: 35.58840560913086\n",
      "Epoch [196/200] Loss: 35.26167297363281\n",
      "Epoch [197/200] Loss: 34.939273834228516\n",
      "Epoch [198/200] Loss: 34.621131896972656\n",
      "Epoch [199/200] Loss: 34.30723571777344\n",
      "Epoch [200/200] Loss: 33.997528076171875\n",
      "Predicted days_remaining for parent_id 215: 14.36201000213623\n",
      "Training for parent_id 218...\n",
      "Epoch [1/200] Loss: 901.7694091796875\n",
      "Epoch [2/200] Loss: 887.26318359375\n",
      "Epoch [3/200] Loss: 872.9402465820312\n",
      "Epoch [4/200] Loss: 858.8543090820312\n",
      "Epoch [5/200] Loss: 845.0618286132812\n",
      "Epoch [6/200] Loss: 831.611328125\n",
      "Epoch [7/200] Loss: 818.5347290039062\n",
      "Epoch [8/200] Loss: 805.8522338867188\n",
      "Epoch [9/200] Loss: 793.5759887695312\n",
      "Epoch [10/200] Loss: 781.7105712890625\n",
      "Epoch [11/200] Loss: 770.2545776367188\n",
      "Epoch [12/200] Loss: 759.20458984375\n",
      "Epoch [13/200] Loss: 748.5587158203125\n",
      "Epoch [14/200] Loss: 738.315185546875\n",
      "Epoch [15/200] Loss: 728.47265625\n",
      "Epoch [16/200] Loss: 719.0279541015625\n",
      "Epoch [17/200] Loss: 709.9755859375\n",
      "Epoch [18/200] Loss: 701.3064575195312\n",
      "Epoch [19/200] Loss: 693.009521484375\n",
      "Epoch [20/200] Loss: 685.0711059570312\n",
      "Epoch [21/200] Loss: 677.4754028320312\n",
      "Epoch [22/200] Loss: 670.20654296875\n",
      "Epoch [23/200] Loss: 663.2471923828125\n",
      "Epoch [24/200] Loss: 656.5806274414062\n",
      "Epoch [25/200] Loss: 650.1904296875\n",
      "Epoch [26/200] Loss: 644.060791015625\n",
      "Epoch [27/200] Loss: 638.1763916015625\n",
      "Epoch [28/200] Loss: 632.5227661132812\n",
      "Epoch [29/200] Loss: 627.0851440429688\n",
      "Epoch [30/200] Loss: 621.848876953125\n",
      "Epoch [31/200] Loss: 616.799072265625\n",
      "Epoch [32/200] Loss: 611.9203491210938\n",
      "Epoch [33/200] Loss: 607.1976928710938\n",
      "Epoch [34/200] Loss: 602.6158447265625\n",
      "Epoch [35/200] Loss: 598.160888671875\n",
      "Epoch [36/200] Loss: 593.8193359375\n",
      "Epoch [37/200] Loss: 589.5790405273438\n",
      "Epoch [38/200] Loss: 585.4292602539062\n",
      "Epoch [39/200] Loss: 581.3605346679688\n",
      "Epoch [40/200] Loss: 577.3648071289062\n",
      "Epoch [41/200] Loss: 573.4351806640625\n",
      "Epoch [42/200] Loss: 569.5654296875\n",
      "Epoch [43/200] Loss: 565.7508544921875\n",
      "Epoch [44/200] Loss: 561.9868774414062\n",
      "Epoch [45/200] Loss: 558.27001953125\n",
      "Epoch [46/200] Loss: 554.5970458984375\n",
      "Epoch [47/200] Loss: 550.965576171875\n",
      "Epoch [48/200] Loss: 547.372802734375\n",
      "Epoch [49/200] Loss: 543.8170776367188\n",
      "Epoch [50/200] Loss: 540.2964477539062\n",
      "Epoch [51/200] Loss: 536.8094482421875\n",
      "Epoch [52/200] Loss: 533.3544311523438\n",
      "Epoch [53/200] Loss: 529.9303588867188\n",
      "Epoch [54/200] Loss: 526.5360717773438\n",
      "Epoch [55/200] Loss: 523.1703491210938\n",
      "Epoch [56/200] Loss: 519.83251953125\n",
      "Epoch [57/200] Loss: 516.521484375\n",
      "Epoch [58/200] Loss: 513.2366333007812\n",
      "Epoch [59/200] Loss: 509.977294921875\n",
      "Epoch [60/200] Loss: 506.7427978515625\n",
      "Epoch [61/200] Loss: 503.5325927734375\n",
      "Epoch [62/200] Loss: 500.34613037109375\n",
      "Epoch [63/200] Loss: 497.1830749511719\n",
      "Epoch [64/200] Loss: 494.04296875\n",
      "Epoch [65/200] Loss: 490.9253234863281\n",
      "Epoch [66/200] Loss: 487.8297424316406\n",
      "Epoch [67/200] Loss: 484.7560119628906\n",
      "Epoch [68/200] Loss: 481.7038269042969\n",
      "Epoch [69/200] Loss: 478.67279052734375\n",
      "Epoch [70/200] Loss: 475.6624450683594\n",
      "Epoch [71/200] Loss: 472.67279052734375\n",
      "Epoch [72/200] Loss: 469.703369140625\n",
      "Epoch [73/200] Loss: 466.75396728515625\n",
      "Epoch [74/200] Loss: 463.8243408203125\n",
      "Epoch [75/200] Loss: 460.9140930175781\n",
      "Epoch [76/200] Loss: 458.02301025390625\n",
      "Epoch [77/200] Loss: 455.1510925292969\n",
      "Epoch [78/200] Loss: 452.2978820800781\n",
      "Epoch [79/200] Loss: 449.4631652832031\n",
      "Epoch [80/200] Loss: 446.646728515625\n",
      "Epoch [81/200] Loss: 443.84844970703125\n",
      "Epoch [82/200] Loss: 441.06805419921875\n",
      "Epoch [83/200] Loss: 438.30523681640625\n",
      "Epoch [84/200] Loss: 435.5600280761719\n",
      "Epoch [85/200] Loss: 432.8321228027344\n",
      "Epoch [86/200] Loss: 430.1211242675781\n",
      "Epoch [87/200] Loss: 427.4271545410156\n",
      "Epoch [88/200] Loss: 424.75\n",
      "Epoch [89/200] Loss: 422.08941650390625\n",
      "Epoch [90/200] Loss: 419.4452209472656\n",
      "Epoch [91/200] Loss: 416.8172607421875\n",
      "Epoch [92/200] Loss: 414.2054443359375\n",
      "Epoch [93/200] Loss: 411.6095886230469\n",
      "Epoch [94/200] Loss: 409.0294189453125\n",
      "Epoch [95/200] Loss: 406.46490478515625\n",
      "Epoch [96/200] Loss: 403.916015625\n",
      "Epoch [97/200] Loss: 401.3824157714844\n",
      "Epoch [98/200] Loss: 398.8641052246094\n",
      "Epoch [99/200] Loss: 396.3607177734375\n",
      "Epoch [100/200] Loss: 393.8723449707031\n",
      "Epoch [101/200] Loss: 391.3988342285156\n",
      "Epoch [102/200] Loss: 388.9400939941406\n",
      "Epoch [103/200] Loss: 386.4958801269531\n",
      "Epoch [104/200] Loss: 384.06622314453125\n",
      "Epoch [105/200] Loss: 381.6506652832031\n",
      "Epoch [106/200] Loss: 379.2496032714844\n",
      "Epoch [107/200] Loss: 376.862548828125\n",
      "Epoch [108/200] Loss: 374.4895935058594\n",
      "Epoch [109/200] Loss: 372.1304626464844\n",
      "Epoch [110/200] Loss: 369.7852478027344\n",
      "Epoch [111/200] Loss: 367.45361328125\n",
      "Epoch [112/200] Loss: 365.1355895996094\n",
      "Epoch [113/200] Loss: 362.83099365234375\n",
      "Epoch [114/200] Loss: 360.5399475097656\n",
      "Epoch [115/200] Loss: 358.2620544433594\n",
      "Epoch [116/200] Loss: 355.99749755859375\n",
      "Epoch [117/200] Loss: 353.7459716796875\n",
      "Epoch [118/200] Loss: 351.50750732421875\n",
      "Epoch [119/200] Loss: 349.2819519042969\n",
      "Epoch [120/200] Loss: 347.06927490234375\n",
      "Epoch [121/200] Loss: 344.8692932128906\n",
      "Epoch [122/200] Loss: 342.6820373535156\n",
      "Epoch [123/200] Loss: 340.50750732421875\n",
      "Epoch [124/200] Loss: 338.3452453613281\n",
      "Epoch [125/200] Loss: 336.1954345703125\n",
      "Epoch [126/200] Loss: 334.0580139160156\n",
      "Epoch [127/200] Loss: 331.9328308105469\n",
      "Epoch [128/200] Loss: 329.8199462890625\n",
      "Epoch [129/200] Loss: 327.71917724609375\n",
      "Epoch [130/200] Loss: 325.63031005859375\n",
      "Epoch [131/200] Loss: 323.553466796875\n",
      "Epoch [132/200] Loss: 321.4884948730469\n",
      "Epoch [133/200] Loss: 319.43536376953125\n",
      "Epoch [134/200] Loss: 317.3939514160156\n",
      "Epoch [135/200] Loss: 315.3642578125\n",
      "Epoch [136/200] Loss: 313.34613037109375\n",
      "Epoch [137/200] Loss: 311.339599609375\n",
      "Epoch [138/200] Loss: 309.3445129394531\n",
      "Epoch [139/200] Loss: 307.36077880859375\n",
      "Epoch [140/200] Loss: 305.388427734375\n",
      "Epoch [141/200] Loss: 303.4273986816406\n",
      "Epoch [142/200] Loss: 301.4774475097656\n",
      "Epoch [143/200] Loss: 299.5387268066406\n",
      "Epoch [144/200] Loss: 297.6109924316406\n",
      "Epoch [145/200] Loss: 295.6943359375\n",
      "Epoch [146/200] Loss: 293.7887268066406\n",
      "Epoch [147/200] Loss: 291.89398193359375\n",
      "Epoch [148/200] Loss: 290.0099792480469\n",
      "Epoch [149/200] Loss: 288.1368408203125\n",
      "Epoch [150/200] Loss: 286.27435302734375\n",
      "Epoch [151/200] Loss: 284.4225769042969\n",
      "Epoch [152/200] Loss: 282.5813903808594\n",
      "Epoch [153/200] Loss: 280.750732421875\n",
      "Epoch [154/200] Loss: 278.9305725097656\n",
      "Epoch [155/200] Loss: 277.1208801269531\n",
      "Epoch [156/200] Loss: 275.3215026855469\n",
      "Epoch [157/200] Loss: 273.532470703125\n",
      "Epoch [158/200] Loss: 271.7537536621094\n",
      "Epoch [159/200] Loss: 269.9851989746094\n",
      "Epoch [160/200] Loss: 268.22686767578125\n",
      "Epoch [161/200] Loss: 266.47857666015625\n",
      "Epoch [162/200] Loss: 264.74041748046875\n",
      "Epoch [163/200] Loss: 263.0121765136719\n",
      "Epoch [164/200] Loss: 261.2939453125\n",
      "Epoch [165/200] Loss: 259.58563232421875\n",
      "Epoch [166/200] Loss: 257.88714599609375\n",
      "Epoch [167/200] Loss: 256.198486328125\n",
      "Epoch [168/200] Loss: 254.51950073242188\n",
      "Epoch [169/200] Loss: 252.85037231445312\n",
      "Epoch [170/200] Loss: 251.1907501220703\n",
      "Epoch [171/200] Loss: 249.54078674316406\n",
      "Epoch [172/200] Loss: 247.90037536621094\n",
      "Epoch [173/200] Loss: 246.26950073242188\n",
      "Epoch [174/200] Loss: 244.64808654785156\n",
      "Epoch [175/200] Loss: 243.0361328125\n",
      "Epoch [176/200] Loss: 241.43350219726562\n",
      "Epoch [177/200] Loss: 239.8401641845703\n",
      "Epoch [178/200] Loss: 238.2561798095703\n",
      "Epoch [179/200] Loss: 236.68138122558594\n",
      "Epoch [180/200] Loss: 235.1157684326172\n",
      "Epoch [181/200] Loss: 233.55926513671875\n",
      "Epoch [182/200] Loss: 232.01193237304688\n",
      "Epoch [183/200] Loss: 230.47360229492188\n",
      "Epoch [184/200] Loss: 228.9442901611328\n",
      "Epoch [185/200] Loss: 227.42393493652344\n",
      "Epoch [186/200] Loss: 225.9125518798828\n",
      "Epoch [187/200] Loss: 224.41000366210938\n",
      "Epoch [188/200] Loss: 222.91635131835938\n",
      "Epoch [189/200] Loss: 221.4314422607422\n",
      "Epoch [190/200] Loss: 219.9552459716797\n",
      "Epoch [191/200] Loss: 218.48782348632812\n",
      "Epoch [192/200] Loss: 217.029052734375\n",
      "Epoch [193/200] Loss: 215.57887268066406\n",
      "Epoch [194/200] Loss: 214.13731384277344\n",
      "Epoch [195/200] Loss: 212.704345703125\n",
      "Epoch [196/200] Loss: 211.27976989746094\n",
      "Epoch [197/200] Loss: 209.86373901367188\n",
      "Epoch [198/200] Loss: 208.45606994628906\n",
      "Epoch [199/200] Loss: 207.05679321289062\n",
      "Epoch [200/200] Loss: 205.66587829589844\n",
      "Predicted days_remaining for parent_id 218: 15.971548080444336\n",
      "Training for parent_id 234...\n",
      "Epoch [1/200] Loss: 1292.069580078125\n",
      "Epoch [2/200] Loss: 1273.7884521484375\n",
      "Epoch [3/200] Loss: 1256.0802001953125\n",
      "Epoch [4/200] Loss: 1239.0390625\n",
      "Epoch [5/200] Loss: 1222.6553955078125\n",
      "Epoch [6/200] Loss: 1206.84033203125\n",
      "Epoch [7/200] Loss: 1191.501708984375\n",
      "Epoch [8/200] Loss: 1176.565673828125\n",
      "Epoch [9/200] Loss: 1161.981201171875\n",
      "Epoch [10/200] Loss: 1147.7269287109375\n",
      "Epoch [11/200] Loss: 1133.8115234375\n",
      "Epoch [12/200] Loss: 1120.2694091796875\n",
      "Epoch [13/200] Loss: 1107.148193359375\n",
      "Epoch [14/200] Loss: 1094.49658203125\n",
      "Epoch [15/200] Loss: 1082.3521728515625\n",
      "Epoch [16/200] Loss: 1070.7393798828125\n",
      "Epoch [17/200] Loss: 1059.666015625\n",
      "Epoch [18/200] Loss: 1049.1265869140625\n",
      "Epoch [19/200] Loss: 1039.1048583984375\n",
      "Epoch [20/200] Loss: 1029.57763671875\n",
      "Epoch [21/200] Loss: 1020.5175170898438\n",
      "Epoch [22/200] Loss: 1011.8949584960938\n",
      "Epoch [23/200] Loss: 1003.6803588867188\n",
      "Epoch [24/200] Loss: 995.8445434570312\n",
      "Epoch [25/200] Loss: 988.359375\n",
      "Epoch [26/200] Loss: 981.197509765625\n",
      "Epoch [27/200] Loss: 974.332763671875\n",
      "Epoch [28/200] Loss: 967.7401123046875\n",
      "Epoch [29/200] Loss: 961.395751953125\n",
      "Epoch [30/200] Loss: 955.2762451171875\n",
      "Epoch [31/200] Loss: 949.36083984375\n",
      "Epoch [32/200] Loss: 943.6289672851562\n",
      "Epoch [33/200] Loss: 938.062255859375\n",
      "Epoch [34/200] Loss: 932.6434936523438\n",
      "Epoch [35/200] Loss: 927.3571166992188\n",
      "Epoch [36/200] Loss: 922.1891479492188\n",
      "Epoch [37/200] Loss: 917.12744140625\n",
      "Epoch [38/200] Loss: 912.1604614257812\n",
      "Epoch [39/200] Loss: 907.27880859375\n",
      "Epoch [40/200] Loss: 902.4736328125\n",
      "Epoch [41/200] Loss: 897.7376708984375\n",
      "Epoch [42/200] Loss: 893.0645751953125\n",
      "Epoch [43/200] Loss: 888.4481201171875\n",
      "Epoch [44/200] Loss: 883.8836059570312\n",
      "Epoch [45/200] Loss: 879.3665161132812\n",
      "Epoch [46/200] Loss: 874.8928833007812\n",
      "Epoch [47/200] Loss: 870.45947265625\n",
      "Epoch [48/200] Loss: 866.0629272460938\n",
      "Epoch [49/200] Loss: 861.7006225585938\n",
      "Epoch [50/200] Loss: 857.3701171875\n",
      "Epoch [51/200] Loss: 853.069580078125\n",
      "Epoch [52/200] Loss: 848.7967529296875\n",
      "Epoch [53/200] Loss: 844.5504150390625\n",
      "Epoch [54/200] Loss: 840.3291015625\n",
      "Epoch [55/200] Loss: 836.131591796875\n",
      "Epoch [56/200] Loss: 831.9572143554688\n",
      "Epoch [57/200] Loss: 827.8050537109375\n",
      "Epoch [58/200] Loss: 823.6748657226562\n",
      "Epoch [59/200] Loss: 819.5662841796875\n",
      "Epoch [60/200] Loss: 815.4793701171875\n",
      "Epoch [61/200] Loss: 811.4136962890625\n",
      "Epoch [62/200] Loss: 807.36962890625\n",
      "Epoch [63/200] Loss: 803.346923828125\n",
      "Epoch [64/200] Loss: 799.345947265625\n",
      "Epoch [65/200] Loss: 795.366455078125\n",
      "Epoch [66/200] Loss: 791.40869140625\n",
      "Epoch [67/200] Loss: 787.4725341796875\n",
      "Epoch [68/200] Loss: 783.55810546875\n",
      "Epoch [69/200] Loss: 779.6651000976562\n",
      "Epoch [70/200] Loss: 775.79345703125\n",
      "Epoch [71/200] Loss: 771.943115234375\n",
      "Epoch [72/200] Loss: 768.1137084960938\n",
      "Epoch [73/200] Loss: 764.3052368164062\n",
      "Epoch [74/200] Loss: 760.517578125\n",
      "Epoch [75/200] Loss: 756.750244140625\n",
      "Epoch [76/200] Loss: 753.0031127929688\n",
      "Epoch [77/200] Loss: 749.2760620117188\n",
      "Epoch [78/200] Loss: 745.569091796875\n",
      "Epoch [79/200] Loss: 741.8814697265625\n",
      "Epoch [80/200] Loss: 738.213134765625\n",
      "Epoch [81/200] Loss: 734.564208984375\n",
      "Epoch [82/200] Loss: 730.9342651367188\n",
      "Epoch [83/200] Loss: 727.3228149414062\n",
      "Epoch [84/200] Loss: 723.7301635742188\n",
      "Epoch [85/200] Loss: 720.1558837890625\n",
      "Epoch [86/200] Loss: 716.5997924804688\n",
      "Epoch [87/200] Loss: 713.0615234375\n",
      "Epoch [88/200] Loss: 709.5413208007812\n",
      "Epoch [89/200] Loss: 706.0386352539062\n",
      "Epoch [90/200] Loss: 702.55322265625\n",
      "Epoch [91/200] Loss: 699.0852661132812\n",
      "Epoch [92/200] Loss: 695.6343994140625\n",
      "Epoch [93/200] Loss: 692.2007446289062\n",
      "Epoch [94/200] Loss: 688.7835693359375\n",
      "Epoch [95/200] Loss: 685.3831787109375\n",
      "Epoch [96/200] Loss: 681.9993286132812\n",
      "Epoch [97/200] Loss: 678.6317749023438\n",
      "Epoch [98/200] Loss: 675.280517578125\n",
      "Epoch [99/200] Loss: 671.945068359375\n",
      "Epoch [100/200] Loss: 668.6258544921875\n",
      "Epoch [101/200] Loss: 665.3223266601562\n",
      "Epoch [102/200] Loss: 662.0345458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [103/200] Loss: 658.7623291015625\n",
      "Epoch [104/200] Loss: 655.50537109375\n",
      "Epoch [105/200] Loss: 652.2637939453125\n",
      "Epoch [106/200] Loss: 649.0371704101562\n",
      "Epoch [107/200] Loss: 645.8258666992188\n",
      "Epoch [108/200] Loss: 642.62939453125\n",
      "Epoch [109/200] Loss: 639.4478759765625\n",
      "Epoch [110/200] Loss: 636.2809448242188\n",
      "Epoch [111/200] Loss: 633.1287231445312\n",
      "Epoch [112/200] Loss: 629.9909057617188\n",
      "Epoch [113/200] Loss: 626.8676147460938\n",
      "Epoch [114/200] Loss: 623.758544921875\n",
      "Epoch [115/200] Loss: 620.6637573242188\n",
      "Epoch [116/200] Loss: 617.5830078125\n",
      "Epoch [117/200] Loss: 614.5162963867188\n",
      "Epoch [118/200] Loss: 611.4635620117188\n",
      "Epoch [119/200] Loss: 608.4247436523438\n",
      "Epoch [120/200] Loss: 605.3993530273438\n",
      "Epoch [121/200] Loss: 602.3878784179688\n",
      "Epoch [122/200] Loss: 599.3900146484375\n",
      "Epoch [123/200] Loss: 596.405517578125\n",
      "Epoch [124/200] Loss: 593.4344482421875\n",
      "Epoch [125/200] Loss: 590.476806640625\n",
      "Epoch [126/200] Loss: 587.5323486328125\n",
      "Epoch [127/200] Loss: 584.6010131835938\n",
      "Epoch [128/200] Loss: 581.682861328125\n",
      "Epoch [129/200] Loss: 578.777587890625\n",
      "Epoch [130/200] Loss: 575.8854370117188\n",
      "Epoch [131/200] Loss: 573.006103515625\n",
      "Epoch [132/200] Loss: 570.1395263671875\n",
      "Epoch [133/200] Loss: 567.2857055664062\n",
      "Epoch [134/200] Loss: 564.4444580078125\n",
      "Epoch [135/200] Loss: 561.6160278320312\n",
      "Epoch [136/200] Loss: 558.7998046875\n",
      "Epoch [137/200] Loss: 555.9962768554688\n",
      "Epoch [138/200] Loss: 553.2051391601562\n",
      "Epoch [139/200] Loss: 550.4263305664062\n",
      "Epoch [140/200] Loss: 547.65966796875\n",
      "Epoch [141/200] Loss: 544.9052124023438\n",
      "Epoch [142/200] Loss: 542.1630249023438\n",
      "Epoch [143/200] Loss: 539.432861328125\n",
      "Epoch [144/200] Loss: 536.7146606445312\n",
      "Epoch [145/200] Loss: 534.0084838867188\n",
      "Epoch [146/200] Loss: 531.314208984375\n",
      "Epoch [147/200] Loss: 528.6318359375\n",
      "Epoch [148/200] Loss: 525.9611206054688\n",
      "Epoch [149/200] Loss: 523.3021850585938\n",
      "Epoch [150/200] Loss: 520.6550903320312\n",
      "Epoch [151/200] Loss: 518.0194091796875\n",
      "Epoch [152/200] Loss: 515.3953857421875\n",
      "Epoch [153/200] Loss: 512.7828369140625\n",
      "Epoch [154/200] Loss: 510.18182373046875\n",
      "Epoch [155/200] Loss: 507.5921936035156\n",
      "Epoch [156/200] Loss: 505.01397705078125\n",
      "Epoch [157/200] Loss: 502.44696044921875\n",
      "Epoch [158/200] Loss: 499.89129638671875\n",
      "Epoch [159/200] Loss: 497.3467712402344\n",
      "Epoch [160/200] Loss: 494.8135070800781\n",
      "Epoch [161/200] Loss: 492.2912902832031\n",
      "Epoch [162/200] Loss: 489.78009033203125\n",
      "Epoch [163/200] Loss: 487.27996826171875\n",
      "Epoch [164/200] Loss: 484.790771484375\n",
      "Epoch [165/200] Loss: 482.3126220703125\n",
      "Epoch [166/200] Loss: 479.8453063964844\n",
      "Epoch [167/200] Loss: 477.388671875\n",
      "Epoch [168/200] Loss: 474.9429931640625\n",
      "Epoch [169/200] Loss: 472.50799560546875\n",
      "Epoch [170/200] Loss: 470.0837097167969\n",
      "Epoch [171/200] Loss: 467.67010498046875\n",
      "Epoch [172/200] Loss: 465.26702880859375\n",
      "Epoch [173/200] Loss: 462.8744812011719\n",
      "Epoch [174/200] Loss: 460.4925842285156\n",
      "Epoch [175/200] Loss: 458.12103271484375\n",
      "Epoch [176/200] Loss: 455.76007080078125\n",
      "Epoch [177/200] Loss: 453.4093933105469\n",
      "Epoch [178/200] Loss: 451.06915283203125\n",
      "Epoch [179/200] Loss: 448.7391357421875\n",
      "Epoch [180/200] Loss: 446.4194030761719\n",
      "Epoch [181/200] Loss: 444.10992431640625\n",
      "Epoch [182/200] Loss: 441.81060791015625\n",
      "Epoch [183/200] Loss: 439.5215148925781\n",
      "Epoch [184/200] Loss: 437.2424621582031\n",
      "Epoch [185/200] Loss: 434.97357177734375\n",
      "Epoch [186/200] Loss: 432.714599609375\n",
      "Epoch [187/200] Loss: 430.4656066894531\n",
      "Epoch [188/200] Loss: 428.22662353515625\n",
      "Epoch [189/200] Loss: 425.9976501464844\n",
      "Epoch [190/200] Loss: 423.77838134765625\n",
      "Epoch [191/200] Loss: 421.5689697265625\n",
      "Epoch [192/200] Loss: 419.36944580078125\n",
      "Epoch [193/200] Loss: 417.17974853515625\n",
      "Epoch [194/200] Loss: 414.9996643066406\n",
      "Epoch [195/200] Loss: 412.8292541503906\n",
      "Epoch [196/200] Loss: 410.6685791015625\n",
      "Epoch [197/200] Loss: 408.5174560546875\n",
      "Epoch [198/200] Loss: 406.3759765625\n",
      "Epoch [199/200] Loss: 404.24395751953125\n",
      "Epoch [200/200] Loss: 402.12152099609375\n",
      "Predicted days_remaining for parent_id 234: 16.114038467407227\n",
      "Training for parent_id 238...\n",
      "Epoch [1/200] Loss: 820.9628295898438\n",
      "Epoch [2/200] Loss: 804.9614868164062\n",
      "Epoch [3/200] Loss: 789.7330932617188\n",
      "Epoch [4/200] Loss: 775.33056640625\n",
      "Epoch [5/200] Loss: 761.7399291992188\n",
      "Epoch [6/200] Loss: 748.9064331054688\n",
      "Epoch [7/200] Loss: 736.75244140625\n",
      "Epoch [8/200] Loss: 725.1993408203125\n",
      "Epoch [9/200] Loss: 714.1752319335938\n",
      "Epoch [10/200] Loss: 703.617431640625\n",
      "Epoch [11/200] Loss: 693.4757690429688\n",
      "Epoch [12/200] Loss: 683.7152099609375\n",
      "Epoch [13/200] Loss: 674.3167114257812\n",
      "Epoch [14/200] Loss: 665.2762451171875\n",
      "Epoch [15/200] Loss: 656.599853515625\n",
      "Epoch [16/200] Loss: 648.2988891601562\n",
      "Epoch [17/200] Loss: 640.3840942382812\n",
      "Epoch [18/200] Loss: 632.8602294921875\n",
      "Epoch [19/200] Loss: 625.7230224609375\n",
      "Epoch [20/200] Loss: 618.9586791992188\n",
      "Epoch [21/200] Loss: 612.5452270507812\n",
      "Epoch [22/200] Loss: 606.4559936523438\n",
      "Epoch [23/200] Loss: 600.66162109375\n",
      "Epoch [24/200] Loss: 595.13232421875\n",
      "Epoch [25/200] Loss: 589.8400268554688\n",
      "Epoch [26/200] Loss: 584.7586059570312\n",
      "Epoch [27/200] Loss: 579.8645629882812\n",
      "Epoch [28/200] Loss: 575.1365966796875\n",
      "Epoch [29/200] Loss: 570.5563354492188\n",
      "Epoch [30/200] Loss: 566.1074829101562\n",
      "Epoch [31/200] Loss: 561.7755126953125\n",
      "Epoch [32/200] Loss: 557.5480346679688\n",
      "Epoch [33/200] Loss: 553.4141235351562\n",
      "Epoch [34/200] Loss: 549.3643188476562\n",
      "Epoch [35/200] Loss: 545.3902587890625\n",
      "Epoch [36/200] Loss: 541.4846801757812\n",
      "Epoch [37/200] Loss: 537.6414184570312\n",
      "Epoch [38/200] Loss: 533.854736328125\n",
      "Epoch [39/200] Loss: 530.1199951171875\n",
      "Epoch [40/200] Loss: 526.43310546875\n",
      "Epoch [41/200] Loss: 522.790283203125\n",
      "Epoch [42/200] Loss: 519.1886596679688\n",
      "Epoch [43/200] Loss: 515.6256103515625\n",
      "Epoch [44/200] Loss: 512.09912109375\n",
      "Epoch [45/200] Loss: 508.607177734375\n",
      "Epoch [46/200] Loss: 505.1482849121094\n",
      "Epoch [47/200] Loss: 501.72119140625\n",
      "Epoch [48/200] Loss: 498.3247375488281\n",
      "Epoch [49/200] Loss: 494.9580993652344\n",
      "Epoch [50/200] Loss: 491.62042236328125\n",
      "Epoch [51/200] Loss: 488.3108825683594\n",
      "Epoch [52/200] Loss: 485.02899169921875\n",
      "Epoch [53/200] Loss: 481.7741394042969\n",
      "Epoch [54/200] Loss: 478.5458679199219\n",
      "Epoch [55/200] Loss: 475.3435974121094\n",
      "Epoch [56/200] Loss: 472.1669616699219\n",
      "Epoch [57/200] Loss: 469.01568603515625\n",
      "Epoch [58/200] Loss: 465.88909912109375\n",
      "Epoch [59/200] Loss: 462.7872009277344\n",
      "Epoch [60/200] Loss: 459.7093505859375\n",
      "Epoch [61/200] Loss: 456.6552734375\n",
      "Epoch [62/200] Loss: 453.62469482421875\n",
      "Epoch [63/200] Loss: 450.6173095703125\n",
      "Epoch [64/200] Loss: 447.6326599121094\n",
      "Epoch [65/200] Loss: 444.6705017089844\n",
      "Epoch [66/200] Loss: 441.7306213378906\n",
      "Epoch [67/200] Loss: 438.81268310546875\n",
      "Epoch [68/200] Loss: 435.9162292480469\n",
      "Epoch [69/200] Loss: 433.0411071777344\n",
      "Epoch [70/200] Loss: 430.18707275390625\n",
      "Epoch [71/200] Loss: 427.3537902832031\n",
      "Epoch [72/200] Loss: 424.5408020019531\n",
      "Epoch [73/200] Loss: 421.748291015625\n",
      "Epoch [74/200] Loss: 418.97552490234375\n",
      "Epoch [75/200] Loss: 416.2224426269531\n",
      "Epoch [76/200] Loss: 413.4888916015625\n",
      "Epoch [77/200] Loss: 410.7744445800781\n",
      "Epoch [78/200] Loss: 408.07904052734375\n",
      "Epoch [79/200] Loss: 405.4021911621094\n",
      "Epoch [80/200] Loss: 402.74395751953125\n",
      "Epoch [81/200] Loss: 400.10382080078125\n",
      "Epoch [82/200] Loss: 397.4818115234375\n",
      "Epoch [83/200] Loss: 394.8775939941406\n",
      "Epoch [84/200] Loss: 392.29095458984375\n",
      "Epoch [85/200] Loss: 389.7217102050781\n",
      "Epoch [86/200] Loss: 387.1695861816406\n",
      "Epoch [87/200] Loss: 384.6345520019531\n",
      "Epoch [88/200] Loss: 382.1162414550781\n",
      "Epoch [89/200] Loss: 379.61456298828125\n",
      "Epoch [90/200] Loss: 377.1292724609375\n",
      "Epoch [91/200] Loss: 374.66021728515625\n",
      "Epoch [92/200] Loss: 372.2072448730469\n",
      "Epoch [93/200] Loss: 369.77020263671875\n",
      "Epoch [94/200] Loss: 367.34881591796875\n",
      "Epoch [95/200] Loss: 364.9431457519531\n",
      "Epoch [96/200] Loss: 362.5528259277344\n",
      "Epoch [97/200] Loss: 360.1776428222656\n",
      "Epoch [98/200] Loss: 357.8176574707031\n",
      "Epoch [99/200] Loss: 355.47265625\n",
      "Epoch [100/200] Loss: 353.1424865722656\n",
      "Epoch [101/200] Loss: 350.82684326171875\n",
      "Epoch [102/200] Loss: 348.5258483886719\n",
      "Epoch [103/200] Loss: 346.2391357421875\n",
      "Epoch [104/200] Loss: 343.9668273925781\n",
      "Epoch [105/200] Loss: 341.7086486816406\n",
      "Epoch [106/200] Loss: 339.4645080566406\n",
      "Epoch [107/200] Loss: 337.2342529296875\n",
      "Epoch [108/200] Loss: 335.0177307128906\n",
      "Epoch [109/200] Loss: 332.81488037109375\n",
      "Epoch [110/200] Loss: 330.6257019042969\n",
      "Epoch [111/200] Loss: 328.44976806640625\n",
      "Epoch [112/200] Loss: 326.2872009277344\n",
      "Epoch [113/200] Loss: 324.1379699707031\n",
      "Epoch [114/200] Loss: 322.0018005371094\n",
      "Epoch [115/200] Loss: 319.8786926269531\n",
      "Epoch [116/200] Loss: 317.7684326171875\n",
      "Epoch [117/200] Loss: 315.6710205078125\n",
      "Epoch [118/200] Loss: 313.5863952636719\n",
      "Epoch [119/200] Loss: 311.5142822265625\n",
      "Epoch [120/200] Loss: 309.4548034667969\n",
      "Epoch [121/200] Loss: 307.40771484375\n",
      "Epoch [122/200] Loss: 305.37310791015625\n",
      "Epoch [123/200] Loss: 303.3506164550781\n",
      "Epoch [124/200] Loss: 301.3403625488281\n",
      "Epoch [125/200] Loss: 299.34228515625\n",
      "Epoch [126/200] Loss: 297.3561706542969\n",
      "Epoch [127/200] Loss: 295.3819885253906\n",
      "Epoch [128/200] Loss: 293.4196472167969\n",
      "Epoch [129/200] Loss: 291.46917724609375\n",
      "Epoch [130/200] Loss: 289.5303039550781\n",
      "Epoch [131/200] Loss: 287.6030578613281\n",
      "Epoch [132/200] Loss: 285.6875\n",
      "Epoch [133/200] Loss: 283.7833251953125\n",
      "Epoch [134/200] Loss: 281.8905944824219\n",
      "Epoch [135/200] Loss: 280.0091552734375\n",
      "Epoch [136/200] Loss: 278.13897705078125\n",
      "Epoch [137/200] Loss: 276.2800598144531\n",
      "Epoch [138/200] Loss: 274.4322204589844\n",
      "Epoch [139/200] Loss: 272.595458984375\n",
      "Epoch [140/200] Loss: 270.76971435546875\n",
      "Epoch [141/200] Loss: 268.95489501953125\n",
      "Epoch [142/200] Loss: 267.15093994140625\n",
      "Epoch [143/200] Loss: 265.3577880859375\n",
      "Epoch [144/200] Loss: 263.5753173828125\n",
      "Epoch [145/200] Loss: 261.8035888671875\n",
      "Epoch [146/200] Loss: 260.04248046875\n",
      "Epoch [147/200] Loss: 258.2918701171875\n",
      "Epoch [148/200] Loss: 256.5517883300781\n",
      "Epoch [149/200] Loss: 254.8221435546875\n",
      "Epoch [150/200] Loss: 253.10289001464844\n",
      "Epoch [151/200] Loss: 251.3938751220703\n",
      "Epoch [152/200] Loss: 249.69517517089844\n",
      "Epoch [153/200] Loss: 248.0066375732422\n",
      "Epoch [154/200] Loss: 246.32823181152344\n",
      "Epoch [155/200] Loss: 244.6600341796875\n",
      "Epoch [156/200] Loss: 243.0018310546875\n",
      "Epoch [157/200] Loss: 241.35353088378906\n",
      "Epoch [158/200] Loss: 239.71522521972656\n",
      "Epoch [159/200] Loss: 238.0867919921875\n",
      "Epoch [160/200] Loss: 236.4681854248047\n",
      "Epoch [161/200] Loss: 234.8592987060547\n",
      "Epoch [162/200] Loss: 233.26016235351562\n",
      "Epoch [163/200] Loss: 231.67062377929688\n",
      "Epoch [164/200] Loss: 230.09078979492188\n",
      "Epoch [165/200] Loss: 228.52052307128906\n",
      "Epoch [166/200] Loss: 226.95968627929688\n",
      "Epoch [167/200] Loss: 225.40838623046875\n",
      "Epoch [168/200] Loss: 223.86647033691406\n",
      "Epoch [169/200] Loss: 222.33387756347656\n",
      "Epoch [170/200] Loss: 220.81063842773438\n",
      "Epoch [171/200] Loss: 219.29672241210938\n",
      "Epoch [172/200] Loss: 217.79193115234375\n",
      "Epoch [173/200] Loss: 216.29637145996094\n",
      "Epoch [174/200] Loss: 214.8098602294922\n",
      "Epoch [175/200] Loss: 213.33255004882812\n",
      "Epoch [176/200] Loss: 211.86412048339844\n",
      "Epoch [177/200] Loss: 210.4048309326172\n",
      "Epoch [178/200] Loss: 208.95436096191406\n",
      "Epoch [179/200] Loss: 207.51283264160156\n",
      "Epoch [180/200] Loss: 206.08018493652344\n",
      "Epoch [181/200] Loss: 204.6562957763672\n",
      "Epoch [182/200] Loss: 203.2411651611328\n",
      "Epoch [183/200] Loss: 201.8347625732422\n",
      "Epoch [184/200] Loss: 200.43699645996094\n",
      "Epoch [185/200] Loss: 199.04794311523438\n",
      "Epoch [186/200] Loss: 197.66737365722656\n",
      "Epoch [187/200] Loss: 196.2953643798828\n",
      "Epoch [188/200] Loss: 194.931884765625\n",
      "Epoch [189/200] Loss: 193.57691955566406\n",
      "Epoch [190/200] Loss: 192.23028564453125\n",
      "Epoch [191/200] Loss: 190.89208984375\n",
      "Epoch [192/200] Loss: 189.5621337890625\n",
      "Epoch [193/200] Loss: 188.2405242919922\n",
      "Epoch [194/200] Loss: 186.92709350585938\n",
      "Epoch [195/200] Loss: 185.6219482421875\n",
      "Epoch [196/200] Loss: 184.32493591308594\n",
      "Epoch [197/200] Loss: 183.03604125976562\n",
      "Epoch [198/200] Loss: 181.7552032470703\n",
      "Epoch [199/200] Loss: 180.48248291015625\n",
      "Epoch [200/200] Loss: 179.21768188476562\n",
      "Predicted days_remaining for parent_id 238: 15.962373733520508\n",
      "Training for parent_id 245...\n",
      "Epoch [1/200] Loss: 1346.6239013671875\n",
      "Epoch [2/200] Loss: 1327.4072265625\n",
      "Epoch [3/200] Loss: 1308.672607421875\n",
      "Epoch [4/200] Loss: 1290.51953125\n",
      "Epoch [5/200] Loss: 1272.998046875\n",
      "Epoch [6/200] Loss: 1256.1358642578125\n",
      "Epoch [7/200] Loss: 1239.95703125\n",
      "Epoch [8/200] Loss: 1224.4716796875\n",
      "Epoch [9/200] Loss: 1209.67333984375\n",
      "Epoch [10/200] Loss: 1195.5439453125\n",
      "Epoch [11/200] Loss: 1182.0638427734375\n",
      "Epoch [12/200] Loss: 1169.210205078125\n",
      "Epoch [13/200] Loss: 1156.9560546875\n",
      "Epoch [14/200] Loss: 1145.26904296875\n",
      "Epoch [15/200] Loss: 1134.11328125\n",
      "Epoch [16/200] Loss: 1123.452880859375\n",
      "Epoch [17/200] Loss: 1113.2576904296875\n",
      "Epoch [18/200] Loss: 1103.5029296875\n",
      "Epoch [19/200] Loss: 1094.1722412109375\n",
      "Epoch [20/200] Loss: 1085.253173828125\n",
      "Epoch [21/200] Loss: 1076.7349853515625\n",
      "Epoch [22/200] Loss: 1068.60302734375\n",
      "Epoch [23/200] Loss: 1060.83544921875\n",
      "Epoch [24/200] Loss: 1053.404052734375\n",
      "Epoch [25/200] Loss: 1046.275146484375\n",
      "Epoch [26/200] Loss: 1039.4130859375\n",
      "Epoch [27/200] Loss: 1032.782958984375\n",
      "Epoch [28/200] Loss: 1026.353515625\n",
      "Epoch [29/200] Loss: 1020.0975341796875\n",
      "Epoch [30/200] Loss: 1013.9923095703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200] Loss: 1008.0193481445312\n",
      "Epoch [32/200] Loss: 1002.1644897460938\n",
      "Epoch [33/200] Loss: 996.4150390625\n",
      "Epoch [34/200] Loss: 990.7617797851562\n",
      "Epoch [35/200] Loss: 985.1963500976562\n",
      "Epoch [36/200] Loss: 979.7112426757812\n",
      "Epoch [37/200] Loss: 974.3011474609375\n",
      "Epoch [38/200] Loss: 968.9600219726562\n",
      "Epoch [39/200] Loss: 963.6834106445312\n",
      "Epoch [40/200] Loss: 958.467041015625\n",
      "Epoch [41/200] Loss: 953.3074951171875\n",
      "Epoch [42/200] Loss: 948.2014770507812\n",
      "Epoch [43/200] Loss: 943.146728515625\n",
      "Epoch [44/200] Loss: 938.1405639648438\n",
      "Epoch [45/200] Loss: 933.1810302734375\n",
      "Epoch [46/200] Loss: 928.26708984375\n",
      "Epoch [47/200] Loss: 923.3971557617188\n",
      "Epoch [48/200] Loss: 918.5699462890625\n",
      "Epoch [49/200] Loss: 913.784423828125\n",
      "Epoch [50/200] Loss: 909.0398559570312\n",
      "Epoch [51/200] Loss: 904.3348388671875\n",
      "Epoch [52/200] Loss: 899.668701171875\n",
      "Epoch [53/200] Loss: 895.0403442382812\n",
      "Epoch [54/200] Loss: 890.4489135742188\n",
      "Epoch [55/200] Loss: 885.8933715820312\n",
      "Epoch [56/200] Loss: 881.372802734375\n",
      "Epoch [57/200] Loss: 876.8863525390625\n",
      "Epoch [58/200] Loss: 872.43310546875\n",
      "Epoch [59/200] Loss: 868.0123901367188\n",
      "Epoch [60/200] Loss: 863.623291015625\n",
      "Epoch [61/200] Loss: 859.2649536132812\n",
      "Epoch [62/200] Loss: 854.9368286132812\n",
      "Epoch [63/200] Loss: 850.6383056640625\n",
      "Epoch [64/200] Loss: 846.3685913085938\n",
      "Epoch [65/200] Loss: 842.127197265625\n",
      "Epoch [66/200] Loss: 837.9133911132812\n",
      "Epoch [67/200] Loss: 833.7269897460938\n",
      "Epoch [68/200] Loss: 829.5670776367188\n",
      "Epoch [69/200] Loss: 825.433349609375\n",
      "Epoch [70/200] Loss: 821.3253173828125\n",
      "Epoch [71/200] Loss: 817.2424926757812\n",
      "Epoch [72/200] Loss: 813.1845092773438\n",
      "Epoch [73/200] Loss: 809.150634765625\n",
      "Epoch [74/200] Loss: 805.1407470703125\n",
      "Epoch [75/200] Loss: 801.1544189453125\n",
      "Epoch [76/200] Loss: 797.191162109375\n",
      "Epoch [77/200] Loss: 793.2508544921875\n",
      "Epoch [78/200] Loss: 789.3325805664062\n",
      "Epoch [79/200] Loss: 785.4367065429688\n",
      "Epoch [80/200] Loss: 781.5625610351562\n",
      "Epoch [81/200] Loss: 777.7098999023438\n",
      "Epoch [82/200] Loss: 773.8784790039062\n",
      "Epoch [83/200] Loss: 770.06787109375\n",
      "Epoch [84/200] Loss: 766.2777099609375\n",
      "Epoch [85/200] Loss: 762.5081176757812\n",
      "Epoch [86/200] Loss: 758.7586669921875\n",
      "Epoch [87/200] Loss: 755.0289916992188\n",
      "Epoch [88/200] Loss: 751.3189697265625\n",
      "Epoch [89/200] Loss: 747.6282958984375\n",
      "Epoch [90/200] Loss: 743.9569091796875\n",
      "Epoch [91/200] Loss: 740.3045043945312\n",
      "Epoch [92/200] Loss: 736.6708984375\n",
      "Epoch [93/200] Loss: 733.0560302734375\n",
      "Epoch [94/200] Loss: 729.4593505859375\n",
      "Epoch [95/200] Loss: 725.8809814453125\n",
      "Epoch [96/200] Loss: 722.320556640625\n",
      "Epoch [97/200] Loss: 718.7780151367188\n",
      "Epoch [98/200] Loss: 715.2532348632812\n",
      "Epoch [99/200] Loss: 711.745849609375\n",
      "Epoch [100/200] Loss: 708.255859375\n",
      "Epoch [101/200] Loss: 704.7828979492188\n",
      "Epoch [102/200] Loss: 701.3271484375\n",
      "Epoch [103/200] Loss: 697.8883056640625\n",
      "Epoch [104/200] Loss: 694.466064453125\n",
      "Epoch [105/200] Loss: 691.0604858398438\n",
      "Epoch [106/200] Loss: 687.67138671875\n",
      "Epoch [107/200] Loss: 684.2986450195312\n",
      "Epoch [108/200] Loss: 680.9420166015625\n",
      "Epoch [109/200] Loss: 677.6013793945312\n",
      "Epoch [110/200] Loss: 674.2767944335938\n",
      "Epoch [111/200] Loss: 670.9678344726562\n",
      "Epoch [112/200] Loss: 667.6749267578125\n",
      "Epoch [113/200] Loss: 664.3972778320312\n",
      "Epoch [114/200] Loss: 661.135009765625\n",
      "Epoch [115/200] Loss: 657.8882446289062\n",
      "Epoch [116/200] Loss: 654.6566162109375\n",
      "Epoch [117/200] Loss: 651.4401245117188\n",
      "Epoch [118/200] Loss: 648.2385864257812\n",
      "Epoch [119/200] Loss: 645.0519409179688\n",
      "Epoch [120/200] Loss: 641.8801879882812\n",
      "Epoch [121/200] Loss: 638.7229614257812\n",
      "Epoch [122/200] Loss: 635.580322265625\n",
      "Epoch [123/200] Loss: 632.4521484375\n",
      "Epoch [124/200] Loss: 629.3385620117188\n",
      "Epoch [125/200] Loss: 626.2390747070312\n",
      "Epoch [126/200] Loss: 623.15380859375\n",
      "Epoch [127/200] Loss: 620.08251953125\n",
      "Epoch [128/200] Loss: 617.025390625\n",
      "Epoch [129/200] Loss: 613.9819946289062\n",
      "Epoch [130/200] Loss: 610.9525756835938\n",
      "Epoch [131/200] Loss: 607.9368896484375\n",
      "Epoch [132/200] Loss: 604.9346923828125\n",
      "Epoch [133/200] Loss: 601.9462890625\n",
      "Epoch [134/200] Loss: 598.9712524414062\n",
      "Epoch [135/200] Loss: 596.0096435546875\n",
      "Epoch [136/200] Loss: 593.061279296875\n",
      "Epoch [137/200] Loss: 590.1262817382812\n",
      "Epoch [138/200] Loss: 587.204345703125\n",
      "Epoch [139/200] Loss: 584.2955932617188\n",
      "Epoch [140/200] Loss: 581.3997802734375\n",
      "Epoch [141/200] Loss: 578.5169067382812\n",
      "Epoch [142/200] Loss: 575.6468505859375\n",
      "Epoch [143/200] Loss: 572.7897338867188\n",
      "Epoch [144/200] Loss: 569.9451293945312\n",
      "Epoch [145/200] Loss: 567.1134033203125\n",
      "Epoch [146/200] Loss: 564.2940673828125\n",
      "Epoch [147/200] Loss: 561.4873046875\n",
      "Epoch [148/200] Loss: 558.6929931640625\n",
      "Epoch [149/200] Loss: 555.9110717773438\n",
      "Epoch [150/200] Loss: 553.141357421875\n",
      "Epoch [151/200] Loss: 550.3840942382812\n",
      "Epoch [152/200] Loss: 547.6387939453125\n",
      "Epoch [153/200] Loss: 544.9058837890625\n",
      "Epoch [154/200] Loss: 542.184814453125\n",
      "Epoch [155/200] Loss: 539.4757690429688\n",
      "Epoch [156/200] Loss: 536.7787475585938\n",
      "Epoch [157/200] Loss: 534.0936279296875\n",
      "Epoch [158/200] Loss: 531.4200439453125\n",
      "Epoch [159/200] Loss: 528.7584838867188\n",
      "Epoch [160/200] Loss: 526.1085815429688\n",
      "Epoch [161/200] Loss: 523.4703369140625\n",
      "Epoch [162/200] Loss: 520.8436889648438\n",
      "Epoch [163/200] Loss: 518.228515625\n",
      "Epoch [164/200] Loss: 515.6248168945312\n",
      "Epoch [165/200] Loss: 513.0325927734375\n",
      "Epoch [166/200] Loss: 510.4517517089844\n",
      "Epoch [167/200] Loss: 507.8822326660156\n",
      "Epoch [168/200] Loss: 505.323974609375\n",
      "Epoch [169/200] Loss: 502.7769775390625\n",
      "Epoch [170/200] Loss: 500.24102783203125\n",
      "Epoch [171/200] Loss: 497.7161865234375\n",
      "Epoch [172/200] Loss: 495.2024841308594\n",
      "Epoch [173/200] Loss: 492.6998291015625\n",
      "Epoch [174/200] Loss: 490.20806884765625\n",
      "Epoch [175/200] Loss: 487.727294921875\n",
      "Epoch [176/200] Loss: 485.2573547363281\n",
      "Epoch [177/200] Loss: 482.79815673828125\n",
      "Epoch [178/200] Loss: 480.349853515625\n",
      "Epoch [179/200] Loss: 477.9121398925781\n",
      "Epoch [180/200] Loss: 475.48529052734375\n",
      "Epoch [181/200] Loss: 473.06890869140625\n",
      "Epoch [182/200] Loss: 470.6631774902344\n",
      "Epoch [183/200] Loss: 468.2680358886719\n",
      "Epoch [184/200] Loss: 465.8833923339844\n",
      "Epoch [185/200] Loss: 463.5091552734375\n",
      "Epoch [186/200] Loss: 461.1452941894531\n",
      "Epoch [187/200] Loss: 458.79193115234375\n",
      "Epoch [188/200] Loss: 456.4486999511719\n",
      "Epoch [189/200] Loss: 454.1158752441406\n",
      "Epoch [190/200] Loss: 451.79339599609375\n",
      "Epoch [191/200] Loss: 449.48095703125\n",
      "Epoch [192/200] Loss: 447.1787109375\n",
      "Epoch [193/200] Loss: 444.8865051269531\n",
      "Epoch [194/200] Loss: 442.60455322265625\n",
      "Epoch [195/200] Loss: 440.33251953125\n",
      "Epoch [196/200] Loss: 438.0705261230469\n",
      "Epoch [197/200] Loss: 435.8185119628906\n",
      "Epoch [198/200] Loss: 433.5763854980469\n",
      "Epoch [199/200] Loss: 431.3440856933594\n",
      "Epoch [200/200] Loss: 429.1217956542969\n",
      "Predicted days_remaining for parent_id 245: 16.440603256225586\n",
      "Training for parent_id 252...\n",
      "Epoch [1/200] Loss: 126.86981964111328\n",
      "Epoch [2/200] Loss: 121.50706481933594\n",
      "Epoch [3/200] Loss: 116.39093017578125\n",
      "Epoch [4/200] Loss: 111.569580078125\n",
      "Epoch [5/200] Loss: 107.0553970336914\n",
      "Epoch [6/200] Loss: 102.84081268310547\n",
      "Epoch [7/200] Loss: 98.90684509277344\n",
      "Epoch [8/200] Loss: 95.22740936279297\n",
      "Epoch [9/200] Loss: 91.77335357666016\n",
      "Epoch [10/200] Loss: 88.5173110961914\n",
      "Epoch [11/200] Loss: 85.4374771118164\n",
      "Epoch [12/200] Loss: 82.51930236816406\n",
      "Epoch [13/200] Loss: 79.75458526611328\n",
      "Epoch [14/200] Loss: 77.13910675048828\n",
      "Epoch [15/200] Loss: 74.66996002197266\n",
      "Epoch [16/200] Loss: 72.34346771240234\n",
      "Epoch [17/200] Loss: 70.15419006347656\n",
      "Epoch [18/200] Loss: 68.09487915039062\n",
      "Epoch [19/200] Loss: 66.15705108642578\n",
      "Epoch [20/200] Loss: 64.3316421508789\n",
      "Epoch [21/200] Loss: 62.609737396240234\n",
      "Epoch [22/200] Loss: 60.98286437988281\n",
      "Epoch [23/200] Loss: 59.44322967529297\n",
      "Epoch [24/200] Loss: 57.98374557495117\n",
      "Epoch [25/200] Loss: 56.5979118347168\n",
      "Epoch [26/200] Loss: 55.27977752685547\n",
      "Epoch [27/200] Loss: 54.02384567260742\n",
      "Epoch [28/200] Loss: 52.824974060058594\n",
      "Epoch [29/200] Loss: 51.678340911865234\n",
      "Epoch [30/200] Loss: 50.57956314086914\n",
      "Epoch [31/200] Loss: 49.524505615234375\n",
      "Epoch [32/200] Loss: 48.509456634521484\n",
      "Epoch [33/200] Loss: 47.53111267089844\n",
      "Epoch [34/200] Loss: 46.58649826049805\n",
      "Epoch [35/200] Loss: 45.67303466796875\n",
      "Epoch [36/200] Loss: 44.78844451904297\n",
      "Epoch [37/200] Loss: 43.9307746887207\n",
      "Epoch [38/200] Loss: 43.09833526611328\n",
      "Epoch [39/200] Loss: 42.2896614074707\n",
      "Epoch [40/200] Loss: 41.50345230102539\n",
      "Epoch [41/200] Loss: 40.7386360168457\n",
      "Epoch [42/200] Loss: 39.99422836303711\n",
      "Epoch [43/200] Loss: 39.269386291503906\n",
      "Epoch [44/200] Loss: 38.56336212158203\n",
      "Epoch [45/200] Loss: 37.87549591064453\n",
      "Epoch [46/200] Loss: 37.20517349243164\n",
      "Epoch [47/200] Loss: 36.55187225341797\n",
      "Epoch [48/200] Loss: 35.915077209472656\n",
      "Epoch [49/200] Loss: 35.29434585571289\n",
      "Epoch [50/200] Loss: 34.689239501953125\n",
      "Epoch [51/200] Loss: 34.099365234375\n",
      "Epoch [52/200] Loss: 33.524330139160156\n",
      "Epoch [53/200] Loss: 32.96379470825195\n",
      "Epoch [54/200] Loss: 32.41739273071289\n",
      "Epoch [55/200] Loss: 31.884811401367188\n",
      "Epoch [56/200] Loss: 31.36572265625\n",
      "Epoch [57/200] Loss: 30.85982322692871\n",
      "Epoch [58/200] Loss: 30.3668155670166\n",
      "Epoch [59/200] Loss: 29.88641357421875\n",
      "Epoch [60/200] Loss: 29.4183406829834\n",
      "Epoch [61/200] Loss: 28.962322235107422\n",
      "Epoch [62/200] Loss: 28.518102645874023\n",
      "Epoch [63/200] Loss: 28.085420608520508\n",
      "Epoch [64/200] Loss: 27.664026260375977\n",
      "Epoch [65/200] Loss: 27.253684997558594\n",
      "Epoch [66/200] Loss: 26.854141235351562\n",
      "Epoch [67/200] Loss: 26.465171813964844\n",
      "Epoch [68/200] Loss: 26.0865535736084\n",
      "Epoch [69/200] Loss: 25.718050003051758\n",
      "Epoch [70/200] Loss: 25.359458923339844\n",
      "Epoch [71/200] Loss: 25.01053810119629\n",
      "Epoch [72/200] Loss: 24.671104431152344\n",
      "Epoch [73/200] Loss: 24.34093475341797\n",
      "Epoch [74/200] Loss: 24.01982307434082\n",
      "Epoch [75/200] Loss: 23.70758056640625\n",
      "Epoch [76/200] Loss: 23.40399932861328\n",
      "Epoch [77/200] Loss: 23.10889434814453\n",
      "Epoch [78/200] Loss: 22.822067260742188\n",
      "Epoch [79/200] Loss: 22.5433349609375\n",
      "Epoch [80/200] Loss: 22.272510528564453\n",
      "Epoch [81/200] Loss: 22.009416580200195\n",
      "Epoch [82/200] Loss: 21.753868103027344\n",
      "Epoch [83/200] Loss: 21.50571060180664\n",
      "Epoch [84/200] Loss: 21.264739990234375\n",
      "Epoch [85/200] Loss: 21.030811309814453\n",
      "Epoch [86/200] Loss: 20.803752899169922\n",
      "Epoch [87/200] Loss: 20.583410263061523\n",
      "Epoch [88/200] Loss: 20.36960220336914\n",
      "Epoch [89/200] Loss: 20.162189483642578\n",
      "Epoch [90/200] Loss: 19.961009979248047\n",
      "Epoch [91/200] Loss: 19.765907287597656\n",
      "Epoch [92/200] Loss: 19.57674217224121\n",
      "Epoch [93/200] Loss: 19.39336585998535\n",
      "Epoch [94/200] Loss: 19.21563720703125\n",
      "Epoch [95/200] Loss: 19.043399810791016\n",
      "Epoch [96/200] Loss: 18.876529693603516\n",
      "Epoch [97/200] Loss: 18.714885711669922\n",
      "Epoch [98/200] Loss: 18.558334350585938\n",
      "Epoch [99/200] Loss: 18.4067440032959\n",
      "Epoch [100/200] Loss: 18.259986877441406\n",
      "Epoch [101/200] Loss: 18.11794090270996\n",
      "Epoch [102/200] Loss: 17.980480194091797\n",
      "Epoch [103/200] Loss: 17.847471237182617\n",
      "Epoch [104/200] Loss: 17.71881103515625\n",
      "Epoch [105/200] Loss: 17.59437370300293\n",
      "Epoch [106/200] Loss: 17.474048614501953\n",
      "Epoch [107/200] Loss: 17.35772705078125\n",
      "Epoch [108/200] Loss: 17.245296478271484\n",
      "Epoch [109/200] Loss: 17.136640548706055\n",
      "Epoch [110/200] Loss: 17.031661987304688\n",
      "Epoch [111/200] Loss: 16.930259704589844\n",
      "Epoch [112/200] Loss: 16.83233070373535\n",
      "Epoch [113/200] Loss: 16.737773895263672\n",
      "Epoch [114/200] Loss: 16.646495819091797\n",
      "Epoch [115/200] Loss: 16.558395385742188\n",
      "Epoch [116/200] Loss: 16.473386764526367\n",
      "Epoch [117/200] Loss: 16.391376495361328\n",
      "Epoch [118/200] Loss: 16.312273025512695\n",
      "Epoch [119/200] Loss: 16.23599624633789\n",
      "Epoch [120/200] Loss: 16.162458419799805\n",
      "Epoch [121/200] Loss: 16.091571807861328\n",
      "Epoch [122/200] Loss: 16.02326011657715\n",
      "Epoch [123/200] Loss: 15.957447052001953\n",
      "Epoch [124/200] Loss: 15.894054412841797\n",
      "Epoch [125/200] Loss: 15.832998275756836\n",
      "Epoch [126/200] Loss: 15.774219512939453\n",
      "Epoch [127/200] Loss: 15.71763801574707\n",
      "Epoch [128/200] Loss: 15.663182258605957\n",
      "Epoch [129/200] Loss: 15.610790252685547\n",
      "Epoch [130/200] Loss: 15.560384750366211\n",
      "Epoch [131/200] Loss: 15.51191520690918\n",
      "Epoch [132/200] Loss: 15.465307235717773\n",
      "Epoch [133/200] Loss: 15.420507431030273\n",
      "Epoch [134/200] Loss: 15.377450942993164\n",
      "Epoch [135/200] Loss: 15.336079597473145\n",
      "Epoch [136/200] Loss: 15.29633903503418\n",
      "Epoch [137/200] Loss: 15.25816822052002\n",
      "Epoch [138/200] Loss: 15.221522331237793\n",
      "Epoch [139/200] Loss: 15.1863431930542\n",
      "Epoch [140/200] Loss: 15.152580261230469\n",
      "Epoch [141/200] Loss: 15.120183944702148\n",
      "Epoch [142/200] Loss: 15.089107513427734\n",
      "Epoch [143/200] Loss: 15.059308052062988\n",
      "Epoch [144/200] Loss: 15.030731201171875\n",
      "Epoch [145/200] Loss: 15.003339767456055\n",
      "Epoch [146/200] Loss: 14.977088928222656\n",
      "Epoch [147/200] Loss: 14.951937675476074\n",
      "Epoch [148/200] Loss: 14.927845001220703\n",
      "Epoch [149/200] Loss: 14.90477466583252\n",
      "Epoch [150/200] Loss: 14.882684707641602\n",
      "Epoch [151/200] Loss: 14.861538887023926\n",
      "Epoch [152/200] Loss: 14.841306686401367\n",
      "Epoch [153/200] Loss: 14.82194709777832\n",
      "Epoch [154/200] Loss: 14.803428649902344\n",
      "Epoch [155/200] Loss: 14.785720825195312\n",
      "Epoch [156/200] Loss: 14.768793106079102\n",
      "Epoch [157/200] Loss: 14.752613067626953\n",
      "Epoch [158/200] Loss: 14.737154006958008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [159/200] Loss: 14.722382545471191\n",
      "Epoch [160/200] Loss: 14.708276748657227\n",
      "Epoch [161/200] Loss: 14.694807052612305\n",
      "Epoch [162/200] Loss: 14.6819486618042\n",
      "Epoch [163/200] Loss: 14.669677734375\n",
      "Epoch [164/200] Loss: 14.657968521118164\n",
      "Epoch [165/200] Loss: 14.646799087524414\n",
      "Epoch [166/200] Loss: 14.636149406433105\n",
      "Epoch [167/200] Loss: 14.625991821289062\n",
      "Epoch [168/200] Loss: 14.616313934326172\n",
      "Epoch [169/200] Loss: 14.60708999633789\n",
      "Epoch [170/200] Loss: 14.598302841186523\n",
      "Epoch [171/200] Loss: 14.589932441711426\n",
      "Epoch [172/200] Loss: 14.581964492797852\n",
      "Epoch [173/200] Loss: 14.574377059936523\n",
      "Epoch [174/200] Loss: 14.567155838012695\n",
      "Epoch [175/200] Loss: 14.560285568237305\n",
      "Epoch [176/200] Loss: 14.553750991821289\n",
      "Epoch [177/200] Loss: 14.54753589630127\n",
      "Epoch [178/200] Loss: 14.541627883911133\n",
      "Epoch [179/200] Loss: 14.5360107421875\n",
      "Epoch [180/200] Loss: 14.530675888061523\n",
      "Epoch [181/200] Loss: 14.525606155395508\n",
      "Epoch [182/200] Loss: 14.520790100097656\n",
      "Epoch [183/200] Loss: 14.516218185424805\n",
      "Epoch [184/200] Loss: 14.511877059936523\n",
      "Epoch [185/200] Loss: 14.507758140563965\n",
      "Epoch [186/200] Loss: 14.503852844238281\n",
      "Epoch [187/200] Loss: 14.500144958496094\n",
      "Epoch [188/200] Loss: 14.49662971496582\n",
      "Epoch [189/200] Loss: 14.49329948425293\n",
      "Epoch [190/200] Loss: 14.490140914916992\n",
      "Epoch [191/200] Loss: 14.487149238586426\n",
      "Epoch [192/200] Loss: 14.484315872192383\n",
      "Epoch [193/200] Loss: 14.481630325317383\n",
      "Epoch [194/200] Loss: 14.479090690612793\n",
      "Epoch [195/200] Loss: 14.4766845703125\n",
      "Epoch [196/200] Loss: 14.474410057067871\n",
      "Epoch [197/200] Loss: 14.472256660461426\n",
      "Epoch [198/200] Loss: 14.470220565795898\n",
      "Epoch [199/200] Loss: 14.46829605102539\n",
      "Epoch [200/200] Loss: 14.466475486755371\n",
      "Predicted days_remaining for parent_id 252: 10.584905624389648\n",
      "Training for parent_id 256...\n",
      "Epoch [1/200] Loss: 111.53324127197266\n",
      "Epoch [2/200] Loss: 107.14293670654297\n",
      "Epoch [3/200] Loss: 102.8567123413086\n",
      "Epoch [4/200] Loss: 98.67791748046875\n",
      "Epoch [5/200] Loss: 94.60694122314453\n",
      "Epoch [6/200] Loss: 90.64529418945312\n",
      "Epoch [7/200] Loss: 86.7986068725586\n",
      "Epoch [8/200] Loss: 83.07658386230469\n",
      "Epoch [9/200] Loss: 79.49108123779297\n",
      "Epoch [10/200] Loss: 76.05421447753906\n",
      "Epoch [11/200] Loss: 72.7768783569336\n",
      "Epoch [12/200] Loss: 69.66777801513672\n",
      "Epoch [13/200] Loss: 66.73258972167969\n",
      "Epoch [14/200] Loss: 63.97343063354492\n",
      "Epoch [15/200] Loss: 61.388648986816406\n",
      "Epoch [16/200] Loss: 58.97325897216797\n",
      "Epoch [17/200] Loss: 56.719703674316406\n",
      "Epoch [18/200] Loss: 54.61871337890625\n",
      "Epoch [19/200] Loss: 52.66014862060547\n",
      "Epoch [20/200] Loss: 50.83361053466797\n",
      "Epoch [21/200] Loss: 49.12882995605469\n",
      "Epoch [22/200] Loss: 47.53593826293945\n",
      "Epoch [23/200] Loss: 46.04560852050781\n",
      "Epoch [24/200] Loss: 44.64910888671875\n",
      "Epoch [25/200] Loss: 43.33833694458008\n",
      "Epoch [26/200] Loss: 42.105777740478516\n",
      "Epoch [27/200] Loss: 40.944541931152344\n",
      "Epoch [28/200] Loss: 39.8482666015625\n",
      "Epoch [29/200] Loss: 38.811222076416016\n",
      "Epoch [30/200] Loss: 37.828189849853516\n",
      "Epoch [31/200] Loss: 36.894493103027344\n",
      "Epoch [32/200] Loss: 36.005958557128906\n",
      "Epoch [33/200] Loss: 35.15889358520508\n",
      "Epoch [34/200] Loss: 34.350006103515625\n",
      "Epoch [35/200] Loss: 33.57640075683594\n",
      "Epoch [36/200] Loss: 32.835487365722656\n",
      "Epoch [37/200] Loss: 32.12498474121094\n",
      "Epoch [38/200] Loss: 31.442859649658203\n",
      "Epoch [39/200] Loss: 30.787311553955078\n",
      "Epoch [40/200] Loss: 30.156755447387695\n",
      "Epoch [41/200] Loss: 29.54976463317871\n",
      "Epoch [42/200] Loss: 28.965105056762695\n",
      "Epoch [43/200] Loss: 28.401649475097656\n",
      "Epoch [44/200] Loss: 27.858417510986328\n",
      "Epoch [45/200] Loss: 27.334508895874023\n",
      "Epoch [46/200] Loss: 26.829145431518555\n",
      "Epoch [47/200] Loss: 26.341581344604492\n",
      "Epoch [48/200] Loss: 25.87115478515625\n",
      "Epoch [49/200] Loss: 25.41724967956543\n",
      "Epoch [50/200] Loss: 24.979286193847656\n",
      "Epoch [51/200] Loss: 24.556730270385742\n",
      "Epoch [52/200] Loss: 24.149066925048828\n",
      "Epoch [53/200] Loss: 23.755817413330078\n",
      "Epoch [54/200] Loss: 23.376510620117188\n",
      "Epoch [55/200] Loss: 23.010705947875977\n",
      "Epoch [56/200] Loss: 22.657974243164062\n",
      "Epoch [57/200] Loss: 22.317907333374023\n",
      "Epoch [58/200] Loss: 21.990095138549805\n",
      "Epoch [59/200] Loss: 21.674156188964844\n",
      "Epoch [60/200] Loss: 21.36971664428711\n",
      "Epoch [61/200] Loss: 21.0764102935791\n",
      "Epoch [62/200] Loss: 20.793882369995117\n",
      "Epoch [63/200] Loss: 20.521799087524414\n",
      "Epoch [64/200] Loss: 20.259817123413086\n",
      "Epoch [65/200] Loss: 20.00762176513672\n",
      "Epoch [66/200] Loss: 19.764890670776367\n",
      "Epoch [67/200] Loss: 19.531333923339844\n",
      "Epoch [68/200] Loss: 19.306642532348633\n",
      "Epoch [69/200] Loss: 19.09054183959961\n",
      "Epoch [70/200] Loss: 18.88274383544922\n",
      "Epoch [71/200] Loss: 18.682981491088867\n",
      "Epoch [72/200] Loss: 18.490999221801758\n",
      "Epoch [73/200] Loss: 18.306533813476562\n",
      "Epoch [74/200] Loss: 18.129343032836914\n",
      "Epoch [75/200] Loss: 17.959186553955078\n",
      "Epoch [76/200] Loss: 17.795825958251953\n",
      "Epoch [77/200] Loss: 17.639036178588867\n",
      "Epoch [78/200] Loss: 17.48859405517578\n",
      "Epoch [79/200] Loss: 17.34429359436035\n",
      "Epoch [80/200] Loss: 17.205915451049805\n",
      "Epoch [81/200] Loss: 17.073261260986328\n",
      "Epoch [82/200] Loss: 16.94613265991211\n",
      "Epoch [83/200] Loss: 16.8243408203125\n",
      "Epoch [84/200] Loss: 16.70769500732422\n",
      "Epoch [85/200] Loss: 16.596012115478516\n",
      "Epoch [86/200] Loss: 16.489124298095703\n",
      "Epoch [87/200] Loss: 16.386852264404297\n",
      "Epoch [88/200] Loss: 16.289033889770508\n",
      "Epoch [89/200] Loss: 16.195497512817383\n",
      "Epoch [90/200] Loss: 16.106101989746094\n",
      "Epoch [91/200] Loss: 16.020679473876953\n",
      "Epoch [92/200] Loss: 15.93908977508545\n",
      "Epoch [93/200] Loss: 15.861189842224121\n",
      "Epoch [94/200] Loss: 15.786834716796875\n",
      "Epoch [95/200] Loss: 15.715890884399414\n",
      "Epoch [96/200] Loss: 15.648225784301758\n",
      "Epoch [97/200] Loss: 15.58371353149414\n",
      "Epoch [98/200] Loss: 15.522228240966797\n",
      "Epoch [99/200] Loss: 15.463650703430176\n",
      "Epoch [100/200] Loss: 15.407865524291992\n",
      "Epoch [101/200] Loss: 15.354758262634277\n",
      "Epoch [102/200] Loss: 15.304220199584961\n",
      "Epoch [103/200] Loss: 15.256149291992188\n",
      "Epoch [104/200] Loss: 15.21043586730957\n",
      "Epoch [105/200] Loss: 15.16698932647705\n",
      "Epoch [106/200] Loss: 15.12570858001709\n",
      "Epoch [107/200] Loss: 15.08650016784668\n",
      "Epoch [108/200] Loss: 15.04927921295166\n",
      "Epoch [109/200] Loss: 15.013959884643555\n",
      "Epoch [110/200] Loss: 14.980456352233887\n",
      "Epoch [111/200] Loss: 14.948686599731445\n",
      "Epoch [112/200] Loss: 14.91857624053955\n",
      "Epoch [113/200] Loss: 14.890050888061523\n",
      "Epoch [114/200] Loss: 14.863038063049316\n",
      "Epoch [115/200] Loss: 14.837467193603516\n",
      "Epoch [116/200] Loss: 14.813271522521973\n",
      "Epoch [117/200] Loss: 14.790388107299805\n",
      "Epoch [118/200] Loss: 14.768754959106445\n",
      "Epoch [119/200] Loss: 14.748312950134277\n",
      "Epoch [120/200] Loss: 14.729002952575684\n",
      "Epoch [121/200] Loss: 14.710773468017578\n",
      "Epoch [122/200] Loss: 14.69356918334961\n",
      "Epoch [123/200] Loss: 14.677340507507324\n",
      "Epoch [124/200] Loss: 14.662040710449219\n",
      "Epoch [125/200] Loss: 14.64762020111084\n",
      "Epoch [126/200] Loss: 14.634035110473633\n",
      "Epoch [127/200] Loss: 14.621244430541992\n",
      "Epoch [128/200] Loss: 14.609207153320312\n",
      "Epoch [129/200] Loss: 14.597883224487305\n",
      "Epoch [130/200] Loss: 14.587234497070312\n",
      "Epoch [131/200] Loss: 14.577228546142578\n",
      "Epoch [132/200] Loss: 14.567825317382812\n",
      "Epoch [133/200] Loss: 14.559000015258789\n",
      "Epoch [134/200] Loss: 14.550716400146484\n",
      "Epoch [135/200] Loss: 14.542945861816406\n",
      "Epoch [136/200] Loss: 14.535660743713379\n",
      "Epoch [137/200] Loss: 14.528833389282227\n",
      "Epoch [138/200] Loss: 14.522436141967773\n",
      "Epoch [139/200] Loss: 14.516449928283691\n",
      "Epoch [140/200] Loss: 14.510848999023438\n",
      "Epoch [141/200] Loss: 14.505609512329102\n",
      "Epoch [142/200] Loss: 14.500711441040039\n",
      "Epoch [143/200] Loss: 14.496133804321289\n",
      "Epoch [144/200] Loss: 14.491859436035156\n",
      "Epoch [145/200] Loss: 14.487871170043945\n",
      "Epoch [146/200] Loss: 14.484150886535645\n",
      "Epoch [147/200] Loss: 14.480682373046875\n",
      "Epoch [148/200] Loss: 14.477449417114258\n",
      "Epoch [149/200] Loss: 14.474437713623047\n",
      "Epoch [150/200] Loss: 14.471633911132812\n",
      "Epoch [151/200] Loss: 14.469026565551758\n",
      "Epoch [152/200] Loss: 14.46660041809082\n",
      "Epoch [153/200] Loss: 14.464346885681152\n",
      "Epoch [154/200] Loss: 14.462252616882324\n",
      "Epoch [155/200] Loss: 14.460309982299805\n",
      "Epoch [156/200] Loss: 14.458505630493164\n",
      "Epoch [157/200] Loss: 14.456833839416504\n",
      "Epoch [158/200] Loss: 14.455284118652344\n",
      "Epoch [159/200] Loss: 14.453847885131836\n",
      "Epoch [160/200] Loss: 14.452518463134766\n",
      "Epoch [161/200] Loss: 14.451289176940918\n",
      "Epoch [162/200] Loss: 14.450152397155762\n",
      "Epoch [163/200] Loss: 14.449102401733398\n",
      "Epoch [164/200] Loss: 14.448131561279297\n",
      "Epoch [165/200] Loss: 14.447237014770508\n",
      "Epoch [166/200] Loss: 14.4464111328125\n",
      "Epoch [167/200] Loss: 14.445649147033691\n",
      "Epoch [168/200] Loss: 14.444948196411133\n",
      "Epoch [169/200] Loss: 14.444302558898926\n",
      "Epoch [170/200] Loss: 14.443708419799805\n",
      "Epoch [171/200] Loss: 14.44316291809082\n",
      "Epoch [172/200] Loss: 14.442662239074707\n",
      "Epoch [173/200] Loss: 14.442201614379883\n",
      "Epoch [174/200] Loss: 14.441777229309082\n",
      "Epoch [175/200] Loss: 14.441389083862305\n",
      "Epoch [176/200] Loss: 14.441035270690918\n",
      "Epoch [177/200] Loss: 14.440710067749023\n",
      "Epoch [178/200] Loss: 14.440412521362305\n",
      "Epoch [179/200] Loss: 14.440140724182129\n",
      "Epoch [180/200] Loss: 14.439891815185547\n",
      "Epoch [181/200] Loss: 14.439663887023926\n",
      "Epoch [182/200] Loss: 14.439456939697266\n",
      "Epoch [183/200] Loss: 14.439269065856934\n",
      "Epoch [184/200] Loss: 14.439096450805664\n",
      "Epoch [185/200] Loss: 14.43894100189209\n",
      "Epoch [186/200] Loss: 14.438797950744629\n",
      "Epoch [187/200] Loss: 14.438669204711914\n",
      "Epoch [188/200] Loss: 14.438551902770996\n",
      "Epoch [189/200] Loss: 14.438446044921875\n",
      "Epoch [190/200] Loss: 14.438348770141602\n",
      "Epoch [191/200] Loss: 14.438261032104492\n",
      "Epoch [192/200] Loss: 14.438182830810547\n",
      "Epoch [193/200] Loss: 14.438109397888184\n",
      "Epoch [194/200] Loss: 14.438045501708984\n",
      "Epoch [195/200] Loss: 14.437986373901367\n",
      "Epoch [196/200] Loss: 14.437934875488281\n",
      "Epoch [197/200] Loss: 14.437886238098145\n",
      "Epoch [198/200] Loss: 14.437844276428223\n",
      "Epoch [199/200] Loss: 14.43780517578125\n",
      "Epoch [200/200] Loss: 14.43777084350586\n",
      "Predicted days_remaining for parent_id 256: 9.734482765197754\n",
      "Training for parent_id 258...\n",
      "Epoch [1/200] Loss: 2288.88330078125\n",
      "Epoch [2/200] Loss: 2264.084228515625\n",
      "Epoch [3/200] Loss: 2239.973388671875\n",
      "Epoch [4/200] Loss: 2216.9033203125\n",
      "Epoch [5/200] Loss: 2195.058837890625\n",
      "Epoch [6/200] Loss: 2174.496337890625\n",
      "Epoch [7/200] Loss: 2155.157470703125\n",
      "Epoch [8/200] Loss: 2136.902099609375\n",
      "Epoch [9/200] Loss: 2119.561767578125\n",
      "Epoch [10/200] Loss: 2102.989013671875\n",
      "Epoch [11/200] Loss: 2087.08154296875\n",
      "Epoch [12/200] Loss: 2071.77783203125\n",
      "Epoch [13/200] Loss: 2057.048583984375\n",
      "Epoch [14/200] Loss: 2042.87744140625\n",
      "Epoch [15/200] Loss: 2029.251708984375\n",
      "Epoch [16/200] Loss: 2016.1561279296875\n",
      "Epoch [17/200] Loss: 2003.571533203125\n",
      "Epoch [18/200] Loss: 1991.4730224609375\n",
      "Epoch [19/200] Loss: 1979.83349609375\n",
      "Epoch [20/200] Loss: 1968.6221923828125\n",
      "Epoch [21/200] Loss: 1957.80859375\n",
      "Epoch [22/200] Loss: 1947.3619384765625\n",
      "Epoch [23/200] Loss: 1937.253173828125\n",
      "Epoch [24/200] Loss: 1927.455078125\n",
      "Epoch [25/200] Loss: 1917.944091796875\n",
      "Epoch [26/200] Loss: 1908.6993408203125\n",
      "Epoch [27/200] Loss: 1899.7044677734375\n",
      "Epoch [28/200] Loss: 1890.94482421875\n",
      "Epoch [29/200] Loss: 1882.4083251953125\n",
      "Epoch [30/200] Loss: 1874.083740234375\n",
      "Epoch [31/200] Loss: 1865.96044921875\n",
      "Epoch [32/200] Loss: 1858.0284423828125\n",
      "Epoch [33/200] Loss: 1850.27587890625\n",
      "Epoch [34/200] Loss: 1842.6910400390625\n",
      "Epoch [35/200] Loss: 1835.26025390625\n",
      "Epoch [36/200] Loss: 1827.970947265625\n",
      "Epoch [37/200] Loss: 1820.8095703125\n",
      "Epoch [38/200] Loss: 1813.7623291015625\n",
      "Epoch [39/200] Loss: 1806.8179931640625\n",
      "Epoch [40/200] Loss: 1799.9647216796875\n",
      "Epoch [41/200] Loss: 1793.1939697265625\n",
      "Epoch [42/200] Loss: 1786.49658203125\n",
      "Epoch [43/200] Loss: 1779.8660888671875\n",
      "Epoch [44/200] Loss: 1773.296142578125\n",
      "Epoch [45/200] Loss: 1766.78076171875\n",
      "Epoch [46/200] Loss: 1760.3155517578125\n",
      "Epoch [47/200] Loss: 1753.89599609375\n",
      "Epoch [48/200] Loss: 1747.517333984375\n",
      "Epoch [49/200] Loss: 1741.176025390625\n",
      "Epoch [50/200] Loss: 1734.868408203125\n",
      "Epoch [51/200] Loss: 1728.5928955078125\n",
      "Epoch [52/200] Loss: 1722.3470458984375\n",
      "Epoch [53/200] Loss: 1716.1300048828125\n",
      "Epoch [54/200] Loss: 1709.940673828125\n",
      "Epoch [55/200] Loss: 1703.7786865234375\n",
      "Epoch [56/200] Loss: 1697.643798828125\n",
      "Epoch [57/200] Loss: 1691.53662109375\n",
      "Epoch [58/200] Loss: 1685.4560546875\n",
      "Epoch [59/200] Loss: 1679.40283203125\n",
      "Epoch [60/200] Loss: 1673.3765869140625\n",
      "Epoch [61/200] Loss: 1667.37744140625\n",
      "Epoch [62/200] Loss: 1661.4056396484375\n",
      "Epoch [63/200] Loss: 1655.4608154296875\n",
      "Epoch [64/200] Loss: 1649.54248046875\n",
      "Epoch [65/200] Loss: 1643.6510009765625\n",
      "Epoch [66/200] Loss: 1637.7862548828125\n",
      "Epoch [67/200] Loss: 1631.9478759765625\n",
      "Epoch [68/200] Loss: 1626.1357421875\n",
      "Epoch [69/200] Loss: 1620.349365234375\n",
      "Epoch [70/200] Loss: 1614.5888671875\n",
      "Epoch [71/200] Loss: 1608.85400390625\n",
      "Epoch [72/200] Loss: 1603.144287109375\n",
      "Epoch [73/200] Loss: 1597.45947265625\n",
      "Epoch [74/200] Loss: 1591.7996826171875\n",
      "Epoch [75/200] Loss: 1586.16455078125\n",
      "Epoch [76/200] Loss: 1580.5537109375\n",
      "Epoch [77/200] Loss: 1574.966552734375\n",
      "Epoch [78/200] Loss: 1569.4036865234375\n",
      "Epoch [79/200] Loss: 1563.8641357421875\n",
      "Epoch [80/200] Loss: 1558.347900390625\n",
      "Epoch [81/200] Loss: 1552.854736328125\n",
      "Epoch [82/200] Loss: 1547.3843994140625\n",
      "Epoch [83/200] Loss: 1541.9368896484375\n",
      "Epoch [84/200] Loss: 1536.5111083984375\n",
      "Epoch [85/200] Loss: 1531.1080322265625\n",
      "Epoch [86/200] Loss: 1525.726806640625\n",
      "Epoch [87/200] Loss: 1520.3668212890625\n",
      "Epoch [88/200] Loss: 1515.0286865234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/200] Loss: 1509.711181640625\n",
      "Epoch [90/200] Loss: 1504.4150390625\n",
      "Epoch [91/200] Loss: 1499.139404296875\n",
      "Epoch [92/200] Loss: 1493.884521484375\n",
      "Epoch [93/200] Loss: 1488.6500244140625\n",
      "Epoch [94/200] Loss: 1483.435302734375\n",
      "Epoch [95/200] Loss: 1478.2408447265625\n",
      "Epoch [96/200] Loss: 1473.06591796875\n",
      "Epoch [97/200] Loss: 1467.91064453125\n",
      "Epoch [98/200] Loss: 1462.7744140625\n",
      "Epoch [99/200] Loss: 1457.6573486328125\n",
      "Epoch [100/200] Loss: 1452.5595703125\n",
      "Epoch [101/200] Loss: 1447.4803466796875\n",
      "Epoch [102/200] Loss: 1442.4197998046875\n",
      "Epoch [103/200] Loss: 1437.3779296875\n",
      "Epoch [104/200] Loss: 1432.3541259765625\n",
      "Epoch [105/200] Loss: 1427.348388671875\n",
      "Epoch [106/200] Loss: 1422.36083984375\n",
      "Epoch [107/200] Loss: 1417.390869140625\n",
      "Epoch [108/200] Loss: 1412.4385986328125\n",
      "Epoch [109/200] Loss: 1407.504150390625\n",
      "Epoch [110/200] Loss: 1402.5869140625\n",
      "Epoch [111/200] Loss: 1397.68701171875\n",
      "Epoch [112/200] Loss: 1392.803955078125\n",
      "Epoch [113/200] Loss: 1387.93798828125\n",
      "Epoch [114/200] Loss: 1383.088623046875\n",
      "Epoch [115/200] Loss: 1378.256103515625\n",
      "Epoch [116/200] Loss: 1373.4404296875\n",
      "Epoch [117/200] Loss: 1368.64111328125\n",
      "Epoch [118/200] Loss: 1363.85791015625\n",
      "Epoch [119/200] Loss: 1359.09130859375\n",
      "Epoch [120/200] Loss: 1354.3406982421875\n",
      "Epoch [121/200] Loss: 1349.6058349609375\n",
      "Epoch [122/200] Loss: 1344.886962890625\n",
      "Epoch [123/200] Loss: 1340.1842041015625\n",
      "Epoch [124/200] Loss: 1335.4967041015625\n",
      "Epoch [125/200] Loss: 1330.8250732421875\n",
      "Epoch [126/200] Loss: 1326.168701171875\n",
      "Epoch [127/200] Loss: 1321.527587890625\n",
      "Epoch [128/200] Loss: 1316.9022216796875\n",
      "Epoch [129/200] Loss: 1312.291748046875\n",
      "Epoch [130/200] Loss: 1307.6966552734375\n",
      "Epoch [131/200] Loss: 1303.1162109375\n",
      "Epoch [132/200] Loss: 1298.55078125\n",
      "Epoch [133/200] Loss: 1294.000244140625\n",
      "Epoch [134/200] Loss: 1289.4644775390625\n",
      "Epoch [135/200] Loss: 1284.9434814453125\n",
      "Epoch [136/200] Loss: 1280.4371337890625\n",
      "Epoch [137/200] Loss: 1275.94482421875\n",
      "Epoch [138/200] Loss: 1271.4674072265625\n",
      "Epoch [139/200] Loss: 1267.0040283203125\n",
      "Epoch [140/200] Loss: 1262.55517578125\n",
      "Epoch [141/200] Loss: 1258.120361328125\n",
      "Epoch [142/200] Loss: 1253.6995849609375\n",
      "Epoch [143/200] Loss: 1249.293212890625\n",
      "Epoch [144/200] Loss: 1244.9005126953125\n",
      "Epoch [145/200] Loss: 1240.5218505859375\n",
      "Epoch [146/200] Loss: 1236.1568603515625\n",
      "Epoch [147/200] Loss: 1231.805908203125\n",
      "Epoch [148/200] Loss: 1227.4683837890625\n",
      "Epoch [149/200] Loss: 1223.144775390625\n",
      "Epoch [150/200] Loss: 1218.8345947265625\n",
      "Epoch [151/200] Loss: 1214.5380859375\n",
      "Epoch [152/200] Loss: 1210.2548828125\n",
      "Epoch [153/200] Loss: 1205.9852294921875\n",
      "Epoch [154/200] Loss: 1201.728759765625\n",
      "Epoch [155/200] Loss: 1197.485595703125\n",
      "Epoch [156/200] Loss: 1193.2557373046875\n",
      "Epoch [157/200] Loss: 1189.0390625\n",
      "Epoch [158/200] Loss: 1184.83544921875\n",
      "Epoch [159/200] Loss: 1180.6448974609375\n",
      "Epoch [160/200] Loss: 1176.467041015625\n",
      "Epoch [161/200] Loss: 1172.302490234375\n",
      "Epoch [162/200] Loss: 1168.150634765625\n",
      "Epoch [163/200] Loss: 1164.0118408203125\n",
      "Epoch [164/200] Loss: 1159.8857421875\n",
      "Epoch [165/200] Loss: 1155.772216796875\n",
      "Epoch [166/200] Loss: 1151.6715087890625\n",
      "Epoch [167/200] Loss: 1147.583251953125\n",
      "Epoch [168/200] Loss: 1143.5079345703125\n",
      "Epoch [169/200] Loss: 1139.4449462890625\n",
      "Epoch [170/200] Loss: 1135.3944091796875\n",
      "Epoch [171/200] Loss: 1131.356201171875\n",
      "Epoch [172/200] Loss: 1127.33056640625\n",
      "Epoch [173/200] Loss: 1123.3173828125\n",
      "Epoch [174/200] Loss: 1119.316162109375\n",
      "Epoch [175/200] Loss: 1115.3272705078125\n",
      "Epoch [176/200] Loss: 1111.350830078125\n",
      "Epoch [177/200] Loss: 1107.38623046875\n",
      "Epoch [178/200] Loss: 1103.4339599609375\n",
      "Epoch [179/200] Loss: 1099.4937744140625\n",
      "Epoch [180/200] Loss: 1095.565673828125\n",
      "Epoch [181/200] Loss: 1091.6494140625\n",
      "Epoch [182/200] Loss: 1087.7452392578125\n",
      "Epoch [183/200] Loss: 1083.852783203125\n",
      "Epoch [184/200] Loss: 1079.97216796875\n",
      "Epoch [185/200] Loss: 1076.1036376953125\n",
      "Epoch [186/200] Loss: 1072.2467041015625\n",
      "Epoch [187/200] Loss: 1068.401611328125\n",
      "Epoch [188/200] Loss: 1064.568115234375\n",
      "Epoch [189/200] Loss: 1060.74658203125\n",
      "Epoch [190/200] Loss: 1056.936279296875\n",
      "Epoch [191/200] Loss: 1053.1375732421875\n",
      "Epoch [192/200] Loss: 1049.3507080078125\n",
      "Epoch [193/200] Loss: 1045.5751953125\n",
      "Epoch [194/200] Loss: 1041.81103515625\n",
      "Epoch [195/200] Loss: 1038.05859375\n",
      "Epoch [196/200] Loss: 1034.3175048828125\n",
      "Epoch [197/200] Loss: 1030.5875244140625\n",
      "Epoch [198/200] Loss: 1026.8692626953125\n",
      "Epoch [199/200] Loss: 1023.1619262695312\n",
      "Epoch [200/200] Loss: 1019.4661254882812\n",
      "Predicted days_remaining for parent_id 258: 16.105979919433594\n",
      "Training for parent_id 265...\n",
      "Epoch [1/200] Loss: 105.76319122314453\n",
      "Epoch [2/200] Loss: 100.96324920654297\n",
      "Epoch [3/200] Loss: 96.37620544433594\n",
      "Epoch [4/200] Loss: 92.01790618896484\n",
      "Epoch [5/200] Loss: 87.9021224975586\n",
      "Epoch [6/200] Loss: 84.0325698852539\n",
      "Epoch [7/200] Loss: 80.40245819091797\n",
      "Epoch [8/200] Loss: 76.99794006347656\n",
      "Epoch [9/200] Loss: 73.80252838134766\n",
      "Epoch [10/200] Loss: 70.8003921508789\n",
      "Epoch [11/200] Loss: 67.97766876220703\n",
      "Epoch [12/200] Loss: 65.3227767944336\n",
      "Epoch [13/200] Loss: 62.8260612487793\n",
      "Epoch [14/200] Loss: 60.479095458984375\n",
      "Epoch [15/200] Loss: 58.274169921875\n",
      "Epoch [16/200] Loss: 56.203704833984375\n",
      "Epoch [17/200] Loss: 54.26005935668945\n",
      "Epoch [18/200] Loss: 52.43545913696289\n",
      "Epoch [19/200] Loss: 50.72216033935547\n",
      "Epoch [20/200] Loss: 49.11261749267578\n",
      "Epoch [21/200] Loss: 47.59967041015625\n",
      "Epoch [22/200] Loss: 46.17662811279297\n",
      "Epoch [23/200] Loss: 44.837379455566406\n",
      "Epoch [24/200] Loss: 43.57630157470703\n",
      "Epoch [25/200] Loss: 42.38820266723633\n",
      "Epoch [26/200] Loss: 41.268165588378906\n",
      "Epoch [27/200] Loss: 40.21154022216797\n",
      "Epoch [28/200] Loss: 39.21379089355469\n",
      "Epoch [29/200] Loss: 38.27049255371094\n",
      "Epoch [30/200] Loss: 37.37738800048828\n",
      "Epoch [31/200] Loss: 36.53034210205078\n",
      "Epoch [32/200] Loss: 35.72547912597656\n",
      "Epoch [33/200] Loss: 34.95914840698242\n",
      "Epoch [34/200] Loss: 34.22800827026367\n",
      "Epoch [35/200] Loss: 33.52903366088867\n",
      "Epoch [36/200] Loss: 32.85951614379883\n",
      "Epoch [37/200] Loss: 32.21701431274414\n",
      "Epoch [38/200] Loss: 31.599411010742188\n",
      "Epoch [39/200] Loss: 31.00481414794922\n",
      "Epoch [40/200] Loss: 30.431570053100586\n",
      "Epoch [41/200] Loss: 29.878250122070312\n",
      "Epoch [42/200] Loss: 29.343555450439453\n",
      "Epoch [43/200] Loss: 28.826385498046875\n",
      "Epoch [44/200] Loss: 28.325763702392578\n",
      "Epoch [45/200] Loss: 27.840822219848633\n",
      "Epoch [46/200] Loss: 27.37082862854004\n",
      "Epoch [47/200] Loss: 26.915096282958984\n",
      "Epoch [48/200] Loss: 26.47305679321289\n",
      "Epoch [49/200] Loss: 26.044164657592773\n",
      "Epoch [50/200] Loss: 25.62797737121582\n",
      "Epoch [51/200] Loss: 25.224048614501953\n",
      "Epoch [52/200] Loss: 24.832006454467773\n",
      "Epoch [53/200] Loss: 24.451494216918945\n",
      "Epoch [54/200] Loss: 24.08218002319336\n",
      "Epoch [55/200] Loss: 23.723752975463867\n",
      "Epoch [56/200] Loss: 23.375926971435547\n",
      "Epoch [57/200] Loss: 23.038429260253906\n",
      "Epoch [58/200] Loss: 22.71100616455078\n",
      "Epoch [59/200] Loss: 22.393402099609375\n",
      "Epoch [60/200] Loss: 22.08540153503418\n",
      "Epoch [61/200] Loss: 21.786766052246094\n",
      "Epoch [62/200] Loss: 21.49729347229004\n",
      "Epoch [63/200] Loss: 21.216785430908203\n",
      "Epoch [64/200] Loss: 20.94503402709961\n",
      "Epoch [65/200] Loss: 20.681854248046875\n",
      "Epoch [66/200] Loss: 20.427066802978516\n",
      "Epoch [67/200] Loss: 20.18048667907715\n",
      "Epoch [68/200] Loss: 19.941936492919922\n",
      "Epoch [69/200] Loss: 19.71125030517578\n",
      "Epoch [70/200] Loss: 19.488258361816406\n",
      "Epoch [71/200] Loss: 19.272785186767578\n",
      "Epoch [72/200] Loss: 19.064678192138672\n",
      "Epoch [73/200] Loss: 18.863767623901367\n",
      "Epoch [74/200] Loss: 18.669889450073242\n",
      "Epoch [75/200] Loss: 18.482887268066406\n",
      "Epoch [76/200] Loss: 18.302597045898438\n",
      "Epoch [77/200] Loss: 18.128868103027344\n",
      "Epoch [78/200] Loss: 17.961528778076172\n",
      "Epoch [79/200] Loss: 17.800432205200195\n",
      "Epoch [80/200] Loss: 17.645408630371094\n",
      "Epoch [81/200] Loss: 17.496313095092773\n",
      "Epoch [82/200] Loss: 17.35297966003418\n",
      "Epoch [83/200] Loss: 17.215255737304688\n",
      "Epoch [84/200] Loss: 17.082984924316406\n",
      "Epoch [85/200] Loss: 16.956008911132812\n",
      "Epoch [86/200] Loss: 16.83417510986328\n",
      "Epoch [87/200] Loss: 16.717327117919922\n",
      "Epoch [88/200] Loss: 16.605314254760742\n",
      "Epoch [89/200] Loss: 16.497987747192383\n",
      "Epoch [90/200] Loss: 16.395198822021484\n",
      "Epoch [91/200] Loss: 16.296794891357422\n",
      "Epoch [92/200] Loss: 16.20263671875\n",
      "Epoch [93/200] Loss: 16.11258316040039\n",
      "Epoch [94/200] Loss: 16.02648162841797\n",
      "Epoch [95/200] Loss: 15.944211959838867\n",
      "Epoch [96/200] Loss: 15.865623474121094\n",
      "Epoch [97/200] Loss: 15.790594100952148\n",
      "Epoch [98/200] Loss: 15.718992233276367\n",
      "Epoch [99/200] Loss: 15.650684356689453\n",
      "Epoch [100/200] Loss: 15.585554122924805\n",
      "Epoch [101/200] Loss: 15.523479461669922\n",
      "Epoch [102/200] Loss: 15.46434211730957\n",
      "Epoch [103/200] Loss: 15.408027648925781\n",
      "Epoch [104/200] Loss: 15.354425430297852\n",
      "Epoch [105/200] Loss: 15.303426742553711\n",
      "Epoch [106/200] Loss: 15.254923820495605\n",
      "Epoch [107/200] Loss: 15.20881462097168\n",
      "Epoch [108/200] Loss: 15.165006637573242\n",
      "Epoch [109/200] Loss: 15.123395919799805\n",
      "Epoch [110/200] Loss: 15.083892822265625\n",
      "Epoch [111/200] Loss: 15.046409606933594\n",
      "Epoch [112/200] Loss: 15.01085090637207\n",
      "Epoch [113/200] Loss: 14.97713851928711\n",
      "Epoch [114/200] Loss: 14.9451904296875\n",
      "Epoch [115/200] Loss: 14.91493034362793\n",
      "Epoch [116/200] Loss: 14.886276245117188\n",
      "Epoch [117/200] Loss: 14.85915756225586\n",
      "Epoch [118/200] Loss: 14.833503723144531\n",
      "Epoch [119/200] Loss: 14.809244155883789\n",
      "Epoch [120/200] Loss: 14.7863187789917\n",
      "Epoch [121/200] Loss: 14.764659881591797\n",
      "Epoch [122/200] Loss: 14.744206428527832\n",
      "Epoch [123/200] Loss: 14.724903106689453\n",
      "Epoch [124/200] Loss: 14.70669174194336\n",
      "Epoch [125/200] Loss: 14.689517974853516\n",
      "Epoch [126/200] Loss: 14.673334121704102\n",
      "Epoch [127/200] Loss: 14.658084869384766\n",
      "Epoch [128/200] Loss: 14.643726348876953\n",
      "Epoch [129/200] Loss: 14.630212783813477\n",
      "Epoch [130/200] Loss: 14.617497444152832\n",
      "Epoch [131/200] Loss: 14.60554313659668\n",
      "Epoch [132/200] Loss: 14.594306945800781\n",
      "Epoch [133/200] Loss: 14.583751678466797\n",
      "Epoch [134/200] Loss: 14.57383918762207\n",
      "Epoch [135/200] Loss: 14.564538955688477\n",
      "Epoch [136/200] Loss: 14.555814743041992\n",
      "Epoch [137/200] Loss: 14.54763412475586\n",
      "Epoch [138/200] Loss: 14.539966583251953\n",
      "Epoch [139/200] Loss: 14.532787322998047\n",
      "Epoch [140/200] Loss: 14.526063919067383\n",
      "Epoch [141/200] Loss: 14.519775390625\n",
      "Epoch [142/200] Loss: 14.513891220092773\n",
      "Epoch [143/200] Loss: 14.508393287658691\n",
      "Epoch [144/200] Loss: 14.503255844116211\n",
      "Epoch [145/200] Loss: 14.498457908630371\n",
      "Epoch [146/200] Loss: 14.493980407714844\n",
      "Epoch [147/200] Loss: 14.489803314208984\n",
      "Epoch [148/200] Loss: 14.485909461975098\n",
      "Epoch [149/200] Loss: 14.482280731201172\n",
      "Epoch [150/200] Loss: 14.478900909423828\n",
      "Epoch [151/200] Loss: 14.47575569152832\n",
      "Epoch [152/200] Loss: 14.472829818725586\n",
      "Epoch [153/200] Loss: 14.470108032226562\n",
      "Epoch [154/200] Loss: 14.46757984161377\n",
      "Epoch [155/200] Loss: 14.465229988098145\n",
      "Epoch [156/200] Loss: 14.46304988861084\n",
      "Epoch [157/200] Loss: 14.461027145385742\n",
      "Epoch [158/200] Loss: 14.459152221679688\n",
      "Epoch [159/200] Loss: 14.457414627075195\n",
      "Epoch [160/200] Loss: 14.455804824829102\n",
      "Epoch [161/200] Loss: 14.454314231872559\n",
      "Epoch [162/200] Loss: 14.452936172485352\n",
      "Epoch [163/200] Loss: 14.45166301727295\n",
      "Epoch [164/200] Loss: 14.450485229492188\n",
      "Epoch [165/200] Loss: 14.449398040771484\n",
      "Epoch [166/200] Loss: 14.448394775390625\n",
      "Epoch [167/200] Loss: 14.447469711303711\n",
      "Epoch [168/200] Loss: 14.446617126464844\n",
      "Epoch [169/200] Loss: 14.445831298828125\n",
      "Epoch [170/200] Loss: 14.445108413696289\n",
      "Epoch [171/200] Loss: 14.444442749023438\n",
      "Epoch [172/200] Loss: 14.443831443786621\n",
      "Epoch [173/200] Loss: 14.443268775939941\n",
      "Epoch [174/200] Loss: 14.442754745483398\n",
      "Epoch [175/200] Loss: 14.442281723022461\n",
      "Epoch [176/200] Loss: 14.441848754882812\n",
      "Epoch [177/200] Loss: 14.441450119018555\n",
      "Epoch [178/200] Loss: 14.441085815429688\n",
      "Epoch [179/200] Loss: 14.440752029418945\n",
      "Epoch [180/200] Loss: 14.440448760986328\n",
      "Epoch [181/200] Loss: 14.440170288085938\n",
      "Epoch [182/200] Loss: 14.439916610717773\n",
      "Epoch [183/200] Loss: 14.439685821533203\n",
      "Epoch [184/200] Loss: 14.439474105834961\n",
      "Epoch [185/200] Loss: 14.439281463623047\n",
      "Epoch [186/200] Loss: 14.439106941223145\n",
      "Epoch [187/200] Loss: 14.438947677612305\n",
      "Epoch [188/200] Loss: 14.438804626464844\n",
      "Epoch [189/200] Loss: 14.43867301940918\n",
      "Epoch [190/200] Loss: 14.438554763793945\n",
      "Epoch [191/200] Loss: 14.438444137573242\n",
      "Epoch [192/200] Loss: 14.438347816467285\n",
      "Epoch [193/200] Loss: 14.438260078430176\n",
      "Epoch [194/200] Loss: 14.438179016113281\n",
      "Epoch [195/200] Loss: 14.43810749053955\n",
      "Epoch [196/200] Loss: 14.438041687011719\n",
      "Epoch [197/200] Loss: 14.437983512878418\n",
      "Epoch [198/200] Loss: 14.4379301071167\n",
      "Epoch [199/200] Loss: 14.437882423400879\n",
      "Epoch [200/200] Loss: 14.437841415405273\n",
      "Predicted days_remaining for parent_id 265: 9.732614517211914\n",
      "Training for parent_id 266...\n",
      "Epoch [1/200] Loss: 5660.13037109375\n",
      "Epoch [2/200] Loss: 5619.1767578125\n",
      "Epoch [3/200] Loss: 5579.6435546875\n",
      "Epoch [4/200] Loss: 5541.74951171875\n",
      "Epoch [5/200] Loss: 5505.4404296875\n",
      "Epoch [6/200] Loss: 5470.6015625\n",
      "Epoch [7/200] Loss: 5437.10986328125\n",
      "Epoch [8/200] Loss: 5404.8349609375\n",
      "Epoch [9/200] Loss: 5373.6767578125\n",
      "Epoch [10/200] Loss: 5343.5869140625\n",
      "Epoch [11/200] Loss: 5314.5615234375\n",
      "Epoch [12/200] Loss: 5286.63427734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/200] Loss: 5259.837890625\n",
      "Epoch [14/200] Loss: 5234.20068359375\n",
      "Epoch [15/200] Loss: 5209.73876953125\n",
      "Epoch [16/200] Loss: 5186.4462890625\n",
      "Epoch [17/200] Loss: 5164.3046875\n",
      "Epoch [18/200] Loss: 5143.27978515625\n",
      "Epoch [19/200] Loss: 5123.32666015625\n",
      "Epoch [20/200] Loss: 5104.39013671875\n",
      "Epoch [21/200] Loss: 5086.40283203125\n",
      "Epoch [22/200] Loss: 5069.294921875\n",
      "Epoch [23/200] Loss: 5052.9912109375\n",
      "Epoch [24/200] Loss: 5037.41748046875\n",
      "Epoch [25/200] Loss: 5022.49853515625\n",
      "Epoch [26/200] Loss: 5008.1611328125\n",
      "Epoch [27/200] Loss: 4994.33837890625\n",
      "Epoch [28/200] Loss: 4980.96826171875\n",
      "Epoch [29/200] Loss: 4967.99267578125\n",
      "Epoch [30/200] Loss: 4955.36328125\n",
      "Epoch [31/200] Loss: 4943.033203125\n",
      "Epoch [32/200] Loss: 4930.96240234375\n",
      "Epoch [33/200] Loss: 4919.1181640625\n",
      "Epoch [34/200] Loss: 4907.470703125\n",
      "Epoch [35/200] Loss: 4895.99365234375\n",
      "Epoch [36/200] Loss: 4884.66748046875\n",
      "Epoch [37/200] Loss: 4873.4736328125\n",
      "Epoch [38/200] Loss: 4862.39697265625\n",
      "Epoch [39/200] Loss: 4851.4248046875\n",
      "Epoch [40/200] Loss: 4840.54833984375\n",
      "Epoch [41/200] Loss: 4829.7578125\n",
      "Epoch [42/200] Loss: 4819.0478515625\n",
      "Epoch [43/200] Loss: 4808.40966796875\n",
      "Epoch [44/200] Loss: 4797.84228515625\n",
      "Epoch [45/200] Loss: 4787.33935546875\n",
      "Epoch [46/200] Loss: 4776.89697265625\n",
      "Epoch [47/200] Loss: 4766.51220703125\n",
      "Epoch [48/200] Loss: 4756.18359375\n",
      "Epoch [49/200] Loss: 4745.9072265625\n",
      "Epoch [50/200] Loss: 4735.681640625\n",
      "Epoch [51/200] Loss: 4725.50634765625\n",
      "Epoch [52/200] Loss: 4715.375\n",
      "Epoch [53/200] Loss: 4705.2900390625\n",
      "Epoch [54/200] Loss: 4695.24951171875\n",
      "Epoch [55/200] Loss: 4685.25\n",
      "Epoch [56/200] Loss: 4675.29150390625\n",
      "Epoch [57/200] Loss: 4665.37353515625\n",
      "Epoch [58/200] Loss: 4655.49365234375\n",
      "Epoch [59/200] Loss: 4645.6513671875\n",
      "Epoch [60/200] Loss: 4635.84619140625\n",
      "Epoch [61/200] Loss: 4626.07568359375\n",
      "Epoch [62/200] Loss: 4616.34130859375\n",
      "Epoch [63/200] Loss: 4606.64013671875\n",
      "Epoch [64/200] Loss: 4596.97314453125\n",
      "Epoch [65/200] Loss: 4587.3388671875\n",
      "Epoch [66/200] Loss: 4577.736328125\n",
      "Epoch [67/200] Loss: 4568.16552734375\n",
      "Epoch [68/200] Loss: 4558.62548828125\n",
      "Epoch [69/200] Loss: 4549.11572265625\n",
      "Epoch [70/200] Loss: 4539.63525390625\n",
      "Epoch [71/200] Loss: 4530.18505859375\n",
      "Epoch [72/200] Loss: 4520.7626953125\n",
      "Epoch [73/200] Loss: 4511.369140625\n",
      "Epoch [74/200] Loss: 4502.00341796875\n",
      "Epoch [75/200] Loss: 4492.6640625\n",
      "Epoch [76/200] Loss: 4483.3525390625\n",
      "Epoch [77/200] Loss: 4474.06689453125\n",
      "Epoch [78/200] Loss: 4464.80810546875\n",
      "Epoch [79/200] Loss: 4455.57421875\n",
      "Epoch [80/200] Loss: 4446.36572265625\n",
      "Epoch [81/200] Loss: 4437.1826171875\n",
      "Epoch [82/200] Loss: 4428.0244140625\n",
      "Epoch [83/200] Loss: 4418.890625\n",
      "Epoch [84/200] Loss: 4409.77978515625\n",
      "Epoch [85/200] Loss: 4400.6943359375\n",
      "Epoch [86/200] Loss: 4391.63134765625\n",
      "Epoch [87/200] Loss: 4382.59130859375\n",
      "Epoch [88/200] Loss: 4373.57421875\n",
      "Epoch [89/200] Loss: 4364.5791015625\n",
      "Epoch [90/200] Loss: 4355.60791015625\n",
      "Epoch [91/200] Loss: 4346.65771484375\n",
      "Epoch [92/200] Loss: 4337.72900390625\n",
      "Epoch [93/200] Loss: 4328.822265625\n",
      "Epoch [94/200] Loss: 4319.9365234375\n",
      "Epoch [95/200] Loss: 4311.0732421875\n",
      "Epoch [96/200] Loss: 4302.22998046875\n",
      "Epoch [97/200] Loss: 4293.4072265625\n",
      "Epoch [98/200] Loss: 4284.60498046875\n",
      "Epoch [99/200] Loss: 4275.82421875\n",
      "Epoch [100/200] Loss: 4267.06201171875\n",
      "Epoch [101/200] Loss: 4258.3212890625\n",
      "Epoch [102/200] Loss: 4249.5986328125\n",
      "Epoch [103/200] Loss: 4240.89697265625\n",
      "Epoch [104/200] Loss: 4232.2138671875\n",
      "Epoch [105/200] Loss: 4223.54931640625\n",
      "Epoch [106/200] Loss: 4214.90576171875\n",
      "Epoch [107/200] Loss: 4206.27978515625\n",
      "Epoch [108/200] Loss: 4197.67333984375\n",
      "Epoch [109/200] Loss: 4189.0849609375\n",
      "Epoch [110/200] Loss: 4180.515625\n",
      "Epoch [111/200] Loss: 4171.9638671875\n",
      "Epoch [112/200] Loss: 4163.43115234375\n",
      "Epoch [113/200] Loss: 4154.916015625\n",
      "Epoch [114/200] Loss: 4146.4189453125\n",
      "Epoch [115/200] Loss: 4137.9404296875\n",
      "Epoch [116/200] Loss: 4129.478515625\n",
      "Epoch [117/200] Loss: 4121.0341796875\n",
      "Epoch [118/200] Loss: 4112.60791015625\n",
      "Epoch [119/200] Loss: 4104.19873046875\n",
      "Epoch [120/200] Loss: 4095.806884765625\n",
      "Epoch [121/200] Loss: 4087.4326171875\n",
      "Epoch [122/200] Loss: 4079.074951171875\n",
      "Epoch [123/200] Loss: 4070.733642578125\n",
      "Epoch [124/200] Loss: 4062.409912109375\n",
      "Epoch [125/200] Loss: 4054.1025390625\n",
      "Epoch [126/200] Loss: 4045.81201171875\n",
      "Epoch [127/200] Loss: 4037.537841796875\n",
      "Epoch [128/200] Loss: 4029.280029296875\n",
      "Epoch [129/200] Loss: 4021.0390625\n",
      "Epoch [130/200] Loss: 4012.8134765625\n",
      "Epoch [131/200] Loss: 4004.604736328125\n",
      "Epoch [132/200] Loss: 3996.412109375\n",
      "Epoch [133/200] Loss: 3988.23486328125\n",
      "Epoch [134/200] Loss: 3980.074462890625\n",
      "Epoch [135/200] Loss: 3971.92919921875\n",
      "Epoch [136/200] Loss: 3963.79931640625\n",
      "Epoch [137/200] Loss: 3955.686279296875\n",
      "Epoch [138/200] Loss: 3947.587890625\n",
      "Epoch [139/200] Loss: 3939.5048828125\n",
      "Epoch [140/200] Loss: 3931.43798828125\n",
      "Epoch [141/200] Loss: 3923.38720703125\n",
      "Epoch [142/200] Loss: 3915.35009765625\n",
      "Epoch [143/200] Loss: 3907.329345703125\n",
      "Epoch [144/200] Loss: 3899.322998046875\n",
      "Epoch [145/200] Loss: 3891.33251953125\n",
      "Epoch [146/200] Loss: 3883.356201171875\n",
      "Epoch [147/200] Loss: 3875.3955078125\n",
      "Epoch [148/200] Loss: 3867.44921875\n",
      "Epoch [149/200] Loss: 3859.518310546875\n",
      "Epoch [150/200] Loss: 3851.602294921875\n",
      "Epoch [151/200] Loss: 3843.70068359375\n",
      "Epoch [152/200] Loss: 3835.81396484375\n",
      "Epoch [153/200] Loss: 3827.94140625\n",
      "Epoch [154/200] Loss: 3820.083251953125\n",
      "Epoch [155/200] Loss: 3812.240478515625\n",
      "Epoch [156/200] Loss: 3804.412109375\n",
      "Epoch [157/200] Loss: 3796.59765625\n",
      "Epoch [158/200] Loss: 3788.79736328125\n",
      "Epoch [159/200] Loss: 3781.011474609375\n",
      "Epoch [160/200] Loss: 3773.240234375\n",
      "Epoch [161/200] Loss: 3765.483154296875\n",
      "Epoch [162/200] Loss: 3757.739990234375\n",
      "Epoch [163/200] Loss: 3750.0107421875\n",
      "Epoch [164/200] Loss: 3742.295654296875\n",
      "Epoch [165/200] Loss: 3734.594482421875\n",
      "Epoch [166/200] Loss: 3726.9072265625\n",
      "Epoch [167/200] Loss: 3719.234130859375\n",
      "Epoch [168/200] Loss: 3711.5751953125\n",
      "Epoch [169/200] Loss: 3703.9296875\n",
      "Epoch [170/200] Loss: 3696.29736328125\n",
      "Epoch [171/200] Loss: 3688.679443359375\n",
      "Epoch [172/200] Loss: 3681.0751953125\n",
      "Epoch [173/200] Loss: 3673.484375\n",
      "Epoch [174/200] Loss: 3665.906982421875\n",
      "Epoch [175/200] Loss: 3658.34375\n",
      "Epoch [176/200] Loss: 3650.7939453125\n",
      "Epoch [177/200] Loss: 3643.25732421875\n",
      "Epoch [178/200] Loss: 3635.73388671875\n",
      "Epoch [179/200] Loss: 3628.223876953125\n",
      "Epoch [180/200] Loss: 3620.72705078125\n",
      "Epoch [181/200] Loss: 3613.244140625\n",
      "Epoch [182/200] Loss: 3605.7744140625\n",
      "Epoch [183/200] Loss: 3598.31787109375\n",
      "Epoch [184/200] Loss: 3590.874267578125\n",
      "Epoch [185/200] Loss: 3583.444091796875\n",
      "Epoch [186/200] Loss: 3576.02685546875\n",
      "Epoch [187/200] Loss: 3568.622802734375\n",
      "Epoch [188/200] Loss: 3561.23193359375\n",
      "Epoch [189/200] Loss: 3553.85400390625\n",
      "Epoch [190/200] Loss: 3546.488525390625\n",
      "Epoch [191/200] Loss: 3539.13671875\n",
      "Epoch [192/200] Loss: 3531.798095703125\n",
      "Epoch [193/200] Loss: 3524.47119140625\n",
      "Epoch [194/200] Loss: 3517.15771484375\n",
      "Epoch [195/200] Loss: 3509.857421875\n",
      "Epoch [196/200] Loss: 3502.5693359375\n",
      "Epoch [197/200] Loss: 3495.29443359375\n",
      "Epoch [198/200] Loss: 3488.0322265625\n",
      "Epoch [199/200] Loss: 3480.782470703125\n",
      "Epoch [200/200] Loss: 3473.54541015625\n",
      "Predicted days_remaining for parent_id 266: 16.997264862060547\n",
      "Training for parent_id 277...\n",
      "Epoch [1/200] Loss: 579.43701171875\n",
      "Epoch [2/200] Loss: 565.9342651367188\n",
      "Epoch [3/200] Loss: 553.1361694335938\n",
      "Epoch [4/200] Loss: 540.962890625\n",
      "Epoch [5/200] Loss: 529.3563232421875\n",
      "Epoch [6/200] Loss: 518.279296875\n",
      "Epoch [7/200] Loss: 507.70440673828125\n",
      "Epoch [8/200] Loss: 497.6007385253906\n",
      "Epoch [9/200] Loss: 487.9356384277344\n",
      "Epoch [10/200] Loss: 478.68182373046875\n",
      "Epoch [11/200] Loss: 469.81903076171875\n",
      "Epoch [12/200] Loss: 461.333740234375\n",
      "Epoch [13/200] Loss: 453.2159423828125\n",
      "Epoch [14/200] Loss: 445.4563903808594\n",
      "Epoch [15/200] Loss: 438.0452880859375\n",
      "Epoch [16/200] Loss: 430.9721374511719\n",
      "Epoch [17/200] Loss: 424.2269592285156\n",
      "Epoch [18/200] Loss: 417.80120849609375\n",
      "Epoch [19/200] Loss: 411.6863708496094\n",
      "Epoch [20/200] Loss: 405.8737487792969\n",
      "Epoch [21/200] Loss: 400.35186767578125\n",
      "Epoch [22/200] Loss: 395.1061096191406\n",
      "Epoch [23/200] Loss: 390.1181640625\n",
      "Epoch [24/200] Loss: 385.3668212890625\n",
      "Epoch [25/200] Loss: 380.8294982910156\n",
      "Epoch [26/200] Loss: 376.4837646484375\n",
      "Epoch [27/200] Loss: 372.308349609375\n",
      "Epoch [28/200] Loss: 368.28436279296875\n",
      "Epoch [29/200] Loss: 364.3954162597656\n",
      "Epoch [30/200] Loss: 360.6278076171875\n",
      "Epoch [31/200] Loss: 356.96990966796875\n",
      "Epoch [32/200] Loss: 353.4120178222656\n",
      "Epoch [33/200] Loss: 349.9457092285156\n",
      "Epoch [34/200] Loss: 346.56353759765625\n",
      "Epoch [35/200] Loss: 343.2594299316406\n",
      "Epoch [36/200] Loss: 340.02862548828125\n",
      "Epoch [37/200] Loss: 336.8675537109375\n",
      "Epoch [38/200] Loss: 333.7744445800781\n",
      "Epoch [39/200] Loss: 330.7484130859375\n",
      "Epoch [40/200] Loss: 327.7889709472656\n",
      "Epoch [41/200] Loss: 324.8957214355469\n",
      "Epoch [42/200] Loss: 322.06683349609375\n",
      "Epoch [43/200] Loss: 319.2997741699219\n",
      "Epoch [44/200] Loss: 316.59063720703125\n",
      "Epoch [45/200] Loss: 313.9347839355469\n",
      "Epoch [46/200] Loss: 311.3271179199219\n",
      "Epoch [47/200] Loss: 308.7626953125\n",
      "Epoch [48/200] Loss: 306.2371520996094\n",
      "Epoch [49/200] Loss: 303.7464294433594\n",
      "Epoch [50/200] Loss: 301.2875671386719\n",
      "Epoch [51/200] Loss: 298.8576354980469\n",
      "Epoch [52/200] Loss: 296.4544982910156\n",
      "Epoch [53/200] Loss: 294.0764465332031\n",
      "Epoch [54/200] Loss: 291.7221374511719\n",
      "Epoch [55/200] Loss: 289.3902587890625\n",
      "Epoch [56/200] Loss: 287.0801086425781\n",
      "Epoch [57/200] Loss: 284.79071044921875\n",
      "Epoch [58/200] Loss: 282.52154541015625\n",
      "Epoch [59/200] Loss: 280.2719421386719\n",
      "Epoch [60/200] Loss: 278.04156494140625\n",
      "Epoch [61/200] Loss: 275.829833984375\n",
      "Epoch [62/200] Loss: 273.63653564453125\n",
      "Epoch [63/200] Loss: 271.4612731933594\n",
      "Epoch [64/200] Loss: 269.30377197265625\n",
      "Epoch [65/200] Loss: 267.1636657714844\n",
      "Epoch [66/200] Loss: 265.040771484375\n",
      "Epoch [67/200] Loss: 262.9347839355469\n",
      "Epoch [68/200] Loss: 260.84564208984375\n",
      "Epoch [69/200] Loss: 258.77288818359375\n",
      "Epoch [70/200] Loss: 256.7165832519531\n",
      "Epoch [71/200] Loss: 254.67623901367188\n",
      "Epoch [72/200] Loss: 252.65191650390625\n",
      "Epoch [73/200] Loss: 250.64328002929688\n",
      "Epoch [74/200] Loss: 248.65023803710938\n",
      "Epoch [75/200] Loss: 246.67259216308594\n",
      "Epoch [76/200] Loss: 244.71011352539062\n",
      "Epoch [77/200] Loss: 242.7627716064453\n",
      "Epoch [78/200] Loss: 240.83033752441406\n",
      "Epoch [79/200] Loss: 238.91262817382812\n",
      "Epoch [80/200] Loss: 237.009521484375\n",
      "Epoch [81/200] Loss: 235.12088012695312\n",
      "Epoch [82/200] Loss: 233.24661254882812\n",
      "Epoch [83/200] Loss: 231.38644409179688\n",
      "Epoch [84/200] Loss: 229.54043579101562\n",
      "Epoch [85/200] Loss: 227.70828247070312\n",
      "Epoch [86/200] Loss: 225.88995361328125\n",
      "Epoch [87/200] Loss: 224.0852508544922\n",
      "Epoch [88/200] Loss: 222.29408264160156\n",
      "Epoch [89/200] Loss: 220.516357421875\n",
      "Epoch [90/200] Loss: 218.75192260742188\n",
      "Epoch [91/200] Loss: 217.0006561279297\n",
      "Epoch [92/200] Loss: 215.26251220703125\n",
      "Epoch [93/200] Loss: 213.53732299804688\n",
      "Epoch [94/200] Loss: 211.82496643066406\n",
      "Epoch [95/200] Loss: 210.1253662109375\n",
      "Epoch [96/200] Loss: 208.4384002685547\n",
      "Epoch [97/200] Loss: 206.76400756835938\n",
      "Epoch [98/200] Loss: 205.1020050048828\n",
      "Epoch [99/200] Loss: 203.452392578125\n",
      "Epoch [100/200] Loss: 201.81497192382812\n",
      "Epoch [101/200] Loss: 200.18972778320312\n",
      "Epoch [102/200] Loss: 198.57655334472656\n",
      "Epoch [103/200] Loss: 196.97523498535156\n",
      "Epoch [104/200] Loss: 195.3859100341797\n",
      "Epoch [105/200] Loss: 193.80831909179688\n",
      "Epoch [106/200] Loss: 192.24237060546875\n",
      "Epoch [107/200] Loss: 190.68807983398438\n",
      "Epoch [108/200] Loss: 189.1453094482422\n",
      "Epoch [109/200] Loss: 187.6139678955078\n",
      "Epoch [110/200] Loss: 186.09396362304688\n",
      "Epoch [111/200] Loss: 184.5852813720703\n",
      "Epoch [112/200] Loss: 183.08770751953125\n",
      "Epoch [113/200] Loss: 181.60128784179688\n",
      "Epoch [114/200] Loss: 180.12591552734375\n",
      "Epoch [115/200] Loss: 178.66146850585938\n",
      "Epoch [116/200] Loss: 177.2079620361328\n",
      "Epoch [117/200] Loss: 175.76516723632812\n",
      "Epoch [118/200] Loss: 174.33316040039062\n",
      "Epoch [119/200] Loss: 172.91177368164062\n",
      "Epoch [120/200] Loss: 171.5010223388672\n",
      "Epoch [121/200] Loss: 170.10073852539062\n",
      "Epoch [122/200] Loss: 168.7108917236328\n",
      "Epoch [123/200] Loss: 167.3314971923828\n",
      "Epoch [124/200] Loss: 165.96231079101562\n",
      "Epoch [125/200] Loss: 164.6033935546875\n",
      "Epoch [126/200] Loss: 163.25466918945312\n",
      "Epoch [127/200] Loss: 161.91603088378906\n",
      "Epoch [128/200] Loss: 160.58741760253906\n",
      "Epoch [129/200] Loss: 159.26876831054688\n",
      "Epoch [130/200] Loss: 157.96006774902344\n",
      "Epoch [131/200] Loss: 156.6611785888672\n",
      "Epoch [132/200] Loss: 155.37205505371094\n",
      "Epoch [133/200] Loss: 154.0926513671875\n",
      "Epoch [134/200] Loss: 152.8229217529297\n",
      "Epoch [135/200] Loss: 151.5626983642578\n",
      "Epoch [136/200] Loss: 150.31211853027344\n",
      "Epoch [137/200] Loss: 149.07095336914062\n",
      "Epoch [138/200] Loss: 147.8392333984375\n",
      "Epoch [139/200] Loss: 146.61683654785156\n",
      "Epoch [140/200] Loss: 145.40374755859375\n",
      "Epoch [141/200] Loss: 144.19985961914062\n",
      "Epoch [142/200] Loss: 143.005126953125\n",
      "Epoch [143/200] Loss: 141.819580078125\n",
      "Epoch [144/200] Loss: 140.64305114746094\n",
      "Epoch [145/200] Loss: 139.4755096435547\n",
      "Epoch [146/200] Loss: 138.31695556640625\n",
      "Epoch [147/200] Loss: 137.16726684570312\n",
      "Epoch [148/200] Loss: 136.0264129638672\n",
      "Epoch [149/200] Loss: 134.89437866210938\n",
      "Epoch [150/200] Loss: 133.7710418701172\n",
      "Epoch [151/200] Loss: 132.6563720703125\n",
      "Epoch [152/200] Loss: 131.55029296875\n",
      "Epoch [153/200] Loss: 130.45281982421875\n",
      "Epoch [154/200] Loss: 129.36387634277344\n",
      "Epoch [155/200] Loss: 128.28334045410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [156/200] Loss: 127.21127319335938\n",
      "Epoch [157/200] Loss: 126.14749145507812\n",
      "Epoch [158/200] Loss: 125.092041015625\n",
      "Epoch [159/200] Loss: 124.04484558105469\n",
      "Epoch [160/200] Loss: 123.00584411621094\n",
      "Epoch [161/200] Loss: 121.97502899169922\n",
      "Epoch [162/200] Loss: 120.95227813720703\n",
      "Epoch [163/200] Loss: 119.93755340576172\n",
      "Epoch [164/200] Loss: 118.9308853149414\n",
      "Epoch [165/200] Loss: 117.9321060180664\n",
      "Epoch [166/200] Loss: 116.94129180908203\n",
      "Epoch [167/200] Loss: 115.95830535888672\n",
      "Epoch [168/200] Loss: 114.98308563232422\n",
      "Epoch [169/200] Loss: 114.01567840576172\n",
      "Epoch [170/200] Loss: 113.05594635009766\n",
      "Epoch [171/200] Loss: 112.10386657714844\n",
      "Epoch [172/200] Loss: 111.15943145751953\n",
      "Epoch [173/200] Loss: 110.22252655029297\n",
      "Epoch [174/200] Loss: 109.29315948486328\n",
      "Epoch [175/200] Loss: 108.37128448486328\n",
      "Epoch [176/200] Loss: 107.456787109375\n",
      "Epoch [177/200] Loss: 106.5496597290039\n",
      "Epoch [178/200] Loss: 105.6499252319336\n",
      "Epoch [179/200] Loss: 104.7574234008789\n",
      "Epoch [180/200] Loss: 103.8721923828125\n",
      "Epoch [181/200] Loss: 102.9941177368164\n",
      "Epoch [182/200] Loss: 102.12324523925781\n",
      "Epoch [183/200] Loss: 101.25946044921875\n",
      "Epoch [184/200] Loss: 100.40271759033203\n",
      "Epoch [185/200] Loss: 99.5530014038086\n",
      "Epoch [186/200] Loss: 98.71025848388672\n",
      "Epoch [187/200] Loss: 97.87445068359375\n",
      "Epoch [188/200] Loss: 97.04551696777344\n",
      "Epoch [189/200] Loss: 96.2234115600586\n",
      "Epoch [190/200] Loss: 95.40815734863281\n",
      "Epoch [191/200] Loss: 94.59956359863281\n",
      "Epoch [192/200] Loss: 93.79776763916016\n",
      "Epoch [193/200] Loss: 93.00257873535156\n",
      "Epoch [194/200] Loss: 92.21403503417969\n",
      "Epoch [195/200] Loss: 91.43209838867188\n",
      "Epoch [196/200] Loss: 90.65665435791016\n",
      "Epoch [197/200] Loss: 89.88775634765625\n",
      "Epoch [198/200] Loss: 89.12527465820312\n",
      "Epoch [199/200] Loss: 88.36925506591797\n",
      "Epoch [200/200] Loss: 87.61957550048828\n",
      "Predicted days_remaining for parent_id 277: 15.23890495300293\n",
      "Training for parent_id 281...\n",
      "Epoch [1/200] Loss: 263.8227844238281\n",
      "Epoch [2/200] Loss: 256.2756652832031\n",
      "Epoch [3/200] Loss: 248.77374267578125\n",
      "Epoch [4/200] Loss: 241.40501403808594\n",
      "Epoch [5/200] Loss: 234.2537841796875\n",
      "Epoch [6/200] Loss: 227.36700439453125\n",
      "Epoch [7/200] Loss: 220.7596893310547\n",
      "Epoch [8/200] Loss: 214.42568969726562\n",
      "Epoch [9/200] Loss: 208.347412109375\n",
      "Epoch [10/200] Loss: 202.50717163085938\n",
      "Epoch [11/200] Loss: 196.89366149902344\n",
      "Epoch [12/200] Loss: 191.5020294189453\n",
      "Epoch [13/200] Loss: 186.33119201660156\n",
      "Epoch [14/200] Loss: 181.3810577392578\n",
      "Epoch [15/200] Loss: 176.65052795410156\n",
      "Epoch [16/200] Loss: 172.13665771484375\n",
      "Epoch [17/200] Loss: 167.8340301513672\n",
      "Epoch [18/200] Loss: 163.73521423339844\n",
      "Epoch [19/200] Loss: 159.8311309814453\n",
      "Epoch [20/200] Loss: 156.11204528808594\n",
      "Epoch [21/200] Loss: 152.5684051513672\n",
      "Epoch [22/200] Loss: 149.1911163330078\n",
      "Epoch [23/200] Loss: 145.97174072265625\n",
      "Epoch [24/200] Loss: 142.90269470214844\n",
      "Epoch [25/200] Loss: 139.97705078125\n",
      "Epoch [26/200] Loss: 137.18841552734375\n",
      "Epoch [27/200] Loss: 134.53060913085938\n",
      "Epoch [28/200] Loss: 131.99745178222656\n",
      "Epoch [29/200] Loss: 129.5823516845703\n",
      "Epoch [30/200] Loss: 127.27820587158203\n",
      "Epoch [31/200] Loss: 125.0771713256836\n",
      "Epoch [32/200] Loss: 122.9709701538086\n",
      "Epoch [33/200] Loss: 120.9510726928711\n",
      "Epoch [34/200] Loss: 119.00904083251953\n",
      "Epoch [35/200] Loss: 117.13697814941406\n",
      "Epoch [36/200] Loss: 115.32766723632812\n",
      "Epoch [37/200] Loss: 113.57469177246094\n",
      "Epoch [38/200] Loss: 111.87248992919922\n",
      "Epoch [39/200] Loss: 110.21623992919922\n",
      "Epoch [40/200] Loss: 108.60181427001953\n",
      "Epoch [41/200] Loss: 107.02569580078125\n",
      "Epoch [42/200] Loss: 105.48486328125\n",
      "Epoch [43/200] Loss: 103.97673797607422\n",
      "Epoch [44/200] Loss: 102.49915313720703\n",
      "Epoch [45/200] Loss: 101.05020904541016\n",
      "Epoch [46/200] Loss: 99.62833404541016\n",
      "Epoch [47/200] Loss: 98.2321548461914\n",
      "Epoch [48/200] Loss: 96.86048126220703\n",
      "Epoch [49/200] Loss: 95.51234436035156\n",
      "Epoch [50/200] Loss: 94.18685913085938\n",
      "Epoch [51/200] Loss: 92.88334655761719\n",
      "Epoch [52/200] Loss: 91.60108184814453\n",
      "Epoch [53/200] Loss: 90.33953094482422\n",
      "Epoch [54/200] Loss: 89.0981674194336\n",
      "Epoch [55/200] Loss: 87.87649536132812\n",
      "Epoch [56/200] Loss: 86.67411041259766\n",
      "Epoch [57/200] Loss: 85.49057006835938\n",
      "Epoch [58/200] Loss: 84.32551574707031\n",
      "Epoch [59/200] Loss: 83.17860412597656\n",
      "Epoch [60/200] Loss: 82.0494155883789\n",
      "Epoch [61/200] Loss: 80.93766784667969\n",
      "Epoch [62/200] Loss: 79.84302520751953\n",
      "Epoch [63/200] Loss: 78.7651138305664\n",
      "Epoch [64/200] Loss: 77.70368194580078\n",
      "Epoch [65/200] Loss: 76.65841674804688\n",
      "Epoch [66/200] Loss: 75.6290283203125\n",
      "Epoch [67/200] Loss: 74.61524200439453\n",
      "Epoch [68/200] Loss: 73.61678314208984\n",
      "Epoch [69/200] Loss: 72.63334655761719\n",
      "Epoch [70/200] Loss: 71.66473388671875\n",
      "Epoch [71/200] Loss: 70.71067810058594\n",
      "Epoch [72/200] Loss: 69.77091979980469\n",
      "Epoch [73/200] Loss: 68.8452377319336\n",
      "Epoch [74/200] Loss: 67.93342590332031\n",
      "Epoch [75/200] Loss: 67.03523254394531\n",
      "Epoch [76/200] Loss: 66.15040588378906\n",
      "Epoch [77/200] Loss: 65.27883911132812\n",
      "Epoch [78/200] Loss: 64.42024230957031\n",
      "Epoch [79/200] Loss: 63.574459075927734\n",
      "Epoch [80/200] Loss: 62.74127960205078\n",
      "Epoch [81/200] Loss: 61.920536041259766\n",
      "Epoch [82/200] Loss: 61.11202621459961\n",
      "Epoch [83/200] Loss: 60.31555938720703\n",
      "Epoch [84/200] Loss: 59.530982971191406\n",
      "Epoch [85/200] Loss: 58.75810623168945\n",
      "Epoch [86/200] Loss: 57.99677658081055\n",
      "Epoch [87/200] Loss: 57.246803283691406\n",
      "Epoch [88/200] Loss: 56.50807571411133\n",
      "Epoch [89/200] Loss: 55.7804069519043\n",
      "Epoch [90/200] Loss: 55.06361770629883\n",
      "Epoch [91/200] Loss: 54.357601165771484\n",
      "Epoch [92/200] Loss: 53.66218566894531\n",
      "Epoch [93/200] Loss: 52.97718811035156\n",
      "Epoch [94/200] Loss: 52.302574157714844\n",
      "Epoch [95/200] Loss: 51.63809585571289\n",
      "Epoch [96/200] Loss: 50.983665466308594\n",
      "Epoch [97/200] Loss: 50.339115142822266\n",
      "Epoch [98/200] Loss: 49.704349517822266\n",
      "Epoch [99/200] Loss: 49.0792236328125\n",
      "Epoch [100/200] Loss: 48.46360397338867\n",
      "Epoch [101/200] Loss: 47.857364654541016\n",
      "Epoch [102/200] Loss: 47.260372161865234\n",
      "Epoch [103/200] Loss: 46.67253112792969\n",
      "Epoch [104/200] Loss: 46.09368133544922\n",
      "Epoch [105/200] Loss: 45.52375030517578\n",
      "Epoch [106/200] Loss: 44.96256637573242\n",
      "Epoch [107/200] Loss: 44.410057067871094\n",
      "Epoch [108/200] Loss: 43.86609649658203\n",
      "Epoch [109/200] Loss: 43.3305778503418\n",
      "Epoch [110/200] Loss: 42.80337142944336\n",
      "Epoch [111/200] Loss: 42.28438186645508\n",
      "Epoch [112/200] Loss: 41.77349853515625\n",
      "Epoch [113/200] Loss: 41.2706184387207\n",
      "Epoch [114/200] Loss: 40.77562713623047\n",
      "Epoch [115/200] Loss: 40.28843307495117\n",
      "Epoch [116/200] Loss: 39.80893325805664\n",
      "Epoch [117/200] Loss: 39.33701705932617\n",
      "Epoch [118/200] Loss: 38.872596740722656\n",
      "Epoch [119/200] Loss: 38.41556930541992\n",
      "Epoch [120/200] Loss: 37.96581268310547\n",
      "Epoch [121/200] Loss: 37.523277282714844\n",
      "Epoch [122/200] Loss: 37.087833404541016\n",
      "Epoch [123/200] Loss: 36.65940856933594\n",
      "Epoch [124/200] Loss: 36.23788833618164\n",
      "Epoch [125/200] Loss: 35.823204040527344\n",
      "Epoch [126/200] Loss: 35.41525650024414\n",
      "Epoch [127/200] Loss: 35.013938903808594\n",
      "Epoch [128/200] Loss: 34.619178771972656\n",
      "Epoch [129/200] Loss: 34.23090362548828\n",
      "Epoch [130/200] Loss: 33.849002838134766\n",
      "Epoch [131/200] Loss: 33.47339630126953\n",
      "Epoch [132/200] Loss: 33.1040153503418\n",
      "Epoch [133/200] Loss: 32.740753173828125\n",
      "Epoch [134/200] Loss: 32.3835334777832\n",
      "Epoch [135/200] Loss: 32.03228759765625\n",
      "Epoch [136/200] Loss: 31.686920166015625\n",
      "Epoch [137/200] Loss: 31.347362518310547\n",
      "Epoch [138/200] Loss: 31.013526916503906\n",
      "Epoch [139/200] Loss: 30.68534278869629\n",
      "Epoch [140/200] Loss: 30.362720489501953\n",
      "Epoch [141/200] Loss: 30.045581817626953\n",
      "Epoch [142/200] Loss: 29.733890533447266\n",
      "Epoch [143/200] Loss: 29.42751693725586\n",
      "Epoch [144/200] Loss: 29.126422882080078\n",
      "Epoch [145/200] Loss: 28.830514907836914\n",
      "Epoch [146/200] Loss: 28.539724349975586\n",
      "Epoch [147/200] Loss: 28.254003524780273\n",
      "Epoch [148/200] Loss: 27.973247528076172\n",
      "Epoch [149/200] Loss: 27.69740104675293\n",
      "Epoch [150/200] Loss: 27.4263916015625\n",
      "Epoch [151/200] Loss: 27.16015625\n",
      "Epoch [152/200] Loss: 26.89862060546875\n",
      "Epoch [153/200] Loss: 26.64171600341797\n",
      "Epoch [154/200] Loss: 26.389381408691406\n",
      "Epoch [155/200] Loss: 26.141551971435547\n",
      "Epoch [156/200] Loss: 25.89815902709961\n",
      "Epoch [157/200] Loss: 25.659133911132812\n",
      "Epoch [158/200] Loss: 25.424427032470703\n",
      "Epoch [159/200] Loss: 25.193958282470703\n",
      "Epoch [160/200] Loss: 24.96766471862793\n",
      "Epoch [161/200] Loss: 24.745502471923828\n",
      "Epoch [162/200] Loss: 24.527400970458984\n",
      "Epoch [163/200] Loss: 24.31328582763672\n",
      "Epoch [164/200] Loss: 24.103120803833008\n",
      "Epoch [165/200] Loss: 23.89682388305664\n",
      "Epoch [166/200] Loss: 23.694360733032227\n",
      "Epoch [167/200] Loss: 23.495655059814453\n",
      "Epoch [168/200] Loss: 23.300655364990234\n",
      "Epoch [169/200] Loss: 23.109302520751953\n",
      "Epoch [170/200] Loss: 22.921552658081055\n",
      "Epoch [171/200] Loss: 22.737340927124023\n",
      "Epoch [172/200] Loss: 22.55659294128418\n",
      "Epoch [173/200] Loss: 22.379295349121094\n",
      "Epoch [174/200] Loss: 22.205368041992188\n",
      "Epoch [175/200] Loss: 22.034751892089844\n",
      "Epoch [176/200] Loss: 21.867420196533203\n",
      "Epoch [177/200] Loss: 21.703292846679688\n",
      "Epoch [178/200] Loss: 21.542354583740234\n",
      "Epoch [179/200] Loss: 21.384521484375\n",
      "Epoch [180/200] Loss: 21.22975730895996\n",
      "Epoch [181/200] Loss: 21.078004837036133\n",
      "Epoch [182/200] Loss: 20.929229736328125\n",
      "Epoch [183/200] Loss: 20.783370971679688\n",
      "Epoch [184/200] Loss: 20.64038848876953\n",
      "Epoch [185/200] Loss: 20.50023078918457\n",
      "Epoch [186/200] Loss: 20.36285400390625\n",
      "Epoch [187/200] Loss: 20.228214263916016\n",
      "Epoch [188/200] Loss: 20.09626007080078\n",
      "Epoch [189/200] Loss: 19.966947555541992\n",
      "Epoch [190/200] Loss: 19.840242385864258\n",
      "Epoch [191/200] Loss: 19.716087341308594\n",
      "Epoch [192/200] Loss: 19.594451904296875\n",
      "Epoch [193/200] Loss: 19.475276947021484\n",
      "Epoch [194/200] Loss: 19.35854148864746\n",
      "Epoch [195/200] Loss: 19.244192123413086\n",
      "Epoch [196/200] Loss: 19.132183074951172\n",
      "Epoch [197/200] Loss: 19.022480010986328\n",
      "Epoch [198/200] Loss: 18.91504669189453\n",
      "Epoch [199/200] Loss: 18.809837341308594\n",
      "Epoch [200/200] Loss: 18.706817626953125\n",
      "Predicted days_remaining for parent_id 281: 13.708322525024414\n",
      "Training for parent_id 284...\n",
      "Epoch [1/200] Loss: 305.4383239746094\n",
      "Epoch [2/200] Loss: 296.51776123046875\n",
      "Epoch [3/200] Loss: 287.9325866699219\n",
      "Epoch [4/200] Loss: 279.75628662109375\n",
      "Epoch [5/200] Loss: 272.0164794921875\n",
      "Epoch [6/200] Loss: 264.7159729003906\n",
      "Epoch [7/200] Loss: 257.8367919921875\n",
      "Epoch [8/200] Loss: 251.34799194335938\n",
      "Epoch [9/200] Loss: 245.2132110595703\n",
      "Epoch [10/200] Loss: 239.39617919921875\n",
      "Epoch [11/200] Loss: 233.86483764648438\n",
      "Epoch [12/200] Loss: 228.59410095214844\n",
      "Epoch [13/200] Loss: 223.5655975341797\n",
      "Epoch [14/200] Loss: 218.76646423339844\n",
      "Epoch [15/200] Loss: 214.1868133544922\n",
      "Epoch [16/200] Loss: 209.81834411621094\n",
      "Epoch [17/200] Loss: 205.65281677246094\n",
      "Epoch [18/200] Loss: 201.68161010742188\n",
      "Epoch [19/200] Loss: 197.89573669433594\n",
      "Epoch [20/200] Loss: 194.28565979003906\n",
      "Epoch [21/200] Loss: 190.8417205810547\n",
      "Epoch [22/200] Loss: 187.5540771484375\n",
      "Epoch [23/200] Loss: 184.4127960205078\n",
      "Epoch [24/200] Loss: 181.40802001953125\n",
      "Epoch [25/200] Loss: 178.52992248535156\n",
      "Epoch [26/200] Loss: 175.76898193359375\n",
      "Epoch [27/200] Loss: 173.11611938476562\n",
      "Epoch [28/200] Loss: 170.5625457763672\n",
      "Epoch [29/200] Loss: 168.10012817382812\n",
      "Epoch [30/200] Loss: 165.7213897705078\n",
      "Epoch [31/200] Loss: 163.4194793701172\n",
      "Epoch [32/200] Loss: 161.1881103515625\n",
      "Epoch [33/200] Loss: 159.02166748046875\n",
      "Epoch [34/200] Loss: 156.9150848388672\n",
      "Epoch [35/200] Loss: 154.8636932373047\n",
      "Epoch [36/200] Loss: 152.86325073242188\n",
      "Epoch [37/200] Loss: 150.90985107421875\n",
      "Epoch [38/200] Loss: 148.9996795654297\n",
      "Epoch [39/200] Loss: 147.129150390625\n",
      "Epoch [40/200] Loss: 145.2947235107422\n",
      "Epoch [41/200] Loss: 143.4933319091797\n",
      "Epoch [42/200] Loss: 141.7220001220703\n",
      "Epoch [43/200] Loss: 139.97828674316406\n",
      "Epoch [44/200] Loss: 138.2601318359375\n",
      "Epoch [45/200] Loss: 136.56591796875\n",
      "Epoch [46/200] Loss: 134.89450073242188\n",
      "Epoch [47/200] Loss: 133.2450408935547\n",
      "Epoch [48/200] Loss: 131.616943359375\n",
      "Epoch [49/200] Loss: 130.00990295410156\n",
      "Epoch [50/200] Loss: 128.42356872558594\n",
      "Epoch [51/200] Loss: 126.85770416259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/200] Loss: 125.3121109008789\n",
      "Epoch [53/200] Loss: 123.78645324707031\n",
      "Epoch [54/200] Loss: 122.28047943115234\n",
      "Epoch [55/200] Loss: 120.79388427734375\n",
      "Epoch [56/200] Loss: 119.32630157470703\n",
      "Epoch [57/200] Loss: 117.87738037109375\n",
      "Epoch [58/200] Loss: 116.44674682617188\n",
      "Epoch [59/200] Loss: 115.0340805053711\n",
      "Epoch [60/200] Loss: 113.63904571533203\n",
      "Epoch [61/200] Loss: 112.26128387451172\n",
      "Epoch [62/200] Loss: 110.90055847167969\n",
      "Epoch [63/200] Loss: 109.55658721923828\n",
      "Epoch [64/200] Loss: 108.22905731201172\n",
      "Epoch [65/200] Loss: 106.91788482666016\n",
      "Epoch [66/200] Loss: 105.62275695800781\n",
      "Epoch [67/200] Loss: 104.34351348876953\n",
      "Epoch [68/200] Loss: 103.0800552368164\n",
      "Epoch [69/200] Loss: 101.8321533203125\n",
      "Epoch [70/200] Loss: 100.5996322631836\n",
      "Epoch [71/200] Loss: 99.3824462890625\n",
      "Epoch [72/200] Loss: 98.18037414550781\n",
      "Epoch [73/200] Loss: 96.99327087402344\n",
      "Epoch [74/200] Loss: 95.82099151611328\n",
      "Epoch [75/200] Loss: 94.66343688964844\n",
      "Epoch [76/200] Loss: 93.52037811279297\n",
      "Epoch [77/200] Loss: 92.39171600341797\n",
      "Epoch [78/200] Loss: 91.27727508544922\n",
      "Epoch [79/200] Loss: 90.17688751220703\n",
      "Epoch [80/200] Loss: 89.0904312133789\n",
      "Epoch [81/200] Loss: 88.01769256591797\n",
      "Epoch [82/200] Loss: 86.95858001708984\n",
      "Epoch [83/200] Loss: 85.912841796875\n",
      "Epoch [84/200] Loss: 84.88041687011719\n",
      "Epoch [85/200] Loss: 83.86109924316406\n",
      "Epoch [86/200] Loss: 82.85472106933594\n",
      "Epoch [87/200] Loss: 81.86116790771484\n",
      "Epoch [88/200] Loss: 80.8802490234375\n",
      "Epoch [89/200] Loss: 79.91185760498047\n",
      "Epoch [90/200] Loss: 78.95578002929688\n",
      "Epoch [91/200] Loss: 78.01191711425781\n",
      "Epoch [92/200] Loss: 77.08013153076172\n",
      "Epoch [93/200] Loss: 76.16024780273438\n",
      "Epoch [94/200] Loss: 75.25215148925781\n",
      "Epoch [95/200] Loss: 74.35568237304688\n",
      "Epoch [96/200] Loss: 73.47071838378906\n",
      "Epoch [97/200] Loss: 72.5970687866211\n",
      "Epoch [98/200] Loss: 71.73467254638672\n",
      "Epoch [99/200] Loss: 70.88339233398438\n",
      "Epoch [100/200] Loss: 70.04303741455078\n",
      "Epoch [101/200] Loss: 69.21354675292969\n",
      "Epoch [102/200] Loss: 68.39476776123047\n",
      "Epoch [103/200] Loss: 67.58651733398438\n",
      "Epoch [104/200] Loss: 66.78878784179688\n",
      "Epoch [105/200] Loss: 66.00135803222656\n",
      "Epoch [106/200] Loss: 65.22415924072266\n",
      "Epoch [107/200] Loss: 64.45702362060547\n",
      "Epoch [108/200] Loss: 63.69992446899414\n",
      "Epoch [109/200] Loss: 62.95265579223633\n",
      "Epoch [110/200] Loss: 62.21514129638672\n",
      "Epoch [111/200] Loss: 61.48726272583008\n",
      "Epoch [112/200] Loss: 60.768917083740234\n",
      "Epoch [113/200] Loss: 60.05997848510742\n",
      "Epoch [114/200] Loss: 59.360374450683594\n",
      "Epoch [115/200] Loss: 58.66996383666992\n",
      "Epoch [116/200] Loss: 57.98862838745117\n",
      "Epoch [117/200] Loss: 57.316307067871094\n",
      "Epoch [118/200] Loss: 56.652870178222656\n",
      "Epoch [119/200] Loss: 55.998233795166016\n",
      "Epoch [120/200] Loss: 55.35226821899414\n",
      "Epoch [121/200] Loss: 54.71491241455078\n",
      "Epoch [122/200] Loss: 54.086055755615234\n",
      "Epoch [123/200] Loss: 53.46556854248047\n",
      "Epoch [124/200] Loss: 52.85340118408203\n",
      "Epoch [125/200] Loss: 52.24943542480469\n",
      "Epoch [126/200] Loss: 51.65359115600586\n",
      "Epoch [127/200] Loss: 51.06576156616211\n",
      "Epoch [128/200] Loss: 50.48586654663086\n",
      "Epoch [129/200] Loss: 49.913814544677734\n",
      "Epoch [130/200] Loss: 49.34950637817383\n",
      "Epoch [131/200] Loss: 48.7928581237793\n",
      "Epoch [132/200] Loss: 48.243804931640625\n",
      "Epoch [133/200] Loss: 47.70222473144531\n",
      "Epoch [134/200] Loss: 47.168060302734375\n",
      "Epoch [135/200] Loss: 46.641212463378906\n",
      "Epoch [136/200] Loss: 46.121612548828125\n",
      "Epoch [137/200] Loss: 45.609169006347656\n",
      "Epoch [138/200] Loss: 45.10379409790039\n",
      "Epoch [139/200] Loss: 44.60541534423828\n",
      "Epoch [140/200] Loss: 44.11394119262695\n",
      "Epoch [141/200] Loss: 43.629329681396484\n",
      "Epoch [142/200] Loss: 43.15143585205078\n",
      "Epoch [143/200] Loss: 42.680240631103516\n",
      "Epoch [144/200] Loss: 42.21566390991211\n",
      "Epoch [145/200] Loss: 41.75758743286133\n",
      "Epoch [146/200] Loss: 41.30597686767578\n",
      "Epoch [147/200] Loss: 40.86075210571289\n",
      "Epoch [148/200] Loss: 40.42180633544922\n",
      "Epoch [149/200] Loss: 39.98912048339844\n",
      "Epoch [150/200] Loss: 39.56258010864258\n",
      "Epoch [151/200] Loss: 39.142127990722656\n",
      "Epoch [152/200] Loss: 38.72769546508789\n",
      "Epoch [153/200] Loss: 38.319217681884766\n",
      "Epoch [154/200] Loss: 37.91659164428711\n",
      "Epoch [155/200] Loss: 37.51979064941406\n",
      "Epoch [156/200] Loss: 37.128746032714844\n",
      "Epoch [157/200] Loss: 36.74333190917969\n",
      "Epoch [158/200] Loss: 36.36355209350586\n",
      "Epoch [159/200] Loss: 35.989322662353516\n",
      "Epoch [160/200] Loss: 35.620548248291016\n",
      "Epoch [161/200] Loss: 35.25719451904297\n",
      "Epoch [162/200] Loss: 34.8991813659668\n",
      "Epoch [163/200] Loss: 34.54644012451172\n",
      "Epoch [164/200] Loss: 34.19893264770508\n",
      "Epoch [165/200] Loss: 33.856571197509766\n",
      "Epoch [166/200] Loss: 33.51930618286133\n",
      "Epoch [167/200] Loss: 33.187068939208984\n",
      "Epoch [168/200] Loss: 32.85981750488281\n",
      "Epoch [169/200] Loss: 32.537452697753906\n",
      "Epoch [170/200] Loss: 32.21995544433594\n",
      "Epoch [171/200] Loss: 31.907238006591797\n",
      "Epoch [172/200] Loss: 31.599258422851562\n",
      "Epoch [173/200] Loss: 31.29596710205078\n",
      "Epoch [174/200] Loss: 30.997276306152344\n",
      "Epoch [175/200] Loss: 30.703149795532227\n",
      "Epoch [176/200] Loss: 30.413532257080078\n",
      "Epoch [177/200] Loss: 30.128355026245117\n",
      "Epoch [178/200] Loss: 29.84757423400879\n",
      "Epoch [179/200] Loss: 29.571130752563477\n",
      "Epoch [180/200] Loss: 29.298973083496094\n",
      "Epoch [181/200] Loss: 29.031036376953125\n",
      "Epoch [182/200] Loss: 28.767269134521484\n",
      "Epoch [183/200] Loss: 28.507633209228516\n",
      "Epoch [184/200] Loss: 28.252073287963867\n",
      "Epoch [185/200] Loss: 28.000511169433594\n",
      "Epoch [186/200] Loss: 27.752931594848633\n",
      "Epoch [187/200] Loss: 27.509254455566406\n",
      "Epoch [188/200] Loss: 27.269447326660156\n",
      "Epoch [189/200] Loss: 27.03346061706543\n",
      "Epoch [190/200] Loss: 26.801237106323242\n",
      "Epoch [191/200] Loss: 26.572723388671875\n",
      "Epoch [192/200] Loss: 26.34787368774414\n",
      "Epoch [193/200] Loss: 26.12666130065918\n",
      "Epoch [194/200] Loss: 25.909006118774414\n",
      "Epoch [195/200] Loss: 25.694883346557617\n",
      "Epoch [196/200] Loss: 25.484233856201172\n",
      "Epoch [197/200] Loss: 25.27700424194336\n",
      "Epoch [198/200] Loss: 25.073165893554688\n",
      "Epoch [199/200] Loss: 24.8726749420166\n",
      "Epoch [200/200] Loss: 24.675472259521484\n",
      "Predicted days_remaining for parent_id 284: 13.580769538879395\n",
      "Training for parent_id 289...\n",
      "Epoch [1/200] Loss: 195.5566864013672\n",
      "Epoch [2/200] Loss: 188.3006591796875\n",
      "Epoch [3/200] Loss: 181.1198272705078\n",
      "Epoch [4/200] Loss: 174.0543670654297\n",
      "Epoch [5/200] Loss: 167.16143798828125\n",
      "Epoch [6/200] Loss: 160.5037078857422\n",
      "Epoch [7/200] Loss: 154.1339569091797\n",
      "Epoch [8/200] Loss: 148.08489990234375\n",
      "Epoch [9/200] Loss: 142.37428283691406\n",
      "Epoch [10/200] Loss: 137.00975036621094\n",
      "Epoch [11/200] Loss: 131.9888458251953\n",
      "Epoch [12/200] Loss: 127.29847717285156\n",
      "Epoch [13/200] Loss: 122.91754150390625\n",
      "Epoch [14/200] Loss: 118.82157135009766\n",
      "Epoch [15/200] Loss: 114.9874496459961\n",
      "Epoch [16/200] Loss: 111.39527130126953\n",
      "Epoch [17/200] Loss: 108.0283203125\n",
      "Epoch [18/200] Loss: 104.87208557128906\n",
      "Epoch [19/200] Loss: 101.91311645507812\n",
      "Epoch [20/200] Loss: 99.13839721679688\n",
      "Epoch [21/200] Loss: 96.53515625\n",
      "Epoch [22/200] Loss: 94.091064453125\n",
      "Epoch [23/200] Loss: 91.79434204101562\n",
      "Epoch [24/200] Loss: 89.63381958007812\n",
      "Epoch [25/200] Loss: 87.5986099243164\n",
      "Epoch [26/200] Loss: 85.67796325683594\n",
      "Epoch [27/200] Loss: 83.86113739013672\n",
      "Epoch [28/200] Loss: 82.1375503540039\n",
      "Epoch [29/200] Loss: 80.49710083007812\n",
      "Epoch [30/200] Loss: 78.93051147460938\n",
      "Epoch [31/200] Loss: 77.42945098876953\n",
      "Epoch [32/200] Loss: 75.98681640625\n",
      "Epoch [33/200] Loss: 74.59650421142578\n",
      "Epoch [34/200] Loss: 73.2533187866211\n",
      "Epoch [35/200] Loss: 71.95281219482422\n",
      "Epoch [36/200] Loss: 70.69121551513672\n",
      "Epoch [37/200] Loss: 69.46520233154297\n",
      "Epoch [38/200] Loss: 68.27204132080078\n",
      "Epoch [39/200] Loss: 67.10941314697266\n",
      "Epoch [40/200] Loss: 65.97550201416016\n",
      "Epoch [41/200] Loss: 64.86874389648438\n",
      "Epoch [42/200] Loss: 63.787986755371094\n",
      "Epoch [43/200] Loss: 62.73221969604492\n",
      "Epoch [44/200] Loss: 61.7005500793457\n",
      "Epoch [45/200] Loss: 60.692176818847656\n",
      "Epoch [46/200] Loss: 59.70626449584961\n",
      "Epoch [47/200] Loss: 58.74197769165039\n",
      "Epoch [48/200] Loss: 57.7984619140625\n",
      "Epoch [49/200] Loss: 56.87489318847656\n",
      "Epoch [50/200] Loss: 55.970458984375\n",
      "Epoch [51/200] Loss: 55.084407806396484\n",
      "Epoch [52/200] Loss: 54.2160758972168\n",
      "Epoch [53/200] Loss: 53.36481475830078\n",
      "Epoch [54/200] Loss: 52.530059814453125\n",
      "Epoch [55/200] Loss: 51.71133804321289\n",
      "Epoch [56/200] Loss: 50.908180236816406\n",
      "Epoch [57/200] Loss: 50.12020492553711\n",
      "Epoch [58/200] Loss: 49.347023010253906\n",
      "Epoch [59/200] Loss: 48.588321685791016\n",
      "Epoch [60/200] Loss: 47.843807220458984\n",
      "Epoch [61/200] Loss: 47.113162994384766\n",
      "Epoch [62/200] Loss: 46.396183013916016\n",
      "Epoch [63/200] Loss: 45.692588806152344\n",
      "Epoch [64/200] Loss: 45.00215530395508\n",
      "Epoch [65/200] Loss: 44.324703216552734\n",
      "Epoch [66/200] Loss: 43.65997314453125\n",
      "Epoch [67/200] Loss: 43.00779342651367\n",
      "Epoch [68/200] Loss: 42.367950439453125\n",
      "Epoch [69/200] Loss: 41.74027633666992\n",
      "Epoch [70/200] Loss: 41.124568939208984\n",
      "Epoch [71/200] Loss: 40.5206298828125\n",
      "Epoch [72/200] Loss: 39.92828369140625\n",
      "Epoch [73/200] Loss: 39.34735870361328\n",
      "Epoch [74/200] Loss: 38.77766799926758\n",
      "Epoch [75/200] Loss: 38.21901321411133\n",
      "Epoch [76/200] Loss: 37.67124557495117\n",
      "Epoch [77/200] Loss: 37.134193420410156\n",
      "Epoch [78/200] Loss: 36.607666015625\n",
      "Epoch [79/200] Loss: 36.091495513916016\n",
      "Epoch [80/200] Loss: 35.585540771484375\n",
      "Epoch [81/200] Loss: 35.0896110534668\n",
      "Epoch [82/200] Loss: 34.60354995727539\n",
      "Epoch [83/200] Loss: 34.127201080322266\n",
      "Epoch [84/200] Loss: 33.6604118347168\n",
      "Epoch [85/200] Loss: 33.20302200317383\n",
      "Epoch [86/200] Loss: 32.754878997802734\n",
      "Epoch [87/200] Loss: 32.315834045410156\n",
      "Epoch [88/200] Loss: 31.88573455810547\n",
      "Epoch [89/200] Loss: 31.464441299438477\n",
      "Epoch [90/200] Loss: 31.051809310913086\n",
      "Epoch [91/200] Loss: 30.647680282592773\n",
      "Epoch [92/200] Loss: 30.251928329467773\n",
      "Epoch [93/200] Loss: 29.86440658569336\n",
      "Epoch [94/200] Loss: 29.484989166259766\n",
      "Epoch [95/200] Loss: 29.113523483276367\n",
      "Epoch [96/200] Loss: 28.7498722076416\n",
      "Epoch [97/200] Loss: 28.39392852783203\n",
      "Epoch [98/200] Loss: 28.045547485351562\n",
      "Epoch [99/200] Loss: 27.704599380493164\n",
      "Epoch [100/200] Loss: 27.370948791503906\n",
      "Epoch [101/200] Loss: 27.044485092163086\n",
      "Epoch [102/200] Loss: 26.725072860717773\n",
      "Epoch [103/200] Loss: 26.412586212158203\n",
      "Epoch [104/200] Loss: 26.106914520263672\n",
      "Epoch [105/200] Loss: 25.807933807373047\n",
      "Epoch [106/200] Loss: 25.515522003173828\n",
      "Epoch [107/200] Loss: 25.229568481445312\n",
      "Epoch [108/200] Loss: 24.949966430664062\n",
      "Epoch [109/200] Loss: 24.676570892333984\n",
      "Epoch [110/200] Loss: 24.4093074798584\n",
      "Epoch [111/200] Loss: 24.14803123474121\n",
      "Epoch [112/200] Loss: 23.892654418945312\n",
      "Epoch [113/200] Loss: 23.64305877685547\n",
      "Epoch [114/200] Loss: 23.399147033691406\n",
      "Epoch [115/200] Loss: 23.160808563232422\n",
      "Epoch [116/200] Loss: 22.927936553955078\n",
      "Epoch [117/200] Loss: 22.700437545776367\n",
      "Epoch [118/200] Loss: 22.478206634521484\n",
      "Epoch [119/200] Loss: 22.26113510131836\n",
      "Epoch [120/200] Loss: 22.049137115478516\n",
      "Epoch [121/200] Loss: 21.842105865478516\n",
      "Epoch [122/200] Loss: 21.639963150024414\n",
      "Epoch [123/200] Loss: 21.442596435546875\n",
      "Epoch [124/200] Loss: 21.249916076660156\n",
      "Epoch [125/200] Loss: 21.061840057373047\n",
      "Epoch [126/200] Loss: 20.87827491760254\n",
      "Epoch [127/200] Loss: 20.699127197265625\n",
      "Epoch [128/200] Loss: 20.524309158325195\n",
      "Epoch [129/200] Loss: 20.35373878479004\n",
      "Epoch [130/200] Loss: 20.18733024597168\n",
      "Epoch [131/200] Loss: 20.025001525878906\n",
      "Epoch [132/200] Loss: 19.866661071777344\n",
      "Epoch [133/200] Loss: 19.71224594116211\n",
      "Epoch [134/200] Loss: 19.561655044555664\n",
      "Epoch [135/200] Loss: 19.414823532104492\n",
      "Epoch [136/200] Loss: 19.271671295166016\n",
      "Epoch [137/200] Loss: 19.132116317749023\n",
      "Epoch [138/200] Loss: 18.9960880279541\n",
      "Epoch [139/200] Loss: 18.86351776123047\n",
      "Epoch [140/200] Loss: 18.73432159423828\n",
      "Epoch [141/200] Loss: 18.60843276977539\n",
      "Epoch [142/200] Loss: 18.485782623291016\n",
      "Epoch [143/200] Loss: 18.366300582885742\n",
      "Epoch [144/200] Loss: 18.249914169311523\n",
      "Epoch [145/200] Loss: 18.13656234741211\n",
      "Epoch [146/200] Loss: 18.02618408203125\n",
      "Epoch [147/200] Loss: 17.918697357177734\n",
      "Epoch [148/200] Loss: 17.814048767089844\n",
      "Epoch [149/200] Loss: 17.712177276611328\n",
      "Epoch [150/200] Loss: 17.613014221191406\n",
      "Epoch [151/200] Loss: 17.516508102416992\n",
      "Epoch [152/200] Loss: 17.422590255737305\n",
      "Epoch [153/200] Loss: 17.33121109008789\n",
      "Epoch [154/200] Loss: 17.24230194091797\n",
      "Epoch [155/200] Loss: 17.155811309814453\n",
      "Epoch [156/200] Loss: 17.071687698364258\n",
      "Epoch [157/200] Loss: 16.9898681640625\n",
      "Epoch [158/200] Loss: 16.910314559936523\n",
      "Epoch [159/200] Loss: 16.83295249938965\n",
      "Epoch [160/200] Loss: 16.75774574279785\n",
      "Epoch [161/200] Loss: 16.68463706970215\n",
      "Epoch [162/200] Loss: 16.61357879638672\n",
      "Epoch [163/200] Loss: 16.544523239135742\n",
      "Epoch [164/200] Loss: 16.477420806884766\n",
      "Epoch [165/200] Loss: 16.412220001220703\n",
      "Epoch [166/200] Loss: 16.3488826751709\n",
      "Epoch [167/200] Loss: 16.287355422973633\n",
      "Epoch [168/200] Loss: 16.227596282958984\n",
      "Epoch [169/200] Loss: 16.169567108154297\n",
      "Epoch [170/200] Loss: 16.11322021484375\n",
      "Epoch [171/200] Loss: 16.058513641357422\n",
      "Epoch [172/200] Loss: 16.00540542602539\n",
      "Epoch [173/200] Loss: 15.95386028289795\n",
      "Epoch [174/200] Loss: 15.90383529663086\n",
      "Epoch [175/200] Loss: 15.855293273925781\n",
      "Epoch [176/200] Loss: 15.808189392089844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [177/200] Loss: 15.76249885559082\n",
      "Epoch [178/200] Loss: 15.718169212341309\n",
      "Epoch [179/200] Loss: 15.675180435180664\n",
      "Epoch [180/200] Loss: 15.633491516113281\n",
      "Epoch [181/200] Loss: 15.59306526184082\n",
      "Epoch [182/200] Loss: 15.553871154785156\n",
      "Epoch [183/200] Loss: 15.515876770019531\n",
      "Epoch [184/200] Loss: 15.479048728942871\n",
      "Epoch [185/200] Loss: 15.443361282348633\n",
      "Epoch [186/200] Loss: 15.408773422241211\n",
      "Epoch [187/200] Loss: 15.375264167785645\n",
      "Epoch [188/200] Loss: 15.342802047729492\n",
      "Epoch [189/200] Loss: 15.311354637145996\n",
      "Epoch [190/200] Loss: 15.280900955200195\n",
      "Epoch [191/200] Loss: 15.251407623291016\n",
      "Epoch [192/200] Loss: 15.222848892211914\n",
      "Epoch [193/200] Loss: 15.195199012756348\n",
      "Epoch [194/200] Loss: 15.168436050415039\n",
      "Epoch [195/200] Loss: 15.142528533935547\n",
      "Epoch [196/200] Loss: 15.117460250854492\n",
      "Epoch [197/200] Loss: 15.09320068359375\n",
      "Epoch [198/200] Loss: 15.069729804992676\n",
      "Epoch [199/200] Loss: 15.047025680541992\n",
      "Epoch [200/200] Loss: 15.02506160736084\n",
      "Predicted days_remaining for parent_id 289: 11.997457504272461\n",
      "Training for parent_id 299...\n",
      "Epoch [1/200] Loss: 605.2676391601562\n",
      "Epoch [2/200] Loss: 592.02978515625\n",
      "Epoch [3/200] Loss: 579.0841064453125\n",
      "Epoch [4/200] Loss: 566.55615234375\n",
      "Epoch [5/200] Loss: 554.5718994140625\n",
      "Epoch [6/200] Loss: 543.2005615234375\n",
      "Epoch [7/200] Loss: 532.4473876953125\n",
      "Epoch [8/200] Loss: 522.2646484375\n",
      "Epoch [9/200] Loss: 512.5763549804688\n",
      "Epoch [10/200] Loss: 503.30450439453125\n",
      "Epoch [11/200] Loss: 494.3846435546875\n",
      "Epoch [12/200] Loss: 485.771240234375\n",
      "Epoch [13/200] Loss: 477.4382019042969\n",
      "Epoch [14/200] Loss: 469.3755798339844\n",
      "Epoch [15/200] Loss: 461.585693359375\n",
      "Epoch [16/200] Loss: 454.0782165527344\n",
      "Epoch [17/200] Loss: 446.8662414550781\n",
      "Epoch [18/200] Loss: 439.96142578125\n",
      "Epoch [19/200] Loss: 433.37030029296875\n",
      "Epoch [20/200] Loss: 427.092041015625\n",
      "Epoch [21/200] Loss: 421.1178283691406\n",
      "Epoch [22/200] Loss: 415.4322204589844\n",
      "Epoch [23/200] Loss: 410.01568603515625\n",
      "Epoch [24/200] Loss: 404.8476867675781\n",
      "Epoch [25/200] Loss: 399.907958984375\n",
      "Epoch [26/200] Loss: 395.17864990234375\n",
      "Epoch [27/200] Loss: 390.643798828125\n",
      "Epoch [28/200] Loss: 386.2891540527344\n",
      "Epoch [29/200] Loss: 382.1015625\n",
      "Epoch [30/200] Loss: 378.06878662109375\n",
      "Epoch [31/200] Loss: 374.17919921875\n",
      "Epoch [32/200] Loss: 370.4223937988281\n",
      "Epoch [33/200] Loss: 366.7880859375\n",
      "Epoch [34/200] Loss: 363.2667541503906\n",
      "Epoch [35/200] Loss: 359.84912109375\n",
      "Epoch [36/200] Loss: 356.5259094238281\n",
      "Epoch [37/200] Loss: 353.2886657714844\n",
      "Epoch [38/200] Loss: 350.1295166015625\n",
      "Epoch [39/200] Loss: 347.04144287109375\n",
      "Epoch [40/200] Loss: 344.0182189941406\n",
      "Epoch [41/200] Loss: 341.05450439453125\n",
      "Epoch [42/200] Loss: 338.14544677734375\n",
      "Epoch [43/200] Loss: 335.28692626953125\n",
      "Epoch [44/200] Loss: 332.47528076171875\n",
      "Epoch [45/200] Loss: 329.70709228515625\n",
      "Epoch [46/200] Loss: 326.97930908203125\n",
      "Epoch [47/200] Loss: 324.2891845703125\n",
      "Epoch [48/200] Loss: 321.634033203125\n",
      "Epoch [49/200] Loss: 319.01165771484375\n",
      "Epoch [50/200] Loss: 316.4198913574219\n",
      "Epoch [51/200] Loss: 313.8566589355469\n",
      "Epoch [52/200] Loss: 311.3201904296875\n",
      "Epoch [53/200] Loss: 308.80877685546875\n",
      "Epoch [54/200] Loss: 306.3209533691406\n",
      "Epoch [55/200] Loss: 303.855224609375\n",
      "Epoch [56/200] Loss: 301.41033935546875\n",
      "Epoch [57/200] Loss: 298.98529052734375\n",
      "Epoch [58/200] Loss: 296.5790710449219\n",
      "Epoch [59/200] Loss: 294.1907653808594\n",
      "Epoch [60/200] Loss: 291.81988525390625\n",
      "Epoch [61/200] Loss: 289.4659423828125\n",
      "Epoch [62/200] Loss: 287.12847900390625\n",
      "Epoch [63/200] Loss: 284.80743408203125\n",
      "Epoch [64/200] Loss: 282.5027770996094\n",
      "Epoch [65/200] Loss: 280.2144775390625\n",
      "Epoch [66/200] Loss: 277.9425964355469\n",
      "Epoch [67/200] Loss: 275.68731689453125\n",
      "Epoch [68/200] Loss: 273.4487609863281\n",
      "Epoch [69/200] Loss: 271.2269592285156\n",
      "Epoch [70/200] Loss: 269.02215576171875\n",
      "Epoch [71/200] Loss: 266.8343200683594\n",
      "Epoch [72/200] Loss: 264.6634216308594\n",
      "Epoch [73/200] Loss: 262.5096130371094\n",
      "Epoch [74/200] Loss: 260.37274169921875\n",
      "Epoch [75/200] Loss: 258.2528381347656\n",
      "Epoch [76/200] Loss: 256.14984130859375\n",
      "Epoch [77/200] Loss: 254.0635223388672\n",
      "Epoch [78/200] Loss: 251.99395751953125\n",
      "Epoch [79/200] Loss: 249.94093322753906\n",
      "Epoch [80/200] Loss: 247.90431213378906\n",
      "Epoch [81/200] Loss: 245.8840789794922\n",
      "Epoch [82/200] Loss: 243.8799591064453\n",
      "Epoch [83/200] Loss: 241.89193725585938\n",
      "Epoch [84/200] Loss: 239.91986083984375\n",
      "Epoch [85/200] Loss: 237.96353149414062\n",
      "Epoch [86/200] Loss: 236.02285766601562\n",
      "Epoch [87/200] Loss: 234.0977020263672\n",
      "Epoch [88/200] Loss: 232.1878662109375\n",
      "Epoch [89/200] Loss: 230.2933349609375\n",
      "Epoch [90/200] Loss: 228.41387939453125\n",
      "Epoch [91/200] Loss: 226.5494384765625\n",
      "Epoch [92/200] Loss: 224.6997528076172\n",
      "Epoch [93/200] Loss: 222.86474609375\n",
      "Epoch [94/200] Loss: 221.0443572998047\n",
      "Epoch [95/200] Loss: 219.23838806152344\n",
      "Epoch [96/200] Loss: 217.4466094970703\n",
      "Epoch [97/200] Loss: 215.6690673828125\n",
      "Epoch [98/200] Loss: 213.9055938720703\n",
      "Epoch [99/200] Loss: 212.156005859375\n",
      "Epoch [100/200] Loss: 210.42018127441406\n",
      "Epoch [101/200] Loss: 208.697998046875\n",
      "Epoch [102/200] Loss: 206.98934936523438\n",
      "Epoch [103/200] Loss: 205.29412841796875\n",
      "Epoch [104/200] Loss: 203.61221313476562\n",
      "Epoch [105/200] Loss: 201.9434814453125\n",
      "Epoch [106/200] Loss: 200.2877960205078\n",
      "Epoch [107/200] Loss: 198.64503479003906\n",
      "Epoch [108/200] Loss: 197.01513671875\n",
      "Epoch [109/200] Loss: 195.39794921875\n",
      "Epoch [110/200] Loss: 193.7933349609375\n",
      "Epoch [111/200] Loss: 192.2012481689453\n",
      "Epoch [112/200] Loss: 190.62159729003906\n",
      "Epoch [113/200] Loss: 189.0542755126953\n",
      "Epoch [114/200] Loss: 187.49905395507812\n",
      "Epoch [115/200] Loss: 185.9559326171875\n",
      "Epoch [116/200] Loss: 184.4248504638672\n",
      "Epoch [117/200] Loss: 182.9056396484375\n",
      "Epoch [118/200] Loss: 181.3982391357422\n",
      "Epoch [119/200] Loss: 179.9025115966797\n",
      "Epoch [120/200] Loss: 178.4183807373047\n",
      "Epoch [121/200] Loss: 176.94580078125\n",
      "Epoch [122/200] Loss: 175.48460388183594\n",
      "Epoch [123/200] Loss: 174.0347900390625\n",
      "Epoch [124/200] Loss: 172.59620666503906\n",
      "Epoch [125/200] Loss: 171.16876220703125\n",
      "Epoch [126/200] Loss: 169.75238037109375\n",
      "Epoch [127/200] Loss: 168.3470001220703\n",
      "Epoch [128/200] Loss: 166.9525146484375\n",
      "Epoch [129/200] Loss: 165.56884765625\n",
      "Epoch [130/200] Loss: 164.19589233398438\n",
      "Epoch [131/200] Loss: 162.8336639404297\n",
      "Epoch [132/200] Loss: 161.48193359375\n",
      "Epoch [133/200] Loss: 160.1407928466797\n",
      "Epoch [134/200] Loss: 158.80999755859375\n",
      "Epoch [135/200] Loss: 157.48959350585938\n",
      "Epoch [136/200] Loss: 156.1793975830078\n",
      "Epoch [137/200] Loss: 154.87945556640625\n",
      "Epoch [138/200] Loss: 153.5896453857422\n",
      "Epoch [139/200] Loss: 152.3098602294922\n",
      "Epoch [140/200] Loss: 151.04002380371094\n",
      "Epoch [141/200] Loss: 149.7801513671875\n",
      "Epoch [142/200] Loss: 148.5301055908203\n",
      "Epoch [143/200] Loss: 147.28981018066406\n",
      "Epoch [144/200] Loss: 146.05917358398438\n",
      "Epoch [145/200] Loss: 144.83824157714844\n",
      "Epoch [146/200] Loss: 143.62684631347656\n",
      "Epoch [147/200] Loss: 142.42494201660156\n",
      "Epoch [148/200] Loss: 141.23248291015625\n",
      "Epoch [149/200] Loss: 140.0493621826172\n",
      "Epoch [150/200] Loss: 138.8755645751953\n",
      "Epoch [151/200] Loss: 137.7109832763672\n",
      "Epoch [152/200] Loss: 136.55557250976562\n",
      "Epoch [153/200] Loss: 135.40931701660156\n",
      "Epoch [154/200] Loss: 134.2720947265625\n",
      "Epoch [155/200] Loss: 133.14381408691406\n",
      "Epoch [156/200] Loss: 132.02450561523438\n",
      "Epoch [157/200] Loss: 130.91409301757812\n",
      "Epoch [158/200] Loss: 129.8124237060547\n",
      "Epoch [159/200] Loss: 128.7195281982422\n",
      "Epoch [160/200] Loss: 127.63533782958984\n",
      "Epoch [161/200] Loss: 126.55973815917969\n",
      "Epoch [162/200] Loss: 125.49278259277344\n",
      "Epoch [163/200] Loss: 124.43428039550781\n",
      "Epoch [164/200] Loss: 123.38426971435547\n",
      "Epoch [165/200] Loss: 122.34261322021484\n",
      "Epoch [166/200] Loss: 121.3093490600586\n",
      "Epoch [167/200] Loss: 120.28438568115234\n",
      "Epoch [168/200] Loss: 119.26763916015625\n",
      "Epoch [169/200] Loss: 118.25904083251953\n",
      "Epoch [170/200] Loss: 117.25862121582031\n",
      "Epoch [171/200] Loss: 116.26627349853516\n",
      "Epoch [172/200] Loss: 115.28189849853516\n",
      "Epoch [173/200] Loss: 114.30552673339844\n",
      "Epoch [174/200] Loss: 113.33708953857422\n",
      "Epoch [175/200] Loss: 112.3764877319336\n",
      "Epoch [176/200] Loss: 111.42369842529297\n",
      "Epoch [177/200] Loss: 110.47867584228516\n",
      "Epoch [178/200] Loss: 109.54135131835938\n",
      "Epoch [179/200] Loss: 108.61168670654297\n",
      "Epoch [180/200] Loss: 107.68961334228516\n",
      "Epoch [181/200] Loss: 106.7751235961914\n",
      "Epoch [182/200] Loss: 105.86811065673828\n",
      "Epoch [183/200] Loss: 104.96857452392578\n",
      "Epoch [184/200] Loss: 104.07644653320312\n",
      "Epoch [185/200] Loss: 103.191650390625\n",
      "Epoch [186/200] Loss: 102.31421661376953\n",
      "Epoch [187/200] Loss: 101.44398498535156\n",
      "Epoch [188/200] Loss: 100.58100128173828\n",
      "Epoch [189/200] Loss: 99.72518157958984\n",
      "Epoch [190/200] Loss: 98.8764877319336\n",
      "Epoch [191/200] Loss: 98.03486633300781\n",
      "Epoch [192/200] Loss: 97.20024108886719\n",
      "Epoch [193/200] Loss: 96.37259674072266\n",
      "Epoch [194/200] Loss: 95.5519027709961\n",
      "Epoch [195/200] Loss: 94.73809051513672\n",
      "Epoch [196/200] Loss: 93.93112182617188\n",
      "Epoch [197/200] Loss: 93.13094329833984\n",
      "Epoch [198/200] Loss: 92.33751678466797\n",
      "Epoch [199/200] Loss: 91.55077362060547\n",
      "Epoch [200/200] Loss: 90.77075958251953\n",
      "Predicted days_remaining for parent_id 299: 15.057487487792969\n",
      "Training for parent_id 302...\n",
      "Epoch [1/200] Loss: 133.404052734375\n",
      "Epoch [2/200] Loss: 128.82066345214844\n",
      "Epoch [3/200] Loss: 124.4046630859375\n",
      "Epoch [4/200] Loss: 120.16509246826172\n",
      "Epoch [5/200] Loss: 116.09577941894531\n",
      "Epoch [6/200] Loss: 112.18214416503906\n",
      "Epoch [7/200] Loss: 108.4108657836914\n",
      "Epoch [8/200] Loss: 104.77021789550781\n",
      "Epoch [9/200] Loss: 101.24990844726562\n",
      "Epoch [10/200] Loss: 97.84221649169922\n",
      "Epoch [11/200] Loss: 94.54290771484375\n",
      "Epoch [12/200] Loss: 91.35091400146484\n",
      "Epoch [13/200] Loss: 88.26751708984375\n",
      "Epoch [14/200] Loss: 85.29528045654297\n",
      "Epoch [15/200] Loss: 82.43719482421875\n",
      "Epoch [16/200] Loss: 79.6957015991211\n",
      "Epoch [17/200] Loss: 77.07209777832031\n",
      "Epoch [18/200] Loss: 74.5662841796875\n",
      "Epoch [19/200] Loss: 72.17704010009766\n",
      "Epoch [20/200] Loss: 69.90223693847656\n",
      "Epoch [21/200] Loss: 67.7389907836914\n",
      "Epoch [22/200] Loss: 65.68388366699219\n",
      "Epoch [23/200] Loss: 63.73303985595703\n",
      "Epoch [24/200] Loss: 61.88212966918945\n",
      "Epoch [25/200] Loss: 60.12662124633789\n",
      "Epoch [26/200] Loss: 58.46176528930664\n",
      "Epoch [27/200] Loss: 56.88267135620117\n",
      "Epoch [28/200] Loss: 55.38438415527344\n",
      "Epoch [29/200] Loss: 53.96193313598633\n",
      "Epoch [30/200] Loss: 52.61033630371094\n",
      "Epoch [31/200] Loss: 51.32476806640625\n",
      "Epoch [32/200] Loss: 50.10044479370117\n",
      "Epoch [33/200] Loss: 48.932823181152344\n",
      "Epoch [34/200] Loss: 47.817501068115234\n",
      "Epoch [35/200] Loss: 46.750492095947266\n",
      "Epoch [36/200] Loss: 45.72799301147461\n",
      "Epoch [37/200] Loss: 44.746612548828125\n",
      "Epoch [38/200] Loss: 43.80327224731445\n",
      "Epoch [39/200] Loss: 42.89519500732422\n",
      "Epoch [40/200] Loss: 42.019920349121094\n",
      "Epoch [41/200] Loss: 41.175254821777344\n",
      "Epoch [42/200] Loss: 40.35923767089844\n",
      "Epoch [43/200] Loss: 39.57016372680664\n",
      "Epoch [44/200] Loss: 38.80646514892578\n",
      "Epoch [45/200] Loss: 38.066776275634766\n",
      "Epoch [46/200] Loss: 37.349853515625\n",
      "Epoch [47/200] Loss: 36.65460968017578\n",
      "Epoch [48/200] Loss: 35.980064392089844\n",
      "Epoch [49/200] Loss: 35.32529830932617\n",
      "Epoch [50/200] Loss: 34.689491271972656\n",
      "Epoch [51/200] Loss: 34.071937561035156\n",
      "Epoch [52/200] Loss: 33.471920013427734\n",
      "Epoch [53/200] Loss: 32.888851165771484\n",
      "Epoch [54/200] Loss: 32.32212829589844\n",
      "Epoch [55/200] Loss: 31.77122688293457\n",
      "Epoch [56/200] Loss: 31.23566246032715\n",
      "Epoch [57/200] Loss: 30.714950561523438\n",
      "Epoch [58/200] Loss: 30.208663940429688\n",
      "Epoch [59/200] Loss: 29.71638298034668\n",
      "Epoch [60/200] Loss: 29.237720489501953\n",
      "Epoch [61/200] Loss: 28.772296905517578\n",
      "Epoch [62/200] Loss: 28.31976318359375\n",
      "Epoch [63/200] Loss: 27.879770278930664\n",
      "Epoch [64/200] Loss: 27.451993942260742\n",
      "Epoch [65/200] Loss: 27.0361270904541\n",
      "Epoch [66/200] Loss: 26.631858825683594\n",
      "Epoch [67/200] Loss: 26.2388858795166\n",
      "Epoch [68/200] Loss: 25.856935501098633\n",
      "Epoch [69/200] Loss: 25.485729217529297\n",
      "Epoch [70/200] Loss: 25.1250057220459\n",
      "Epoch [71/200] Loss: 24.774492263793945\n",
      "Epoch [72/200] Loss: 24.433944702148438\n",
      "Epoch [73/200] Loss: 24.103120803833008\n",
      "Epoch [74/200] Loss: 23.78177261352539\n",
      "Epoch [75/200] Loss: 23.46967315673828\n",
      "Epoch [76/200] Loss: 23.166584014892578\n",
      "Epoch [77/200] Loss: 22.87229347229004\n",
      "Epoch [78/200] Loss: 22.586584091186523\n",
      "Epoch [79/200] Loss: 22.309236526489258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/200] Loss: 22.040058135986328\n",
      "Epoch [81/200] Loss: 21.778827667236328\n",
      "Epoch [82/200] Loss: 21.525352478027344\n",
      "Epoch [83/200] Loss: 21.279449462890625\n",
      "Epoch [84/200] Loss: 21.040924072265625\n",
      "Epoch [85/200] Loss: 20.809589385986328\n",
      "Epoch [86/200] Loss: 20.58525276184082\n",
      "Epoch [87/200] Loss: 20.36775779724121\n",
      "Epoch [88/200] Loss: 20.15692138671875\n",
      "Epoch [89/200] Loss: 19.952573776245117\n",
      "Epoch [90/200] Loss: 19.754554748535156\n",
      "Epoch [91/200] Loss: 19.562681198120117\n",
      "Epoch [92/200] Loss: 19.37682342529297\n",
      "Epoch [93/200] Loss: 19.196802139282227\n",
      "Epoch [94/200] Loss: 19.022480010986328\n",
      "Epoch [95/200] Loss: 18.853700637817383\n",
      "Epoch [96/200] Loss: 18.690311431884766\n",
      "Epoch [97/200] Loss: 18.532176971435547\n",
      "Epoch [98/200] Loss: 18.37916374206543\n",
      "Epoch [99/200] Loss: 18.23111915588379\n",
      "Epoch [100/200] Loss: 18.087913513183594\n",
      "Epoch [101/200] Loss: 17.949424743652344\n",
      "Epoch [102/200] Loss: 17.81551742553711\n",
      "Epoch [103/200] Loss: 17.686054229736328\n",
      "Epoch [104/200] Loss: 17.560928344726562\n",
      "Epoch [105/200] Loss: 17.440017700195312\n",
      "Epoch [106/200] Loss: 17.323192596435547\n",
      "Epoch [107/200] Loss: 17.210351943969727\n",
      "Epoch [108/200] Loss: 17.101375579833984\n",
      "Epoch [109/200] Loss: 16.99614715576172\n",
      "Epoch [110/200] Loss: 16.89456558227539\n",
      "Epoch [111/200] Loss: 16.79652214050293\n",
      "Epoch [112/200] Loss: 16.701921463012695\n",
      "Epoch [113/200] Loss: 16.61064910888672\n",
      "Epoch [114/200] Loss: 16.52261734008789\n",
      "Epoch [115/200] Loss: 16.437721252441406\n",
      "Epoch [116/200] Loss: 16.355876922607422\n",
      "Epoch [117/200] Loss: 16.276981353759766\n",
      "Epoch [118/200] Loss: 16.20095443725586\n",
      "Epoch [119/200] Loss: 16.127700805664062\n",
      "Epoch [120/200] Loss: 16.057140350341797\n",
      "Epoch [121/200] Loss: 15.989181518554688\n",
      "Epoch [122/200] Loss: 15.923749923706055\n",
      "Epoch [123/200] Loss: 15.860772132873535\n",
      "Epoch [124/200] Loss: 15.800149917602539\n",
      "Epoch [125/200] Loss: 15.741825103759766\n",
      "Epoch [126/200] Loss: 15.685721397399902\n",
      "Epoch [127/200] Loss: 15.631759643554688\n",
      "Epoch [128/200] Loss: 15.579874038696289\n",
      "Epoch [129/200] Loss: 15.529998779296875\n",
      "Epoch [130/200] Loss: 15.48206615447998\n",
      "Epoch [131/200] Loss: 15.436010360717773\n",
      "Epoch [132/200] Loss: 15.391764640808105\n",
      "Epoch [133/200] Loss: 15.349272727966309\n",
      "Epoch [134/200] Loss: 15.30847454071045\n",
      "Epoch [135/200] Loss: 15.269309997558594\n",
      "Epoch [136/200] Loss: 15.23172378540039\n",
      "Epoch [137/200] Loss: 15.195660591125488\n",
      "Epoch [138/200] Loss: 15.161064147949219\n",
      "Epoch [139/200] Loss: 15.127890586853027\n",
      "Epoch [140/200] Loss: 15.096078872680664\n",
      "Epoch [141/200] Loss: 15.06558895111084\n",
      "Epoch [142/200] Loss: 15.036368370056152\n",
      "Epoch [143/200] Loss: 15.008373260498047\n",
      "Epoch [144/200] Loss: 14.981559753417969\n",
      "Epoch [145/200] Loss: 14.955879211425781\n",
      "Epoch [146/200] Loss: 14.931295394897461\n",
      "Epoch [147/200] Loss: 14.907766342163086\n",
      "Epoch [148/200] Loss: 14.885249137878418\n",
      "Epoch [149/200] Loss: 14.86370849609375\n",
      "Epoch [150/200] Loss: 14.843107223510742\n",
      "Epoch [151/200] Loss: 14.823408126831055\n",
      "Epoch [152/200] Loss: 14.804576873779297\n",
      "Epoch [153/200] Loss: 14.786579132080078\n",
      "Epoch [154/200] Loss: 14.769383430480957\n",
      "Epoch [155/200] Loss: 14.752958297729492\n",
      "Epoch [156/200] Loss: 14.737271308898926\n",
      "Epoch [157/200] Loss: 14.722293853759766\n",
      "Epoch [158/200] Loss: 14.708000183105469\n",
      "Epoch [159/200] Loss: 14.694358825683594\n",
      "Epoch [160/200] Loss: 14.681346893310547\n",
      "Epoch [161/200] Loss: 14.668933868408203\n",
      "Epoch [162/200] Loss: 14.657099723815918\n",
      "Epoch [163/200] Loss: 14.645817756652832\n",
      "Epoch [164/200] Loss: 14.635066986083984\n",
      "Epoch [165/200] Loss: 14.624822616577148\n",
      "Epoch [166/200] Loss: 14.615066528320312\n",
      "Epoch [167/200] Loss: 14.605775833129883\n",
      "Epoch [168/200] Loss: 14.596929550170898\n",
      "Epoch [169/200] Loss: 14.588509559631348\n",
      "Epoch [170/200] Loss: 14.580497741699219\n",
      "Epoch [171/200] Loss: 14.572877883911133\n",
      "Epoch [172/200] Loss: 14.565629959106445\n",
      "Epoch [173/200] Loss: 14.55873966217041\n",
      "Epoch [174/200] Loss: 14.552190780639648\n",
      "Epoch [175/200] Loss: 14.545965194702148\n",
      "Epoch [176/200] Loss: 14.54005241394043\n",
      "Epoch [177/200] Loss: 14.534436225891113\n",
      "Epoch [178/200] Loss: 14.529106140136719\n",
      "Epoch [179/200] Loss: 14.524044036865234\n",
      "Epoch [180/200] Loss: 14.51923942565918\n",
      "Epoch [181/200] Loss: 14.51468276977539\n",
      "Epoch [182/200] Loss: 14.510360717773438\n",
      "Epoch [183/200] Loss: 14.506261825561523\n",
      "Epoch [184/200] Loss: 14.502376556396484\n",
      "Epoch [185/200] Loss: 14.498695373535156\n",
      "Epoch [186/200] Loss: 14.495206832885742\n",
      "Epoch [187/200] Loss: 14.491902351379395\n",
      "Epoch [188/200] Loss: 14.48877239227295\n",
      "Epoch [189/200] Loss: 14.485811233520508\n",
      "Epoch [190/200] Loss: 14.48300838470459\n",
      "Epoch [191/200] Loss: 14.480354309082031\n",
      "Epoch [192/200] Loss: 14.477846145629883\n",
      "Epoch [193/200] Loss: 14.47547435760498\n",
      "Epoch [194/200] Loss: 14.47323226928711\n",
      "Epoch [195/200] Loss: 14.471111297607422\n",
      "Epoch [196/200] Loss: 14.469110488891602\n",
      "Epoch [197/200] Loss: 14.467218399047852\n",
      "Epoch [198/200] Loss: 14.465431213378906\n",
      "Epoch [199/200] Loss: 14.4637451171875\n",
      "Epoch [200/200] Loss: 14.46215534210205\n",
      "Predicted days_remaining for parent_id 302: 10.597837448120117\n",
      "Training for parent_id 310...\n",
      "Epoch [1/200] Loss: 3015.35888671875\n",
      "Epoch [2/200] Loss: 2983.0283203125\n",
      "Epoch [3/200] Loss: 2950.91796875\n",
      "Epoch [4/200] Loss: 2919.499267578125\n",
      "Epoch [5/200] Loss: 2889.2900390625\n",
      "Epoch [6/200] Loss: 2860.649169921875\n",
      "Epoch [7/200] Loss: 2833.7197265625\n",
      "Epoch [8/200] Loss: 2808.448486328125\n",
      "Epoch [9/200] Loss: 2784.66259765625\n",
      "Epoch [10/200] Loss: 2762.171630859375\n",
      "Epoch [11/200] Loss: 2740.81591796875\n",
      "Epoch [12/200] Loss: 2720.4755859375\n",
      "Epoch [13/200] Loss: 2701.062255859375\n",
      "Epoch [14/200] Loss: 2682.51123046875\n",
      "Epoch [15/200] Loss: 2664.78173828125\n",
      "Epoch [16/200] Loss: 2647.852783203125\n",
      "Epoch [17/200] Loss: 2631.718994140625\n",
      "Epoch [18/200] Loss: 2616.379638671875\n",
      "Epoch [19/200] Loss: 2601.832763671875\n",
      "Epoch [20/200] Loss: 2588.06640625\n",
      "Epoch [21/200] Loss: 2575.05615234375\n",
      "Epoch [22/200] Loss: 2562.75927734375\n",
      "Epoch [23/200] Loss: 2551.120361328125\n",
      "Epoch [24/200] Loss: 2540.073486328125\n",
      "Epoch [25/200] Loss: 2529.550048828125\n",
      "Epoch [26/200] Loss: 2519.48193359375\n",
      "Epoch [27/200] Loss: 2509.80810546875\n",
      "Epoch [28/200] Loss: 2500.47265625\n",
      "Epoch [29/200] Loss: 2491.4296875\n",
      "Epoch [30/200] Loss: 2482.6376953125\n",
      "Epoch [31/200] Loss: 2474.06005859375\n",
      "Epoch [32/200] Loss: 2465.6669921875\n",
      "Epoch [33/200] Loss: 2457.43017578125\n",
      "Epoch [34/200] Loss: 2449.3271484375\n",
      "Epoch [35/200] Loss: 2441.337890625\n",
      "Epoch [36/200] Loss: 2433.44677734375\n",
      "Epoch [37/200] Loss: 2425.639892578125\n",
      "Epoch [38/200] Loss: 2417.90673828125\n",
      "Epoch [39/200] Loss: 2410.237548828125\n",
      "Epoch [40/200] Loss: 2402.625732421875\n",
      "Epoch [41/200] Loss: 2395.065185546875\n",
      "Epoch [42/200] Loss: 2387.551513671875\n",
      "Epoch [43/200] Loss: 2380.081787109375\n",
      "Epoch [44/200] Loss: 2372.653076171875\n",
      "Epoch [45/200] Loss: 2365.263916015625\n",
      "Epoch [46/200] Loss: 2357.912109375\n",
      "Epoch [47/200] Loss: 2350.59765625\n",
      "Epoch [48/200] Loss: 2343.318359375\n",
      "Epoch [49/200] Loss: 2336.0751953125\n",
      "Epoch [50/200] Loss: 2328.8662109375\n",
      "Epoch [51/200] Loss: 2321.691650390625\n",
      "Epoch [52/200] Loss: 2314.550537109375\n",
      "Epoch [53/200] Loss: 2307.4423828125\n",
      "Epoch [54/200] Loss: 2300.36669921875\n",
      "Epoch [55/200] Loss: 2293.322021484375\n",
      "Epoch [56/200] Loss: 2286.308349609375\n",
      "Epoch [57/200] Loss: 2279.32470703125\n",
      "Epoch [58/200] Loss: 2272.3701171875\n",
      "Epoch [59/200] Loss: 2265.44384765625\n",
      "Epoch [60/200] Loss: 2258.54541015625\n",
      "Epoch [61/200] Loss: 2251.67333984375\n",
      "Epoch [62/200] Loss: 2244.82763671875\n",
      "Epoch [63/200] Loss: 2238.00732421875\n",
      "Epoch [64/200] Loss: 2231.2119140625\n",
      "Epoch [65/200] Loss: 2224.440185546875\n",
      "Epoch [66/200] Loss: 2217.692138671875\n",
      "Epoch [67/200] Loss: 2210.966064453125\n",
      "Epoch [68/200] Loss: 2204.26220703125\n",
      "Epoch [69/200] Loss: 2197.578857421875\n",
      "Epoch [70/200] Loss: 2190.916259765625\n",
      "Epoch [71/200] Loss: 2184.2734375\n",
      "Epoch [72/200] Loss: 2177.64990234375\n",
      "Epoch [73/200] Loss: 2171.045654296875\n",
      "Epoch [74/200] Loss: 2164.46044921875\n",
      "Epoch [75/200] Loss: 2157.894775390625\n",
      "Epoch [76/200] Loss: 2151.348388671875\n",
      "Epoch [77/200] Loss: 2144.82177734375\n",
      "Epoch [78/200] Loss: 2138.3154296875\n",
      "Epoch [79/200] Loss: 2131.829345703125\n",
      "Epoch [80/200] Loss: 2125.364013671875\n",
      "Epoch [81/200] Loss: 2118.919677734375\n",
      "Epoch [82/200] Loss: 2112.49609375\n",
      "Epoch [83/200] Loss: 2106.09423828125\n",
      "Epoch [84/200] Loss: 2099.7138671875\n",
      "Epoch [85/200] Loss: 2093.354736328125\n",
      "Epoch [86/200] Loss: 2087.0166015625\n",
      "Epoch [87/200] Loss: 2080.700439453125\n",
      "Epoch [88/200] Loss: 2074.405517578125\n",
      "Epoch [89/200] Loss: 2068.132080078125\n",
      "Epoch [90/200] Loss: 2061.8798828125\n",
      "Epoch [91/200] Loss: 2055.648193359375\n",
      "Epoch [92/200] Loss: 2049.437744140625\n",
      "Epoch [93/200] Loss: 2043.2489013671875\n",
      "Epoch [94/200] Loss: 2037.079833984375\n",
      "Epoch [95/200] Loss: 2030.93212890625\n",
      "Epoch [96/200] Loss: 2024.8040771484375\n",
      "Epoch [97/200] Loss: 2018.6971435546875\n",
      "Epoch [98/200] Loss: 2012.610107421875\n",
      "Epoch [99/200] Loss: 2006.5430908203125\n",
      "Epoch [100/200] Loss: 2000.496337890625\n",
      "Epoch [101/200] Loss: 1994.4688720703125\n",
      "Epoch [102/200] Loss: 1988.4613037109375\n",
      "Epoch [103/200] Loss: 1982.4732666015625\n",
      "Epoch [104/200] Loss: 1976.5042724609375\n",
      "Epoch [105/200] Loss: 1970.5546875\n",
      "Epoch [106/200] Loss: 1964.6239013671875\n",
      "Epoch [107/200] Loss: 1958.712158203125\n",
      "Epoch [108/200] Loss: 1952.8192138671875\n",
      "Epoch [109/200] Loss: 1946.945068359375\n",
      "Epoch [110/200] Loss: 1941.08935546875\n",
      "Epoch [111/200] Loss: 1935.251708984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [112/200] Loss: 1929.4326171875\n",
      "Epoch [113/200] Loss: 1923.6312255859375\n",
      "Epoch [114/200] Loss: 1917.8480224609375\n",
      "Epoch [115/200] Loss: 1912.0826416015625\n",
      "Epoch [116/200] Loss: 1906.3350830078125\n",
      "Epoch [117/200] Loss: 1900.604736328125\n",
      "Epoch [118/200] Loss: 1894.89208984375\n",
      "Epoch [119/200] Loss: 1889.196533203125\n",
      "Epoch [120/200] Loss: 1883.5185546875\n",
      "Epoch [121/200] Loss: 1877.8572998046875\n",
      "Epoch [122/200] Loss: 1872.213134765625\n",
      "Epoch [123/200] Loss: 1866.5860595703125\n",
      "Epoch [124/200] Loss: 1860.9755859375\n",
      "Epoch [125/200] Loss: 1855.381591796875\n",
      "Epoch [126/200] Loss: 1849.803955078125\n",
      "Epoch [127/200] Loss: 1844.2432861328125\n",
      "Epoch [128/200] Loss: 1838.698486328125\n",
      "Epoch [129/200] Loss: 1833.1700439453125\n",
      "Epoch [130/200] Loss: 1827.657958984375\n",
      "Epoch [131/200] Loss: 1822.1611328125\n",
      "Epoch [132/200] Loss: 1816.6806640625\n",
      "Epoch [133/200] Loss: 1811.216064453125\n",
      "Epoch [134/200] Loss: 1805.76708984375\n",
      "Epoch [135/200] Loss: 1800.3336181640625\n",
      "Epoch [136/200] Loss: 1794.9158935546875\n",
      "Epoch [137/200] Loss: 1789.513671875\n",
      "Epoch [138/200] Loss: 1784.126708984375\n",
      "Epoch [139/200] Loss: 1778.754638671875\n",
      "Epoch [140/200] Loss: 1773.398193359375\n",
      "Epoch [141/200] Loss: 1768.0567626953125\n",
      "Epoch [142/200] Loss: 1762.730224609375\n",
      "Epoch [143/200] Loss: 1757.41845703125\n",
      "Epoch [144/200] Loss: 1752.1220703125\n",
      "Epoch [145/200] Loss: 1746.8399658203125\n",
      "Epoch [146/200] Loss: 1741.572998046875\n",
      "Epoch [147/200] Loss: 1736.3203125\n",
      "Epoch [148/200] Loss: 1731.0821533203125\n",
      "Epoch [149/200] Loss: 1725.8585205078125\n",
      "Epoch [150/200] Loss: 1720.649658203125\n",
      "Epoch [151/200] Loss: 1715.45458984375\n",
      "Epoch [152/200] Loss: 1710.2738037109375\n",
      "Epoch [153/200] Loss: 1705.1075439453125\n",
      "Epoch [154/200] Loss: 1699.95556640625\n",
      "Epoch [155/200] Loss: 1694.817138671875\n",
      "Epoch [156/200] Loss: 1689.6929931640625\n",
      "Epoch [157/200] Loss: 1684.5826416015625\n",
      "Epoch [158/200] Loss: 1679.4864501953125\n",
      "Epoch [159/200] Loss: 1674.4039306640625\n",
      "Epoch [160/200] Loss: 1669.3349609375\n",
      "Epoch [161/200] Loss: 1664.2801513671875\n",
      "Epoch [162/200] Loss: 1659.2384033203125\n",
      "Epoch [163/200] Loss: 1654.21044921875\n",
      "Epoch [164/200] Loss: 1649.1962890625\n",
      "Epoch [165/200] Loss: 1644.195556640625\n",
      "Epoch [166/200] Loss: 1639.2080078125\n",
      "Epoch [167/200] Loss: 1634.23388671875\n",
      "Epoch [168/200] Loss: 1629.2728271484375\n",
      "Epoch [169/200] Loss: 1624.3251953125\n",
      "Epoch [170/200] Loss: 1619.390625\n",
      "Epoch [171/200] Loss: 1614.4696044921875\n",
      "Epoch [172/200] Loss: 1609.5615234375\n",
      "Epoch [173/200] Loss: 1604.6663818359375\n",
      "Epoch [174/200] Loss: 1599.7840576171875\n",
      "Epoch [175/200] Loss: 1594.914794921875\n",
      "Epoch [176/200] Loss: 1590.05859375\n",
      "Epoch [177/200] Loss: 1585.215087890625\n",
      "Epoch [178/200] Loss: 1580.3846435546875\n",
      "Epoch [179/200] Loss: 1575.566650390625\n",
      "Epoch [180/200] Loss: 1570.761474609375\n",
      "Epoch [181/200] Loss: 1565.968994140625\n",
      "Epoch [182/200] Loss: 1561.1890869140625\n",
      "Epoch [183/200] Loss: 1556.4215087890625\n",
      "Epoch [184/200] Loss: 1551.666748046875\n",
      "Epoch [185/200] Loss: 1546.924560546875\n",
      "Epoch [186/200] Loss: 1542.194580078125\n",
      "Epoch [187/200] Loss: 1537.4769287109375\n",
      "Epoch [188/200] Loss: 1532.77197265625\n",
      "Epoch [189/200] Loss: 1528.079345703125\n",
      "Epoch [190/200] Loss: 1523.398681640625\n",
      "Epoch [191/200] Loss: 1518.7305908203125\n",
      "Epoch [192/200] Loss: 1514.0743408203125\n",
      "Epoch [193/200] Loss: 1509.430419921875\n",
      "Epoch [194/200] Loss: 1504.798583984375\n",
      "Epoch [195/200] Loss: 1500.1790771484375\n",
      "Epoch [196/200] Loss: 1495.571533203125\n",
      "Epoch [197/200] Loss: 1490.9757080078125\n",
      "Epoch [198/200] Loss: 1486.3919677734375\n",
      "Epoch [199/200] Loss: 1481.8204345703125\n",
      "Epoch [200/200] Loss: 1477.2603759765625\n",
      "Predicted days_remaining for parent_id 310: 16.562633514404297\n",
      "Training for parent_id 312...\n",
      "Epoch [1/200] Loss: 1304.2156982421875\n",
      "Epoch [2/200] Loss: 1285.377197265625\n",
      "Epoch [3/200] Loss: 1267.023193359375\n",
      "Epoch [4/200] Loss: 1249.269287109375\n",
      "Epoch [5/200] Loss: 1232.171142578125\n",
      "Epoch [6/200] Loss: 1215.754638671875\n",
      "Epoch [7/200] Loss: 1200.0240478515625\n",
      "Epoch [8/200] Loss: 1184.9677734375\n",
      "Epoch [9/200] Loss: 1170.562744140625\n",
      "Epoch [10/200] Loss: 1156.777587890625\n",
      "Epoch [11/200] Loss: 1143.576416015625\n",
      "Epoch [12/200] Loss: 1130.9232177734375\n",
      "Epoch [13/200] Loss: 1118.7884521484375\n",
      "Epoch [14/200] Loss: 1107.1468505859375\n",
      "Epoch [15/200] Loss: 1095.97900390625\n",
      "Epoch [16/200] Loss: 1085.2685546875\n",
      "Epoch [17/200] Loss: 1075.0010986328125\n",
      "Epoch [18/200] Loss: 1065.163818359375\n",
      "Epoch [19/200] Loss: 1055.7454833984375\n",
      "Epoch [20/200] Loss: 1046.734619140625\n",
      "Epoch [21/200] Loss: 1038.120361328125\n",
      "Epoch [22/200] Loss: 1029.8896484375\n",
      "Epoch [23/200] Loss: 1022.0283203125\n",
      "Epoch [24/200] Loss: 1014.5192260742188\n",
      "Epoch [25/200] Loss: 1007.3427124023438\n",
      "Epoch [26/200] Loss: 1000.4768676757812\n",
      "Epoch [27/200] Loss: 993.8977661132812\n",
      "Epoch [28/200] Loss: 987.5802612304688\n",
      "Epoch [29/200] Loss: 981.4988403320312\n",
      "Epoch [30/200] Loss: 975.6285400390625\n",
      "Epoch [31/200] Loss: 969.945556640625\n",
      "Epoch [32/200] Loss: 964.427490234375\n",
      "Epoch [33/200] Loss: 959.0540771484375\n",
      "Epoch [34/200] Loss: 953.806884765625\n",
      "Epoch [35/200] Loss: 948.6701049804688\n",
      "Epoch [36/200] Loss: 943.6294555664062\n",
      "Epoch [37/200] Loss: 938.6727294921875\n",
      "Epoch [38/200] Loss: 933.7897338867188\n",
      "Epoch [39/200] Loss: 928.9710693359375\n",
      "Epoch [40/200] Loss: 924.2096557617188\n",
      "Epoch [41/200] Loss: 919.4991455078125\n",
      "Epoch [42/200] Loss: 914.8342895507812\n",
      "Epoch [43/200] Loss: 910.2111206054688\n",
      "Epoch [44/200] Loss: 905.62646484375\n",
      "Epoch [45/200] Loss: 901.0780029296875\n",
      "Epoch [46/200] Loss: 896.5635375976562\n",
      "Epoch [47/200] Loss: 892.0821533203125\n",
      "Epoch [48/200] Loss: 887.632568359375\n",
      "Epoch [49/200] Loss: 883.2139892578125\n",
      "Epoch [50/200] Loss: 878.825927734375\n",
      "Epoch [51/200] Loss: 874.4675903320312\n",
      "Epoch [52/200] Loss: 870.1385498046875\n",
      "Epoch [53/200] Loss: 865.83837890625\n",
      "Epoch [54/200] Loss: 861.5662841796875\n",
      "Epoch [55/200] Loss: 857.32177734375\n",
      "Epoch [56/200] Loss: 853.1046752929688\n",
      "Epoch [57/200] Loss: 848.9141235351562\n",
      "Epoch [58/200] Loss: 844.7496337890625\n",
      "Epoch [59/200] Loss: 840.61083984375\n",
      "Epoch [60/200] Loss: 836.4970703125\n",
      "Epoch [61/200] Loss: 832.4080200195312\n",
      "Epoch [62/200] Loss: 828.34326171875\n",
      "Epoch [63/200] Loss: 824.30224609375\n",
      "Epoch [64/200] Loss: 820.2847290039062\n",
      "Epoch [65/200] Loss: 816.2901000976562\n",
      "Epoch [66/200] Loss: 812.3182373046875\n",
      "Epoch [67/200] Loss: 808.3685913085938\n",
      "Epoch [68/200] Loss: 804.4409790039062\n",
      "Epoch [69/200] Loss: 800.5350341796875\n",
      "Epoch [70/200] Loss: 796.6505126953125\n",
      "Epoch [71/200] Loss: 792.786865234375\n",
      "Epoch [72/200] Loss: 788.944091796875\n",
      "Epoch [73/200] Loss: 785.1218872070312\n",
      "Epoch [74/200] Loss: 781.3198852539062\n",
      "Epoch [75/200] Loss: 777.537841796875\n",
      "Epoch [76/200] Loss: 773.7754516601562\n",
      "Epoch [77/200] Loss: 770.0326538085938\n",
      "Epoch [78/200] Loss: 766.308837890625\n",
      "Epoch [79/200] Loss: 762.6043090820312\n",
      "Epoch [80/200] Loss: 758.91845703125\n",
      "Epoch [81/200] Loss: 755.251220703125\n",
      "Epoch [82/200] Loss: 751.6025390625\n",
      "Epoch [83/200] Loss: 747.9718627929688\n",
      "Epoch [84/200] Loss: 744.3591918945312\n",
      "Epoch [85/200] Loss: 740.7642211914062\n",
      "Epoch [86/200] Loss: 737.1871337890625\n",
      "Epoch [87/200] Loss: 733.6273193359375\n",
      "Epoch [88/200] Loss: 730.084716796875\n",
      "Epoch [89/200] Loss: 726.5592041015625\n",
      "Epoch [90/200] Loss: 723.0506591796875\n",
      "Epoch [91/200] Loss: 719.5589599609375\n",
      "Epoch [92/200] Loss: 716.0836791992188\n",
      "Epoch [93/200] Loss: 712.6251831054688\n",
      "Epoch [94/200] Loss: 709.1826171875\n",
      "Epoch [95/200] Loss: 705.7562866210938\n",
      "Epoch [96/200] Loss: 702.3461303710938\n",
      "Epoch [97/200] Loss: 698.9517211914062\n",
      "Epoch [98/200] Loss: 695.5731201171875\n",
      "Epoch [99/200] Loss: 692.2101440429688\n",
      "Epoch [100/200] Loss: 688.8626708984375\n",
      "Epoch [101/200] Loss: 685.5305786132812\n",
      "Epoch [102/200] Loss: 682.2136840820312\n",
      "Epoch [103/200] Loss: 678.911865234375\n",
      "Epoch [104/200] Loss: 675.6251220703125\n",
      "Epoch [105/200] Loss: 672.3532104492188\n",
      "Epoch [106/200] Loss: 669.09619140625\n",
      "Epoch [107/200] Loss: 665.853759765625\n",
      "Epoch [108/200] Loss: 662.6259765625\n",
      "Epoch [109/200] Loss: 659.41259765625\n",
      "Epoch [110/200] Loss: 656.213623046875\n",
      "Epoch [111/200] Loss: 653.0288696289062\n",
      "Epoch [112/200] Loss: 649.8582763671875\n",
      "Epoch [113/200] Loss: 646.701904296875\n",
      "Epoch [114/200] Loss: 643.5593872070312\n",
      "Epoch [115/200] Loss: 640.4308471679688\n",
      "Epoch [116/200] Loss: 637.3161010742188\n",
      "Epoch [117/200] Loss: 634.215087890625\n",
      "Epoch [118/200] Loss: 631.1276245117188\n",
      "Epoch [119/200] Loss: 628.0537719726562\n",
      "Epoch [120/200] Loss: 624.993408203125\n",
      "Epoch [121/200] Loss: 621.9462890625\n",
      "Epoch [122/200] Loss: 618.9126586914062\n",
      "Epoch [123/200] Loss: 615.8922119140625\n",
      "Epoch [124/200] Loss: 612.885009765625\n",
      "Epoch [125/200] Loss: 609.8906860351562\n",
      "Epoch [126/200] Loss: 606.909423828125\n",
      "Epoch [127/200] Loss: 603.9412231445312\n",
      "Epoch [128/200] Loss: 600.9857788085938\n",
      "Epoch [129/200] Loss: 598.0431518554688\n",
      "Epoch [130/200] Loss: 595.11328125\n",
      "Epoch [131/200] Loss: 592.196044921875\n",
      "Epoch [132/200] Loss: 589.2914428710938\n",
      "Epoch [133/200] Loss: 586.3992309570312\n",
      "Epoch [134/200] Loss: 583.5195922851562\n",
      "Epoch [135/200] Loss: 580.65234375\n",
      "Epoch [136/200] Loss: 577.7974243164062\n",
      "Epoch [137/200] Loss: 574.954833984375\n",
      "Epoch [138/200] Loss: 572.1243896484375\n",
      "Epoch [139/200] Loss: 569.3060913085938\n",
      "Epoch [140/200] Loss: 566.4998168945312\n",
      "Epoch [141/200] Loss: 563.705810546875\n",
      "Epoch [142/200] Loss: 560.9235229492188\n",
      "Epoch [143/200] Loss: 558.1533203125\n",
      "Epoch [144/200] Loss: 555.39501953125\n",
      "Epoch [145/200] Loss: 552.6483154296875\n",
      "Epoch [146/200] Loss: 549.91357421875\n",
      "Epoch [147/200] Loss: 547.1903686523438\n",
      "Epoch [148/200] Loss: 544.4789428710938\n",
      "Epoch [149/200] Loss: 541.7791748046875\n",
      "Epoch [150/200] Loss: 539.0908203125\n",
      "Epoch [151/200] Loss: 536.4140014648438\n",
      "Epoch [152/200] Loss: 533.74853515625\n",
      "Epoch [153/200] Loss: 531.0945434570312\n",
      "Epoch [154/200] Loss: 528.451904296875\n",
      "Epoch [155/200] Loss: 525.820556640625\n",
      "Epoch [156/200] Loss: 523.2005004882812\n",
      "Epoch [157/200] Loss: 520.591552734375\n",
      "Epoch [158/200] Loss: 517.9938354492188\n",
      "Epoch [159/200] Loss: 515.4071044921875\n",
      "Epoch [160/200] Loss: 512.8316040039062\n",
      "Epoch [161/200] Loss: 510.2668762207031\n",
      "Epoch [162/200] Loss: 507.7132568359375\n",
      "Epoch [163/200] Loss: 505.1706237792969\n",
      "Epoch [164/200] Loss: 502.6387023925781\n",
      "Epoch [165/200] Loss: 500.11773681640625\n",
      "Epoch [166/200] Loss: 497.6075134277344\n",
      "Epoch [167/200] Loss: 495.1080322265625\n",
      "Epoch [168/200] Loss: 492.6192626953125\n",
      "Epoch [169/200] Loss: 490.1410827636719\n",
      "Epoch [170/200] Loss: 487.6736145019531\n",
      "Epoch [171/200] Loss: 485.21673583984375\n",
      "Epoch [172/200] Loss: 482.7702941894531\n",
      "Epoch [173/200] Loss: 480.3343505859375\n",
      "Epoch [174/200] Loss: 477.90887451171875\n",
      "Epoch [175/200] Loss: 475.4938049316406\n",
      "Epoch [176/200] Loss: 473.0890808105469\n",
      "Epoch [177/200] Loss: 470.6947937011719\n",
      "Epoch [178/200] Loss: 468.3106384277344\n",
      "Epoch [179/200] Loss: 465.9368591308594\n",
      "Epoch [180/200] Loss: 463.57318115234375\n",
      "Epoch [181/200] Loss: 461.2197570800781\n",
      "Epoch [182/200] Loss: 458.87652587890625\n",
      "Epoch [183/200] Loss: 456.543212890625\n",
      "Epoch [184/200] Loss: 454.22003173828125\n",
      "Epoch [185/200] Loss: 451.9068908691406\n",
      "Epoch [186/200] Loss: 449.60369873046875\n",
      "Epoch [187/200] Loss: 447.310546875\n",
      "Epoch [188/200] Loss: 445.0271301269531\n",
      "Epoch [189/200] Loss: 442.75372314453125\n",
      "Epoch [190/200] Loss: 440.49005126953125\n",
      "Epoch [191/200] Loss: 438.236328125\n",
      "Epoch [192/200] Loss: 435.9922790527344\n",
      "Epoch [193/200] Loss: 433.7579040527344\n",
      "Epoch [194/200] Loss: 431.5332946777344\n",
      "Epoch [195/200] Loss: 429.3182678222656\n",
      "Epoch [196/200] Loss: 427.1129455566406\n",
      "Epoch [197/200] Loss: 424.91717529296875\n",
      "Epoch [198/200] Loss: 422.7309265136719\n",
      "Epoch [199/200] Loss: 420.55413818359375\n",
      "Epoch [200/200] Loss: 418.38690185546875\n",
      "Predicted days_remaining for parent_id 312: 15.705259323120117\n",
      "Training for parent_id 315...\n",
      "Epoch [1/200] Loss: 1536.7735595703125\n",
      "Epoch [2/200] Loss: 1518.907470703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200] Loss: 1501.39208984375\n",
      "Epoch [4/200] Loss: 1484.337890625\n",
      "Epoch [5/200] Loss: 1467.809814453125\n",
      "Epoch [6/200] Loss: 1451.8355712890625\n",
      "Epoch [7/200] Loss: 1436.4288330078125\n",
      "Epoch [8/200] Loss: 1421.5802001953125\n",
      "Epoch [9/200] Loss: 1407.2564697265625\n",
      "Epoch [10/200] Loss: 1393.408203125\n",
      "Epoch [11/200] Loss: 1379.987060546875\n",
      "Epoch [12/200] Loss: 1366.9583740234375\n",
      "Epoch [13/200] Loss: 1354.304931640625\n",
      "Epoch [14/200] Loss: 1342.0250244140625\n",
      "Epoch [15/200] Loss: 1330.1270751953125\n",
      "Epoch [16/200] Loss: 1318.6217041015625\n",
      "Epoch [17/200] Loss: 1307.5159912109375\n",
      "Epoch [18/200] Loss: 1296.81005859375\n",
      "Epoch [19/200] Loss: 1286.497802734375\n",
      "Epoch [20/200] Loss: 1276.5675048828125\n",
      "Epoch [21/200] Loss: 1267.0040283203125\n",
      "Epoch [22/200] Loss: 1257.7889404296875\n",
      "Epoch [23/200] Loss: 1248.90380859375\n",
      "Epoch [24/200] Loss: 1240.3291015625\n",
      "Epoch [25/200] Loss: 1232.044189453125\n",
      "Epoch [26/200] Loss: 1224.0301513671875\n",
      "Epoch [27/200] Loss: 1216.2662353515625\n",
      "Epoch [28/200] Loss: 1208.7322998046875\n",
      "Epoch [29/200] Loss: 1201.407958984375\n",
      "Epoch [30/200] Loss: 1194.27392578125\n",
      "Epoch [31/200] Loss: 1187.3118896484375\n",
      "Epoch [32/200] Loss: 1180.505615234375\n",
      "Epoch [33/200] Loss: 1173.8397216796875\n",
      "Epoch [34/200] Loss: 1167.301513671875\n",
      "Epoch [35/200] Loss: 1160.8802490234375\n",
      "Epoch [36/200] Loss: 1154.5665283203125\n",
      "Epoch [37/200] Loss: 1148.352294921875\n",
      "Epoch [38/200] Loss: 1142.2313232421875\n",
      "Epoch [39/200] Loss: 1136.197509765625\n",
      "Epoch [40/200] Loss: 1130.24658203125\n",
      "Epoch [41/200] Loss: 1124.3736572265625\n",
      "Epoch [42/200] Loss: 1118.5753173828125\n",
      "Epoch [43/200] Loss: 1112.8477783203125\n",
      "Epoch [44/200] Loss: 1107.1878662109375\n",
      "Epoch [45/200] Loss: 1101.59228515625\n",
      "Epoch [46/200] Loss: 1096.058349609375\n",
      "Epoch [47/200] Loss: 1090.5831298828125\n",
      "Epoch [48/200] Loss: 1085.16455078125\n",
      "Epoch [49/200] Loss: 1079.799560546875\n",
      "Epoch [50/200] Loss: 1074.486083984375\n",
      "Epoch [51/200] Loss: 1069.2218017578125\n",
      "Epoch [52/200] Loss: 1064.0050048828125\n",
      "Epoch [53/200] Loss: 1058.8338623046875\n",
      "Epoch [54/200] Loss: 1053.7064208984375\n",
      "Epoch [55/200] Loss: 1048.621337890625\n",
      "Epoch [56/200] Loss: 1043.5775146484375\n",
      "Epoch [57/200] Loss: 1038.572998046875\n",
      "Epoch [58/200] Loss: 1033.6070556640625\n",
      "Epoch [59/200] Loss: 1028.67822265625\n",
      "Epoch [60/200] Loss: 1023.7858276367188\n",
      "Epoch [61/200] Loss: 1018.9287109375\n",
      "Epoch [62/200] Loss: 1014.106201171875\n",
      "Epoch [63/200] Loss: 1009.3172607421875\n",
      "Epoch [64/200] Loss: 1004.5611572265625\n",
      "Epoch [65/200] Loss: 999.8370971679688\n",
      "Epoch [66/200] Loss: 995.1447143554688\n",
      "Epoch [67/200] Loss: 990.482666015625\n",
      "Epoch [68/200] Loss: 985.8512573242188\n",
      "Epoch [69/200] Loss: 981.2490844726562\n",
      "Epoch [70/200] Loss: 976.676025390625\n",
      "Epoch [71/200] Loss: 972.1312866210938\n",
      "Epoch [72/200] Loss: 967.6144409179688\n",
      "Epoch [73/200] Loss: 963.1250610351562\n",
      "Epoch [74/200] Loss: 958.6626586914062\n",
      "Epoch [75/200] Loss: 954.2266235351562\n",
      "Epoch [76/200] Loss: 949.8165283203125\n",
      "Epoch [77/200] Loss: 945.4323120117188\n",
      "Epoch [78/200] Loss: 941.0730590820312\n",
      "Epoch [79/200] Loss: 936.7386474609375\n",
      "Epoch [80/200] Loss: 932.428466796875\n",
      "Epoch [81/200] Loss: 928.1425170898438\n",
      "Epoch [82/200] Loss: 923.8800659179688\n",
      "Epoch [83/200] Loss: 919.6412353515625\n",
      "Epoch [84/200] Loss: 915.4251708984375\n",
      "Epoch [85/200] Loss: 911.231689453125\n",
      "Epoch [86/200] Loss: 907.0606689453125\n",
      "Epoch [87/200] Loss: 902.91162109375\n",
      "Epoch [88/200] Loss: 898.7843627929688\n",
      "Epoch [89/200] Loss: 894.678466796875\n",
      "Epoch [90/200] Loss: 890.59375\n",
      "Epoch [91/200] Loss: 886.5299682617188\n",
      "Epoch [92/200] Loss: 882.4868774414062\n",
      "Epoch [93/200] Loss: 878.464111328125\n",
      "Epoch [94/200] Loss: 874.4613647460938\n",
      "Epoch [95/200] Loss: 870.478515625\n",
      "Epoch [96/200] Loss: 866.515380859375\n",
      "Epoch [97/200] Loss: 862.5717163085938\n",
      "Epoch [98/200] Loss: 858.6471557617188\n",
      "Epoch [99/200] Loss: 854.74169921875\n",
      "Epoch [100/200] Loss: 850.8550415039062\n",
      "Epoch [101/200] Loss: 846.98681640625\n",
      "Epoch [102/200] Loss: 843.1370849609375\n",
      "Epoch [103/200] Loss: 839.305419921875\n",
      "Epoch [104/200] Loss: 835.4917602539062\n",
      "Epoch [105/200] Loss: 831.6961669921875\n",
      "Epoch [106/200] Loss: 827.9180297851562\n",
      "Epoch [107/200] Loss: 824.1574096679688\n",
      "Epoch [108/200] Loss: 820.4140625\n",
      "Epoch [109/200] Loss: 816.6878051757812\n",
      "Epoch [110/200] Loss: 812.9788208007812\n",
      "Epoch [111/200] Loss: 809.2863159179688\n",
      "Epoch [112/200] Loss: 805.610595703125\n",
      "Epoch [113/200] Loss: 801.951416015625\n",
      "Epoch [114/200] Loss: 798.308837890625\n",
      "Epoch [115/200] Loss: 794.6822509765625\n",
      "Epoch [116/200] Loss: 791.07177734375\n",
      "Epoch [117/200] Loss: 787.4775390625\n",
      "Epoch [118/200] Loss: 783.8990478515625\n",
      "Epoch [119/200] Loss: 780.3361206054688\n",
      "Epoch [120/200] Loss: 776.7889404296875\n",
      "Epoch [121/200] Loss: 773.2572021484375\n",
      "Epoch [122/200] Loss: 769.7406616210938\n",
      "Epoch [123/200] Loss: 766.2396240234375\n",
      "Epoch [124/200] Loss: 762.753662109375\n",
      "Epoch [125/200] Loss: 759.2826538085938\n",
      "Epoch [126/200] Loss: 755.82666015625\n",
      "Epoch [127/200] Loss: 752.3853759765625\n",
      "Epoch [128/200] Loss: 748.958740234375\n",
      "Epoch [129/200] Loss: 745.5469360351562\n",
      "Epoch [130/200] Loss: 742.1493530273438\n",
      "Epoch [131/200] Loss: 738.7664794921875\n",
      "Epoch [132/200] Loss: 735.397705078125\n",
      "Epoch [133/200] Loss: 732.0431518554688\n",
      "Epoch [134/200] Loss: 728.7027587890625\n",
      "Epoch [135/200] Loss: 725.3765869140625\n",
      "Epoch [136/200] Loss: 722.064208984375\n",
      "Epoch [137/200] Loss: 718.765625\n",
      "Epoch [138/200] Loss: 715.48095703125\n",
      "Epoch [139/200] Loss: 712.2098999023438\n",
      "Epoch [140/200] Loss: 708.9525146484375\n",
      "Epoch [141/200] Loss: 705.708740234375\n",
      "Epoch [142/200] Loss: 702.4783935546875\n",
      "Epoch [143/200] Loss: 699.26123046875\n",
      "Epoch [144/200] Loss: 696.0576782226562\n",
      "Epoch [145/200] Loss: 692.8670654296875\n",
      "Epoch [146/200] Loss: 689.6898803710938\n",
      "Epoch [147/200] Loss: 686.5254516601562\n",
      "Epoch [148/200] Loss: 683.3744506835938\n",
      "Epoch [149/200] Loss: 680.236083984375\n",
      "Epoch [150/200] Loss: 677.1107177734375\n",
      "Epoch [151/200] Loss: 673.998046875\n",
      "Epoch [152/200] Loss: 670.8980102539062\n",
      "Epoch [153/200] Loss: 667.8110961914062\n",
      "Epoch [154/200] Loss: 664.736572265625\n",
      "Epoch [155/200] Loss: 661.6744995117188\n",
      "Epoch [156/200] Loss: 658.6249389648438\n",
      "Epoch [157/200] Loss: 655.5877685546875\n",
      "Epoch [158/200] Loss: 652.56298828125\n",
      "Epoch [159/200] Loss: 649.5506591796875\n",
      "Epoch [160/200] Loss: 646.5504760742188\n",
      "Epoch [161/200] Loss: 643.5625\n",
      "Epoch [162/200] Loss: 640.58642578125\n",
      "Epoch [163/200] Loss: 637.6226196289062\n",
      "Epoch [164/200] Loss: 634.6708374023438\n",
      "Epoch [165/200] Loss: 631.73095703125\n",
      "Epoch [166/200] Loss: 628.8031005859375\n",
      "Epoch [167/200] Loss: 625.8870239257812\n",
      "Epoch [168/200] Loss: 622.9827270507812\n",
      "Epoch [169/200] Loss: 620.0901489257812\n",
      "Epoch [170/200] Loss: 617.2092895507812\n",
      "Epoch [171/200] Loss: 614.3400268554688\n",
      "Epoch [172/200] Loss: 611.4823608398438\n",
      "Epoch [173/200] Loss: 608.63623046875\n",
      "Epoch [174/200] Loss: 605.801513671875\n",
      "Epoch [175/200] Loss: 602.9784545898438\n",
      "Epoch [176/200] Loss: 600.1666870117188\n",
      "Epoch [177/200] Loss: 597.3662109375\n",
      "Epoch [178/200] Loss: 594.5771484375\n",
      "Epoch [179/200] Loss: 591.7991333007812\n",
      "Epoch [180/200] Loss: 589.032470703125\n",
      "Epoch [181/200] Loss: 586.2769165039062\n",
      "Epoch [182/200] Loss: 583.5325317382812\n",
      "Epoch [183/200] Loss: 580.7991943359375\n",
      "Epoch [184/200] Loss: 578.0767822265625\n",
      "Epoch [185/200] Loss: 575.3654174804688\n",
      "Epoch [186/200] Loss: 572.6649169921875\n",
      "Epoch [187/200] Loss: 569.9752807617188\n",
      "Epoch [188/200] Loss: 567.2965087890625\n",
      "Epoch [189/200] Loss: 564.628662109375\n",
      "Epoch [190/200] Loss: 561.971435546875\n",
      "Epoch [191/200] Loss: 559.3250732421875\n",
      "Epoch [192/200] Loss: 556.6890869140625\n",
      "Epoch [193/200] Loss: 554.0640258789062\n",
      "Epoch [194/200] Loss: 551.449462890625\n",
      "Epoch [195/200] Loss: 548.8453979492188\n",
      "Epoch [196/200] Loss: 546.2520141601562\n",
      "Epoch [197/200] Loss: 543.6689453125\n",
      "Epoch [198/200] Loss: 541.0963134765625\n",
      "Epoch [199/200] Loss: 538.5341796875\n",
      "Epoch [200/200] Loss: 535.9823608398438\n",
      "Predicted days_remaining for parent_id 315: 15.968358039855957\n",
      "Training for parent_id 316...\n",
      "Epoch [1/200] Loss: 301.51641845703125\n",
      "Epoch [2/200] Loss: 295.3838806152344\n",
      "Epoch [3/200] Loss: 289.36273193359375\n",
      "Epoch [4/200] Loss: 283.4854431152344\n",
      "Epoch [5/200] Loss: 277.7528381347656\n",
      "Epoch [6/200] Loss: 272.1565246582031\n",
      "Epoch [7/200] Loss: 266.69110107421875\n",
      "Epoch [8/200] Loss: 261.355224609375\n",
      "Epoch [9/200] Loss: 256.1501770019531\n",
      "Epoch [10/200] Loss: 251.0785675048828\n",
      "Epoch [11/200] Loss: 246.14358520507812\n",
      "Epoch [12/200] Loss: 241.34877014160156\n",
      "Epoch [13/200] Loss: 236.69692993164062\n",
      "Epoch [14/200] Loss: 232.18893432617188\n",
      "Epoch [15/200] Loss: 227.82322692871094\n",
      "Epoch [16/200] Loss: 223.59622192382812\n",
      "Epoch [17/200] Loss: 219.5026397705078\n",
      "Epoch [18/200] Loss: 215.53688049316406\n",
      "Epoch [19/200] Loss: 211.6934814453125\n",
      "Epoch [20/200] Loss: 207.9678192138672\n",
      "Epoch [21/200] Loss: 204.3565216064453\n",
      "Epoch [22/200] Loss: 200.85726928710938\n",
      "Epoch [23/200] Loss: 197.46829223632812\n",
      "Epoch [24/200] Loss: 194.18777465820312\n",
      "Epoch [25/200] Loss: 191.01332092285156\n",
      "Epoch [26/200] Loss: 187.9415283203125\n",
      "Epoch [27/200] Loss: 184.96791076660156\n",
      "Epoch [28/200] Loss: 182.0871124267578\n",
      "Epoch [29/200] Loss: 179.29327392578125\n",
      "Epoch [30/200] Loss: 176.5803985595703\n",
      "Epoch [31/200] Loss: 173.9426727294922\n",
      "Epoch [32/200] Loss: 171.3747100830078\n",
      "Epoch [33/200] Loss: 168.87184143066406\n",
      "Epoch [34/200] Loss: 166.42996215820312\n",
      "Epoch [35/200] Loss: 164.0458221435547\n",
      "Epoch [36/200] Loss: 161.71688842773438\n",
      "Epoch [37/200] Loss: 159.4412841796875\n",
      "Epoch [38/200] Loss: 157.2177276611328\n",
      "Epoch [39/200] Loss: 155.0452117919922\n",
      "Epoch [40/200] Loss: 152.9230499267578\n",
      "Epoch [41/200] Loss: 150.8504638671875\n",
      "Epoch [42/200] Loss: 148.82647705078125\n",
      "Epoch [43/200] Loss: 146.84988403320312\n",
      "Epoch [44/200] Loss: 144.9191131591797\n",
      "Epoch [45/200] Loss: 143.0323028564453\n",
      "Epoch [46/200] Loss: 141.1873779296875\n",
      "Epoch [47/200] Loss: 139.38206481933594\n",
      "Epoch [48/200] Loss: 137.6140899658203\n",
      "Epoch [49/200] Loss: 135.8812713623047\n",
      "Epoch [50/200] Loss: 134.18154907226562\n",
      "Epoch [51/200] Loss: 132.51292419433594\n",
      "Epoch [52/200] Loss: 130.87362670898438\n",
      "Epoch [53/200] Loss: 129.26219177246094\n",
      "Epoch [54/200] Loss: 127.67716217041016\n",
      "Epoch [55/200] Loss: 126.1172866821289\n",
      "Epoch [56/200] Loss: 124.5815200805664\n",
      "Epoch [57/200] Loss: 123.06886291503906\n",
      "Epoch [58/200] Loss: 121.57847595214844\n",
      "Epoch [59/200] Loss: 120.10960388183594\n",
      "Epoch [60/200] Loss: 118.66155242919922\n",
      "Epoch [61/200] Loss: 117.23377227783203\n",
      "Epoch [62/200] Loss: 115.82559967041016\n",
      "Epoch [63/200] Loss: 114.43666076660156\n",
      "Epoch [64/200] Loss: 113.0664291381836\n",
      "Epoch [65/200] Loss: 111.71448516845703\n",
      "Epoch [66/200] Loss: 110.38042449951172\n",
      "Epoch [67/200] Loss: 109.06392669677734\n",
      "Epoch [68/200] Loss: 107.7646255493164\n",
      "Epoch [69/200] Loss: 106.48218536376953\n",
      "Epoch [70/200] Loss: 105.2163314819336\n",
      "Epoch [71/200] Loss: 103.96675872802734\n",
      "Epoch [72/200] Loss: 102.7331771850586\n",
      "Epoch [73/200] Loss: 101.51535034179688\n",
      "Epoch [74/200] Loss: 100.31298828125\n",
      "Epoch [75/200] Loss: 99.12588500976562\n",
      "Epoch [76/200] Loss: 97.95378112792969\n",
      "Epoch [77/200] Loss: 96.79647064208984\n",
      "Epoch [78/200] Loss: 95.65370178222656\n",
      "Epoch [79/200] Loss: 94.5252914428711\n",
      "Epoch [80/200] Loss: 93.4110336303711\n",
      "Epoch [81/200] Loss: 92.31066131591797\n",
      "Epoch [82/200] Loss: 91.22409057617188\n",
      "Epoch [83/200] Loss: 90.15103912353516\n",
      "Epoch [84/200] Loss: 89.0914077758789\n",
      "Epoch [85/200] Loss: 88.04492950439453\n",
      "Epoch [86/200] Loss: 87.01145935058594\n",
      "Epoch [87/200] Loss: 85.99082946777344\n",
      "Epoch [88/200] Loss: 84.98291015625\n",
      "Epoch [89/200] Loss: 83.98751068115234\n",
      "Epoch [90/200] Loss: 83.00442504882812\n",
      "Epoch [91/200] Loss: 82.03352355957031\n",
      "Epoch [92/200] Loss: 81.07470703125\n",
      "Epoch [93/200] Loss: 80.12777709960938\n",
      "Epoch [94/200] Loss: 79.19257354736328\n",
      "Epoch [95/200] Loss: 78.26895904541016\n",
      "Epoch [96/200] Loss: 77.35682678222656\n",
      "Epoch [97/200] Loss: 76.4560317993164\n",
      "Epoch [98/200] Loss: 75.56641387939453\n",
      "Epoch [99/200] Loss: 74.68785095214844\n",
      "Epoch [100/200] Loss: 73.82019805908203\n",
      "Epoch [101/200] Loss: 72.96336364746094\n",
      "Epoch [102/200] Loss: 72.1171875\n",
      "Epoch [103/200] Loss: 71.28155517578125\n",
      "Epoch [104/200] Loss: 70.45635223388672\n",
      "Epoch [105/200] Loss: 69.64144897460938\n",
      "Epoch [106/200] Loss: 68.83672332763672\n",
      "Epoch [107/200] Loss: 68.04206848144531\n",
      "Epoch [108/200] Loss: 67.25740051269531\n",
      "Epoch [109/200] Loss: 66.48253631591797\n",
      "Epoch [110/200] Loss: 65.71742248535156\n",
      "Epoch [111/200] Loss: 64.96194458007812\n",
      "Epoch [112/200] Loss: 64.21598052978516\n",
      "Epoch [113/200] Loss: 63.47940444946289\n",
      "Epoch [114/200] Loss: 62.752166748046875\n",
      "Epoch [115/200] Loss: 62.03412628173828\n",
      "Epoch [116/200] Loss: 61.32517623901367\n",
      "Epoch [117/200] Loss: 60.625244140625\n",
      "Epoch [118/200] Loss: 59.93422317504883\n",
      "Epoch [119/200] Loss: 59.251991271972656\n",
      "Epoch [120/200] Loss: 58.5784912109375\n",
      "Epoch [121/200] Loss: 57.913597106933594\n",
      "Epoch [122/200] Loss: 57.2572135925293\n",
      "Epoch [123/200] Loss: 56.60929489135742\n",
      "Epoch [124/200] Loss: 55.969669342041016\n",
      "Epoch [125/200] Loss: 55.338321685791016\n",
      "Epoch [126/200] Loss: 54.71510314941406\n",
      "Epoch [127/200] Loss: 54.09999465942383\n",
      "Epoch [128/200] Loss: 53.492862701416016\n",
      "Epoch [129/200] Loss: 52.89361572265625\n",
      "Epoch [130/200] Loss: 52.302162170410156\n",
      "Epoch [131/200] Loss: 51.718475341796875\n",
      "Epoch [132/200] Loss: 51.14240646362305\n",
      "Epoch [133/200] Loss: 50.573917388916016\n",
      "Epoch [134/200] Loss: 50.01288986206055\n",
      "Epoch [135/200] Loss: 49.45925521850586\n",
      "Epoch [136/200] Loss: 48.912967681884766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [137/200] Loss: 48.37388610839844\n",
      "Epoch [138/200] Loss: 47.84199523925781\n",
      "Epoch [139/200] Loss: 47.31716537475586\n",
      "Epoch [140/200] Loss: 46.799346923828125\n",
      "Epoch [141/200] Loss: 46.28847885131836\n",
      "Epoch [142/200] Loss: 45.78443145751953\n",
      "Epoch [143/200] Loss: 45.28718185424805\n",
      "Epoch [144/200] Loss: 44.79663848876953\n",
      "Epoch [145/200] Loss: 44.312713623046875\n",
      "Epoch [146/200] Loss: 43.83535385131836\n",
      "Epoch [147/200] Loss: 43.364498138427734\n",
      "Epoch [148/200] Loss: 42.900028228759766\n",
      "Epoch [149/200] Loss: 42.441932678222656\n",
      "Epoch [150/200] Loss: 41.990089416503906\n",
      "Epoch [151/200] Loss: 41.54446029663086\n",
      "Epoch [152/200] Loss: 41.10498046875\n",
      "Epoch [153/200] Loss: 40.67155075073242\n",
      "Epoch [154/200] Loss: 40.244140625\n",
      "Epoch [155/200] Loss: 39.822669982910156\n",
      "Epoch [156/200] Loss: 39.407039642333984\n",
      "Epoch [157/200] Loss: 38.99722671508789\n",
      "Epoch [158/200] Loss: 38.59315490722656\n",
      "Epoch [159/200] Loss: 38.19475555419922\n",
      "Epoch [160/200] Loss: 37.80196762084961\n",
      "Epoch [161/200] Loss: 37.414710998535156\n",
      "Epoch [162/200] Loss: 37.0329475402832\n",
      "Epoch [163/200] Loss: 36.6566047668457\n",
      "Epoch [164/200] Loss: 36.285614013671875\n",
      "Epoch [165/200] Loss: 35.91991424560547\n",
      "Epoch [166/200] Loss: 35.55945587158203\n",
      "Epoch [167/200] Loss: 35.204185485839844\n",
      "Epoch [168/200] Loss: 34.8540153503418\n",
      "Epoch [169/200] Loss: 34.50890350341797\n",
      "Epoch [170/200] Loss: 34.16880798339844\n",
      "Epoch [171/200] Loss: 33.83362579345703\n",
      "Epoch [172/200] Loss: 33.50334548950195\n",
      "Epoch [173/200] Loss: 33.17787551879883\n",
      "Epoch [174/200] Loss: 32.857173919677734\n",
      "Epoch [175/200] Loss: 32.541202545166016\n",
      "Epoch [176/200] Loss: 32.22986602783203\n",
      "Epoch [177/200] Loss: 31.923141479492188\n",
      "Epoch [178/200] Loss: 31.620956420898438\n",
      "Epoch [179/200] Loss: 31.32326889038086\n",
      "Epoch [180/200] Loss: 31.030012130737305\n",
      "Epoch [181/200] Loss: 30.74114227294922\n",
      "Epoch [182/200] Loss: 30.45661163330078\n",
      "Epoch [183/200] Loss: 30.176353454589844\n",
      "Epoch [184/200] Loss: 29.900306701660156\n",
      "Epoch [185/200] Loss: 29.628467559814453\n",
      "Epoch [186/200] Loss: 29.3607177734375\n",
      "Epoch [187/200] Loss: 29.09706687927246\n",
      "Epoch [188/200] Loss: 28.837432861328125\n",
      "Epoch [189/200] Loss: 28.581756591796875\n",
      "Epoch [190/200] Loss: 28.330020904541016\n",
      "Epoch [191/200] Loss: 28.082164764404297\n",
      "Epoch [192/200] Loss: 27.838111877441406\n",
      "Epoch [193/200] Loss: 27.597848892211914\n",
      "Epoch [194/200] Loss: 27.361328125\n",
      "Epoch [195/200] Loss: 27.12848472595215\n",
      "Epoch [196/200] Loss: 26.89927864074707\n",
      "Epoch [197/200] Loss: 26.673656463623047\n",
      "Epoch [198/200] Loss: 26.45159149169922\n",
      "Epoch [199/200] Loss: 26.23301887512207\n",
      "Epoch [200/200] Loss: 26.017894744873047\n",
      "Predicted days_remaining for parent_id 316: 13.378252029418945\n",
      "Training for parent_id 317...\n",
      "Epoch [1/200] Loss: 1430.2015380859375\n",
      "Epoch [2/200] Loss: 1414.681396484375\n",
      "Epoch [3/200] Loss: 1399.626220703125\n",
      "Epoch [4/200] Loss: 1384.96875\n",
      "Epoch [5/200] Loss: 1370.679443359375\n",
      "Epoch [6/200] Loss: 1356.730224609375\n",
      "Epoch [7/200] Loss: 1343.0987548828125\n",
      "Epoch [8/200] Loss: 1329.7796630859375\n",
      "Epoch [9/200] Loss: 1316.7855224609375\n",
      "Epoch [10/200] Loss: 1304.1343994140625\n",
      "Epoch [11/200] Loss: 1291.842041015625\n",
      "Epoch [12/200] Loss: 1279.9210205078125\n",
      "Epoch [13/200] Loss: 1268.375244140625\n",
      "Epoch [14/200] Loss: 1257.20166015625\n",
      "Epoch [15/200] Loss: 1246.3912353515625\n",
      "Epoch [16/200] Loss: 1235.92919921875\n",
      "Epoch [17/200] Loss: 1225.7998046875\n",
      "Epoch [18/200] Loss: 1215.987060546875\n",
      "Epoch [19/200] Loss: 1206.476318359375\n",
      "Epoch [20/200] Loss: 1197.2540283203125\n",
      "Epoch [21/200] Loss: 1188.30810546875\n",
      "Epoch [22/200] Loss: 1179.629150390625\n",
      "Epoch [23/200] Loss: 1171.2080078125\n",
      "Epoch [24/200] Loss: 1163.0369873046875\n",
      "Epoch [25/200] Loss: 1155.1083984375\n",
      "Epoch [26/200] Loss: 1147.414306640625\n",
      "Epoch [27/200] Loss: 1139.9462890625\n",
      "Epoch [28/200] Loss: 1132.6947021484375\n",
      "Epoch [29/200] Loss: 1125.6495361328125\n",
      "Epoch [30/200] Loss: 1118.7989501953125\n",
      "Epoch [31/200] Loss: 1112.130615234375\n",
      "Epoch [32/200] Loss: 1105.632568359375\n",
      "Epoch [33/200] Loss: 1099.2923583984375\n",
      "Epoch [34/200] Loss: 1093.0972900390625\n",
      "Epoch [35/200] Loss: 1087.0361328125\n",
      "Epoch [36/200] Loss: 1081.09765625\n",
      "Epoch [37/200] Loss: 1075.271728515625\n",
      "Epoch [38/200] Loss: 1069.548828125\n",
      "Epoch [39/200] Loss: 1063.9207763671875\n",
      "Epoch [40/200] Loss: 1058.380126953125\n",
      "Epoch [41/200] Loss: 1052.9200439453125\n",
      "Epoch [42/200] Loss: 1047.5347900390625\n",
      "Epoch [43/200] Loss: 1042.21875\n",
      "Epoch [44/200] Loss: 1036.967529296875\n",
      "Epoch [45/200] Loss: 1031.7767333984375\n",
      "Epoch [46/200] Loss: 1026.6427001953125\n",
      "Epoch [47/200] Loss: 1021.5621337890625\n",
      "Epoch [48/200] Loss: 1016.5317993164062\n",
      "Epoch [49/200] Loss: 1011.5489501953125\n",
      "Epoch [50/200] Loss: 1006.6115112304688\n",
      "Epoch [51/200] Loss: 1001.7171020507812\n",
      "Epoch [52/200] Loss: 996.86376953125\n",
      "Epoch [53/200] Loss: 992.0498046875\n",
      "Epoch [54/200] Loss: 987.2734985351562\n",
      "Epoch [55/200] Loss: 982.53369140625\n",
      "Epoch [56/200] Loss: 977.8289184570312\n",
      "Epoch [57/200] Loss: 973.1583251953125\n",
      "Epoch [58/200] Loss: 968.5204467773438\n",
      "Epoch [59/200] Loss: 963.9146118164062\n",
      "Epoch [60/200] Loss: 959.3399047851562\n",
      "Epoch [61/200] Loss: 954.79541015625\n",
      "Epoch [62/200] Loss: 950.2808837890625\n",
      "Epoch [63/200] Loss: 945.794921875\n",
      "Epoch [64/200] Loss: 941.3372802734375\n",
      "Epoch [65/200] Loss: 936.9072875976562\n",
      "Epoch [66/200] Loss: 932.5045776367188\n",
      "Epoch [67/200] Loss: 928.128173828125\n",
      "Epoch [68/200] Loss: 923.7781982421875\n",
      "Epoch [69/200] Loss: 919.4535522460938\n",
      "Epoch [70/200] Loss: 915.1541137695312\n",
      "Epoch [71/200] Loss: 910.87939453125\n",
      "Epoch [72/200] Loss: 906.62890625\n",
      "Epoch [73/200] Loss: 902.4024658203125\n",
      "Epoch [74/200] Loss: 898.1994018554688\n",
      "Epoch [75/200] Loss: 894.0195922851562\n",
      "Epoch [76/200] Loss: 889.8624267578125\n",
      "Epoch [77/200] Loss: 885.7278442382812\n",
      "Epoch [78/200] Loss: 881.6153564453125\n",
      "Epoch [79/200] Loss: 877.5247192382812\n",
      "Epoch [80/200] Loss: 873.45556640625\n",
      "Epoch [81/200] Loss: 869.4073486328125\n",
      "Epoch [82/200] Loss: 865.38037109375\n",
      "Epoch [83/200] Loss: 861.3740234375\n",
      "Epoch [84/200] Loss: 857.3880615234375\n",
      "Epoch [85/200] Loss: 853.422119140625\n",
      "Epoch [86/200] Loss: 849.4760131835938\n",
      "Epoch [87/200] Loss: 845.5496826171875\n",
      "Epoch [88/200] Loss: 841.6427612304688\n",
      "Epoch [89/200] Loss: 837.7550048828125\n",
      "Epoch [90/200] Loss: 833.8862915039062\n",
      "Epoch [91/200] Loss: 830.0361328125\n",
      "Epoch [92/200] Loss: 826.2046508789062\n",
      "Epoch [93/200] Loss: 822.3914184570312\n",
      "Epoch [94/200] Loss: 818.5964965820312\n",
      "Epoch [95/200] Loss: 814.8193359375\n",
      "Epoch [96/200] Loss: 811.06005859375\n",
      "Epoch [97/200] Loss: 807.318359375\n",
      "Epoch [98/200] Loss: 803.5941162109375\n",
      "Epoch [99/200] Loss: 799.886962890625\n",
      "Epoch [100/200] Loss: 796.1973266601562\n",
      "Epoch [101/200] Loss: 792.5242309570312\n",
      "Epoch [102/200] Loss: 788.8681640625\n",
      "Epoch [103/200] Loss: 785.2285766601562\n",
      "Epoch [104/200] Loss: 781.60546875\n",
      "Epoch [105/200] Loss: 777.9988403320312\n",
      "Epoch [106/200] Loss: 774.408447265625\n",
      "Epoch [107/200] Loss: 770.833984375\n",
      "Epoch [108/200] Loss: 767.275390625\n",
      "Epoch [109/200] Loss: 763.7328491210938\n",
      "Epoch [110/200] Loss: 760.2058715820312\n",
      "Epoch [111/200] Loss: 756.6945190429688\n",
      "Epoch [112/200] Loss: 753.1985473632812\n",
      "Epoch [113/200] Loss: 749.7179565429688\n",
      "Epoch [114/200] Loss: 746.25244140625\n",
      "Epoch [115/200] Loss: 742.8021850585938\n",
      "Epoch [116/200] Loss: 739.3668823242188\n",
      "Epoch [117/200] Loss: 735.946533203125\n",
      "Epoch [118/200] Loss: 732.5408935546875\n",
      "Epoch [119/200] Loss: 729.14990234375\n",
      "Epoch [120/200] Loss: 725.7734985351562\n",
      "Epoch [121/200] Loss: 722.41162109375\n",
      "Epoch [122/200] Loss: 719.0640869140625\n",
      "Epoch [123/200] Loss: 715.7308349609375\n",
      "Epoch [124/200] Loss: 712.4119262695312\n",
      "Epoch [125/200] Loss: 709.10693359375\n",
      "Epoch [126/200] Loss: 705.8161010742188\n",
      "Epoch [127/200] Loss: 702.5392456054688\n",
      "Epoch [128/200] Loss: 699.276123046875\n",
      "Epoch [129/200] Loss: 696.0267333984375\n",
      "Epoch [130/200] Loss: 692.7911376953125\n",
      "Epoch [131/200] Loss: 689.569091796875\n",
      "Epoch [132/200] Loss: 686.360595703125\n",
      "Epoch [133/200] Loss: 683.1656494140625\n",
      "Epoch [134/200] Loss: 679.9840087890625\n",
      "Epoch [135/200] Loss: 676.8154907226562\n",
      "Epoch [136/200] Loss: 673.6605224609375\n",
      "Epoch [137/200] Loss: 670.5184936523438\n",
      "Epoch [138/200] Loss: 667.3894653320312\n",
      "Epoch [139/200] Loss: 664.2736206054688\n",
      "Epoch [140/200] Loss: 661.1707153320312\n",
      "Epoch [141/200] Loss: 658.0807495117188\n",
      "Epoch [142/200] Loss: 655.0033569335938\n",
      "Epoch [143/200] Loss: 651.9387817382812\n",
      "Epoch [144/200] Loss: 648.8870849609375\n",
      "Epoch [145/200] Loss: 645.8478393554688\n",
      "Epoch [146/200] Loss: 642.8211059570312\n",
      "Epoch [147/200] Loss: 639.8069458007812\n",
      "Epoch [148/200] Loss: 636.80517578125\n",
      "Epoch [149/200] Loss: 633.8156127929688\n",
      "Epoch [150/200] Loss: 630.838623046875\n",
      "Epoch [151/200] Loss: 627.873779296875\n",
      "Epoch [152/200] Loss: 624.9212036132812\n",
      "Epoch [153/200] Loss: 621.980712890625\n",
      "Epoch [154/200] Loss: 619.05224609375\n",
      "Epoch [155/200] Loss: 616.1357421875\n",
      "Epoch [156/200] Loss: 613.2313232421875\n",
      "Epoch [157/200] Loss: 610.3387451171875\n",
      "Epoch [158/200] Loss: 607.4581909179688\n",
      "Epoch [159/200] Loss: 604.5892333984375\n",
      "Epoch [160/200] Loss: 601.7321166992188\n",
      "Epoch [161/200] Loss: 598.88671875\n",
      "Epoch [162/200] Loss: 596.0529174804688\n",
      "Epoch [163/200] Loss: 593.2306518554688\n",
      "Epoch [164/200] Loss: 590.4201049804688\n",
      "Epoch [165/200] Loss: 587.6209106445312\n",
      "Epoch [166/200] Loss: 584.833251953125\n",
      "Epoch [167/200] Loss: 582.05712890625\n",
      "Epoch [168/200] Loss: 579.2921142578125\n",
      "Epoch [169/200] Loss: 576.5385131835938\n",
      "Epoch [170/200] Loss: 573.796142578125\n",
      "Epoch [171/200] Loss: 571.06494140625\n",
      "Epoch [172/200] Loss: 568.3450927734375\n",
      "Epoch [173/200] Loss: 565.6361694335938\n",
      "Epoch [174/200] Loss: 562.9384155273438\n",
      "Epoch [175/200] Loss: 560.2515869140625\n",
      "Epoch [176/200] Loss: 557.5758666992188\n",
      "Epoch [177/200] Loss: 554.9110107421875\n",
      "Epoch [178/200] Loss: 552.2570190429688\n",
      "Epoch [179/200] Loss: 549.614013671875\n",
      "Epoch [180/200] Loss: 546.9818115234375\n",
      "Epoch [181/200] Loss: 544.3602905273438\n",
      "Epoch [182/200] Loss: 541.74951171875\n",
      "Epoch [183/200] Loss: 539.1495361328125\n",
      "Epoch [184/200] Loss: 536.5601196289062\n",
      "Epoch [185/200] Loss: 533.9813232421875\n",
      "Epoch [186/200] Loss: 531.4131469726562\n",
      "Epoch [187/200] Loss: 528.8554077148438\n",
      "Epoch [188/200] Loss: 526.30810546875\n",
      "Epoch [189/200] Loss: 523.771240234375\n",
      "Epoch [190/200] Loss: 521.2449340820312\n",
      "Epoch [191/200] Loss: 518.72900390625\n",
      "Epoch [192/200] Loss: 516.2232055664062\n",
      "Epoch [193/200] Loss: 513.7279052734375\n",
      "Epoch [194/200] Loss: 511.2427062988281\n",
      "Epoch [195/200] Loss: 508.7677917480469\n",
      "Epoch [196/200] Loss: 506.3030090332031\n",
      "Epoch [197/200] Loss: 503.848388671875\n",
      "Epoch [198/200] Loss: 501.4038391113281\n",
      "Epoch [199/200] Loss: 498.9693298339844\n",
      "Epoch [200/200] Loss: 496.5450439453125\n",
      "Predicted days_remaining for parent_id 317: 15.848104476928711\n",
      "Training for parent_id 320...\n",
      "Epoch [1/200] Loss: 1888.033447265625\n",
      "Epoch [2/200] Loss: 1866.51220703125\n",
      "Epoch [3/200] Loss: 1845.32666015625\n",
      "Epoch [4/200] Loss: 1824.593505859375\n",
      "Epoch [5/200] Loss: 1804.402099609375\n",
      "Epoch [6/200] Loss: 1784.801513671875\n",
      "Epoch [7/200] Loss: 1765.80810546875\n",
      "Epoch [8/200] Loss: 1747.4219970703125\n",
      "Epoch [9/200] Loss: 1729.6441650390625\n",
      "Epoch [10/200] Loss: 1712.483642578125\n",
      "Epoch [11/200] Loss: 1695.95751953125\n",
      "Epoch [12/200] Loss: 1680.08544921875\n",
      "Epoch [13/200] Loss: 1664.887451171875\n",
      "Epoch [14/200] Loss: 1650.3773193359375\n",
      "Epoch [15/200] Loss: 1636.5640869140625\n",
      "Epoch [16/200] Loss: 1623.444580078125\n",
      "Epoch [17/200] Loss: 1611.0047607421875\n",
      "Epoch [18/200] Loss: 1599.2186279296875\n",
      "Epoch [19/200] Loss: 1588.052978515625\n",
      "Epoch [20/200] Loss: 1577.4664306640625\n",
      "Epoch [21/200] Loss: 1567.414306640625\n",
      "Epoch [22/200] Loss: 1557.8507080078125\n",
      "Epoch [23/200] Loss: 1548.7296142578125\n",
      "Epoch [24/200] Loss: 1540.0076904296875\n",
      "Epoch [25/200] Loss: 1531.642822265625\n",
      "Epoch [26/200] Loss: 1523.597412109375\n",
      "Epoch [27/200] Loss: 1515.83740234375\n",
      "Epoch [28/200] Loss: 1508.330810546875\n",
      "Epoch [29/200] Loss: 1501.0501708984375\n",
      "Epoch [30/200] Loss: 1493.969482421875\n",
      "Epoch [31/200] Loss: 1487.065185546875\n",
      "Epoch [32/200] Loss: 1480.31689453125\n",
      "Epoch [33/200] Loss: 1473.7052001953125\n",
      "Epoch [34/200] Loss: 1467.215087890625\n",
      "Epoch [35/200] Loss: 1460.83203125\n",
      "Epoch [36/200] Loss: 1454.5447998046875\n",
      "Epoch [37/200] Loss: 1448.3447265625\n",
      "Epoch [38/200] Loss: 1442.222900390625\n",
      "Epoch [39/200] Loss: 1436.173095703125\n",
      "Epoch [40/200] Loss: 1430.1898193359375\n",
      "Epoch [41/200] Loss: 1424.2677001953125\n",
      "Epoch [42/200] Loss: 1418.40283203125\n",
      "Epoch [43/200] Loss: 1412.590576171875\n",
      "Epoch [44/200] Loss: 1406.828369140625\n",
      "Epoch [45/200] Loss: 1401.11279296875\n",
      "Epoch [46/200] Loss: 1395.441162109375\n",
      "Epoch [47/200] Loss: 1389.810791015625\n",
      "Epoch [48/200] Loss: 1384.219970703125\n",
      "Epoch [49/200] Loss: 1378.666748046875\n",
      "Epoch [50/200] Loss: 1373.1494140625\n",
      "Epoch [51/200] Loss: 1367.667236328125\n",
      "Epoch [52/200] Loss: 1362.21875\n",
      "Epoch [53/200] Loss: 1356.802734375\n",
      "Epoch [54/200] Loss: 1351.41845703125\n",
      "Epoch [55/200] Loss: 1346.0655517578125\n",
      "Epoch [56/200] Loss: 1340.7423095703125\n",
      "Epoch [57/200] Loss: 1335.44873046875\n",
      "Epoch [58/200] Loss: 1330.1839599609375\n",
      "Epoch [59/200] Loss: 1324.9473876953125\n",
      "Epoch [60/200] Loss: 1319.7386474609375\n",
      "Epoch [61/200] Loss: 1314.5567626953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/200] Loss: 1309.40185546875\n",
      "Epoch [63/200] Loss: 1304.2730712890625\n",
      "Epoch [64/200] Loss: 1299.1697998046875\n",
      "Epoch [65/200] Loss: 1294.0919189453125\n",
      "Epoch [66/200] Loss: 1289.0390625\n",
      "Epoch [67/200] Loss: 1284.0103759765625\n",
      "Epoch [68/200] Loss: 1279.0059814453125\n",
      "Epoch [69/200] Loss: 1274.0252685546875\n",
      "Epoch [70/200] Loss: 1269.068359375\n",
      "Epoch [71/200] Loss: 1264.134033203125\n",
      "Epoch [72/200] Loss: 1259.22265625\n",
      "Epoch [73/200] Loss: 1254.333984375\n",
      "Epoch [74/200] Loss: 1249.467041015625\n",
      "Epoch [75/200] Loss: 1244.6221923828125\n",
      "Epoch [76/200] Loss: 1239.798828125\n",
      "Epoch [77/200] Loss: 1234.99658203125\n",
      "Epoch [78/200] Loss: 1230.2154541015625\n",
      "Epoch [79/200] Loss: 1225.455078125\n",
      "Epoch [80/200] Loss: 1220.71533203125\n",
      "Epoch [81/200] Loss: 1215.9957275390625\n",
      "Epoch [82/200] Loss: 1211.2958984375\n",
      "Epoch [83/200] Loss: 1206.6163330078125\n",
      "Epoch [84/200] Loss: 1201.955810546875\n",
      "Epoch [85/200] Loss: 1197.31494140625\n",
      "Epoch [86/200] Loss: 1192.6932373046875\n",
      "Epoch [87/200] Loss: 1188.090087890625\n",
      "Epoch [88/200] Loss: 1183.5059814453125\n",
      "Epoch [89/200] Loss: 1178.9400634765625\n",
      "Epoch [90/200] Loss: 1174.3927001953125\n",
      "Epoch [91/200] Loss: 1169.8631591796875\n",
      "Epoch [92/200] Loss: 1165.35205078125\n",
      "Epoch [93/200] Loss: 1160.8583984375\n",
      "Epoch [94/200] Loss: 1156.3824462890625\n",
      "Epoch [95/200] Loss: 1151.923828125\n",
      "Epoch [96/200] Loss: 1147.482666015625\n",
      "Epoch [97/200] Loss: 1143.0584716796875\n",
      "Epoch [98/200] Loss: 1138.6512451171875\n",
      "Epoch [99/200] Loss: 1134.2607421875\n",
      "Epoch [100/200] Loss: 1129.8873291015625\n",
      "Epoch [101/200] Loss: 1125.5302734375\n",
      "Epoch [102/200] Loss: 1121.189697265625\n",
      "Epoch [103/200] Loss: 1116.8651123046875\n",
      "Epoch [104/200] Loss: 1112.556884765625\n",
      "Epoch [105/200] Loss: 1108.2646484375\n",
      "Epoch [106/200] Loss: 1103.9881591796875\n",
      "Epoch [107/200] Loss: 1099.727783203125\n",
      "Epoch [108/200] Loss: 1095.4827880859375\n",
      "Epoch [109/200] Loss: 1091.25341796875\n",
      "Epoch [110/200] Loss: 1087.03955078125\n",
      "Epoch [111/200] Loss: 1082.841064453125\n",
      "Epoch [112/200] Loss: 1078.6578369140625\n",
      "Epoch [113/200] Loss: 1074.4896240234375\n",
      "Epoch [114/200] Loss: 1070.33642578125\n",
      "Epoch [115/200] Loss: 1066.197998046875\n",
      "Epoch [116/200] Loss: 1062.0745849609375\n",
      "Epoch [117/200] Loss: 1057.9658203125\n",
      "Epoch [118/200] Loss: 1053.8719482421875\n",
      "Epoch [119/200] Loss: 1049.79248046875\n",
      "Epoch [120/200] Loss: 1045.7271728515625\n",
      "Epoch [121/200] Loss: 1041.6766357421875\n",
      "Epoch [122/200] Loss: 1037.6402587890625\n",
      "Epoch [123/200] Loss: 1033.6180419921875\n",
      "Epoch [124/200] Loss: 1029.610107421875\n",
      "Epoch [125/200] Loss: 1025.6160888671875\n",
      "Epoch [126/200] Loss: 1021.63623046875\n",
      "Epoch [127/200] Loss: 1017.669921875\n",
      "Epoch [128/200] Loss: 1013.7176513671875\n",
      "Epoch [129/200] Loss: 1009.779052734375\n",
      "Epoch [130/200] Loss: 1005.854248046875\n",
      "Epoch [131/200] Loss: 1001.9429931640625\n",
      "Epoch [132/200] Loss: 998.0451049804688\n",
      "Epoch [133/200] Loss: 994.1608276367188\n",
      "Epoch [134/200] Loss: 990.2900390625\n",
      "Epoch [135/200] Loss: 986.4324951171875\n",
      "Epoch [136/200] Loss: 982.5881958007812\n",
      "Epoch [137/200] Loss: 978.7569580078125\n",
      "Epoch [138/200] Loss: 974.9390258789062\n",
      "Epoch [139/200] Loss: 971.1340942382812\n",
      "Epoch [140/200] Loss: 967.34228515625\n",
      "Epoch [141/200] Loss: 963.5634155273438\n",
      "Epoch [142/200] Loss: 959.7974853515625\n",
      "Epoch [143/200] Loss: 956.0440673828125\n",
      "Epoch [144/200] Loss: 952.3037719726562\n",
      "Epoch [145/200] Loss: 948.5762329101562\n",
      "Epoch [146/200] Loss: 944.861083984375\n",
      "Epoch [147/200] Loss: 941.1587524414062\n",
      "Epoch [148/200] Loss: 937.4688720703125\n",
      "Epoch [149/200] Loss: 933.791748046875\n",
      "Epoch [150/200] Loss: 930.1268310546875\n",
      "Epoch [151/200] Loss: 926.474365234375\n",
      "Epoch [152/200] Loss: 922.8344116210938\n",
      "Epoch [153/200] Loss: 919.2064208984375\n",
      "Epoch [154/200] Loss: 915.5909423828125\n",
      "Epoch [155/200] Loss: 911.9876098632812\n",
      "Epoch [156/200] Loss: 908.3965454101562\n",
      "Epoch [157/200] Loss: 904.8173828125\n",
      "Epoch [158/200] Loss: 901.2504272460938\n",
      "Epoch [159/200] Loss: 897.6953735351562\n",
      "Epoch [160/200] Loss: 894.1524047851562\n",
      "Epoch [161/200] Loss: 890.621337890625\n",
      "Epoch [162/200] Loss: 887.1021118164062\n",
      "Epoch [163/200] Loss: 883.5947265625\n",
      "Epoch [164/200] Loss: 880.099365234375\n",
      "Epoch [165/200] Loss: 876.615234375\n",
      "Epoch [166/200] Loss: 873.1431274414062\n",
      "Epoch [167/200] Loss: 869.6827392578125\n",
      "Epoch [168/200] Loss: 866.2338256835938\n",
      "Epoch [169/200] Loss: 862.7965087890625\n",
      "Epoch [170/200] Loss: 859.370849609375\n",
      "Epoch [171/200] Loss: 855.9566040039062\n",
      "Epoch [172/200] Loss: 852.5538330078125\n",
      "Epoch [173/200] Loss: 849.16259765625\n",
      "Epoch [174/200] Loss: 845.782470703125\n",
      "Epoch [175/200] Loss: 842.413818359375\n",
      "Epoch [176/200] Loss: 839.056396484375\n",
      "Epoch [177/200] Loss: 835.7103881835938\n",
      "Epoch [178/200] Loss: 832.3755493164062\n",
      "Epoch [179/200] Loss: 829.0518798828125\n",
      "Epoch [180/200] Loss: 825.7392578125\n",
      "Epoch [181/200] Loss: 822.4378051757812\n",
      "Epoch [182/200] Loss: 819.1476440429688\n",
      "Epoch [183/200] Loss: 815.8681640625\n",
      "Epoch [184/200] Loss: 812.5999755859375\n",
      "Epoch [185/200] Loss: 809.3426513671875\n",
      "Epoch [186/200] Loss: 806.0961303710938\n",
      "Epoch [187/200] Loss: 802.860595703125\n",
      "Epoch [188/200] Loss: 799.635986328125\n",
      "Epoch [189/200] Loss: 796.4220581054688\n",
      "Epoch [190/200] Loss: 793.2190551757812\n",
      "Epoch [191/200] Loss: 790.0267333984375\n",
      "Epoch [192/200] Loss: 786.8451538085938\n",
      "Epoch [193/200] Loss: 783.6744384765625\n",
      "Epoch [194/200] Loss: 780.5140991210938\n",
      "Epoch [195/200] Loss: 777.3645629882812\n",
      "Epoch [196/200] Loss: 774.2254638671875\n",
      "Epoch [197/200] Loss: 771.09716796875\n",
      "Epoch [198/200] Loss: 767.9791259765625\n",
      "Epoch [199/200] Loss: 764.8717651367188\n",
      "Epoch [200/200] Loss: 761.7747802734375\n",
      "Predicted days_remaining for parent_id 320: 16.46904182434082\n",
      "Training for parent_id 321...\n",
      "Epoch [1/200] Loss: 782.7671508789062\n",
      "Epoch [2/200] Loss: 770.5841064453125\n",
      "Epoch [3/200] Loss: 758.3936767578125\n",
      "Epoch [4/200] Loss: 746.2634887695312\n",
      "Epoch [5/200] Loss: 734.24267578125\n",
      "Epoch [6/200] Loss: 722.3583984375\n",
      "Epoch [7/200] Loss: 710.6277465820312\n",
      "Epoch [8/200] Loss: 699.0670166015625\n",
      "Epoch [9/200] Loss: 687.699462890625\n",
      "Epoch [10/200] Loss: 676.55859375\n",
      "Epoch [11/200] Loss: 665.685546875\n",
      "Epoch [12/200] Loss: 655.1212158203125\n",
      "Epoch [13/200] Loss: 644.89892578125\n",
      "Epoch [14/200] Loss: 635.04150390625\n",
      "Epoch [15/200] Loss: 625.5620727539062\n",
      "Epoch [16/200] Loss: 616.4669799804688\n",
      "Epoch [17/200] Loss: 607.7583618164062\n",
      "Epoch [18/200] Loss: 599.4346923828125\n",
      "Epoch [19/200] Loss: 591.4915161132812\n",
      "Epoch [20/200] Loss: 583.9205932617188\n",
      "Epoch [21/200] Loss: 576.7103881835938\n",
      "Epoch [22/200] Loss: 569.8460693359375\n",
      "Epoch [23/200] Loss: 563.3106689453125\n",
      "Epoch [24/200] Loss: 557.0850219726562\n",
      "Epoch [25/200] Loss: 551.1492919921875\n",
      "Epoch [26/200] Loss: 545.4829711914062\n",
      "Epoch [27/200] Loss: 540.0654907226562\n",
      "Epoch [28/200] Loss: 534.8768920898438\n",
      "Epoch [29/200] Loss: 529.8977661132812\n",
      "Epoch [30/200] Loss: 525.1098022460938\n",
      "Epoch [31/200] Loss: 520.4952392578125\n",
      "Epoch [32/200] Loss: 516.037841796875\n",
      "Epoch [33/200] Loss: 511.72210693359375\n",
      "Epoch [34/200] Loss: 507.5336608886719\n",
      "Epoch [35/200] Loss: 503.45953369140625\n",
      "Epoch [36/200] Loss: 499.48779296875\n",
      "Epoch [37/200] Loss: 495.6080017089844\n",
      "Epoch [38/200] Loss: 491.81072998046875\n",
      "Epoch [39/200] Loss: 488.0882568359375\n",
      "Epoch [40/200] Loss: 484.4336242675781\n",
      "Epoch [41/200] Loss: 480.8412170410156\n",
      "Epoch [42/200] Loss: 477.3061218261719\n",
      "Epoch [43/200] Loss: 473.8240966796875\n",
      "Epoch [44/200] Loss: 470.3915710449219\n",
      "Epoch [45/200] Loss: 467.0054016113281\n",
      "Epoch [46/200] Loss: 463.6625671386719\n",
      "Epoch [47/200] Loss: 460.36053466796875\n",
      "Epoch [48/200] Loss: 457.0967102050781\n",
      "Epoch [49/200] Loss: 453.8687438964844\n",
      "Epoch [50/200] Loss: 450.67474365234375\n",
      "Epoch [51/200] Loss: 447.512451171875\n",
      "Epoch [52/200] Loss: 444.380126953125\n",
      "Epoch [53/200] Loss: 441.2761535644531\n",
      "Epoch [54/200] Loss: 438.1990051269531\n",
      "Epoch [55/200] Loss: 435.147216796875\n",
      "Epoch [56/200] Loss: 432.1198425292969\n",
      "Epoch [57/200] Loss: 429.1159973144531\n",
      "Epoch [58/200] Loss: 426.1344909667969\n",
      "Epoch [59/200] Loss: 423.17498779296875\n",
      "Epoch [60/200] Loss: 420.2367858886719\n",
      "Epoch [61/200] Loss: 417.3195495605469\n",
      "Epoch [62/200] Loss: 414.4228210449219\n",
      "Epoch [63/200] Loss: 411.54644775390625\n",
      "Epoch [64/200] Loss: 408.6900939941406\n",
      "Epoch [65/200] Loss: 405.853515625\n",
      "Epoch [66/200] Loss: 403.0365295410156\n",
      "Epoch [67/200] Loss: 400.23895263671875\n",
      "Epoch [68/200] Loss: 397.4606018066406\n",
      "Epoch [69/200] Loss: 394.7011413574219\n",
      "Epoch [70/200] Loss: 391.96038818359375\n",
      "Epoch [71/200] Loss: 389.2382507324219\n",
      "Epoch [72/200] Loss: 386.5343933105469\n",
      "Epoch [73/200] Loss: 383.8487243652344\n",
      "Epoch [74/200] Loss: 381.18109130859375\n",
      "Epoch [75/200] Loss: 378.5312805175781\n",
      "Epoch [76/200] Loss: 375.8993225097656\n",
      "Epoch [77/200] Loss: 373.28497314453125\n",
      "Epoch [78/200] Loss: 370.68829345703125\n",
      "Epoch [79/200] Loss: 368.1090393066406\n",
      "Epoch [80/200] Loss: 365.5472717285156\n",
      "Epoch [81/200] Loss: 363.0027770996094\n",
      "Epoch [82/200] Loss: 360.4757080078125\n",
      "Epoch [83/200] Loss: 357.96588134765625\n",
      "Epoch [84/200] Loss: 355.4732360839844\n",
      "Epoch [85/200] Loss: 352.9976806640625\n",
      "Epoch [86/200] Loss: 350.5390930175781\n",
      "Epoch [87/200] Loss: 348.09735107421875\n",
      "Epoch [88/200] Loss: 345.67236328125\n",
      "Epoch [89/200] Loss: 343.26422119140625\n",
      "Epoch [90/200] Loss: 340.8723449707031\n",
      "Epoch [91/200] Loss: 338.4970703125\n",
      "Epoch [92/200] Loss: 336.1380310058594\n",
      "Epoch [93/200] Loss: 333.79510498046875\n",
      "Epoch [94/200] Loss: 331.4682312011719\n",
      "Epoch [95/200] Loss: 329.1571960449219\n",
      "Epoch [96/200] Loss: 326.8619079589844\n",
      "Epoch [97/200] Loss: 324.5823059082031\n",
      "Epoch [98/200] Loss: 322.3180236816406\n",
      "Epoch [99/200] Loss: 320.0692443847656\n",
      "Epoch [100/200] Loss: 317.8356628417969\n",
      "Epoch [101/200] Loss: 315.6170654296875\n",
      "Epoch [102/200] Loss: 313.4134216308594\n",
      "Epoch [103/200] Loss: 311.2247009277344\n",
      "Epoch [104/200] Loss: 309.05059814453125\n",
      "Epoch [105/200] Loss: 306.8910217285156\n",
      "Epoch [106/200] Loss: 304.74591064453125\n",
      "Epoch [107/200] Loss: 302.6151428222656\n",
      "Epoch [108/200] Loss: 300.49853515625\n",
      "Epoch [109/200] Loss: 298.39605712890625\n",
      "Epoch [110/200] Loss: 296.3074951171875\n",
      "Epoch [111/200] Loss: 294.2327575683594\n",
      "Epoch [112/200] Loss: 292.17181396484375\n",
      "Epoch [113/200] Loss: 290.1244812011719\n",
      "Epoch [114/200] Loss: 288.090576171875\n",
      "Epoch [115/200] Loss: 286.0701904296875\n",
      "Epoch [116/200] Loss: 284.06298828125\n",
      "Epoch [117/200] Loss: 282.06915283203125\n",
      "Epoch [118/200] Loss: 280.08819580078125\n",
      "Epoch [119/200] Loss: 278.1203308105469\n",
      "Epoch [120/200] Loss: 276.165283203125\n",
      "Epoch [121/200] Loss: 274.2230529785156\n",
      "Epoch [122/200] Loss: 272.29351806640625\n",
      "Epoch [123/200] Loss: 270.3765869140625\n",
      "Epoch [124/200] Loss: 268.47216796875\n",
      "Epoch [125/200] Loss: 266.58013916015625\n",
      "Epoch [126/200] Loss: 264.7004699707031\n",
      "Epoch [127/200] Loss: 262.8328857421875\n",
      "Epoch [128/200] Loss: 260.9775695800781\n",
      "Epoch [129/200] Loss: 259.1343078613281\n",
      "Epoch [130/200] Loss: 257.302978515625\n",
      "Epoch [131/200] Loss: 255.48353576660156\n",
      "Epoch [132/200] Loss: 253.67593383789062\n",
      "Epoch [133/200] Loss: 251.8799591064453\n",
      "Epoch [134/200] Loss: 250.0956573486328\n",
      "Epoch [135/200] Loss: 248.322998046875\n",
      "Epoch [136/200] Loss: 246.56175231933594\n",
      "Epoch [137/200] Loss: 244.81195068359375\n",
      "Epoch [138/200] Loss: 243.0734405517578\n",
      "Epoch [139/200] Loss: 241.34617614746094\n",
      "Epoch [140/200] Loss: 239.63009643554688\n",
      "Epoch [141/200] Loss: 237.92518615722656\n",
      "Epoch [142/200] Loss: 236.23126220703125\n",
      "Epoch [143/200] Loss: 234.5482635498047\n",
      "Epoch [144/200] Loss: 232.87615966796875\n",
      "Epoch [145/200] Loss: 231.21495056152344\n",
      "Epoch [146/200] Loss: 229.56448364257812\n",
      "Epoch [147/200] Loss: 227.9247283935547\n",
      "Epoch [148/200] Loss: 226.2955322265625\n",
      "Epoch [149/200] Loss: 224.67691040039062\n",
      "Epoch [150/200] Loss: 223.06875610351562\n",
      "Epoch [151/200] Loss: 221.47105407714844\n",
      "Epoch [152/200] Loss: 219.8836669921875\n",
      "Epoch [153/200] Loss: 218.306640625\n",
      "Epoch [154/200] Loss: 216.73985290527344\n",
      "Epoch [155/200] Loss: 215.18321228027344\n",
      "Epoch [156/200] Loss: 213.6366729736328\n",
      "Epoch [157/200] Loss: 212.10023498535156\n",
      "Epoch [158/200] Loss: 210.57371520996094\n",
      "Epoch [159/200] Loss: 209.0572052001953\n",
      "Epoch [160/200] Loss: 207.55052185058594\n",
      "Epoch [161/200] Loss: 206.05369567871094\n",
      "Epoch [162/200] Loss: 204.56658935546875\n",
      "Epoch [163/200] Loss: 203.0892791748047\n",
      "Epoch [164/200] Loss: 201.62152099609375\n",
      "Epoch [165/200] Loss: 200.1632843017578\n",
      "Epoch [166/200] Loss: 198.7147216796875\n",
      "Epoch [167/200] Loss: 197.27557373046875\n",
      "Epoch [168/200] Loss: 195.8458709716797\n",
      "Epoch [169/200] Loss: 194.42556762695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/200] Loss: 193.01451110839844\n",
      "Epoch [171/200] Loss: 191.61277770996094\n",
      "Epoch [172/200] Loss: 190.22023010253906\n",
      "Epoch [173/200] Loss: 188.83685302734375\n",
      "Epoch [174/200] Loss: 187.46258544921875\n",
      "Epoch [175/200] Loss: 186.09739685058594\n",
      "Epoch [176/200] Loss: 184.74118041992188\n",
      "Epoch [177/200] Loss: 183.39395141601562\n",
      "Epoch [178/200] Loss: 182.05557250976562\n",
      "Epoch [179/200] Loss: 180.72610473632812\n",
      "Epoch [180/200] Loss: 179.4054412841797\n",
      "Epoch [181/200] Loss: 178.0935516357422\n",
      "Epoch [182/200] Loss: 176.79031372070312\n",
      "Epoch [183/200] Loss: 175.49578857421875\n",
      "Epoch [184/200] Loss: 174.2098388671875\n",
      "Epoch [185/200] Loss: 172.9324493408203\n",
      "Epoch [186/200] Loss: 171.66363525390625\n",
      "Epoch [187/200] Loss: 170.40321350097656\n",
      "Epoch [188/200] Loss: 169.1512908935547\n",
      "Epoch [189/200] Loss: 167.90768432617188\n",
      "Epoch [190/200] Loss: 166.67242431640625\n",
      "Epoch [191/200] Loss: 165.4455108642578\n",
      "Epoch [192/200] Loss: 164.2267608642578\n",
      "Epoch [193/200] Loss: 163.01625061035156\n",
      "Epoch [194/200] Loss: 161.81390380859375\n",
      "Epoch [195/200] Loss: 160.61962890625\n",
      "Epoch [196/200] Loss: 159.43341064453125\n",
      "Epoch [197/200] Loss: 158.25518798828125\n",
      "Epoch [198/200] Loss: 157.08497619628906\n",
      "Epoch [199/200] Loss: 155.92271423339844\n",
      "Epoch [200/200] Loss: 154.76832580566406\n",
      "Predicted days_remaining for parent_id 321: 15.952362060546875\n",
      "Training for parent_id 326...\n",
      "Epoch [1/200] Loss: 231.8595428466797\n",
      "Epoch [2/200] Loss: 225.74363708496094\n",
      "Epoch [3/200] Loss: 219.732421875\n",
      "Epoch [4/200] Loss: 213.81898498535156\n",
      "Epoch [5/200] Loss: 208.0025634765625\n",
      "Epoch [6/200] Loss: 202.28927612304688\n",
      "Epoch [7/200] Loss: 196.69314575195312\n",
      "Epoch [8/200] Loss: 191.23414611816406\n",
      "Epoch [9/200] Loss: 185.93479919433594\n",
      "Epoch [10/200] Loss: 180.81698608398438\n",
      "Epoch [11/200] Loss: 175.89984130859375\n",
      "Epoch [12/200] Loss: 171.1976776123047\n",
      "Epoch [13/200] Loss: 166.718994140625\n",
      "Epoch [14/200] Loss: 162.4660186767578\n",
      "Epoch [15/200] Loss: 158.43544006347656\n",
      "Epoch [16/200] Loss: 154.61956787109375\n",
      "Epoch [17/200] Loss: 151.00804138183594\n",
      "Epoch [18/200] Loss: 147.5893096923828\n",
      "Epoch [19/200] Loss: 144.3514862060547\n",
      "Epoch [20/200] Loss: 141.2830047607422\n",
      "Epoch [21/200] Loss: 138.37261962890625\n",
      "Epoch [22/200] Loss: 135.60952758789062\n",
      "Epoch [23/200] Loss: 132.98318481445312\n",
      "Epoch [24/200] Loss: 130.48324584960938\n",
      "Epoch [25/200] Loss: 128.0995330810547\n",
      "Epoch [26/200] Loss: 125.82223510742188\n",
      "Epoch [27/200] Loss: 123.64208221435547\n",
      "Epoch [28/200] Loss: 121.55034637451172\n",
      "Epoch [29/200] Loss: 119.53899383544922\n",
      "Epoch [30/200] Loss: 117.60076141357422\n",
      "Epoch [31/200] Loss: 115.72900390625\n",
      "Epoch [32/200] Loss: 113.91766357421875\n",
      "Epoch [33/200] Loss: 112.16136169433594\n",
      "Epoch [34/200] Loss: 110.45523834228516\n",
      "Epoch [35/200] Loss: 108.79508972167969\n",
      "Epoch [36/200] Loss: 107.17703247070312\n",
      "Epoch [37/200] Loss: 105.59783935546875\n",
      "Epoch [38/200] Loss: 104.05452728271484\n",
      "Epoch [39/200] Loss: 102.54454040527344\n",
      "Epoch [40/200] Loss: 101.06560516357422\n",
      "Epoch [41/200] Loss: 99.61563110351562\n",
      "Epoch [42/200] Loss: 98.19290924072266\n",
      "Epoch [43/200] Loss: 96.79573822021484\n",
      "Epoch [44/200] Loss: 95.4227294921875\n",
      "Epoch [45/200] Loss: 94.07257843017578\n",
      "Epoch [46/200] Loss: 92.744140625\n",
      "Epoch [47/200] Loss: 91.43638610839844\n",
      "Epoch [48/200] Loss: 90.14842224121094\n",
      "Epoch [49/200] Loss: 88.87946319580078\n",
      "Epoch [50/200] Loss: 87.62882995605469\n",
      "Epoch [51/200] Loss: 86.39590454101562\n",
      "Epoch [52/200] Loss: 85.18017578125\n",
      "Epoch [53/200] Loss: 83.981201171875\n",
      "Epoch [54/200] Loss: 82.7986068725586\n",
      "Epoch [55/200] Loss: 81.63209533691406\n",
      "Epoch [56/200] Loss: 80.4814224243164\n",
      "Epoch [57/200] Loss: 79.34634399414062\n",
      "Epoch [58/200] Loss: 78.22676086425781\n",
      "Epoch [59/200] Loss: 77.12254333496094\n",
      "Epoch [60/200] Loss: 76.0335693359375\n",
      "Epoch [61/200] Loss: 74.95982360839844\n",
      "Epoch [62/200] Loss: 73.90126037597656\n",
      "Epoch [63/200] Loss: 72.85784149169922\n",
      "Epoch [64/200] Loss: 71.82955169677734\n",
      "Epoch [65/200] Loss: 70.81636810302734\n",
      "Epoch [66/200] Loss: 69.8182144165039\n",
      "Epoch [67/200] Loss: 68.83507537841797\n",
      "Epoch [68/200] Loss: 67.86687469482422\n",
      "Epoch [69/200] Loss: 66.91349792480469\n",
      "Epoch [70/200] Loss: 65.97486877441406\n",
      "Epoch [71/200] Loss: 65.05085754394531\n",
      "Epoch [72/200] Loss: 64.14132690429688\n",
      "Epoch [73/200] Loss: 63.24614334106445\n",
      "Epoch [74/200] Loss: 62.36510467529297\n",
      "Epoch [75/200] Loss: 61.49810791015625\n",
      "Epoch [76/200] Loss: 60.64491653442383\n",
      "Epoch [77/200] Loss: 59.805389404296875\n",
      "Epoch [78/200] Loss: 58.97933578491211\n",
      "Epoch [79/200] Loss: 58.16656494140625\n",
      "Epoch [80/200] Loss: 57.366905212402344\n",
      "Epoch [81/200] Loss: 56.580142974853516\n",
      "Epoch [82/200] Loss: 55.80612564086914\n",
      "Epoch [83/200] Loss: 55.04463577270508\n",
      "Epoch [84/200] Loss: 54.295509338378906\n",
      "Epoch [85/200] Loss: 53.55854034423828\n",
      "Epoch [86/200] Loss: 52.83357620239258\n",
      "Epoch [87/200] Loss: 52.12044906616211\n",
      "Epoch [88/200] Loss: 51.41893005371094\n",
      "Epoch [89/200] Loss: 50.7288932800293\n",
      "Epoch [90/200] Loss: 50.050148010253906\n",
      "Epoch [91/200] Loss: 49.382530212402344\n",
      "Epoch [92/200] Loss: 48.72586441040039\n",
      "Epoch [93/200] Loss: 48.079994201660156\n",
      "Epoch [94/200] Loss: 47.44478225708008\n",
      "Epoch [95/200] Loss: 46.82001876831055\n",
      "Epoch [96/200] Loss: 46.20559310913086\n",
      "Epoch [97/200] Loss: 45.60133361816406\n",
      "Epoch [98/200] Loss: 45.0070686340332\n",
      "Epoch [99/200] Loss: 44.42268371582031\n",
      "Epoch [100/200] Loss: 43.848026275634766\n",
      "Epoch [101/200] Loss: 43.28293228149414\n",
      "Epoch [102/200] Loss: 42.727272033691406\n",
      "Epoch [103/200] Loss: 42.18092346191406\n",
      "Epoch [104/200] Loss: 41.64371109008789\n",
      "Epoch [105/200] Loss: 41.11553192138672\n",
      "Epoch [106/200] Loss: 40.59624481201172\n",
      "Epoch [107/200] Loss: 40.08571243286133\n",
      "Epoch [108/200] Loss: 39.583797454833984\n",
      "Epoch [109/200] Loss: 39.09040451049805\n",
      "Epoch [110/200] Loss: 38.6053581237793\n",
      "Epoch [111/200] Loss: 38.12859344482422\n",
      "Epoch [112/200] Loss: 37.659942626953125\n",
      "Epoch [113/200] Loss: 37.19931411743164\n",
      "Epoch [114/200] Loss: 36.746578216552734\n",
      "Epoch [115/200] Loss: 36.30160903930664\n",
      "Epoch [116/200] Loss: 35.86430358886719\n",
      "Epoch [117/200] Loss: 35.4345703125\n",
      "Epoch [118/200] Loss: 35.01226043701172\n",
      "Epoch [119/200] Loss: 34.5972900390625\n",
      "Epoch [120/200] Loss: 34.189537048339844\n",
      "Epoch [121/200] Loss: 33.788902282714844\n",
      "Epoch [122/200] Loss: 33.39527130126953\n",
      "Epoch [123/200] Loss: 33.00855255126953\n",
      "Epoch [124/200] Loss: 32.628631591796875\n",
      "Epoch [125/200] Loss: 32.25542449951172\n",
      "Epoch [126/200] Loss: 31.888826370239258\n",
      "Epoch [127/200] Loss: 31.528743743896484\n",
      "Epoch [128/200] Loss: 31.175048828125\n",
      "Epoch [129/200] Loss: 30.827674865722656\n",
      "Epoch [130/200] Loss: 30.48653221130371\n",
      "Epoch [131/200] Loss: 30.151525497436523\n",
      "Epoch [132/200] Loss: 29.822540283203125\n",
      "Epoch [133/200] Loss: 29.499509811401367\n",
      "Epoch [134/200] Loss: 29.182331085205078\n",
      "Epoch [135/200] Loss: 28.87091064453125\n",
      "Epoch [136/200] Loss: 28.565185546875\n",
      "Epoch [137/200] Loss: 28.26504898071289\n",
      "Epoch [138/200] Loss: 27.970436096191406\n",
      "Epoch [139/200] Loss: 27.68124771118164\n",
      "Epoch [140/200] Loss: 27.397382736206055\n",
      "Epoch [141/200] Loss: 27.118797302246094\n",
      "Epoch [142/200] Loss: 26.845399856567383\n",
      "Epoch [143/200] Loss: 26.577091217041016\n",
      "Epoch [144/200] Loss: 26.31381607055664\n",
      "Epoch [145/200] Loss: 26.055477142333984\n",
      "Epoch [146/200] Loss: 25.802017211914062\n",
      "Epoch [147/200] Loss: 25.553346633911133\n",
      "Epoch [148/200] Loss: 25.30938148498535\n",
      "Epoch [149/200] Loss: 25.070085525512695\n",
      "Epoch [150/200] Loss: 24.835336685180664\n",
      "Epoch [151/200] Loss: 24.605098724365234\n",
      "Epoch [152/200] Loss: 24.37929344177246\n",
      "Epoch [153/200] Loss: 24.157835006713867\n",
      "Epoch [154/200] Loss: 23.940673828125\n",
      "Epoch [155/200] Loss: 23.727733612060547\n",
      "Epoch [156/200] Loss: 23.518938064575195\n",
      "Epoch [157/200] Loss: 23.314231872558594\n",
      "Epoch [158/200] Loss: 23.113548278808594\n",
      "Epoch [159/200] Loss: 22.91680908203125\n",
      "Epoch [160/200] Loss: 22.72396469116211\n",
      "Epoch [161/200] Loss: 22.534934997558594\n",
      "Epoch [162/200] Loss: 22.349674224853516\n",
      "Epoch [163/200] Loss: 22.16812515258789\n",
      "Epoch [164/200] Loss: 21.990205764770508\n",
      "Epoch [165/200] Loss: 21.815860748291016\n",
      "Epoch [166/200] Loss: 21.645042419433594\n",
      "Epoch [167/200] Loss: 21.477680206298828\n",
      "Epoch [168/200] Loss: 21.313718795776367\n",
      "Epoch [169/200] Loss: 21.15310287475586\n",
      "Epoch [170/200] Loss: 20.99577522277832\n",
      "Epoch [171/200] Loss: 20.841676712036133\n",
      "Epoch [172/200] Loss: 20.690752029418945\n",
      "Epoch [173/200] Loss: 20.542953491210938\n",
      "Epoch [174/200] Loss: 20.398225784301758\n",
      "Epoch [175/200] Loss: 20.25650405883789\n",
      "Epoch [176/200] Loss: 20.11774444580078\n",
      "Epoch [177/200] Loss: 19.981910705566406\n",
      "Epoch [178/200] Loss: 19.848922729492188\n",
      "Epoch [179/200] Loss: 19.71874237060547\n",
      "Epoch [180/200] Loss: 19.591320037841797\n",
      "Epoch [181/200] Loss: 19.466609954833984\n",
      "Epoch [182/200] Loss: 19.344566345214844\n",
      "Epoch [183/200] Loss: 19.225135803222656\n",
      "Epoch [184/200] Loss: 19.1082706451416\n",
      "Epoch [185/200] Loss: 18.99391746520996\n",
      "Epoch [186/200] Loss: 18.882049560546875\n",
      "Epoch [187/200] Loss: 18.772607803344727\n",
      "Epoch [188/200] Loss: 18.66555404663086\n",
      "Epoch [189/200] Loss: 18.56083869934082\n",
      "Epoch [190/200] Loss: 18.458419799804688\n",
      "Epoch [191/200] Loss: 18.358257293701172\n",
      "Epoch [192/200] Loss: 18.260303497314453\n",
      "Epoch [193/200] Loss: 18.164533615112305\n",
      "Epoch [194/200] Loss: 18.070877075195312\n",
      "Epoch [195/200] Loss: 17.979328155517578\n",
      "Epoch [196/200] Loss: 17.88982391357422\n",
      "Epoch [197/200] Loss: 17.80232810974121\n",
      "Epoch [198/200] Loss: 17.716806411743164\n",
      "Epoch [199/200] Loss: 17.633220672607422\n",
      "Epoch [200/200] Loss: 17.55153465270996\n",
      "Predicted days_remaining for parent_id 326: 13.008101463317871\n",
      "Training for parent_id 332...\n",
      "Epoch [1/200] Loss: 156.57614135742188\n",
      "Epoch [2/200] Loss: 151.33663940429688\n",
      "Epoch [3/200] Loss: 146.2476348876953\n",
      "Epoch [4/200] Loss: 141.3305206298828\n",
      "Epoch [5/200] Loss: 136.60438537597656\n",
      "Epoch [6/200] Loss: 132.07566833496094\n",
      "Epoch [7/200] Loss: 127.74420928955078\n",
      "Epoch [8/200] Loss: 123.6060562133789\n",
      "Epoch [9/200] Loss: 119.6546630859375\n",
      "Epoch [10/200] Loss: 115.88209533691406\n",
      "Epoch [11/200] Loss: 112.27986145019531\n",
      "Epoch [12/200] Loss: 108.8393325805664\n",
      "Epoch [13/200] Loss: 105.5517807006836\n",
      "Epoch [14/200] Loss: 102.40866088867188\n",
      "Epoch [15/200] Loss: 99.40180206298828\n",
      "Epoch [16/200] Loss: 96.52386474609375\n",
      "Epoch [17/200] Loss: 93.76851654052734\n",
      "Epoch [18/200] Loss: 91.13046264648438\n",
      "Epoch [19/200] Loss: 88.60531616210938\n",
      "Epoch [20/200] Loss: 86.18901824951172\n",
      "Epoch [21/200] Loss: 83.87785339355469\n",
      "Epoch [22/200] Loss: 81.668212890625\n",
      "Epoch [23/200] Loss: 79.5564193725586\n",
      "Epoch [24/200] Loss: 77.53866577148438\n",
      "Epoch [25/200] Loss: 75.61085510253906\n",
      "Epoch [26/200] Loss: 73.76840209960938\n",
      "Epoch [27/200] Loss: 72.00635528564453\n",
      "Epoch [28/200] Loss: 70.31949615478516\n",
      "Epoch [29/200] Loss: 68.70244598388672\n",
      "Epoch [30/200] Loss: 67.14998626708984\n",
      "Epoch [31/200] Loss: 65.65711212158203\n",
      "Epoch [32/200] Loss: 64.21928405761719\n",
      "Epoch [33/200] Loss: 62.83242416381836\n",
      "Epoch [34/200] Loss: 61.492942810058594\n",
      "Epoch [35/200] Loss: 60.197792053222656\n",
      "Epoch [36/200] Loss: 58.944217681884766\n",
      "Epoch [37/200] Loss: 57.72995376586914\n",
      "Epoch [38/200] Loss: 56.55298614501953\n",
      "Epoch [39/200] Loss: 55.411598205566406\n",
      "Epoch [40/200] Loss: 54.30423355102539\n",
      "Epoch [41/200] Loss: 53.22953796386719\n",
      "Epoch [42/200] Loss: 52.186317443847656\n",
      "Epoch [43/200] Loss: 51.17342758178711\n",
      "Epoch [44/200] Loss: 50.189842224121094\n",
      "Epoch [45/200] Loss: 49.23457717895508\n",
      "Epoch [46/200] Loss: 48.30668640136719\n",
      "Epoch [47/200] Loss: 47.40522384643555\n",
      "Epoch [48/200] Loss: 46.5292854309082\n",
      "Epoch [49/200] Loss: 45.677921295166016\n",
      "Epoch [50/200] Loss: 44.850215911865234\n",
      "Epoch [51/200] Loss: 44.04521942138672\n",
      "Epoch [52/200] Loss: 43.262001037597656\n",
      "Epoch [53/200] Loss: 42.499656677246094\n",
      "Epoch [54/200] Loss: 41.75729751586914\n",
      "Epoch [55/200] Loss: 41.0340576171875\n",
      "Epoch [56/200] Loss: 40.32917785644531\n",
      "Epoch [57/200] Loss: 39.641910552978516\n",
      "Epoch [58/200] Loss: 38.97162628173828\n",
      "Epoch [59/200] Loss: 38.317726135253906\n",
      "Epoch [60/200] Loss: 37.679710388183594\n",
      "Epoch [61/200] Loss: 37.0571403503418\n",
      "Epoch [62/200] Loss: 36.44960403442383\n",
      "Epoch [63/200] Loss: 35.85675048828125\n",
      "Epoch [64/200] Loss: 35.27824020385742\n",
      "Epoch [65/200] Loss: 34.7137565612793\n",
      "Epoch [66/200] Loss: 34.162994384765625\n",
      "Epoch [67/200] Loss: 33.62565994262695\n",
      "Epoch [68/200] Loss: 33.1014404296875\n",
      "Epoch [69/200] Loss: 32.590049743652344\n",
      "Epoch [70/200] Loss: 32.091190338134766\n",
      "Epoch [71/200] Loss: 31.60457992553711\n",
      "Epoch [72/200] Loss: 31.129905700683594\n",
      "Epoch [73/200] Loss: 30.66691017150879\n",
      "Epoch [74/200] Loss: 30.215299606323242\n",
      "Epoch [75/200] Loss: 29.77481460571289\n",
      "Epoch [76/200] Loss: 29.34518814086914\n",
      "Epoch [77/200] Loss: 28.926158905029297\n",
      "Epoch [78/200] Loss: 28.517471313476562\n",
      "Epoch [79/200] Loss: 28.11890411376953\n",
      "Epoch [80/200] Loss: 27.73019790649414\n",
      "Epoch [81/200] Loss: 27.351146697998047\n",
      "Epoch [82/200] Loss: 26.98151969909668\n",
      "Epoch [83/200] Loss: 26.62111473083496\n",
      "Epoch [84/200] Loss: 26.269695281982422\n",
      "Epoch [85/200] Loss: 25.927085876464844\n",
      "Epoch [86/200] Loss: 25.59308624267578\n",
      "Epoch [87/200] Loss: 25.267486572265625\n",
      "Epoch [88/200] Loss: 24.950130462646484\n",
      "Epoch [89/200] Loss: 24.640806198120117\n",
      "Epoch [90/200] Loss: 24.339359283447266\n",
      "Epoch [91/200] Loss: 24.04558563232422\n",
      "Epoch [92/200] Loss: 23.759353637695312\n",
      "Epoch [93/200] Loss: 23.480466842651367\n",
      "Epoch [94/200] Loss: 23.208778381347656\n",
      "Epoch [95/200] Loss: 22.94411277770996\n",
      "Epoch [96/200] Loss: 22.686342239379883\n",
      "Epoch [97/200] Loss: 22.435283660888672\n",
      "Epoch [98/200] Loss: 22.19080924987793\n",
      "Epoch [99/200] Loss: 21.95276641845703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200] Loss: 21.721012115478516\n",
      "Epoch [101/200] Loss: 21.495397567749023\n",
      "Epoch [102/200] Loss: 21.275798797607422\n",
      "Epoch [103/200] Loss: 21.062091827392578\n",
      "Epoch [104/200] Loss: 20.8541202545166\n",
      "Epoch [105/200] Loss: 20.651763916015625\n",
      "Epoch [106/200] Loss: 20.454912185668945\n",
      "Epoch [107/200] Loss: 20.26342010498047\n",
      "Epoch [108/200] Loss: 20.077184677124023\n",
      "Epoch [109/200] Loss: 19.896074295043945\n",
      "Epoch [110/200] Loss: 19.7199764251709\n",
      "Epoch [111/200] Loss: 19.548778533935547\n",
      "Epoch [112/200] Loss: 19.382369995117188\n",
      "Epoch [113/200] Loss: 19.220640182495117\n",
      "Epoch [114/200] Loss: 19.063486099243164\n",
      "Epoch [115/200] Loss: 18.910789489746094\n",
      "Epoch [116/200] Loss: 18.762466430664062\n",
      "Epoch [117/200] Loss: 18.618392944335938\n",
      "Epoch [118/200] Loss: 18.478477478027344\n",
      "Epoch [119/200] Loss: 18.342636108398438\n",
      "Epoch [120/200] Loss: 18.21075439453125\n",
      "Epoch [121/200] Loss: 18.082740783691406\n",
      "Epoch [122/200] Loss: 17.958505630493164\n",
      "Epoch [123/200] Loss: 17.837966918945312\n",
      "Epoch [124/200] Loss: 17.72101402282715\n",
      "Epoch [125/200] Loss: 17.60757064819336\n",
      "Epoch [126/200] Loss: 17.497554779052734\n",
      "Epoch [127/200] Loss: 17.390871047973633\n",
      "Epoch [128/200] Loss: 17.287445068359375\n",
      "Epoch [129/200] Loss: 17.187191009521484\n",
      "Epoch [130/200] Loss: 17.090030670166016\n",
      "Epoch [131/200] Loss: 16.995880126953125\n",
      "Epoch [132/200] Loss: 16.904666900634766\n",
      "Epoch [133/200] Loss: 16.816312789916992\n",
      "Epoch [134/200] Loss: 16.730741500854492\n",
      "Epoch [135/200] Loss: 16.647884368896484\n",
      "Epoch [136/200] Loss: 16.567668914794922\n",
      "Epoch [137/200] Loss: 16.490022659301758\n",
      "Epoch [138/200] Loss: 16.414878845214844\n",
      "Epoch [139/200] Loss: 16.3421630859375\n",
      "Epoch [140/200] Loss: 16.271820068359375\n",
      "Epoch [141/200] Loss: 16.203773498535156\n",
      "Epoch [142/200] Loss: 16.13797378540039\n",
      "Epoch [143/200] Loss: 16.07434844970703\n",
      "Epoch [144/200] Loss: 16.012834548950195\n",
      "Epoch [145/200] Loss: 15.953376770019531\n",
      "Epoch [146/200] Loss: 15.89592170715332\n",
      "Epoch [147/200] Loss: 15.84040641784668\n",
      "Epoch [148/200] Loss: 15.786775588989258\n",
      "Epoch [149/200] Loss: 15.73497200012207\n",
      "Epoch [150/200] Loss: 15.68494987487793\n",
      "Epoch [151/200] Loss: 15.636648178100586\n",
      "Epoch [152/200] Loss: 15.590021133422852\n",
      "Epoch [153/200] Loss: 15.545021057128906\n",
      "Epoch [154/200] Loss: 15.50159740447998\n",
      "Epoch [155/200] Loss: 15.459699630737305\n",
      "Epoch [156/200] Loss: 15.419279098510742\n",
      "Epoch [157/200] Loss: 15.380298614501953\n",
      "Epoch [158/200] Loss: 15.342711448669434\n",
      "Epoch [159/200] Loss: 15.30647087097168\n",
      "Epoch [160/200] Loss: 15.271537780761719\n",
      "Epoch [161/200] Loss: 15.237874031066895\n",
      "Epoch [162/200] Loss: 15.20543098449707\n",
      "Epoch [163/200] Loss: 15.174175262451172\n",
      "Epoch [164/200] Loss: 15.144075393676758\n",
      "Epoch [165/200] Loss: 15.115080833435059\n",
      "Epoch [166/200] Loss: 15.087167739868164\n",
      "Epoch [167/200] Loss: 15.060293197631836\n",
      "Epoch [168/200] Loss: 15.034428596496582\n",
      "Epoch [169/200] Loss: 15.009536743164062\n",
      "Epoch [170/200] Loss: 14.985586166381836\n",
      "Epoch [171/200] Loss: 14.962547302246094\n",
      "Epoch [172/200] Loss: 14.940386772155762\n",
      "Epoch [173/200] Loss: 14.919076919555664\n",
      "Epoch [174/200] Loss: 14.898590087890625\n",
      "Epoch [175/200] Loss: 14.878898620605469\n",
      "Epoch [176/200] Loss: 14.859970092773438\n",
      "Epoch [177/200] Loss: 14.841782569885254\n",
      "Epoch [178/200] Loss: 14.824310302734375\n",
      "Epoch [179/200] Loss: 14.80752944946289\n",
      "Epoch [180/200] Loss: 14.791411399841309\n",
      "Epoch [181/200] Loss: 14.775934219360352\n",
      "Epoch [182/200] Loss: 14.761075973510742\n",
      "Epoch [183/200] Loss: 14.746816635131836\n",
      "Epoch [184/200] Loss: 14.733132362365723\n",
      "Epoch [185/200] Loss: 14.720001220703125\n",
      "Epoch [186/200] Loss: 14.707405090332031\n",
      "Epoch [187/200] Loss: 14.69532585144043\n",
      "Epoch [188/200] Loss: 14.68374252319336\n",
      "Epoch [189/200] Loss: 14.672637939453125\n",
      "Epoch [190/200] Loss: 14.661992073059082\n",
      "Epoch [191/200] Loss: 14.651790618896484\n",
      "Epoch [192/200] Loss: 14.642013549804688\n",
      "Epoch [193/200] Loss: 14.632649421691895\n",
      "Epoch [194/200] Loss: 14.623678207397461\n",
      "Epoch [195/200] Loss: 14.615089416503906\n",
      "Epoch [196/200] Loss: 14.606863021850586\n",
      "Epoch [197/200] Loss: 14.598989486694336\n",
      "Epoch [198/200] Loss: 14.59145450592041\n",
      "Epoch [199/200] Loss: 14.584243774414062\n",
      "Epoch [200/200] Loss: 14.57734489440918\n",
      "Predicted days_remaining for parent_id 332: 11.384970664978027\n",
      "Training for parent_id 340...\n",
      "Epoch [1/200] Loss: 265.39990234375\n",
      "Epoch [2/200] Loss: 258.3448791503906\n",
      "Epoch [3/200] Loss: 251.60025024414062\n",
      "Epoch [4/200] Loss: 245.19039916992188\n",
      "Epoch [5/200] Loss: 239.1101837158203\n",
      "Epoch [6/200] Loss: 233.32249450683594\n",
      "Epoch [7/200] Loss: 227.7749786376953\n",
      "Epoch [8/200] Loss: 222.41693115234375\n",
      "Epoch [9/200] Loss: 217.20741271972656\n",
      "Epoch [10/200] Loss: 212.117919921875\n",
      "Epoch [11/200] Loss: 207.13201904296875\n",
      "Epoch [12/200] Loss: 202.2437286376953\n",
      "Epoch [13/200] Loss: 197.455078125\n",
      "Epoch [14/200] Loss: 192.77386474609375\n",
      "Epoch [15/200] Loss: 188.2119598388672\n",
      "Epoch [16/200] Loss: 183.7836151123047\n",
      "Epoch [17/200] Loss: 179.5039825439453\n",
      "Epoch [18/200] Loss: 175.3878173828125\n",
      "Epoch [19/200] Loss: 171.4476318359375\n",
      "Epoch [20/200] Loss: 167.6927032470703\n",
      "Epoch [21/200] Loss: 164.12779235839844\n",
      "Epoch [22/200] Loss: 160.7529296875\n",
      "Epoch [23/200] Loss: 157.56361389160156\n",
      "Epoch [24/200] Loss: 154.55120849609375\n",
      "Epoch [25/200] Loss: 151.7044677734375\n",
      "Epoch [26/200] Loss: 149.01023864746094\n",
      "Epoch [27/200] Loss: 146.45481872558594\n",
      "Epoch [28/200] Loss: 144.02442932128906\n",
      "Epoch [29/200] Loss: 141.7059326171875\n",
      "Epoch [30/200] Loss: 139.48712158203125\n",
      "Epoch [31/200] Loss: 137.35684204101562\n",
      "Epoch [32/200] Loss: 135.3050079345703\n",
      "Epoch [33/200] Loss: 133.32275390625\n",
      "Epoch [34/200] Loss: 131.40225219726562\n",
      "Epoch [35/200] Loss: 129.53671264648438\n",
      "Epoch [36/200] Loss: 127.72020721435547\n",
      "Epoch [37/200] Loss: 125.9478759765625\n",
      "Epoch [38/200] Loss: 124.21551513671875\n",
      "Epoch [39/200] Loss: 122.51969146728516\n",
      "Epoch [40/200] Loss: 120.857421875\n",
      "Epoch [41/200] Loss: 119.226318359375\n",
      "Epoch [42/200] Loss: 117.62445831298828\n",
      "Epoch [43/200] Loss: 116.0501708984375\n",
      "Epoch [44/200] Loss: 114.5020523071289\n",
      "Epoch [45/200] Loss: 112.97899627685547\n",
      "Epoch [46/200] Loss: 111.4799575805664\n",
      "Epoch [47/200] Loss: 110.004150390625\n",
      "Epoch [48/200] Loss: 108.55081939697266\n",
      "Epoch [49/200] Loss: 107.119384765625\n",
      "Epoch [50/200] Loss: 105.7093276977539\n",
      "Epoch [51/200] Loss: 104.32012939453125\n",
      "Epoch [52/200] Loss: 102.95143127441406\n",
      "Epoch [53/200] Loss: 101.60279083251953\n",
      "Epoch [54/200] Loss: 100.27389526367188\n",
      "Epoch [55/200] Loss: 98.96437072753906\n",
      "Epoch [56/200] Loss: 97.67390441894531\n",
      "Epoch [57/200] Loss: 96.40214538574219\n",
      "Epoch [58/200] Loss: 95.1487808227539\n",
      "Epoch [59/200] Loss: 93.9134521484375\n",
      "Epoch [60/200] Loss: 92.6958999633789\n",
      "Epoch [61/200] Loss: 91.49580383300781\n",
      "Epoch [62/200] Loss: 90.31279754638672\n",
      "Epoch [63/200] Loss: 89.14669036865234\n",
      "Epoch [64/200] Loss: 87.99710845947266\n",
      "Epoch [65/200] Loss: 86.86381530761719\n",
      "Epoch [66/200] Loss: 85.74653625488281\n",
      "Epoch [67/200] Loss: 84.64501190185547\n",
      "Epoch [68/200] Loss: 83.55896759033203\n",
      "Epoch [69/200] Loss: 82.48820495605469\n",
      "Epoch [70/200] Loss: 81.43241882324219\n",
      "Epoch [71/200] Loss: 80.39144897460938\n",
      "Epoch [72/200] Loss: 79.36499786376953\n",
      "Epoch [73/200] Loss: 78.35289764404297\n",
      "Epoch [74/200] Loss: 77.35493469238281\n",
      "Epoch [75/200] Loss: 76.37083435058594\n",
      "Epoch [76/200] Loss: 75.40046691894531\n",
      "Epoch [77/200] Loss: 74.443603515625\n",
      "Epoch [78/200] Loss: 73.50006103515625\n",
      "Epoch [79/200] Loss: 72.56965637207031\n",
      "Epoch [80/200] Loss: 71.65216064453125\n",
      "Epoch [81/200] Loss: 70.74744415283203\n",
      "Epoch [82/200] Loss: 69.85530090332031\n",
      "Epoch [83/200] Loss: 68.97557067871094\n",
      "Epoch [84/200] Loss: 68.1080551147461\n",
      "Epoch [85/200] Loss: 67.25263214111328\n",
      "Epoch [86/200] Loss: 66.40909576416016\n",
      "Epoch [87/200] Loss: 65.57731628417969\n",
      "Epoch [88/200] Loss: 64.75712585449219\n",
      "Epoch [89/200] Loss: 63.94837188720703\n",
      "Epoch [90/200] Loss: 63.1508903503418\n",
      "Epoch [91/200] Loss: 62.36455154418945\n",
      "Epoch [92/200] Loss: 61.58918762207031\n",
      "Epoch [93/200] Loss: 60.82469940185547\n",
      "Epoch [94/200] Loss: 60.070892333984375\n",
      "Epoch [95/200] Loss: 59.327667236328125\n",
      "Epoch [96/200] Loss: 58.5948486328125\n",
      "Epoch [97/200] Loss: 57.872337341308594\n",
      "Epoch [98/200] Loss: 57.16001510620117\n",
      "Epoch [99/200] Loss: 56.457706451416016\n",
      "Epoch [100/200] Loss: 55.76531982421875\n",
      "Epoch [101/200] Loss: 55.08273696899414\n",
      "Epoch [102/200] Loss: 54.40980911254883\n",
      "Epoch [103/200] Loss: 53.74641799926758\n",
      "Epoch [104/200] Loss: 53.09244155883789\n",
      "Epoch [105/200] Loss: 52.44780349731445\n",
      "Epoch [106/200] Loss: 51.81232833862305\n",
      "Epoch [107/200] Loss: 51.18594741821289\n",
      "Epoch [108/200] Loss: 50.568546295166016\n",
      "Epoch [109/200] Loss: 49.95998764038086\n",
      "Epoch [110/200] Loss: 49.36017608642578\n",
      "Epoch [111/200] Loss: 48.76899337768555\n",
      "Epoch [112/200] Loss: 48.18634796142578\n",
      "Epoch [113/200] Loss: 47.61213684082031\n",
      "Epoch [114/200] Loss: 47.04627227783203\n",
      "Epoch [115/200] Loss: 46.488616943359375\n",
      "Epoch [116/200] Loss: 45.9390754699707\n",
      "Epoch [117/200] Loss: 45.39756393432617\n",
      "Epoch [118/200] Loss: 44.863990783691406\n",
      "Epoch [119/200] Loss: 44.338226318359375\n",
      "Epoch [120/200] Loss: 43.82021713256836\n",
      "Epoch [121/200] Loss: 43.309837341308594\n",
      "Epoch [122/200] Loss: 42.807010650634766\n",
      "Epoch [123/200] Loss: 42.31163787841797\n",
      "Epoch [124/200] Loss: 41.823631286621094\n",
      "Epoch [125/200] Loss: 41.34290313720703\n",
      "Epoch [126/200] Loss: 40.86934280395508\n",
      "Epoch [127/200] Loss: 40.402870178222656\n",
      "Epoch [128/200] Loss: 39.94343185424805\n",
      "Epoch [129/200] Loss: 39.49090576171875\n",
      "Epoch [130/200] Loss: 39.04522705078125\n",
      "Epoch [131/200] Loss: 38.606300354003906\n",
      "Epoch [132/200] Loss: 38.17402648925781\n",
      "Epoch [133/200] Loss: 37.74835205078125\n",
      "Epoch [134/200] Loss: 37.32918930053711\n",
      "Epoch [135/200] Loss: 36.916439056396484\n",
      "Epoch [136/200] Loss: 36.51002883911133\n",
      "Epoch [137/200] Loss: 36.10989761352539\n",
      "Epoch [138/200] Loss: 35.71595764160156\n",
      "Epoch [139/200] Loss: 35.328121185302734\n",
      "Epoch [140/200] Loss: 34.94632339477539\n",
      "Epoch [141/200] Loss: 34.57048416137695\n",
      "Epoch [142/200] Loss: 34.20051574707031\n",
      "Epoch [143/200] Loss: 33.83637237548828\n",
      "Epoch [144/200] Loss: 33.47795104980469\n",
      "Epoch [145/200] Loss: 33.12519073486328\n",
      "Epoch [146/200] Loss: 32.77803421020508\n",
      "Epoch [147/200] Loss: 32.436378479003906\n",
      "Epoch [148/200] Loss: 32.10017395019531\n",
      "Epoch [149/200] Loss: 31.76936149597168\n",
      "Epoch [150/200] Loss: 31.44383430480957\n",
      "Epoch [151/200] Loss: 31.123565673828125\n",
      "Epoch [152/200] Loss: 30.808460235595703\n",
      "Epoch [153/200] Loss: 30.498455047607422\n",
      "Epoch [154/200] Loss: 30.1934871673584\n",
      "Epoch [155/200] Loss: 29.89350128173828\n",
      "Epoch [156/200] Loss: 29.598411560058594\n",
      "Epoch [157/200] Loss: 29.308164596557617\n",
      "Epoch [158/200] Loss: 29.02268409729004\n",
      "Epoch [159/200] Loss: 28.741920471191406\n",
      "Epoch [160/200] Loss: 28.465824127197266\n",
      "Epoch [161/200] Loss: 28.194290161132812\n",
      "Epoch [162/200] Loss: 27.927303314208984\n",
      "Epoch [163/200] Loss: 27.664762496948242\n",
      "Epoch [164/200] Loss: 27.40662956237793\n",
      "Epoch [165/200] Loss: 27.152854919433594\n",
      "Epoch [166/200] Loss: 26.903358459472656\n",
      "Epoch [167/200] Loss: 26.658092498779297\n",
      "Epoch [168/200] Loss: 26.416994094848633\n",
      "Epoch [169/200] Loss: 26.17999839782715\n",
      "Epoch [170/200] Loss: 25.947059631347656\n",
      "Epoch [171/200] Loss: 25.718120574951172\n",
      "Epoch [172/200] Loss: 25.49312400817871\n",
      "Epoch [173/200] Loss: 25.272014617919922\n",
      "Epoch [174/200] Loss: 25.054725646972656\n",
      "Epoch [175/200] Loss: 24.841228485107422\n",
      "Epoch [176/200] Loss: 24.631450653076172\n",
      "Epoch [177/200] Loss: 24.425344467163086\n",
      "Epoch [178/200] Loss: 24.222858428955078\n",
      "Epoch [179/200] Loss: 24.023929595947266\n",
      "Epoch [180/200] Loss: 23.828527450561523\n",
      "Epoch [181/200] Loss: 23.63658905029297\n",
      "Epoch [182/200] Loss: 23.44805908203125\n",
      "Epoch [183/200] Loss: 23.262895584106445\n",
      "Epoch [184/200] Loss: 23.08104133605957\n",
      "Epoch [185/200] Loss: 22.902462005615234\n",
      "Epoch [186/200] Loss: 22.727092742919922\n",
      "Epoch [187/200] Loss: 22.554906845092773\n",
      "Epoch [188/200] Loss: 22.385833740234375\n",
      "Epoch [189/200] Loss: 22.219833374023438\n",
      "Epoch [190/200] Loss: 22.056869506835938\n",
      "Epoch [191/200] Loss: 21.896886825561523\n",
      "Epoch [192/200] Loss: 21.73984718322754\n",
      "Epoch [193/200] Loss: 21.58570098876953\n",
      "Epoch [194/200] Loss: 21.434406280517578\n",
      "Epoch [195/200] Loss: 21.285919189453125\n",
      "Epoch [196/200] Loss: 21.14019203186035\n",
      "Epoch [197/200] Loss: 20.99718475341797\n",
      "Epoch [198/200] Loss: 20.85686492919922\n",
      "Epoch [199/200] Loss: 20.719173431396484\n",
      "Epoch [200/200] Loss: 20.58408546447754\n",
      "Predicted days_remaining for parent_id 340: 13.29764461517334\n",
      "Training for parent_id 346...\n",
      "Epoch [1/200] Loss: 363.2450256347656\n",
      "Epoch [2/200] Loss: 355.75482177734375\n",
      "Epoch [3/200] Loss: 348.3524169921875\n",
      "Epoch [4/200] Loss: 341.04132080078125\n",
      "Epoch [5/200] Loss: 333.8357849121094\n",
      "Epoch [6/200] Loss: 326.7500305175781\n",
      "Epoch [7/200] Loss: 319.794189453125\n",
      "Epoch [8/200] Loss: 312.9783020019531\n",
      "Epoch [9/200] Loss: 306.3166809082031\n",
      "Epoch [10/200] Loss: 299.82733154296875\n",
      "Epoch [11/200] Loss: 293.5294494628906\n",
      "Epoch [12/200] Loss: 287.44122314453125\n",
      "Epoch [13/200] Loss: 281.5775146484375\n",
      "Epoch [14/200] Loss: 275.9479064941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/200] Loss: 270.5569763183594\n",
      "Epoch [16/200] Loss: 265.40411376953125\n",
      "Epoch [17/200] Loss: 260.4842224121094\n",
      "Epoch [18/200] Loss: 255.78834533691406\n",
      "Epoch [19/200] Loss: 251.3050994873047\n",
      "Epoch [20/200] Loss: 247.0218048095703\n",
      "Epoch [21/200] Loss: 242.92556762695312\n",
      "Epoch [22/200] Loss: 239.00384521484375\n",
      "Epoch [23/200] Loss: 235.2447509765625\n",
      "Epoch [24/200] Loss: 231.6372833251953\n",
      "Epoch [25/200] Loss: 228.17123413085938\n",
      "Epoch [26/200] Loss: 224.83706665039062\n",
      "Epoch [27/200] Loss: 221.62594604492188\n",
      "Epoch [28/200] Loss: 218.52931213378906\n",
      "Epoch [29/200] Loss: 215.53912353515625\n",
      "Epoch [30/200] Loss: 212.64743041992188\n",
      "Epoch [31/200] Loss: 209.8468017578125\n",
      "Epoch [32/200] Loss: 207.12998962402344\n",
      "Epoch [33/200] Loss: 204.4905548095703\n",
      "Epoch [34/200] Loss: 201.9224853515625\n",
      "Epoch [35/200] Loss: 199.4204559326172\n",
      "Epoch [36/200] Loss: 196.9798583984375\n",
      "Epoch [37/200] Loss: 194.5966339111328\n",
      "Epoch [38/200] Loss: 192.26719665527344\n",
      "Epoch [39/200] Loss: 189.98826599121094\n",
      "Epoch [40/200] Loss: 187.7569580078125\n",
      "Epoch [41/200] Loss: 185.57064819335938\n",
      "Epoch [42/200] Loss: 183.42674255371094\n",
      "Epoch [43/200] Loss: 181.32298278808594\n",
      "Epoch [44/200] Loss: 179.2572479248047\n",
      "Epoch [45/200] Loss: 177.2274627685547\n",
      "Epoch [46/200] Loss: 175.2317657470703\n",
      "Epoch [47/200] Loss: 173.26844787597656\n",
      "Epoch [48/200] Loss: 171.3358917236328\n",
      "Epoch [49/200] Loss: 169.43280029296875\n",
      "Epoch [50/200] Loss: 167.5577392578125\n",
      "Epoch [51/200] Loss: 165.70957946777344\n",
      "Epoch [52/200] Loss: 163.88731384277344\n",
      "Epoch [53/200] Loss: 162.08998107910156\n",
      "Epoch [54/200] Loss: 160.31674194335938\n",
      "Epoch [55/200] Loss: 158.56680297851562\n",
      "Epoch [56/200] Loss: 156.83946228027344\n",
      "Epoch [57/200] Loss: 155.1341552734375\n",
      "Epoch [58/200] Loss: 153.45025634765625\n",
      "Epoch [59/200] Loss: 151.78721618652344\n",
      "Epoch [60/200] Loss: 150.14451599121094\n",
      "Epoch [61/200] Loss: 148.5218048095703\n",
      "Epoch [62/200] Loss: 146.91856384277344\n",
      "Epoch [63/200] Loss: 145.33436584472656\n",
      "Epoch [64/200] Loss: 143.76895141601562\n",
      "Epoch [65/200] Loss: 142.22181701660156\n",
      "Epoch [66/200] Loss: 140.6927032470703\n",
      "Epoch [67/200] Loss: 139.1812744140625\n",
      "Epoch [68/200] Loss: 137.68722534179688\n",
      "Epoch [69/200] Loss: 136.21022033691406\n",
      "Epoch [70/200] Loss: 134.75003051757812\n",
      "Epoch [71/200] Loss: 133.30638122558594\n",
      "Epoch [72/200] Loss: 131.87893676757812\n",
      "Epoch [73/200] Loss: 130.46751403808594\n",
      "Epoch [74/200] Loss: 129.07186889648438\n",
      "Epoch [75/200] Loss: 127.69175720214844\n",
      "Epoch [76/200] Loss: 126.32693481445312\n",
      "Epoch [77/200] Loss: 124.97724914550781\n",
      "Epoch [78/200] Loss: 123.64237976074219\n",
      "Epoch [79/200] Loss: 122.3222427368164\n",
      "Epoch [80/200] Loss: 121.01654815673828\n",
      "Epoch [81/200] Loss: 119.72515106201172\n",
      "Epoch [82/200] Loss: 118.44784545898438\n",
      "Epoch [83/200] Loss: 117.18446350097656\n",
      "Epoch [84/200] Loss: 115.93480682373047\n",
      "Epoch [85/200] Loss: 114.6987533569336\n",
      "Epoch [86/200] Loss: 113.47606658935547\n",
      "Epoch [87/200] Loss: 112.26663208007812\n",
      "Epoch [88/200] Loss: 111.07026672363281\n",
      "Epoch [89/200] Loss: 109.88684844970703\n",
      "Epoch [90/200] Loss: 108.71617889404297\n",
      "Epoch [91/200] Loss: 107.55815887451172\n",
      "Epoch [92/200] Loss: 106.41259002685547\n",
      "Epoch [93/200] Loss: 105.27938079833984\n",
      "Epoch [94/200] Loss: 104.15835571289062\n",
      "Epoch [95/200] Loss: 103.04940795898438\n",
      "Epoch [96/200] Loss: 101.952392578125\n",
      "Epoch [97/200] Loss: 100.8671875\n",
      "Epoch [98/200] Loss: 99.79364013671875\n",
      "Epoch [99/200] Loss: 98.73162841796875\n",
      "Epoch [100/200] Loss: 97.68106842041016\n",
      "Epoch [101/200] Loss: 96.6418228149414\n",
      "Epoch [102/200] Loss: 95.61372375488281\n",
      "Epoch [103/200] Loss: 94.59672546386719\n",
      "Epoch [104/200] Loss: 93.59068298339844\n",
      "Epoch [105/200] Loss: 92.59547424316406\n",
      "Epoch [106/200] Loss: 91.61100769042969\n",
      "Epoch [107/200] Loss: 90.63715362548828\n",
      "Epoch [108/200] Loss: 89.67384338378906\n",
      "Epoch [109/200] Loss: 88.72093963623047\n",
      "Epoch [110/200] Loss: 87.7783203125\n",
      "Epoch [111/200] Loss: 86.84593963623047\n",
      "Epoch [112/200] Loss: 85.92365264892578\n",
      "Epoch [113/200] Loss: 85.01136779785156\n",
      "Epoch [114/200] Loss: 84.1090087890625\n",
      "Epoch [115/200] Loss: 83.21644592285156\n",
      "Epoch [116/200] Loss: 82.3336181640625\n",
      "Epoch [117/200] Loss: 81.46041107177734\n",
      "Epoch [118/200] Loss: 80.59673309326172\n",
      "Epoch [119/200] Loss: 79.74252319335938\n",
      "Epoch [120/200] Loss: 78.89765930175781\n",
      "Epoch [121/200] Loss: 78.06204986572266\n",
      "Epoch [122/200] Loss: 77.23561096191406\n",
      "Epoch [123/200] Loss: 76.41828918457031\n",
      "Epoch [124/200] Loss: 75.60994720458984\n",
      "Epoch [125/200] Loss: 74.81051635742188\n",
      "Epoch [126/200] Loss: 74.01992797851562\n",
      "Epoch [127/200] Loss: 73.23811340332031\n",
      "Epoch [128/200] Loss: 72.46495056152344\n",
      "Epoch [129/200] Loss: 71.70038604736328\n",
      "Epoch [130/200] Loss: 70.94432067871094\n",
      "Epoch [131/200] Loss: 70.19670104980469\n",
      "Epoch [132/200] Loss: 69.45742797851562\n",
      "Epoch [133/200] Loss: 68.7264175415039\n",
      "Epoch [134/200] Loss: 68.00361633300781\n",
      "Epoch [135/200] Loss: 67.28893280029297\n",
      "Epoch [136/200] Loss: 66.58230590820312\n",
      "Epoch [137/200] Loss: 65.8836669921875\n",
      "Epoch [138/200] Loss: 65.1928939819336\n",
      "Epoch [139/200] Loss: 64.50995635986328\n",
      "Epoch [140/200] Loss: 63.834781646728516\n",
      "Epoch [141/200] Loss: 63.167266845703125\n",
      "Epoch [142/200] Loss: 62.50737380981445\n",
      "Epoch [143/200] Loss: 61.855018615722656\n",
      "Epoch [144/200] Loss: 61.210121154785156\n",
      "Epoch [145/200] Loss: 60.57264709472656\n",
      "Epoch [146/200] Loss: 59.9424934387207\n",
      "Epoch [147/200] Loss: 59.31959915161133\n",
      "Epoch [148/200] Loss: 58.70390701293945\n",
      "Epoch [149/200] Loss: 58.09531784057617\n",
      "Epoch [150/200] Loss: 57.493831634521484\n",
      "Epoch [151/200] Loss: 56.89931869506836\n",
      "Epoch [152/200] Loss: 56.311744689941406\n",
      "Epoch [153/200] Loss: 55.73103332519531\n",
      "Epoch [154/200] Loss: 55.15712356567383\n",
      "Epoch [155/200] Loss: 54.589942932128906\n",
      "Epoch [156/200] Loss: 54.02945327758789\n",
      "Epoch [157/200] Loss: 53.475563049316406\n",
      "Epoch [158/200] Loss: 52.92822265625\n",
      "Epoch [159/200] Loss: 52.387359619140625\n",
      "Epoch [160/200] Loss: 51.852943420410156\n",
      "Epoch [161/200] Loss: 51.32487487792969\n",
      "Epoch [162/200] Loss: 50.80313491821289\n",
      "Epoch [163/200] Loss: 50.28762435913086\n",
      "Epoch [164/200] Loss: 49.77830123901367\n",
      "Epoch [165/200] Loss: 49.27511215209961\n",
      "Epoch [166/200] Loss: 48.777976989746094\n",
      "Epoch [167/200] Loss: 48.28684997558594\n",
      "Epoch [168/200] Loss: 47.801692962646484\n",
      "Epoch [169/200] Loss: 47.322418212890625\n",
      "Epoch [170/200] Loss: 46.848976135253906\n",
      "Epoch [171/200] Loss: 46.38132858276367\n",
      "Epoch [172/200] Loss: 45.91939926147461\n",
      "Epoch [173/200] Loss: 45.46311950683594\n",
      "Epoch [174/200] Loss: 45.012489318847656\n",
      "Epoch [175/200] Loss: 44.56739807128906\n",
      "Epoch [176/200] Loss: 44.12782287597656\n",
      "Epoch [177/200] Loss: 43.69369888305664\n",
      "Epoch [178/200] Loss: 43.264976501464844\n",
      "Epoch [179/200] Loss: 42.84157180786133\n",
      "Epoch [180/200] Loss: 42.42348098754883\n",
      "Epoch [181/200] Loss: 42.01060485839844\n",
      "Epoch [182/200] Loss: 41.60292053222656\n",
      "Epoch [183/200] Loss: 41.20039367675781\n",
      "Epoch [184/200] Loss: 40.80293655395508\n",
      "Epoch [185/200] Loss: 40.41050338745117\n",
      "Epoch [186/200] Loss: 40.02306365966797\n",
      "Epoch [187/200] Loss: 39.640541076660156\n",
      "Epoch [188/200] Loss: 39.26291275024414\n",
      "Epoch [189/200] Loss: 38.89011764526367\n",
      "Epoch [190/200] Loss: 38.52208709716797\n",
      "Epoch [191/200] Loss: 38.1588020324707\n",
      "Epoch [192/200] Loss: 37.80020523071289\n",
      "Epoch [193/200] Loss: 37.44622039794922\n",
      "Epoch [194/200] Loss: 37.09684753417969\n",
      "Epoch [195/200] Loss: 36.75200653076172\n",
      "Epoch [196/200] Loss: 36.41167068481445\n",
      "Epoch [197/200] Loss: 36.075767517089844\n",
      "Epoch [198/200] Loss: 35.74427032470703\n",
      "Epoch [199/200] Loss: 35.41712951660156\n",
      "Epoch [200/200] Loss: 35.09429931640625\n",
      "Predicted days_remaining for parent_id 346: 14.24020767211914\n",
      "Training for parent_id 347...\n",
      "Epoch [1/200] Loss: 330.04620361328125\n",
      "Epoch [2/200] Loss: 321.2008972167969\n",
      "Epoch [3/200] Loss: 312.6880798339844\n",
      "Epoch [4/200] Loss: 304.48974609375\n",
      "Epoch [5/200] Loss: 296.59124755859375\n",
      "Epoch [6/200] Loss: 288.98297119140625\n",
      "Epoch [7/200] Loss: 281.6615905761719\n",
      "Epoch [8/200] Loss: 274.62738037109375\n",
      "Epoch [9/200] Loss: 267.8800354003906\n",
      "Epoch [10/200] Loss: 261.416748046875\n",
      "Epoch [11/200] Loss: 255.23231506347656\n",
      "Epoch [12/200] Loss: 249.31910705566406\n",
      "Epoch [13/200] Loss: 243.66712951660156\n",
      "Epoch [14/200] Loss: 238.2646484375\n",
      "Epoch [15/200] Loss: 233.09913635253906\n",
      "Epoch [16/200] Loss: 228.15830993652344\n",
      "Epoch [17/200] Loss: 223.43121337890625\n",
      "Epoch [18/200] Loss: 218.90843200683594\n",
      "Epoch [19/200] Loss: 214.582275390625\n",
      "Epoch [20/200] Loss: 210.44589233398438\n",
      "Epoch [21/200] Loss: 206.49249267578125\n",
      "Epoch [22/200] Loss: 202.7148895263672\n",
      "Epoch [23/200] Loss: 199.1047821044922\n",
      "Epoch [24/200] Loss: 195.65283203125\n",
      "Epoch [25/200] Loss: 192.348876953125\n",
      "Epoch [26/200] Loss: 189.18223571777344\n",
      "Epoch [27/200] Loss: 186.1419677734375\n",
      "Epoch [28/200] Loss: 183.21755981445312\n",
      "Epoch [29/200] Loss: 180.3990478515625\n",
      "Epoch [30/200] Loss: 177.67730712890625\n",
      "Epoch [31/200] Loss: 175.044189453125\n",
      "Epoch [32/200] Loss: 172.49249267578125\n",
      "Epoch [33/200] Loss: 170.01609802246094\n",
      "Epoch [34/200] Loss: 167.609619140625\n",
      "Epoch [35/200] Loss: 165.2684326171875\n",
      "Epoch [36/200] Loss: 162.988525390625\n",
      "Epoch [37/200] Loss: 160.7662353515625\n",
      "Epoch [38/200] Loss: 158.5982208251953\n",
      "Epoch [39/200] Loss: 156.48133850097656\n",
      "Epoch [40/200] Loss: 154.41259765625\n",
      "Epoch [41/200] Loss: 152.3890838623047\n",
      "Epoch [42/200] Loss: 150.40811157226562\n",
      "Epoch [43/200] Loss: 148.46725463867188\n",
      "Epoch [44/200] Loss: 146.56414794921875\n",
      "Epoch [45/200] Loss: 144.6968231201172\n",
      "Epoch [46/200] Loss: 142.86351013183594\n",
      "Epoch [47/200] Loss: 141.06260681152344\n",
      "Epoch [48/200] Loss: 139.29275512695312\n",
      "Epoch [49/200] Loss: 137.55271911621094\n",
      "Epoch [50/200] Loss: 135.84146118164062\n",
      "Epoch [51/200] Loss: 134.15798950195312\n",
      "Epoch [52/200] Loss: 132.50143432617188\n",
      "Epoch [53/200] Loss: 130.87100219726562\n",
      "Epoch [54/200] Loss: 129.26589965820312\n",
      "Epoch [55/200] Loss: 127.68543243408203\n",
      "Epoch [56/200] Loss: 126.12893676757812\n",
      "Epoch [57/200] Loss: 124.59571838378906\n",
      "Epoch [58/200] Loss: 123.0852279663086\n",
      "Epoch [59/200] Loss: 121.59680938720703\n",
      "Epoch [60/200] Loss: 120.12999725341797\n",
      "Epoch [61/200] Loss: 118.6841812133789\n",
      "Epoch [62/200] Loss: 117.25894927978516\n",
      "Epoch [63/200] Loss: 115.85372924804688\n",
      "Epoch [64/200] Loss: 114.46817779541016\n",
      "Epoch [65/200] Loss: 113.10181427001953\n",
      "Epoch [66/200] Loss: 111.7542724609375\n",
      "Epoch [67/200] Loss: 110.42518615722656\n",
      "Epoch [68/200] Loss: 109.11416625976562\n",
      "Epoch [69/200] Loss: 107.82090759277344\n",
      "Epoch [70/200] Loss: 106.54509735107422\n",
      "Epoch [71/200] Loss: 105.28634643554688\n",
      "Epoch [72/200] Loss: 104.04443359375\n",
      "Epoch [73/200] Loss: 102.81902313232422\n",
      "Epoch [74/200] Loss: 101.60977935791016\n",
      "Epoch [75/200] Loss: 100.41651916503906\n",
      "Epoch [76/200] Loss: 99.23885345458984\n",
      "Epoch [77/200] Loss: 98.07662963867188\n",
      "Epoch [78/200] Loss: 96.92948150634766\n",
      "Epoch [79/200] Loss: 95.7972183227539\n",
      "Epoch [80/200] Loss: 94.67959594726562\n",
      "Epoch [81/200] Loss: 93.5762939453125\n",
      "Epoch [82/200] Loss: 92.4871597290039\n",
      "Epoch [83/200] Loss: 91.41191101074219\n",
      "Epoch [84/200] Loss: 90.350341796875\n",
      "Epoch [85/200] Loss: 89.30225372314453\n",
      "Epoch [86/200] Loss: 88.26738739013672\n",
      "Epoch [87/200] Loss: 87.24559783935547\n",
      "Epoch [88/200] Loss: 86.23662567138672\n",
      "Epoch [89/200] Loss: 85.24028778076172\n",
      "Epoch [90/200] Loss: 84.25642395019531\n",
      "Epoch [91/200] Loss: 83.28484344482422\n",
      "Epoch [92/200] Loss: 82.32534790039062\n",
      "Epoch [93/200] Loss: 81.37781524658203\n",
      "Epoch [94/200] Loss: 80.4420166015625\n",
      "Epoch [95/200] Loss: 79.51782989501953\n",
      "Epoch [96/200] Loss: 78.60505676269531\n",
      "Epoch [97/200] Loss: 77.70360565185547\n",
      "Epoch [98/200] Loss: 76.81327819824219\n",
      "Epoch [99/200] Loss: 75.93397521972656\n",
      "Epoch [100/200] Loss: 75.06548309326172\n",
      "Epoch [101/200] Loss: 74.20772552490234\n",
      "Epoch [102/200] Loss: 73.36052703857422\n",
      "Epoch [103/200] Loss: 72.52379608154297\n",
      "Epoch [104/200] Loss: 71.69734191894531\n",
      "Epoch [105/200] Loss: 70.88109588623047\n",
      "Epoch [106/200] Loss: 70.07492065429688\n",
      "Epoch [107/200] Loss: 69.27867126464844\n",
      "Epoch [108/200] Loss: 68.49226379394531\n",
      "Epoch [109/200] Loss: 67.71552276611328\n",
      "Epoch [110/200] Loss: 66.94837188720703\n",
      "Epoch [111/200] Loss: 66.1906967163086\n",
      "Epoch [112/200] Loss: 65.44236755371094\n",
      "Epoch [113/200] Loss: 64.70327758789062\n",
      "Epoch [114/200] Loss: 63.97331619262695\n",
      "Epoch [115/200] Loss: 63.25239944458008\n",
      "Epoch [116/200] Loss: 62.540367126464844\n",
      "Epoch [117/200] Loss: 61.83714294433594\n",
      "Epoch [118/200] Loss: 61.14262771606445\n",
      "Epoch [119/200] Loss: 60.45668029785156\n",
      "Epoch [120/200] Loss: 59.77926254272461\n",
      "Epoch [121/200] Loss: 59.110206604003906\n",
      "Epoch [122/200] Loss: 58.4494514465332\n",
      "Epoch [123/200] Loss: 57.79689407348633\n",
      "Epoch [124/200] Loss: 57.152400970458984\n",
      "Epoch [125/200] Loss: 56.51593017578125\n",
      "Epoch [126/200] Loss: 55.88730239868164\n",
      "Epoch [127/200] Loss: 55.26653289794922\n",
      "Epoch [128/200] Loss: 54.65343475341797\n",
      "Epoch [129/200] Loss: 54.047935485839844\n",
      "Epoch [130/200] Loss: 53.44997024536133\n",
      "Epoch [131/200] Loss: 52.85945510864258\n",
      "Epoch [132/200] Loss: 52.276268005371094\n",
      "Epoch [133/200] Loss: 51.700321197509766\n",
      "Epoch [134/200] Loss: 51.13156509399414\n",
      "Epoch [135/200] Loss: 50.569889068603516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [136/200] Loss: 50.015228271484375\n",
      "Epoch [137/200] Loss: 49.46749496459961\n",
      "Epoch [138/200] Loss: 48.92664337158203\n",
      "Epoch [139/200] Loss: 48.392555236816406\n",
      "Epoch [140/200] Loss: 47.865196228027344\n",
      "Epoch [141/200] Loss: 47.34446716308594\n",
      "Epoch [142/200] Loss: 46.83033752441406\n",
      "Epoch [143/200] Loss: 46.322731018066406\n",
      "Epoch [144/200] Loss: 45.821598052978516\n",
      "Epoch [145/200] Loss: 45.32683563232422\n",
      "Epoch [146/200] Loss: 44.83842849731445\n",
      "Epoch [147/200] Loss: 44.35631561279297\n",
      "Epoch [148/200] Loss: 43.88041687011719\n",
      "Epoch [149/200] Loss: 43.41072463989258\n",
      "Epoch [150/200] Loss: 42.9471435546875\n",
      "Epoch [151/200] Loss: 42.489646911621094\n",
      "Epoch [152/200] Loss: 42.03817367553711\n",
      "Epoch [153/200] Loss: 41.59269332885742\n",
      "Epoch [154/200] Loss: 41.15312194824219\n",
      "Epoch [155/200] Loss: 40.71943283081055\n",
      "Epoch [156/200] Loss: 40.29157257080078\n",
      "Epoch [157/200] Loss: 39.86948776245117\n",
      "Epoch [158/200] Loss: 39.45313262939453\n",
      "Epoch [159/200] Loss: 39.04245376586914\n",
      "Epoch [160/200] Loss: 38.63739776611328\n",
      "Epoch [161/200] Loss: 38.2379150390625\n",
      "Epoch [162/200] Loss: 37.84396743774414\n",
      "Epoch [163/200] Loss: 37.45547103881836\n",
      "Epoch [164/200] Loss: 37.07240676879883\n",
      "Epoch [165/200] Loss: 36.694705963134766\n",
      "Epoch [166/200] Loss: 36.32233428955078\n",
      "Epoch [167/200] Loss: 35.9552116394043\n",
      "Epoch [168/200] Loss: 35.593299865722656\n",
      "Epoch [169/200] Loss: 35.23655700683594\n",
      "Epoch [170/200] Loss: 34.8848991394043\n",
      "Epoch [171/200] Loss: 34.53830337524414\n",
      "Epoch [172/200] Loss: 34.196712493896484\n",
      "Epoch [173/200] Loss: 33.86005401611328\n",
      "Epoch [174/200] Loss: 33.52827072143555\n",
      "Epoch [175/200] Loss: 33.20133972167969\n",
      "Epoch [176/200] Loss: 32.87919235229492\n",
      "Epoch [177/200] Loss: 32.561767578125\n",
      "Epoch [178/200] Loss: 32.2490348815918\n",
      "Epoch [179/200] Loss: 31.940914154052734\n",
      "Epoch [180/200] Loss: 31.637372970581055\n",
      "Epoch [181/200] Loss: 31.338336944580078\n",
      "Epoch [182/200] Loss: 31.043773651123047\n",
      "Epoch [183/200] Loss: 30.753625869750977\n",
      "Epoch [184/200] Loss: 30.467844009399414\n",
      "Epoch [185/200] Loss: 30.18635368347168\n",
      "Epoch [186/200] Loss: 29.909137725830078\n",
      "Epoch [187/200] Loss: 29.636123657226562\n",
      "Epoch [188/200] Loss: 29.36726951599121\n",
      "Epoch [189/200] Loss: 29.1025333404541\n",
      "Epoch [190/200] Loss: 28.841835021972656\n",
      "Epoch [191/200] Loss: 28.585155487060547\n",
      "Epoch [192/200] Loss: 28.332429885864258\n",
      "Epoch [193/200] Loss: 28.0836181640625\n",
      "Epoch [194/200] Loss: 27.83867073059082\n",
      "Epoch [195/200] Loss: 27.59752082824707\n",
      "Epoch [196/200] Loss: 27.36012840270996\n",
      "Epoch [197/200] Loss: 27.126474380493164\n",
      "Epoch [198/200] Loss: 26.896472930908203\n",
      "Epoch [199/200] Loss: 26.67009735107422\n",
      "Epoch [200/200] Loss: 26.447307586669922\n",
      "Predicted days_remaining for parent_id 347: 14.316266059875488\n",
      "Training for parent_id 348...\n",
      "Epoch [1/200] Loss: 1009.2568359375\n",
      "Epoch [2/200] Loss: 995.3920288085938\n",
      "Epoch [3/200] Loss: 981.8609619140625\n",
      "Epoch [4/200] Loss: 968.6926879882812\n",
      "Epoch [5/200] Loss: 955.8780517578125\n",
      "Epoch [6/200] Loss: 943.39990234375\n",
      "Epoch [7/200] Loss: 931.2628173828125\n",
      "Epoch [8/200] Loss: 919.4796752929688\n",
      "Epoch [9/200] Loss: 908.0514526367188\n",
      "Epoch [10/200] Loss: 896.9669799804688\n",
      "Epoch [11/200] Loss: 886.2109985351562\n",
      "Epoch [12/200] Loss: 875.7750244140625\n",
      "Epoch [13/200] Loss: 865.6618041992188\n",
      "Epoch [14/200] Loss: 855.882080078125\n",
      "Epoch [15/200] Loss: 846.4486694335938\n",
      "Epoch [16/200] Loss: 837.3692016601562\n",
      "Epoch [17/200] Loss: 828.6427001953125\n",
      "Epoch [18/200] Loss: 820.2593383789062\n",
      "Epoch [19/200] Loss: 812.2032470703125\n",
      "Epoch [20/200] Loss: 804.4555053710938\n",
      "Epoch [21/200] Loss: 796.997314453125\n",
      "Epoch [22/200] Loss: 789.8108520507812\n",
      "Epoch [23/200] Loss: 782.8806762695312\n",
      "Epoch [24/200] Loss: 776.1929931640625\n",
      "Epoch [25/200] Loss: 769.7357177734375\n",
      "Epoch [26/200] Loss: 763.4972534179688\n",
      "Epoch [27/200] Loss: 757.467041015625\n",
      "Epoch [28/200] Loss: 751.6349487304688\n",
      "Epoch [29/200] Loss: 745.990478515625\n",
      "Epoch [30/200] Loss: 740.5227661132812\n",
      "Epoch [31/200] Loss: 735.2208251953125\n",
      "Epoch [32/200] Loss: 730.0733642578125\n",
      "Epoch [33/200] Loss: 725.06884765625\n",
      "Epoch [34/200] Loss: 720.1956787109375\n",
      "Epoch [35/200] Loss: 715.4426879882812\n",
      "Epoch [36/200] Loss: 710.7984619140625\n",
      "Epoch [37/200] Loss: 706.2527465820312\n",
      "Epoch [38/200] Loss: 701.7951049804688\n",
      "Epoch [39/200] Loss: 697.4160766601562\n",
      "Epoch [40/200] Loss: 693.1067504882812\n",
      "Epoch [41/200] Loss: 688.859130859375\n",
      "Epoch [42/200] Loss: 684.6661987304688\n",
      "Epoch [43/200] Loss: 680.5216064453125\n",
      "Epoch [44/200] Loss: 676.4202880859375\n",
      "Epoch [45/200] Loss: 672.3580932617188\n",
      "Epoch [46/200] Loss: 668.331787109375\n",
      "Epoch [47/200] Loss: 664.3392333984375\n",
      "Epoch [48/200] Loss: 660.3787841796875\n",
      "Epoch [49/200] Loss: 656.449462890625\n",
      "Epoch [50/200] Loss: 652.5510864257812\n",
      "Epoch [51/200] Loss: 648.6834106445312\n",
      "Epoch [52/200] Loss: 644.8460693359375\n",
      "Epoch [53/200] Loss: 641.039306640625\n",
      "Epoch [54/200] Loss: 637.2626953125\n",
      "Epoch [55/200] Loss: 633.5162963867188\n",
      "Epoch [56/200] Loss: 629.7994995117188\n",
      "Epoch [57/200] Loss: 626.112060546875\n",
      "Epoch [58/200] Loss: 622.453369140625\n",
      "Epoch [59/200] Loss: 618.8229370117188\n",
      "Epoch [60/200] Loss: 615.2200317382812\n",
      "Epoch [61/200] Loss: 611.6442260742188\n",
      "Epoch [62/200] Loss: 608.094970703125\n",
      "Epoch [63/200] Loss: 604.571533203125\n",
      "Epoch [64/200] Loss: 601.073486328125\n",
      "Epoch [65/200] Loss: 597.6004028320312\n",
      "Epoch [66/200] Loss: 594.1514282226562\n",
      "Epoch [67/200] Loss: 590.7265014648438\n",
      "Epoch [68/200] Loss: 587.3250122070312\n",
      "Epoch [69/200] Loss: 583.9464721679688\n",
      "Epoch [70/200] Loss: 580.5906372070312\n",
      "Epoch [71/200] Loss: 577.2569580078125\n",
      "Epoch [72/200] Loss: 573.9450073242188\n",
      "Epoch [73/200] Loss: 570.65478515625\n",
      "Epoch [74/200] Loss: 567.3856811523438\n",
      "Epoch [75/200] Loss: 564.1375122070312\n",
      "Epoch [76/200] Loss: 560.9098510742188\n",
      "Epoch [77/200] Loss: 557.702392578125\n",
      "Epoch [78/200] Loss: 554.5149536132812\n",
      "Epoch [79/200] Loss: 551.3472900390625\n",
      "Epoch [80/200] Loss: 548.1991577148438\n",
      "Epoch [81/200] Loss: 545.070068359375\n",
      "Epoch [82/200] Loss: 541.9598999023438\n",
      "Epoch [83/200] Loss: 538.8685302734375\n",
      "Epoch [84/200] Loss: 535.7955932617188\n",
      "Epoch [85/200] Loss: 532.7408447265625\n",
      "Epoch [86/200] Loss: 529.7041625976562\n",
      "Epoch [87/200] Loss: 526.6853637695312\n",
      "Epoch [88/200] Loss: 523.6842041015625\n",
      "Epoch [89/200] Loss: 520.7003784179688\n",
      "Epoch [90/200] Loss: 517.7337646484375\n",
      "Epoch [91/200] Loss: 514.7841796875\n",
      "Epoch [92/200] Loss: 511.85150146484375\n",
      "Epoch [93/200] Loss: 508.9355163574219\n",
      "Epoch [94/200] Loss: 506.0360107421875\n",
      "Epoch [95/200] Loss: 503.15277099609375\n",
      "Epoch [96/200] Loss: 500.28564453125\n",
      "Epoch [97/200] Loss: 497.4347229003906\n",
      "Epoch [98/200] Loss: 494.599609375\n",
      "Epoch [99/200] Loss: 491.78009033203125\n",
      "Epoch [100/200] Loss: 488.9761657714844\n",
      "Epoch [101/200] Loss: 486.1876525878906\n",
      "Epoch [102/200] Loss: 483.41448974609375\n",
      "Epoch [103/200] Loss: 480.65643310546875\n",
      "Epoch [104/200] Loss: 477.9134521484375\n",
      "Epoch [105/200] Loss: 475.185302734375\n",
      "Epoch [106/200] Loss: 472.4718322753906\n",
      "Epoch [107/200] Loss: 469.77301025390625\n",
      "Epoch [108/200] Loss: 467.0888671875\n",
      "Epoch [109/200] Loss: 464.4189758300781\n",
      "Epoch [110/200] Loss: 461.7633972167969\n",
      "Epoch [111/200] Loss: 459.12200927734375\n",
      "Epoch [112/200] Loss: 456.4947204589844\n",
      "Epoch [113/200] Loss: 453.8812255859375\n",
      "Epoch [114/200] Loss: 451.2818298339844\n",
      "Epoch [115/200] Loss: 448.696044921875\n",
      "Epoch [116/200] Loss: 446.12384033203125\n",
      "Epoch [117/200] Loss: 443.5652160644531\n",
      "Epoch [118/200] Loss: 441.0201721191406\n",
      "Epoch [119/200] Loss: 438.488525390625\n",
      "Epoch [120/200] Loss: 435.96990966796875\n",
      "Epoch [121/200] Loss: 433.46453857421875\n",
      "Epoch [122/200] Loss: 430.9723815917969\n",
      "Epoch [123/200] Loss: 428.4931640625\n",
      "Epoch [124/200] Loss: 426.0268859863281\n",
      "Epoch [125/200] Loss: 423.5733947753906\n",
      "Epoch [126/200] Loss: 421.1326904296875\n",
      "Epoch [127/200] Loss: 418.7046813964844\n",
      "Epoch [128/200] Loss: 416.28912353515625\n",
      "Epoch [129/200] Loss: 413.8862609863281\n",
      "Epoch [130/200] Loss: 411.4957275390625\n",
      "Epoch [131/200] Loss: 409.11761474609375\n",
      "Epoch [132/200] Loss: 406.7516784667969\n",
      "Epoch [133/200] Loss: 404.3981628417969\n",
      "Epoch [134/200] Loss: 402.056640625\n",
      "Epoch [135/200] Loss: 399.7271728515625\n",
      "Epoch [136/200] Loss: 397.4096984863281\n",
      "Epoch [137/200] Loss: 395.104248046875\n",
      "Epoch [138/200] Loss: 392.8106384277344\n",
      "Epoch [139/200] Loss: 390.5288391113281\n",
      "Epoch [140/200] Loss: 388.25872802734375\n",
      "Epoch [141/200] Loss: 386.0003662109375\n",
      "Epoch [142/200] Loss: 383.7535705566406\n",
      "Epoch [143/200] Loss: 381.5183410644531\n",
      "Epoch [144/200] Loss: 379.29449462890625\n",
      "Epoch [145/200] Loss: 377.08221435546875\n",
      "Epoch [146/200] Loss: 374.881103515625\n",
      "Epoch [147/200] Loss: 372.6914367675781\n",
      "Epoch [148/200] Loss: 370.5129089355469\n",
      "Epoch [149/200] Loss: 368.34564208984375\n",
      "Epoch [150/200] Loss: 366.1894836425781\n",
      "Epoch [151/200] Loss: 364.0443420410156\n",
      "Epoch [152/200] Loss: 361.9102478027344\n",
      "Epoch [153/200] Loss: 359.7872314453125\n",
      "Epoch [154/200] Loss: 357.6749267578125\n",
      "Epoch [155/200] Loss: 355.5735168457031\n",
      "Epoch [156/200] Loss: 353.48297119140625\n",
      "Epoch [157/200] Loss: 351.403076171875\n",
      "Epoch [158/200] Loss: 349.3338623046875\n",
      "Epoch [159/200] Loss: 347.2752990722656\n",
      "Epoch [160/200] Loss: 345.22735595703125\n",
      "Epoch [161/200] Loss: 343.1900329589844\n",
      "Epoch [162/200] Loss: 341.1629943847656\n",
      "Epoch [163/200] Loss: 339.146484375\n",
      "Epoch [164/200] Loss: 337.14031982421875\n",
      "Epoch [165/200] Loss: 335.14459228515625\n",
      "Epoch [166/200] Loss: 333.1590881347656\n",
      "Epoch [167/200] Loss: 331.18377685546875\n",
      "Epoch [168/200] Loss: 329.2186584472656\n",
      "Epoch [169/200] Loss: 327.2636413574219\n",
      "Epoch [170/200] Loss: 325.3188781738281\n",
      "Epoch [171/200] Loss: 323.3840026855469\n",
      "Epoch [172/200] Loss: 321.4590148925781\n",
      "Epoch [173/200] Loss: 319.5442199707031\n",
      "Epoch [174/200] Loss: 317.63922119140625\n",
      "Epoch [175/200] Loss: 315.7441101074219\n",
      "Epoch [176/200] Loss: 313.8588562011719\n",
      "Epoch [177/200] Loss: 311.9833068847656\n",
      "Epoch [178/200] Loss: 310.1175842285156\n",
      "Epoch [179/200] Loss: 308.26141357421875\n",
      "Epoch [180/200] Loss: 306.4149169921875\n",
      "Epoch [181/200] Loss: 304.5780944824219\n",
      "Epoch [182/200] Loss: 302.7507019042969\n",
      "Epoch [183/200] Loss: 300.9330139160156\n",
      "Epoch [184/200] Loss: 299.1246032714844\n",
      "Epoch [185/200] Loss: 297.3257751464844\n",
      "Epoch [186/200] Loss: 295.5362243652344\n",
      "Epoch [187/200] Loss: 293.75604248046875\n",
      "Epoch [188/200] Loss: 291.9851379394531\n",
      "Epoch [189/200] Loss: 290.2236633300781\n",
      "Epoch [190/200] Loss: 288.47119140625\n",
      "Epoch [191/200] Loss: 286.72802734375\n",
      "Epoch [192/200] Loss: 284.9939880371094\n",
      "Epoch [193/200] Loss: 283.2689514160156\n",
      "Epoch [194/200] Loss: 281.5531005859375\n",
      "Epoch [195/200] Loss: 279.84625244140625\n",
      "Epoch [196/200] Loss: 278.1484069824219\n",
      "Epoch [197/200] Loss: 276.45947265625\n",
      "Epoch [198/200] Loss: 274.7794189453125\n",
      "Epoch [199/200] Loss: 273.1083984375\n",
      "Epoch [200/200] Loss: 271.44598388671875\n",
      "Predicted days_remaining for parent_id 348: 15.770170211791992\n",
      "Training for parent_id 352...\n",
      "Epoch [1/200] Loss: 210.2876739501953\n",
      "Epoch [2/200] Loss: 205.1432647705078\n",
      "Epoch [3/200] Loss: 200.1953887939453\n",
      "Epoch [4/200] Loss: 195.4369659423828\n",
      "Epoch [5/200] Loss: 190.8524932861328\n",
      "Epoch [6/200] Loss: 186.4206085205078\n",
      "Epoch [7/200] Loss: 182.12193298339844\n",
      "Epoch [8/200] Loss: 177.9405059814453\n",
      "Epoch [9/200] Loss: 173.8631591796875\n",
      "Epoch [10/200] Loss: 169.87855529785156\n",
      "Epoch [11/200] Loss: 165.97750854492188\n",
      "Epoch [12/200] Loss: 162.15403747558594\n",
      "Epoch [13/200] Loss: 158.40589904785156\n",
      "Epoch [14/200] Loss: 154.73377990722656\n",
      "Epoch [15/200] Loss: 151.14024353027344\n",
      "Epoch [16/200] Loss: 147.6290740966797\n",
      "Epoch [17/200] Loss: 144.20431518554688\n",
      "Epoch [18/200] Loss: 140.8700408935547\n",
      "Epoch [19/200] Loss: 137.62954711914062\n",
      "Epoch [20/200] Loss: 134.48526000976562\n",
      "Epoch [21/200] Loss: 131.4384307861328\n",
      "Epoch [22/200] Loss: 128.48902893066406\n",
      "Epoch [23/200] Loss: 125.63578033447266\n",
      "Epoch [24/200] Loss: 122.87633514404297\n",
      "Epoch [25/200] Loss: 120.20759582519531\n",
      "Epoch [26/200] Loss: 117.62592315673828\n",
      "Epoch [27/200] Loss: 115.12739562988281\n",
      "Epoch [28/200] Loss: 112.7080307006836\n",
      "Epoch [29/200] Loss: 110.3639907836914\n",
      "Epoch [30/200] Loss: 108.0916519165039\n",
      "Epoch [31/200] Loss: 105.88754272460938\n",
      "Epoch [32/200] Loss: 103.74858093261719\n",
      "Epoch [33/200] Loss: 101.67171478271484\n",
      "Epoch [34/200] Loss: 99.65421295166016\n",
      "Epoch [35/200] Loss: 97.69344329833984\n",
      "Epoch [36/200] Loss: 95.78697204589844\n",
      "Epoch [37/200] Loss: 93.9324951171875\n",
      "Epoch [38/200] Loss: 92.12781524658203\n",
      "Epoch [39/200] Loss: 90.37091064453125\n",
      "Epoch [40/200] Loss: 88.6598892211914\n",
      "Epoch [41/200] Loss: 86.99296569824219\n",
      "Epoch [42/200] Loss: 85.3684310913086\n",
      "Epoch [43/200] Loss: 83.78475189208984\n",
      "Epoch [44/200] Loss: 82.24036407470703\n",
      "Epoch [45/200] Loss: 80.73394012451172\n",
      "Epoch [46/200] Loss: 79.26412963867188\n",
      "Epoch [47/200] Loss: 77.8297119140625\n",
      "Epoch [48/200] Loss: 76.42958068847656\n",
      "Epoch [49/200] Loss: 75.0626220703125\n",
      "Epoch [50/200] Loss: 73.72792053222656\n",
      "Epoch [51/200] Loss: 72.42448425292969\n",
      "Epoch [52/200] Loss: 71.15144348144531\n",
      "Epoch [53/200] Loss: 69.90798950195312\n",
      "Epoch [54/200] Loss: 68.69327545166016\n",
      "Epoch [55/200] Loss: 67.50650024414062\n",
      "Epoch [56/200] Loss: 66.3469467163086\n",
      "Epoch [57/200] Loss: 65.21383666992188\n",
      "Epoch [58/200] Loss: 64.1064682006836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/200] Loss: 63.02415084838867\n",
      "Epoch [60/200] Loss: 61.96615982055664\n",
      "Epoch [61/200] Loss: 60.93183898925781\n",
      "Epoch [62/200] Loss: 59.92054748535156\n",
      "Epoch [63/200] Loss: 58.93162536621094\n",
      "Epoch [64/200] Loss: 57.964481353759766\n",
      "Epoch [65/200] Loss: 57.018497467041016\n",
      "Epoch [66/200] Loss: 56.09307098388672\n",
      "Epoch [67/200] Loss: 55.18766784667969\n",
      "Epoch [68/200] Loss: 54.30175018310547\n",
      "Epoch [69/200] Loss: 53.43476867675781\n",
      "Epoch [70/200] Loss: 52.586238861083984\n",
      "Epoch [71/200] Loss: 51.755680084228516\n",
      "Epoch [72/200] Loss: 50.94260787963867\n",
      "Epoch [73/200] Loss: 50.146575927734375\n",
      "Epoch [74/200] Loss: 49.36715316772461\n",
      "Epoch [75/200] Loss: 48.603939056396484\n",
      "Epoch [76/200] Loss: 47.856510162353516\n",
      "Epoch [77/200] Loss: 47.124534606933594\n",
      "Epoch [78/200] Loss: 46.4075927734375\n",
      "Epoch [79/200] Loss: 45.70536422729492\n",
      "Epoch [80/200] Loss: 45.017478942871094\n",
      "Epoch [81/200] Loss: 44.3436393737793\n",
      "Epoch [82/200] Loss: 43.68349838256836\n",
      "Epoch [83/200] Loss: 43.03679275512695\n",
      "Epoch [84/200] Loss: 42.4032096862793\n",
      "Epoch [85/200] Loss: 41.78244400024414\n",
      "Epoch [86/200] Loss: 41.174259185791016\n",
      "Epoch [87/200] Loss: 40.5783576965332\n",
      "Epoch [88/200] Loss: 39.994510650634766\n",
      "Epoch [89/200] Loss: 39.42244338989258\n",
      "Epoch [90/200] Loss: 38.86193084716797\n",
      "Epoch [91/200] Loss: 38.31273651123047\n",
      "Epoch [92/200] Loss: 37.77460479736328\n",
      "Epoch [93/200] Loss: 37.24736785888672\n",
      "Epoch [94/200] Loss: 36.730770111083984\n",
      "Epoch [95/200] Loss: 36.22462844848633\n",
      "Epoch [96/200] Loss: 35.72871017456055\n",
      "Epoch [97/200] Loss: 35.242828369140625\n",
      "Epoch [98/200] Loss: 34.76681137084961\n",
      "Epoch [99/200] Loss: 34.30044174194336\n",
      "Epoch [100/200] Loss: 33.84354782104492\n",
      "Epoch [101/200] Loss: 33.39596176147461\n",
      "Epoch [102/200] Loss: 32.957481384277344\n",
      "Epoch [103/200] Loss: 32.527957916259766\n",
      "Epoch [104/200] Loss: 32.107215881347656\n",
      "Epoch [105/200] Loss: 31.695087432861328\n",
      "Epoch [106/200] Loss: 31.291410446166992\n",
      "Epoch [107/200] Loss: 30.896038055419922\n",
      "Epoch [108/200] Loss: 30.50881576538086\n",
      "Epoch [109/200] Loss: 30.129592895507812\n",
      "Epoch [110/200] Loss: 29.75821876525879\n",
      "Epoch [111/200] Loss: 29.39455223083496\n",
      "Epoch [112/200] Loss: 29.03844451904297\n",
      "Epoch [113/200] Loss: 28.689762115478516\n",
      "Epoch [114/200] Loss: 28.348369598388672\n",
      "Epoch [115/200] Loss: 28.014137268066406\n",
      "Epoch [116/200] Loss: 27.686922073364258\n",
      "Epoch [117/200] Loss: 27.366609573364258\n",
      "Epoch [118/200] Loss: 27.053058624267578\n",
      "Epoch [119/200] Loss: 26.746164321899414\n",
      "Epoch [120/200] Loss: 26.445781707763672\n",
      "Epoch [121/200] Loss: 26.151813507080078\n",
      "Epoch [122/200] Loss: 25.864131927490234\n",
      "Epoch [123/200] Loss: 25.582611083984375\n",
      "Epoch [124/200] Loss: 25.307167053222656\n",
      "Epoch [125/200] Loss: 25.037660598754883\n",
      "Epoch [126/200] Loss: 24.773983001708984\n",
      "Epoch [127/200] Loss: 24.516054153442383\n",
      "Epoch [128/200] Loss: 24.263729095458984\n",
      "Epoch [129/200] Loss: 24.01694107055664\n",
      "Epoch [130/200] Loss: 23.775550842285156\n",
      "Epoch [131/200] Loss: 23.53948974609375\n",
      "Epoch [132/200] Loss: 23.308645248413086\n",
      "Epoch [133/200] Loss: 23.082916259765625\n",
      "Epoch [134/200] Loss: 22.862211227416992\n",
      "Epoch [135/200] Loss: 22.646434783935547\n",
      "Epoch [136/200] Loss: 22.435495376586914\n",
      "Epoch [137/200] Loss: 22.229291915893555\n",
      "Epoch [138/200] Loss: 22.027755737304688\n",
      "Epoch [139/200] Loss: 21.83077621459961\n",
      "Epoch [140/200] Loss: 21.638280868530273\n",
      "Epoch [141/200] Loss: 21.450164794921875\n",
      "Epoch [142/200] Loss: 21.266374588012695\n",
      "Epoch [143/200] Loss: 21.08679962158203\n",
      "Epoch [144/200] Loss: 20.9113712310791\n",
      "Epoch [145/200] Loss: 20.739994049072266\n",
      "Epoch [146/200] Loss: 20.572614669799805\n",
      "Epoch [147/200] Loss: 20.409135818481445\n",
      "Epoch [148/200] Loss: 20.24948501586914\n",
      "Epoch [149/200] Loss: 20.093585968017578\n",
      "Epoch [150/200] Loss: 19.941368103027344\n",
      "Epoch [151/200] Loss: 19.792749404907227\n",
      "Epoch [152/200] Loss: 19.647674560546875\n",
      "Epoch [153/200] Loss: 19.506052017211914\n",
      "Epoch [154/200] Loss: 19.367822647094727\n",
      "Epoch [155/200] Loss: 19.232927322387695\n",
      "Epoch [156/200] Loss: 19.10127830505371\n",
      "Epoch [157/200] Loss: 18.972824096679688\n",
      "Epoch [158/200] Loss: 18.847497940063477\n",
      "Epoch [159/200] Loss: 18.725223541259766\n",
      "Epoch [160/200] Loss: 18.605953216552734\n",
      "Epoch [161/200] Loss: 18.48961639404297\n",
      "Epoch [162/200] Loss: 18.376148223876953\n",
      "Epoch [163/200] Loss: 18.2654972076416\n",
      "Epoch [164/200] Loss: 18.15760612487793\n",
      "Epoch [165/200] Loss: 18.05240249633789\n",
      "Epoch [166/200] Loss: 17.94984245300293\n",
      "Epoch [167/200] Loss: 17.84986114501953\n",
      "Epoch [168/200] Loss: 17.752412796020508\n",
      "Epoch [169/200] Loss: 17.657434463500977\n",
      "Epoch [170/200] Loss: 17.56487464904785\n",
      "Epoch [171/200] Loss: 17.474681854248047\n",
      "Epoch [172/200] Loss: 17.386804580688477\n",
      "Epoch [173/200] Loss: 17.301191329956055\n",
      "Epoch [174/200] Loss: 17.217790603637695\n",
      "Epoch [175/200] Loss: 17.13656234741211\n",
      "Epoch [176/200] Loss: 17.05744171142578\n",
      "Epoch [177/200] Loss: 16.980392456054688\n",
      "Epoch [178/200] Loss: 16.905376434326172\n",
      "Epoch [179/200] Loss: 16.83233070373535\n",
      "Epoch [180/200] Loss: 16.761219024658203\n",
      "Epoch [181/200] Loss: 16.691999435424805\n",
      "Epoch [182/200] Loss: 16.62462615966797\n",
      "Epoch [183/200] Loss: 16.55905532836914\n",
      "Epoch [184/200] Loss: 16.495243072509766\n",
      "Epoch [185/200] Loss: 16.433151245117188\n",
      "Epoch [186/200] Loss: 16.372745513916016\n",
      "Epoch [187/200] Loss: 16.313976287841797\n",
      "Epoch [188/200] Loss: 16.256816864013672\n",
      "Epoch [189/200] Loss: 16.201215744018555\n",
      "Epoch [190/200] Loss: 16.14714813232422\n",
      "Epoch [191/200] Loss: 16.09457015991211\n",
      "Epoch [192/200] Loss: 16.043445587158203\n",
      "Epoch [193/200] Loss: 15.993745803833008\n",
      "Epoch [194/200] Loss: 15.945426940917969\n",
      "Epoch [195/200] Loss: 15.898465156555176\n",
      "Epoch [196/200] Loss: 15.852825164794922\n",
      "Epoch [197/200] Loss: 15.808469772338867\n",
      "Epoch [198/200] Loss: 15.76537036895752\n",
      "Epoch [199/200] Loss: 15.723494529724121\n",
      "Epoch [200/200] Loss: 15.682816505432129\n",
      "Predicted days_remaining for parent_id 352: 12.651910781860352\n",
      "Training for parent_id 360...\n",
      "Epoch [1/200] Loss: 495.4632873535156\n",
      "Epoch [2/200] Loss: 482.0880126953125\n",
      "Epoch [3/200] Loss: 469.0857849121094\n",
      "Epoch [4/200] Loss: 456.693603515625\n",
      "Epoch [5/200] Loss: 445.04486083984375\n",
      "Epoch [6/200] Loss: 434.1758728027344\n",
      "Epoch [7/200] Loss: 424.06280517578125\n",
      "Epoch [8/200] Loss: 414.6564025878906\n",
      "Epoch [9/200] Loss: 405.90240478515625\n",
      "Epoch [10/200] Loss: 397.7486877441406\n",
      "Epoch [11/200] Loss: 390.1473083496094\n",
      "Epoch [12/200] Loss: 383.055419921875\n",
      "Epoch [13/200] Loss: 376.4339294433594\n",
      "Epoch [14/200] Loss: 370.24627685546875\n",
      "Epoch [15/200] Loss: 364.4576721191406\n",
      "Epoch [16/200] Loss: 359.034423828125\n",
      "Epoch [17/200] Loss: 353.94317626953125\n",
      "Epoch [18/200] Loss: 349.1507873535156\n",
      "Epoch [19/200] Loss: 344.6251525878906\n",
      "Epoch [20/200] Loss: 340.3365173339844\n",
      "Epoch [21/200] Loss: 336.2585144042969\n",
      "Epoch [22/200] Loss: 332.368408203125\n",
      "Epoch [23/200] Loss: 328.6462707519531\n",
      "Epoch [24/200] Loss: 325.0742492675781\n",
      "Epoch [25/200] Loss: 321.6361389160156\n",
      "Epoch [26/200] Loss: 318.3171691894531\n",
      "Epoch [27/200] Loss: 315.1035461425781\n",
      "Epoch [28/200] Loss: 311.9828796386719\n",
      "Epoch [29/200] Loss: 308.94427490234375\n",
      "Epoch [30/200] Loss: 305.9781494140625\n",
      "Epoch [31/200] Loss: 303.0765075683594\n",
      "Epoch [32/200] Loss: 300.232421875\n",
      "Epoch [33/200] Loss: 297.4407043457031\n",
      "Epoch [34/200] Loss: 294.6968078613281\n",
      "Epoch [35/200] Loss: 291.99713134765625\n",
      "Epoch [36/200] Loss: 289.3385009765625\n",
      "Epoch [37/200] Loss: 286.7183837890625\n",
      "Epoch [38/200] Loss: 284.134521484375\n",
      "Epoch [39/200] Loss: 281.58465576171875\n",
      "Epoch [40/200] Loss: 279.067138671875\n",
      "Epoch [41/200] Loss: 276.58026123046875\n",
      "Epoch [42/200] Loss: 274.1226501464844\n",
      "Epoch [43/200] Loss: 271.69305419921875\n",
      "Epoch [44/200] Loss: 269.2904968261719\n",
      "Epoch [45/200] Loss: 266.9140625\n",
      "Epoch [46/200] Loss: 264.5628356933594\n",
      "Epoch [47/200] Loss: 262.236328125\n",
      "Epoch [48/200] Loss: 259.9338684082031\n",
      "Epoch [49/200] Loss: 257.6549072265625\n",
      "Epoch [50/200] Loss: 255.39889526367188\n",
      "Epoch [51/200] Loss: 253.1654815673828\n",
      "Epoch [52/200] Loss: 250.95419311523438\n",
      "Epoch [53/200] Loss: 248.7646026611328\n",
      "Epoch [54/200] Loss: 246.59632873535156\n",
      "Epoch [55/200] Loss: 244.44894409179688\n",
      "Epoch [56/200] Loss: 242.3221435546875\n",
      "Epoch [57/200] Loss: 240.2155303955078\n",
      "Epoch [58/200] Loss: 238.1287841796875\n",
      "Epoch [59/200] Loss: 236.06153869628906\n",
      "Epoch [60/200] Loss: 234.01351928710938\n",
      "Epoch [61/200] Loss: 231.98446655273438\n",
      "Epoch [62/200] Loss: 229.97401428222656\n",
      "Epoch [63/200] Loss: 227.98190307617188\n",
      "Epoch [64/200] Loss: 226.0078125\n",
      "Epoch [65/200] Loss: 224.05154418945312\n",
      "Epoch [66/200] Loss: 222.11277770996094\n",
      "Epoch [67/200] Loss: 220.19137573242188\n",
      "Epoch [68/200] Loss: 218.28697204589844\n",
      "Epoch [69/200] Loss: 216.3993377685547\n",
      "Epoch [70/200] Loss: 214.52838134765625\n",
      "Epoch [71/200] Loss: 212.6737518310547\n",
      "Epoch [72/200] Loss: 210.83531188964844\n",
      "Epoch [73/200] Loss: 209.0127716064453\n",
      "Epoch [74/200] Loss: 207.2060546875\n",
      "Epoch [75/200] Loss: 205.41482543945312\n",
      "Epoch [76/200] Loss: 203.63902282714844\n",
      "Epoch [77/200] Loss: 201.87832641601562\n",
      "Epoch [78/200] Loss: 200.13270568847656\n",
      "Epoch [79/200] Loss: 198.4019317626953\n",
      "Epoch [80/200] Loss: 196.685791015625\n",
      "Epoch [81/200] Loss: 194.98411560058594\n",
      "Epoch [82/200] Loss: 193.29678344726562\n",
      "Epoch [83/200] Loss: 191.62368774414062\n",
      "Epoch [84/200] Loss: 189.9645233154297\n",
      "Epoch [85/200] Loss: 188.3192901611328\n",
      "Epoch [86/200] Loss: 186.68780517578125\n",
      "Epoch [87/200] Loss: 185.06988525390625\n",
      "Epoch [88/200] Loss: 183.4654541015625\n",
      "Epoch [89/200] Loss: 181.8743133544922\n",
      "Epoch [90/200] Loss: 180.2963409423828\n",
      "Epoch [91/200] Loss: 178.7314453125\n",
      "Epoch [92/200] Loss: 177.1795196533203\n",
      "Epoch [93/200] Loss: 175.64035034179688\n",
      "Epoch [94/200] Loss: 174.11387634277344\n",
      "Epoch [95/200] Loss: 172.6000213623047\n",
      "Epoch [96/200] Loss: 171.09861755371094\n",
      "Epoch [97/200] Loss: 169.60952758789062\n",
      "Epoch [98/200] Loss: 168.13267517089844\n",
      "Epoch [99/200] Loss: 166.66793823242188\n",
      "Epoch [100/200] Loss: 165.2152557373047\n",
      "Epoch [101/200] Loss: 163.77450561523438\n",
      "Epoch [102/200] Loss: 162.3455352783203\n",
      "Epoch [103/200] Loss: 160.9282989501953\n",
      "Epoch [104/200] Loss: 159.52268981933594\n",
      "Epoch [105/200] Loss: 158.1285858154297\n",
      "Epoch [106/200] Loss: 156.74591064453125\n",
      "Epoch [107/200] Loss: 155.37457275390625\n",
      "Epoch [108/200] Loss: 154.0144805908203\n",
      "Epoch [109/200] Loss: 152.66554260253906\n",
      "Epoch [110/200] Loss: 151.32762145996094\n",
      "Epoch [111/200] Loss: 150.00071716308594\n",
      "Epoch [112/200] Loss: 148.68472290039062\n",
      "Epoch [113/200] Loss: 147.37948608398438\n",
      "Epoch [114/200] Loss: 146.08499145507812\n",
      "Epoch [115/200] Loss: 144.80116271972656\n",
      "Epoch [116/200] Loss: 143.52783203125\n",
      "Epoch [117/200] Loss: 142.26504516601562\n",
      "Epoch [118/200] Loss: 141.01258850097656\n",
      "Epoch [119/200] Loss: 139.77052307128906\n",
      "Epoch [120/200] Loss: 138.53866577148438\n",
      "Epoch [121/200] Loss: 137.31700134277344\n",
      "Epoch [122/200] Loss: 136.10537719726562\n",
      "Epoch [123/200] Loss: 134.90382385253906\n",
      "Epoch [124/200] Loss: 133.712158203125\n",
      "Epoch [125/200] Loss: 132.53042602539062\n",
      "Epoch [126/200] Loss: 131.3584442138672\n",
      "Epoch [127/200] Loss: 130.19622802734375\n",
      "Epoch [128/200] Loss: 129.04367065429688\n",
      "Epoch [129/200] Loss: 127.90070343017578\n",
      "Epoch [130/200] Loss: 126.76724243164062\n",
      "Epoch [131/200] Loss: 125.64324951171875\n",
      "Epoch [132/200] Loss: 124.52862548828125\n",
      "Epoch [133/200] Loss: 123.42333984375\n",
      "Epoch [134/200] Loss: 122.32732391357422\n",
      "Epoch [135/200] Loss: 121.2405014038086\n",
      "Epoch [136/200] Loss: 120.16278076171875\n",
      "Epoch [137/200] Loss: 119.09413146972656\n",
      "Epoch [138/200] Loss: 118.03449249267578\n",
      "Epoch [139/200] Loss: 116.9837417602539\n",
      "Epoch [140/200] Loss: 115.94192504882812\n",
      "Epoch [141/200] Loss: 114.90888214111328\n",
      "Epoch [142/200] Loss: 113.88462829589844\n",
      "Epoch [143/200] Loss: 112.86901092529297\n",
      "Epoch [144/200] Loss: 111.86204528808594\n",
      "Epoch [145/200] Loss: 110.86363983154297\n",
      "Epoch [146/200] Loss: 109.87376403808594\n",
      "Epoch [147/200] Loss: 108.89234924316406\n",
      "Epoch [148/200] Loss: 107.91930389404297\n",
      "Epoch [149/200] Loss: 106.95459747314453\n",
      "Epoch [150/200] Loss: 105.9981918334961\n",
      "Epoch [151/200] Loss: 105.04996490478516\n",
      "Epoch [152/200] Loss: 104.10991668701172\n",
      "Epoch [153/200] Loss: 103.17801666259766\n",
      "Epoch [154/200] Loss: 102.2541275024414\n",
      "Epoch [155/200] Loss: 101.33826446533203\n",
      "Epoch [156/200] Loss: 100.4303207397461\n",
      "Epoch [157/200] Loss: 99.53026580810547\n",
      "Epoch [158/200] Loss: 98.63805389404297\n",
      "Epoch [159/200] Loss: 97.75360107421875\n",
      "Epoch [160/200] Loss: 96.87691497802734\n",
      "Epoch [161/200] Loss: 96.00788879394531\n",
      "Epoch [162/200] Loss: 95.14645385742188\n",
      "Epoch [163/200] Loss: 94.2925796508789\n",
      "Epoch [164/200] Loss: 93.4462661743164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [165/200] Loss: 92.60737609863281\n",
      "Epoch [166/200] Loss: 91.77593231201172\n",
      "Epoch [167/200] Loss: 90.9518051147461\n",
      "Epoch [168/200] Loss: 90.13499450683594\n",
      "Epoch [169/200] Loss: 89.32547760009766\n",
      "Epoch [170/200] Loss: 88.52313232421875\n",
      "Epoch [171/200] Loss: 87.72795867919922\n",
      "Epoch [172/200] Loss: 86.93990325927734\n",
      "Epoch [173/200] Loss: 86.15889739990234\n",
      "Epoch [174/200] Loss: 85.38491821289062\n",
      "Epoch [175/200] Loss: 84.61785888671875\n",
      "Epoch [176/200] Loss: 83.85773468017578\n",
      "Epoch [177/200] Loss: 83.1044692993164\n",
      "Epoch [178/200] Loss: 82.35804748535156\n",
      "Epoch [179/200] Loss: 81.61833190917969\n",
      "Epoch [180/200] Loss: 80.88538360595703\n",
      "Epoch [181/200] Loss: 80.15908813476562\n",
      "Epoch [182/200] Loss: 79.43942260742188\n",
      "Epoch [183/200] Loss: 78.72633361816406\n",
      "Epoch [184/200] Loss: 78.01979064941406\n",
      "Epoch [185/200] Loss: 77.31969451904297\n",
      "Epoch [186/200] Loss: 76.62609100341797\n",
      "Epoch [187/200] Loss: 75.9388427734375\n",
      "Epoch [188/200] Loss: 75.25794219970703\n",
      "Epoch [189/200] Loss: 74.5833511352539\n",
      "Epoch [190/200] Loss: 73.91503143310547\n",
      "Epoch [191/200] Loss: 73.25287628173828\n",
      "Epoch [192/200] Loss: 72.59695434570312\n",
      "Epoch [193/200] Loss: 71.94713592529297\n",
      "Epoch [194/200] Loss: 71.3033676147461\n",
      "Epoch [195/200] Loss: 70.66567993164062\n",
      "Epoch [196/200] Loss: 70.033935546875\n",
      "Epoch [197/200] Loss: 69.40814971923828\n",
      "Epoch [198/200] Loss: 68.78828430175781\n",
      "Epoch [199/200] Loss: 68.17427825927734\n",
      "Epoch [200/200] Loss: 67.56607055664062\n",
      "Predicted days_remaining for parent_id 360: 14.5025053024292\n",
      "Training for parent_id 362...\n",
      "Epoch [1/200] Loss: 1171.540771484375\n",
      "Epoch [2/200] Loss: 1158.765380859375\n",
      "Epoch [3/200] Loss: 1146.2760009765625\n",
      "Epoch [4/200] Loss: 1134.0419921875\n",
      "Epoch [5/200] Loss: 1122.0579833984375\n",
      "Epoch [6/200] Loss: 1110.3232421875\n",
      "Epoch [7/200] Loss: 1098.8306884765625\n",
      "Epoch [8/200] Loss: 1087.5643310546875\n",
      "Epoch [9/200] Loss: 1076.5015869140625\n",
      "Epoch [10/200] Loss: 1065.61669921875\n",
      "Epoch [11/200] Loss: 1054.8851318359375\n",
      "Epoch [12/200] Loss: 1044.2847900390625\n",
      "Epoch [13/200] Loss: 1033.7989501953125\n",
      "Epoch [14/200] Loss: 1023.4151611328125\n",
      "Epoch [15/200] Loss: 1013.1265258789062\n",
      "Epoch [16/200] Loss: 1002.93115234375\n",
      "Epoch [17/200] Loss: 992.8319702148438\n",
      "Epoch [18/200] Loss: 982.8357543945312\n",
      "Epoch [19/200] Loss: 972.9531860351562\n",
      "Epoch [20/200] Loss: 963.196533203125\n",
      "Epoch [21/200] Loss: 953.5802001953125\n",
      "Epoch [22/200] Loss: 944.1183471679688\n",
      "Epoch [23/200] Loss: 934.8230590820312\n",
      "Epoch [24/200] Loss: 925.7032470703125\n",
      "Epoch [25/200] Loss: 916.7628173828125\n",
      "Epoch [26/200] Loss: 908.001220703125\n",
      "Epoch [27/200] Loss: 899.4137573242188\n",
      "Epoch [28/200] Loss: 890.99462890625\n",
      "Epoch [29/200] Loss: 882.739501953125\n",
      "Epoch [30/200] Loss: 874.6465454101562\n",
      "Epoch [31/200] Loss: 866.719482421875\n",
      "Epoch [32/200] Loss: 858.966064453125\n",
      "Epoch [33/200] Loss: 851.3973999023438\n",
      "Epoch [34/200] Loss: 844.0253295898438\n",
      "Epoch [35/200] Loss: 836.8599243164062\n",
      "Epoch [36/200] Loss: 829.90869140625\n",
      "Epoch [37/200] Loss: 823.174072265625\n",
      "Epoch [38/200] Loss: 816.6544799804688\n",
      "Epoch [39/200] Loss: 810.3441772460938\n",
      "Epoch [40/200] Loss: 804.2343139648438\n",
      "Epoch [41/200] Loss: 798.3135375976562\n",
      "Epoch [42/200] Loss: 792.569580078125\n",
      "Epoch [43/200] Loss: 786.9892578125\n",
      "Epoch [44/200] Loss: 781.5597534179688\n",
      "Epoch [45/200] Loss: 776.2685546875\n",
      "Epoch [46/200] Loss: 771.1036987304688\n",
      "Epoch [47/200] Loss: 766.054443359375\n",
      "Epoch [48/200] Loss: 761.1102905273438\n",
      "Epoch [49/200] Loss: 756.2625122070312\n",
      "Epoch [50/200] Loss: 751.5025024414062\n",
      "Epoch [51/200] Loss: 746.8232421875\n",
      "Epoch [52/200] Loss: 742.2178955078125\n",
      "Epoch [53/200] Loss: 737.680908203125\n",
      "Epoch [54/200] Loss: 733.2069702148438\n",
      "Epoch [55/200] Loss: 728.7916259765625\n",
      "Epoch [56/200] Loss: 724.4310302734375\n",
      "Epoch [57/200] Loss: 720.1216430664062\n",
      "Epoch [58/200] Loss: 715.8604125976562\n",
      "Epoch [59/200] Loss: 711.6446533203125\n",
      "Epoch [60/200] Loss: 707.4719848632812\n",
      "Epoch [61/200] Loss: 703.3403930664062\n",
      "Epoch [62/200] Loss: 699.2479858398438\n",
      "Epoch [63/200] Loss: 695.1931762695312\n",
      "Epoch [64/200] Loss: 691.174560546875\n",
      "Epoch [65/200] Loss: 687.1906127929688\n",
      "Epoch [66/200] Loss: 683.2404174804688\n",
      "Epoch [67/200] Loss: 679.3226928710938\n",
      "Epoch [68/200] Loss: 675.4366455078125\n",
      "Epoch [69/200] Loss: 671.581298828125\n",
      "Epoch [70/200] Loss: 667.7557373046875\n",
      "Epoch [71/200] Loss: 663.9593505859375\n",
      "Epoch [72/200] Loss: 660.1912841796875\n",
      "Epoch [73/200] Loss: 656.4511108398438\n",
      "Epoch [74/200] Loss: 652.738037109375\n",
      "Epoch [75/200] Loss: 649.0513916015625\n",
      "Epoch [76/200] Loss: 645.390869140625\n",
      "Epoch [77/200] Loss: 641.7559204101562\n",
      "Epoch [78/200] Loss: 638.1459350585938\n",
      "Epoch [79/200] Loss: 634.5604248046875\n",
      "Epoch [80/200] Loss: 630.9990844726562\n",
      "Epoch [81/200] Loss: 627.4616088867188\n",
      "Epoch [82/200] Loss: 623.9470825195312\n",
      "Epoch [83/200] Loss: 620.455810546875\n",
      "Epoch [84/200] Loss: 616.9868774414062\n",
      "Epoch [85/200] Loss: 613.5401611328125\n",
      "Epoch [86/200] Loss: 610.1153564453125\n",
      "Epoch [87/200] Loss: 606.7120971679688\n",
      "Epoch [88/200] Loss: 603.3298950195312\n",
      "Epoch [89/200] Loss: 599.9686889648438\n",
      "Epoch [90/200] Loss: 596.628173828125\n",
      "Epoch [91/200] Loss: 593.3079833984375\n",
      "Epoch [92/200] Loss: 590.0078735351562\n",
      "Epoch [93/200] Loss: 586.7273559570312\n",
      "Epoch [94/200] Loss: 583.466552734375\n",
      "Epoch [95/200] Loss: 580.2250366210938\n",
      "Epoch [96/200] Loss: 577.002685546875\n",
      "Epoch [97/200] Loss: 573.799072265625\n",
      "Epoch [98/200] Loss: 570.6141357421875\n",
      "Epoch [99/200] Loss: 567.4473876953125\n",
      "Epoch [100/200] Loss: 564.2991943359375\n",
      "Epoch [101/200] Loss: 561.1688232421875\n",
      "Epoch [102/200] Loss: 558.0562133789062\n",
      "Epoch [103/200] Loss: 554.9611206054688\n",
      "Epoch [104/200] Loss: 551.8836059570312\n",
      "Epoch [105/200] Loss: 548.8232421875\n",
      "Epoch [106/200] Loss: 545.7799682617188\n",
      "Epoch [107/200] Loss: 542.7536010742188\n",
      "Epoch [108/200] Loss: 539.7439575195312\n",
      "Epoch [109/200] Loss: 536.7508544921875\n",
      "Epoch [110/200] Loss: 533.774169921875\n",
      "Epoch [111/200] Loss: 530.8138427734375\n",
      "Epoch [112/200] Loss: 527.8695678710938\n",
      "Epoch [113/200] Loss: 524.941162109375\n",
      "Epoch [114/200] Loss: 522.028564453125\n",
      "Epoch [115/200] Loss: 519.1317138671875\n",
      "Epoch [116/200] Loss: 516.25048828125\n",
      "Epoch [117/200] Loss: 513.3845825195312\n",
      "Epoch [118/200] Loss: 510.53399658203125\n",
      "Epoch [119/200] Loss: 507.6985168457031\n",
      "Epoch [120/200] Loss: 504.8780822753906\n",
      "Epoch [121/200] Loss: 502.0726318359375\n",
      "Epoch [122/200] Loss: 499.2819519042969\n",
      "Epoch [123/200] Loss: 496.5058898925781\n",
      "Epoch [124/200] Loss: 493.74456787109375\n",
      "Epoch [125/200] Loss: 490.9974670410156\n",
      "Epoch [126/200] Loss: 488.26483154296875\n",
      "Epoch [127/200] Loss: 485.5465087890625\n",
      "Epoch [128/200] Loss: 482.8423156738281\n",
      "Epoch [129/200] Loss: 480.1520690917969\n",
      "Epoch [130/200] Loss: 477.47589111328125\n",
      "Epoch [131/200] Loss: 474.81341552734375\n",
      "Epoch [132/200] Loss: 472.1648254394531\n",
      "Epoch [133/200] Loss: 469.5298156738281\n",
      "Epoch [134/200] Loss: 466.9082946777344\n",
      "Epoch [135/200] Loss: 464.30047607421875\n",
      "Epoch [136/200] Loss: 461.7058410644531\n",
      "Epoch [137/200] Loss: 459.12457275390625\n",
      "Epoch [138/200] Loss: 456.5565185546875\n",
      "Epoch [139/200] Loss: 454.00152587890625\n",
      "Epoch [140/200] Loss: 451.4596862792969\n",
      "Epoch [141/200] Loss: 448.93072509765625\n",
      "Epoch [142/200] Loss: 446.4146423339844\n",
      "Epoch [143/200] Loss: 443.91131591796875\n",
      "Epoch [144/200] Loss: 441.42083740234375\n",
      "Epoch [145/200] Loss: 438.9429016113281\n",
      "Epoch [146/200] Loss: 436.4775695800781\n",
      "Epoch [147/200] Loss: 434.0247802734375\n",
      "Epoch [148/200] Loss: 431.58428955078125\n",
      "Epoch [149/200] Loss: 429.1561584472656\n",
      "Epoch [150/200] Loss: 426.740478515625\n",
      "Epoch [151/200] Loss: 424.3369140625\n",
      "Epoch [152/200] Loss: 421.9454040527344\n",
      "Epoch [153/200] Loss: 419.5660400390625\n",
      "Epoch [154/200] Loss: 417.19873046875\n",
      "Epoch [155/200] Loss: 414.8432922363281\n",
      "Epoch [156/200] Loss: 412.49969482421875\n",
      "Epoch [157/200] Loss: 410.1679382324219\n",
      "Epoch [158/200] Loss: 407.8478698730469\n",
      "Epoch [159/200] Loss: 405.53961181640625\n",
      "Epoch [160/200] Loss: 403.2427978515625\n",
      "Epoch [161/200] Loss: 400.957763671875\n",
      "Epoch [162/200] Loss: 398.68402099609375\n",
      "Epoch [163/200] Loss: 396.42181396484375\n",
      "Epoch [164/200] Loss: 394.1709899902344\n",
      "Epoch [165/200] Loss: 391.9314270019531\n",
      "Epoch [166/200] Loss: 389.7030944824219\n",
      "Epoch [167/200] Loss: 387.4860534667969\n",
      "Epoch [168/200] Loss: 385.2801513671875\n",
      "Epoch [169/200] Loss: 383.085205078125\n",
      "Epoch [170/200] Loss: 380.9013366699219\n",
      "Epoch [171/200] Loss: 378.7285461425781\n",
      "Epoch [172/200] Loss: 376.5666809082031\n",
      "Epoch [173/200] Loss: 374.41552734375\n",
      "Epoch [174/200] Loss: 372.2752990722656\n",
      "Epoch [175/200] Loss: 370.1458435058594\n",
      "Epoch [176/200] Loss: 368.0270690917969\n",
      "Epoch [177/200] Loss: 365.91888427734375\n",
      "Epoch [178/200] Loss: 363.8214111328125\n",
      "Epoch [179/200] Loss: 361.73443603515625\n",
      "Epoch [180/200] Loss: 359.65802001953125\n",
      "Epoch [181/200] Loss: 357.59197998046875\n",
      "Epoch [182/200] Loss: 355.5364685058594\n",
      "Epoch [183/200] Loss: 353.4911804199219\n",
      "Epoch [184/200] Loss: 351.45623779296875\n",
      "Epoch [185/200] Loss: 349.4315185546875\n",
      "Epoch [186/200] Loss: 347.41705322265625\n",
      "Epoch [187/200] Loss: 345.4128112792969\n",
      "Epoch [188/200] Loss: 343.41864013671875\n",
      "Epoch [189/200] Loss: 341.4344482421875\n",
      "Epoch [190/200] Loss: 339.46044921875\n",
      "Epoch [191/200] Loss: 337.4963684082031\n",
      "Epoch [192/200] Loss: 335.5421142578125\n",
      "Epoch [193/200] Loss: 333.5978088378906\n",
      "Epoch [194/200] Loss: 331.66339111328125\n",
      "Epoch [195/200] Loss: 329.7388000488281\n",
      "Epoch [196/200] Loss: 327.823974609375\n",
      "Epoch [197/200] Loss: 325.918701171875\n",
      "Epoch [198/200] Loss: 324.02325439453125\n",
      "Epoch [199/200] Loss: 322.1372985839844\n",
      "Epoch [200/200] Loss: 320.2610168457031\n",
      "Predicted days_remaining for parent_id 362: 16.315643310546875\n",
      "Training for parent_id 366...\n",
      "Epoch [1/200] Loss: 947.0081787109375\n",
      "Epoch [2/200] Loss: 933.3908081054688\n",
      "Epoch [3/200] Loss: 919.9639892578125\n",
      "Epoch [4/200] Loss: 906.7689819335938\n",
      "Epoch [5/200] Loss: 893.8428955078125\n",
      "Epoch [6/200] Loss: 881.229248046875\n",
      "Epoch [7/200] Loss: 868.960205078125\n",
      "Epoch [8/200] Loss: 857.0502319335938\n",
      "Epoch [9/200] Loss: 845.5059204101562\n",
      "Epoch [10/200] Loss: 834.33251953125\n",
      "Epoch [11/200] Loss: 823.5382080078125\n",
      "Epoch [12/200] Loss: 813.1337890625\n",
      "Epoch [13/200] Loss: 803.1310424804688\n",
      "Epoch [14/200] Loss: 793.5401611328125\n",
      "Epoch [15/200] Loss: 784.3668823242188\n",
      "Epoch [16/200] Loss: 775.6122436523438\n",
      "Epoch [17/200] Loss: 767.2722778320312\n",
      "Epoch [18/200] Loss: 759.337646484375\n",
      "Epoch [19/200] Loss: 751.79443359375\n",
      "Epoch [20/200] Loss: 744.6251831054688\n",
      "Epoch [21/200] Loss: 737.80908203125\n",
      "Epoch [22/200] Loss: 731.3244018554688\n",
      "Epoch [23/200] Loss: 725.1480102539062\n",
      "Epoch [24/200] Loss: 719.2567138671875\n",
      "Epoch [25/200] Loss: 713.6267700195312\n",
      "Epoch [26/200] Loss: 708.2344360351562\n",
      "Epoch [27/200] Loss: 703.0567626953125\n",
      "Epoch [28/200] Loss: 698.0703735351562\n",
      "Epoch [29/200] Loss: 693.253662109375\n",
      "Epoch [30/200] Loss: 688.585693359375\n",
      "Epoch [31/200] Loss: 684.0473022460938\n",
      "Epoch [32/200] Loss: 679.6210327148438\n",
      "Epoch [33/200] Loss: 675.2919921875\n",
      "Epoch [34/200] Loss: 671.046875\n",
      "Epoch [35/200] Loss: 666.8743896484375\n",
      "Epoch [36/200] Loss: 662.765625\n",
      "Epoch [37/200] Loss: 658.7127075195312\n",
      "Epoch [38/200] Loss: 654.7097778320312\n",
      "Epoch [39/200] Loss: 650.7518310546875\n",
      "Epoch [40/200] Loss: 646.8355712890625\n",
      "Epoch [41/200] Loss: 642.9579467773438\n",
      "Epoch [42/200] Loss: 639.1173095703125\n",
      "Epoch [43/200] Loss: 635.312255859375\n",
      "Epoch [44/200] Loss: 631.5418701171875\n",
      "Epoch [45/200] Loss: 627.8055419921875\n",
      "Epoch [46/200] Loss: 624.1026000976562\n",
      "Epoch [47/200] Loss: 620.4326782226562\n",
      "Epoch [48/200] Loss: 616.7948608398438\n",
      "Epoch [49/200] Loss: 613.1885375976562\n",
      "Epoch [50/200] Loss: 609.6126098632812\n",
      "Epoch [51/200] Loss: 606.0663452148438\n",
      "Epoch [52/200] Loss: 602.5482788085938\n",
      "Epoch [53/200] Loss: 599.05712890625\n",
      "Epoch [54/200] Loss: 595.591796875\n",
      "Epoch [55/200] Loss: 592.1509399414062\n",
      "Epoch [56/200] Loss: 588.7332153320312\n",
      "Epoch [57/200] Loss: 585.3377685546875\n",
      "Epoch [58/200] Loss: 581.9635009765625\n",
      "Epoch [59/200] Loss: 578.6097412109375\n",
      "Epoch [60/200] Loss: 575.2754516601562\n",
      "Epoch [61/200] Loss: 571.96044921875\n",
      "Epoch [62/200] Loss: 568.6640014648438\n",
      "Epoch [63/200] Loss: 565.3856811523438\n",
      "Epoch [64/200] Loss: 562.1254272460938\n",
      "Epoch [65/200] Loss: 558.8828735351562\n",
      "Epoch [66/200] Loss: 555.6580200195312\n",
      "Epoch [67/200] Loss: 552.4506225585938\n",
      "Epoch [68/200] Loss: 549.2608642578125\n",
      "Epoch [69/200] Loss: 546.0885620117188\n",
      "Epoch [70/200] Loss: 542.933837890625\n",
      "Epoch [71/200] Loss: 539.796630859375\n",
      "Epoch [72/200] Loss: 536.6769409179688\n",
      "Epoch [73/200] Loss: 533.5748291015625\n",
      "Epoch [74/200] Loss: 530.489990234375\n",
      "Epoch [75/200] Loss: 527.4227294921875\n",
      "Epoch [76/200] Loss: 524.3726196289062\n",
      "Epoch [77/200] Loss: 521.33984375\n",
      "Epoch [78/200] Loss: 518.3240966796875\n",
      "Epoch [79/200] Loss: 515.3253784179688\n",
      "Epoch [80/200] Loss: 512.3433227539062\n",
      "Epoch [81/200] Loss: 509.37811279296875\n",
      "Epoch [82/200] Loss: 506.4295349121094\n",
      "Epoch [83/200] Loss: 503.4973449707031\n",
      "Epoch [84/200] Loss: 500.58148193359375\n",
      "Epoch [85/200] Loss: 497.68194580078125\n",
      "Epoch [86/200] Loss: 494.7983703613281\n",
      "Epoch [87/200] Loss: 491.9308776855469\n",
      "Epoch [88/200] Loss: 489.0792236328125\n",
      "Epoch [89/200] Loss: 486.2432861328125\n",
      "Epoch [90/200] Loss: 483.4229736328125\n",
      "Epoch [91/200] Loss: 480.6181640625\n",
      "Epoch [92/200] Loss: 477.8287048339844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/200] Loss: 475.0546569824219\n",
      "Epoch [94/200] Loss: 472.2958068847656\n",
      "Epoch [95/200] Loss: 469.5519104003906\n",
      "Epoch [96/200] Loss: 466.822998046875\n",
      "Epoch [97/200] Loss: 464.1089782714844\n",
      "Epoch [98/200] Loss: 461.4096374511719\n",
      "Epoch [99/200] Loss: 458.72503662109375\n",
      "Epoch [100/200] Loss: 456.05487060546875\n",
      "Epoch [101/200] Loss: 453.39923095703125\n",
      "Epoch [102/200] Loss: 450.75787353515625\n",
      "Epoch [103/200] Loss: 448.130859375\n",
      "Epoch [104/200] Loss: 445.517822265625\n",
      "Epoch [105/200] Loss: 442.9188537597656\n",
      "Epoch [106/200] Loss: 440.3339538574219\n",
      "Epoch [107/200] Loss: 437.7628479003906\n",
      "Epoch [108/200] Loss: 435.2054138183594\n",
      "Epoch [109/200] Loss: 432.66168212890625\n",
      "Epoch [110/200] Loss: 430.1315612792969\n",
      "Epoch [111/200] Loss: 427.61492919921875\n",
      "Epoch [112/200] Loss: 425.111572265625\n",
      "Epoch [113/200] Loss: 422.62164306640625\n",
      "Epoch [114/200] Loss: 420.1448669433594\n",
      "Epoch [115/200] Loss: 417.6812438964844\n",
      "Epoch [116/200] Loss: 415.2307434082031\n",
      "Epoch [117/200] Loss: 412.7930603027344\n",
      "Epoch [118/200] Loss: 410.3683166503906\n",
      "Epoch [119/200] Loss: 407.95648193359375\n",
      "Epoch [120/200] Loss: 405.5572509765625\n",
      "Epoch [121/200] Loss: 403.17071533203125\n",
      "Epoch [122/200] Loss: 400.79681396484375\n",
      "Epoch [123/200] Loss: 398.4353332519531\n",
      "Epoch [124/200] Loss: 396.0862121582031\n",
      "Epoch [125/200] Loss: 393.7494812011719\n",
      "Epoch [126/200] Loss: 391.4250793457031\n",
      "Epoch [127/200] Loss: 389.1128845214844\n",
      "Epoch [128/200] Loss: 386.812744140625\n",
      "Epoch [129/200] Loss: 384.52471923828125\n",
      "Epoch [130/200] Loss: 382.24859619140625\n",
      "Epoch [131/200] Loss: 379.9845886230469\n",
      "Epoch [132/200] Loss: 377.7322998046875\n",
      "Epoch [133/200] Loss: 375.4918518066406\n",
      "Epoch [134/200] Loss: 373.2631530761719\n",
      "Epoch [135/200] Loss: 371.04608154296875\n",
      "Epoch [136/200] Loss: 368.8405456542969\n",
      "Epoch [137/200] Loss: 366.6466064453125\n",
      "Epoch [138/200] Loss: 364.46417236328125\n",
      "Epoch [139/200] Loss: 362.293212890625\n",
      "Epoch [140/200] Loss: 360.1335144042969\n",
      "Epoch [141/200] Loss: 357.98516845703125\n",
      "Epoch [142/200] Loss: 355.84796142578125\n",
      "Epoch [143/200] Loss: 353.7220458984375\n",
      "Epoch [144/200] Loss: 351.60711669921875\n",
      "Epoch [145/200] Loss: 349.50341796875\n",
      "Epoch [146/200] Loss: 347.4106140136719\n",
      "Epoch [147/200] Loss: 345.3288269042969\n",
      "Epoch [148/200] Loss: 343.25787353515625\n",
      "Epoch [149/200] Loss: 341.19775390625\n",
      "Epoch [150/200] Loss: 339.1484069824219\n",
      "Epoch [151/200] Loss: 337.10992431640625\n",
      "Epoch [152/200] Loss: 335.08209228515625\n",
      "Epoch [153/200] Loss: 333.0648498535156\n",
      "Epoch [154/200] Loss: 331.0581359863281\n",
      "Epoch [155/200] Loss: 329.06207275390625\n",
      "Epoch [156/200] Loss: 327.0764465332031\n",
      "Epoch [157/200] Loss: 325.10113525390625\n",
      "Epoch [158/200] Loss: 323.13623046875\n",
      "Epoch [159/200] Loss: 321.18182373046875\n",
      "Epoch [160/200] Loss: 319.237548828125\n",
      "Epoch [161/200] Loss: 317.3034973144531\n",
      "Epoch [162/200] Loss: 315.3796691894531\n",
      "Epoch [163/200] Loss: 313.4659118652344\n",
      "Epoch [164/200] Loss: 311.5622863769531\n",
      "Epoch [165/200] Loss: 309.66876220703125\n",
      "Epoch [166/200] Loss: 307.78509521484375\n",
      "Epoch [167/200] Loss: 305.9114990234375\n",
      "Epoch [168/200] Loss: 304.0477294921875\n",
      "Epoch [169/200] Loss: 302.19384765625\n",
      "Epoch [170/200] Loss: 300.3497314453125\n",
      "Epoch [171/200] Loss: 298.5154724121094\n",
      "Epoch [172/200] Loss: 296.6908874511719\n",
      "Epoch [173/200] Loss: 294.8759765625\n",
      "Epoch [174/200] Loss: 293.07073974609375\n",
      "Epoch [175/200] Loss: 291.2749938964844\n",
      "Epoch [176/200] Loss: 289.48883056640625\n",
      "Epoch [177/200] Loss: 287.71221923828125\n",
      "Epoch [178/200] Loss: 285.945068359375\n",
      "Epoch [179/200] Loss: 284.187255859375\n",
      "Epoch [180/200] Loss: 282.4389343261719\n",
      "Epoch [181/200] Loss: 280.6998596191406\n",
      "Epoch [182/200] Loss: 278.9700622558594\n",
      "Epoch [183/200] Loss: 277.2495422363281\n",
      "Epoch [184/200] Loss: 275.53826904296875\n",
      "Epoch [185/200] Loss: 273.836181640625\n",
      "Epoch [186/200] Loss: 272.14312744140625\n",
      "Epoch [187/200] Loss: 270.459228515625\n",
      "Epoch [188/200] Loss: 268.784423828125\n",
      "Epoch [189/200] Loss: 267.11859130859375\n",
      "Epoch [190/200] Loss: 265.46173095703125\n",
      "Epoch [191/200] Loss: 263.8138122558594\n",
      "Epoch [192/200] Loss: 262.1746826171875\n",
      "Epoch [193/200] Loss: 260.5445556640625\n",
      "Epoch [194/200] Loss: 258.9231872558594\n",
      "Epoch [195/200] Loss: 257.310546875\n",
      "Epoch [196/200] Loss: 255.7067108154297\n",
      "Epoch [197/200] Loss: 254.11154174804688\n",
      "Epoch [198/200] Loss: 252.5250244140625\n",
      "Epoch [199/200] Loss: 250.9471893310547\n",
      "Epoch [200/200] Loss: 249.37791442871094\n",
      "Predicted days_remaining for parent_id 366: 15.473231315612793\n",
      "Training for parent_id 367...\n",
      "Epoch [1/200] Loss: 2204.24560546875\n",
      "Epoch [2/200] Loss: 2183.3564453125\n",
      "Epoch [3/200] Loss: 2163.38232421875\n",
      "Epoch [4/200] Loss: 2144.41015625\n",
      "Epoch [5/200] Loss: 2126.425537109375\n",
      "Epoch [6/200] Loss: 2109.27587890625\n",
      "Epoch [7/200] Loss: 2092.75\n",
      "Epoch [8/200] Loss: 2076.675048828125\n",
      "Epoch [9/200] Loss: 2060.9443359375\n",
      "Epoch [10/200] Loss: 2045.504150390625\n",
      "Epoch [11/200] Loss: 2030.3427734375\n",
      "Epoch [12/200] Loss: 2015.4796142578125\n",
      "Epoch [13/200] Loss: 2000.9481201171875\n",
      "Epoch [14/200] Loss: 1986.784423828125\n",
      "Epoch [15/200] Loss: 1973.017333984375\n",
      "Epoch [16/200] Loss: 1959.66650390625\n",
      "Epoch [17/200] Loss: 1946.74462890625\n",
      "Epoch [18/200] Loss: 1934.256103515625\n",
      "Epoch [19/200] Loss: 1922.1986083984375\n",
      "Epoch [20/200] Loss: 1910.5654296875\n",
      "Epoch [21/200] Loss: 1899.3428955078125\n",
      "Epoch [22/200] Loss: 1888.5108642578125\n",
      "Epoch [23/200] Loss: 1878.04638671875\n",
      "Epoch [24/200] Loss: 1867.922119140625\n",
      "Epoch [25/200] Loss: 1858.1119384765625\n",
      "Epoch [26/200] Loss: 1848.5899658203125\n",
      "Epoch [27/200] Loss: 1839.332275390625\n",
      "Epoch [28/200] Loss: 1830.318115234375\n",
      "Epoch [29/200] Loss: 1821.52783203125\n",
      "Epoch [30/200] Loss: 1812.944580078125\n",
      "Epoch [31/200] Loss: 1804.552734375\n",
      "Epoch [32/200] Loss: 1796.3382568359375\n",
      "Epoch [33/200] Loss: 1788.2882080078125\n",
      "Epoch [34/200] Loss: 1780.3907470703125\n",
      "Epoch [35/200] Loss: 1772.6348876953125\n",
      "Epoch [36/200] Loss: 1765.0108642578125\n",
      "Epoch [37/200] Loss: 1757.5086669921875\n",
      "Epoch [38/200] Loss: 1750.1202392578125\n",
      "Epoch [39/200] Loss: 1742.83740234375\n",
      "Epoch [40/200] Loss: 1735.6533203125\n",
      "Epoch [41/200] Loss: 1728.56201171875\n",
      "Epoch [42/200] Loss: 1721.55712890625\n",
      "Epoch [43/200] Loss: 1714.6346435546875\n",
      "Epoch [44/200] Loss: 1707.7891845703125\n",
      "Epoch [45/200] Loss: 1701.0169677734375\n",
      "Epoch [46/200] Loss: 1694.3135986328125\n",
      "Epoch [47/200] Loss: 1687.6763916015625\n",
      "Epoch [48/200] Loss: 1681.10107421875\n",
      "Epoch [49/200] Loss: 1674.5849609375\n",
      "Epoch [50/200] Loss: 1668.12548828125\n",
      "Epoch [51/200] Loss: 1661.71923828125\n",
      "Epoch [52/200] Loss: 1655.3646240234375\n",
      "Epoch [53/200] Loss: 1649.0589599609375\n",
      "Epoch [54/200] Loss: 1642.800048828125\n",
      "Epoch [55/200] Loss: 1636.5865478515625\n",
      "Epoch [56/200] Loss: 1630.416015625\n",
      "Epoch [57/200] Loss: 1624.2872314453125\n",
      "Epoch [58/200] Loss: 1618.198486328125\n",
      "Epoch [59/200] Loss: 1612.1488037109375\n",
      "Epoch [60/200] Loss: 1606.136962890625\n",
      "Epoch [61/200] Loss: 1600.1612548828125\n",
      "Epoch [62/200] Loss: 1594.22119140625\n",
      "Epoch [63/200] Loss: 1588.315673828125\n",
      "Epoch [64/200] Loss: 1582.44384765625\n",
      "Epoch [65/200] Loss: 1576.6046142578125\n",
      "Epoch [66/200] Loss: 1570.79736328125\n",
      "Epoch [67/200] Loss: 1565.021484375\n",
      "Epoch [68/200] Loss: 1559.276123046875\n",
      "Epoch [69/200] Loss: 1553.5606689453125\n",
      "Epoch [70/200] Loss: 1547.8746337890625\n",
      "Epoch [71/200] Loss: 1542.2171630859375\n",
      "Epoch [72/200] Loss: 1536.5880126953125\n",
      "Epoch [73/200] Loss: 1530.986572265625\n",
      "Epoch [74/200] Loss: 1525.411865234375\n",
      "Epoch [75/200] Loss: 1519.864501953125\n",
      "Epoch [76/200] Loss: 1514.3427734375\n",
      "Epoch [77/200] Loss: 1508.8470458984375\n",
      "Epoch [78/200] Loss: 1503.3768310546875\n",
      "Epoch [79/200] Loss: 1497.931396484375\n",
      "Epoch [80/200] Loss: 1492.5107421875\n",
      "Epoch [81/200] Loss: 1487.1138916015625\n",
      "Epoch [82/200] Loss: 1481.7412109375\n",
      "Epoch [83/200] Loss: 1476.391845703125\n",
      "Epoch [84/200] Loss: 1471.0657958984375\n",
      "Epoch [85/200] Loss: 1465.7625732421875\n",
      "Epoch [86/200] Loss: 1460.48193359375\n",
      "Epoch [87/200] Loss: 1455.2235107421875\n",
      "Epoch [88/200] Loss: 1449.9869384765625\n",
      "Epoch [89/200] Loss: 1444.77197265625\n",
      "Epoch [90/200] Loss: 1439.57861328125\n",
      "Epoch [91/200] Loss: 1434.406494140625\n",
      "Epoch [92/200] Loss: 1429.2548828125\n",
      "Epoch [93/200] Loss: 1424.124267578125\n",
      "Epoch [94/200] Loss: 1419.013671875\n",
      "Epoch [95/200] Loss: 1413.9234619140625\n",
      "Epoch [96/200] Loss: 1408.8536376953125\n",
      "Epoch [97/200] Loss: 1403.8031005859375\n",
      "Epoch [98/200] Loss: 1398.7723388671875\n",
      "Epoch [99/200] Loss: 1393.760498046875\n",
      "Epoch [100/200] Loss: 1388.7679443359375\n",
      "Epoch [101/200] Loss: 1383.7943115234375\n",
      "Epoch [102/200] Loss: 1378.8397216796875\n",
      "Epoch [103/200] Loss: 1373.903564453125\n",
      "Epoch [104/200] Loss: 1368.985595703125\n",
      "Epoch [105/200] Loss: 1364.086181640625\n",
      "Epoch [106/200] Loss: 1359.2044677734375\n",
      "Epoch [107/200] Loss: 1354.3408203125\n",
      "Epoch [108/200] Loss: 1349.494873046875\n",
      "Epoch [109/200] Loss: 1344.6666259765625\n",
      "Epoch [110/200] Loss: 1339.8555908203125\n",
      "Epoch [111/200] Loss: 1335.062255859375\n",
      "Epoch [112/200] Loss: 1330.2855224609375\n",
      "Epoch [113/200] Loss: 1325.526123046875\n",
      "Epoch [114/200] Loss: 1320.783447265625\n",
      "Epoch [115/200] Loss: 1316.057861328125\n",
      "Epoch [116/200] Loss: 1311.348388671875\n",
      "Epoch [117/200] Loss: 1306.656005859375\n",
      "Epoch [118/200] Loss: 1301.9796142578125\n",
      "Epoch [119/200] Loss: 1297.319580078125\n",
      "Epoch [120/200] Loss: 1292.6756591796875\n",
      "Epoch [121/200] Loss: 1288.047607421875\n",
      "Epoch [122/200] Loss: 1283.4356689453125\n",
      "Epoch [123/200] Loss: 1278.83935546875\n",
      "Epoch [124/200] Loss: 1274.259033203125\n",
      "Epoch [125/200] Loss: 1269.6939697265625\n",
      "Epoch [126/200] Loss: 1265.14453125\n",
      "Epoch [127/200] Loss: 1260.6107177734375\n",
      "Epoch [128/200] Loss: 1256.0919189453125\n",
      "Epoch [129/200] Loss: 1251.588623046875\n",
      "Epoch [130/200] Loss: 1247.10009765625\n",
      "Epoch [131/200] Loss: 1242.6265869140625\n",
      "Epoch [132/200] Loss: 1238.1683349609375\n",
      "Epoch [133/200] Loss: 1233.724609375\n",
      "Epoch [134/200] Loss: 1229.295654296875\n",
      "Epoch [135/200] Loss: 1224.8814697265625\n",
      "Epoch [136/200] Loss: 1220.48193359375\n",
      "Epoch [137/200] Loss: 1216.0968017578125\n",
      "Epoch [138/200] Loss: 1211.7261962890625\n",
      "Epoch [139/200] Loss: 1207.3702392578125\n",
      "Epoch [140/200] Loss: 1203.0279541015625\n",
      "Epoch [141/200] Loss: 1198.7001953125\n",
      "Epoch [142/200] Loss: 1194.3865966796875\n",
      "Epoch [143/200] Loss: 1190.0869140625\n",
      "Epoch [144/200] Loss: 1185.801025390625\n",
      "Epoch [145/200] Loss: 1181.529541015625\n",
      "Epoch [146/200] Loss: 1177.2716064453125\n",
      "Epoch [147/200] Loss: 1173.0274658203125\n",
      "Epoch [148/200] Loss: 1168.7972412109375\n",
      "Epoch [149/200] Loss: 1164.58056640625\n",
      "Epoch [150/200] Loss: 1160.377197265625\n",
      "Epoch [151/200] Loss: 1156.1875\n",
      "Epoch [152/200] Loss: 1152.01123046875\n",
      "Epoch [153/200] Loss: 1147.848388671875\n",
      "Epoch [154/200] Loss: 1143.6988525390625\n",
      "Epoch [155/200] Loss: 1139.5626220703125\n",
      "Epoch [156/200] Loss: 1135.4395751953125\n",
      "Epoch [157/200] Loss: 1131.330078125\n",
      "Epoch [158/200] Loss: 1127.2330322265625\n",
      "Epoch [159/200] Loss: 1123.1492919921875\n",
      "Epoch [160/200] Loss: 1119.07861328125\n",
      "Epoch [161/200] Loss: 1115.0208740234375\n",
      "Epoch [162/200] Loss: 1110.9759521484375\n",
      "Epoch [163/200] Loss: 1106.9437255859375\n",
      "Epoch [164/200] Loss: 1102.924560546875\n",
      "Epoch [165/200] Loss: 1098.91796875\n",
      "Epoch [166/200] Loss: 1094.9239501953125\n",
      "Epoch [167/200] Loss: 1090.9425048828125\n",
      "Epoch [168/200] Loss: 1086.9737548828125\n",
      "Epoch [169/200] Loss: 1083.0174560546875\n",
      "Epoch [170/200] Loss: 1079.0738525390625\n",
      "Epoch [171/200] Loss: 1075.1424560546875\n",
      "Epoch [172/200] Loss: 1071.223388671875\n",
      "Epoch [173/200] Loss: 1067.3167724609375\n",
      "Epoch [174/200] Loss: 1063.4222412109375\n",
      "Epoch [175/200] Loss: 1059.540283203125\n",
      "Epoch [176/200] Loss: 1055.6700439453125\n",
      "Epoch [177/200] Loss: 1051.8125\n",
      "Epoch [178/200] Loss: 1047.9666748046875\n",
      "Epoch [179/200] Loss: 1044.1329345703125\n",
      "Epoch [180/200] Loss: 1040.3114013671875\n",
      "Epoch [181/200] Loss: 1036.5015869140625\n",
      "Epoch [182/200] Loss: 1032.703857421875\n",
      "Epoch [183/200] Loss: 1028.91796875\n",
      "Epoch [184/200] Loss: 1025.1439208984375\n",
      "Epoch [185/200] Loss: 1021.3816528320312\n",
      "Epoch [186/200] Loss: 1017.6311645507812\n",
      "Epoch [187/200] Loss: 1013.8922729492188\n",
      "Epoch [188/200] Loss: 1010.1653442382812\n",
      "Epoch [189/200] Loss: 1006.4498291015625\n",
      "Epoch [190/200] Loss: 1002.7462158203125\n",
      "Epoch [191/200] Loss: 999.0537719726562\n",
      "Epoch [192/200] Loss: 995.3729858398438\n",
      "Epoch [193/200] Loss: 991.7037353515625\n",
      "Epoch [194/200] Loss: 988.0458374023438\n",
      "Epoch [195/200] Loss: 984.3994140625\n",
      "Epoch [196/200] Loss: 980.7645263671875\n",
      "Epoch [197/200] Loss: 977.1409301757812\n",
      "Epoch [198/200] Loss: 973.5283813476562\n",
      "Epoch [199/200] Loss: 969.9271850585938\n",
      "Epoch [200/200] Loss: 966.33740234375\n",
      "Predicted days_remaining for parent_id 367: 15.955177307128906\n",
      "Training for parent_id 371...\n",
      "Epoch [1/200] Loss: 294.1785583496094\n",
      "Epoch [2/200] Loss: 287.3902587890625\n",
      "Epoch [3/200] Loss: 280.7635498046875\n",
      "Epoch [4/200] Loss: 274.32666015625\n",
      "Epoch [5/200] Loss: 268.0915222167969\n",
      "Epoch [6/200] Loss: 262.0681457519531\n",
      "Epoch [7/200] Loss: 256.2597961425781\n",
      "Epoch [8/200] Loss: 250.65936279296875\n",
      "Epoch [9/200] Loss: 245.25381469726562\n",
      "Epoch [10/200] Loss: 240.0296173095703\n",
      "Epoch [11/200] Loss: 234.97628784179688\n",
      "Epoch [12/200] Loss: 230.0897979736328\n",
      "Epoch [13/200] Loss: 225.374755859375\n",
      "Epoch [14/200] Loss: 220.84262084960938\n",
      "Epoch [15/200] Loss: 216.50794982910156\n",
      "Epoch [16/200] Loss: 212.3839874267578\n",
      "Epoch [17/200] Loss: 208.4783172607422\n",
      "Epoch [18/200] Loss: 204.79074096679688\n",
      "Epoch [19/200] Loss: 201.312744140625\n",
      "Epoch [20/200] Loss: 198.0294189453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200] Loss: 194.92221069335938\n",
      "Epoch [22/200] Loss: 191.97201538085938\n",
      "Epoch [23/200] Loss: 189.1610870361328\n",
      "Epoch [24/200] Loss: 186.47369384765625\n",
      "Epoch [25/200] Loss: 183.8959197998047\n",
      "Epoch [26/200] Loss: 181.4156494140625\n",
      "Epoch [27/200] Loss: 179.02198791503906\n",
      "Epoch [28/200] Loss: 176.70501708984375\n",
      "Epoch [29/200] Loss: 174.455810546875\n",
      "Epoch [30/200] Loss: 172.26614379882812\n",
      "Epoch [31/200] Loss: 170.1288604736328\n",
      "Epoch [32/200] Loss: 168.03726196289062\n",
      "Epoch [33/200] Loss: 165.98587036132812\n",
      "Epoch [34/200] Loss: 163.969970703125\n",
      "Epoch [35/200] Loss: 161.9858856201172\n",
      "Epoch [36/200] Loss: 160.03077697753906\n",
      "Epoch [37/200] Loss: 158.1028289794922\n",
      "Epoch [38/200] Loss: 156.200927734375\n",
      "Epoch [39/200] Loss: 154.3244171142578\n",
      "Epoch [40/200] Loss: 152.47323608398438\n",
      "Epoch [41/200] Loss: 150.6475067138672\n",
      "Epoch [42/200] Loss: 148.8472137451172\n",
      "Epoch [43/200] Loss: 147.0723419189453\n",
      "Epoch [44/200] Loss: 145.3226318359375\n",
      "Epoch [45/200] Loss: 143.59742736816406\n",
      "Epoch [46/200] Loss: 141.8959503173828\n",
      "Epoch [47/200] Loss: 140.21717834472656\n",
      "Epoch [48/200] Loss: 138.56016540527344\n",
      "Epoch [49/200] Loss: 136.92381286621094\n",
      "Epoch [50/200] Loss: 135.30726623535156\n",
      "Epoch [51/200] Loss: 133.709716796875\n",
      "Epoch [52/200] Loss: 132.13058471679688\n",
      "Epoch [53/200] Loss: 130.56927490234375\n",
      "Epoch [54/200] Loss: 129.02540588378906\n",
      "Epoch [55/200] Loss: 127.49864959716797\n",
      "Epoch [56/200] Loss: 125.98863220214844\n",
      "Epoch [57/200] Loss: 124.4952163696289\n",
      "Epoch [58/200] Loss: 123.01812744140625\n",
      "Epoch [59/200] Loss: 121.55716705322266\n",
      "Epoch [60/200] Loss: 120.11219024658203\n",
      "Epoch [61/200] Loss: 118.68306732177734\n",
      "Epoch [62/200] Loss: 117.26966094970703\n",
      "Epoch [63/200] Loss: 115.87189483642578\n",
      "Epoch [64/200] Loss: 114.48969268798828\n",
      "Epoch [65/200] Loss: 113.1230697631836\n",
      "Epoch [66/200] Loss: 111.77191162109375\n",
      "Epoch [67/200] Loss: 110.43624114990234\n",
      "Epoch [68/200] Loss: 109.11605072021484\n",
      "Epoch [69/200] Loss: 107.8112564086914\n",
      "Epoch [70/200] Loss: 106.52189636230469\n",
      "Epoch [71/200] Loss: 105.24790954589844\n",
      "Epoch [72/200] Loss: 103.98924255371094\n",
      "Epoch [73/200] Loss: 102.74583435058594\n",
      "Epoch [74/200] Loss: 101.51753997802734\n",
      "Epoch [75/200] Loss: 100.30435180664062\n",
      "Epoch [76/200] Loss: 99.10609436035156\n",
      "Epoch [77/200] Loss: 97.92268371582031\n",
      "Epoch [78/200] Loss: 96.75395965576172\n",
      "Epoch [79/200] Loss: 95.59976959228516\n",
      "Epoch [80/200] Loss: 94.45996856689453\n",
      "Epoch [81/200] Loss: 93.33445739746094\n",
      "Epoch [82/200] Loss: 92.2230224609375\n",
      "Epoch [83/200] Loss: 91.1255111694336\n",
      "Epoch [84/200] Loss: 90.0417709350586\n",
      "Epoch [85/200] Loss: 88.9716796875\n",
      "Epoch [86/200] Loss: 87.91500854492188\n",
      "Epoch [87/200] Loss: 86.87163543701172\n",
      "Epoch [88/200] Loss: 85.84139251708984\n",
      "Epoch [89/200] Loss: 84.82412719726562\n",
      "Epoch [90/200] Loss: 83.8196792602539\n",
      "Epoch [91/200] Loss: 82.827880859375\n",
      "Epoch [92/200] Loss: 81.84859466552734\n",
      "Epoch [93/200] Loss: 80.88166046142578\n",
      "Epoch [94/200] Loss: 79.92691802978516\n",
      "Epoch [95/200] Loss: 78.9842529296875\n",
      "Epoch [96/200] Loss: 78.05349731445312\n",
      "Epoch [97/200] Loss: 77.13448333740234\n",
      "Epoch [98/200] Loss: 76.22712707519531\n",
      "Epoch [99/200] Loss: 75.33123016357422\n",
      "Epoch [100/200] Loss: 74.44666290283203\n",
      "Epoch [101/200] Loss: 73.5733413696289\n",
      "Epoch [102/200] Loss: 72.71107482910156\n",
      "Epoch [103/200] Loss: 71.85974884033203\n",
      "Epoch [104/200] Loss: 71.01923370361328\n",
      "Epoch [105/200] Loss: 70.18941497802734\n",
      "Epoch [106/200] Loss: 69.37016296386719\n",
      "Epoch [107/200] Loss: 68.56130981445312\n",
      "Epoch [108/200] Loss: 67.76282501220703\n",
      "Epoch [109/200] Loss: 66.97449493408203\n",
      "Epoch [110/200] Loss: 66.19625091552734\n",
      "Epoch [111/200] Loss: 65.42793273925781\n",
      "Epoch [112/200] Loss: 64.66947174072266\n",
      "Epoch [113/200] Loss: 63.920753479003906\n",
      "Epoch [114/200] Loss: 63.18164825439453\n",
      "Epoch [115/200] Loss: 62.4520149230957\n",
      "Epoch [116/200] Loss: 61.731807708740234\n",
      "Epoch [117/200] Loss: 61.02089309692383\n",
      "Epoch [118/200] Loss: 60.319149017333984\n",
      "Epoch [119/200] Loss: 59.6264762878418\n",
      "Epoch [120/200] Loss: 58.94280242919922\n",
      "Epoch [121/200] Loss: 58.26797103881836\n",
      "Epoch [122/200] Loss: 57.60192108154297\n",
      "Epoch [123/200] Loss: 56.94454574584961\n",
      "Epoch [124/200] Loss: 56.2957649230957\n",
      "Epoch [125/200] Loss: 55.65544509887695\n",
      "Epoch [126/200] Loss: 55.02351379394531\n",
      "Epoch [127/200] Loss: 54.39987564086914\n",
      "Epoch [128/200] Loss: 53.78443145751953\n",
      "Epoch [129/200] Loss: 53.177066802978516\n",
      "Epoch [130/200] Loss: 52.577762603759766\n",
      "Epoch [131/200] Loss: 51.986358642578125\n",
      "Epoch [132/200] Loss: 51.40279769897461\n",
      "Epoch [133/200] Loss: 50.82697296142578\n",
      "Epoch [134/200] Loss: 50.258819580078125\n",
      "Epoch [135/200] Loss: 49.698246002197266\n",
      "Epoch [136/200] Loss: 49.14515686035156\n",
      "Epoch [137/200] Loss: 48.599491119384766\n",
      "Epoch [138/200] Loss: 48.06113052368164\n",
      "Epoch [139/200] Loss: 47.530025482177734\n",
      "Epoch [140/200] Loss: 47.00606155395508\n",
      "Epoch [141/200] Loss: 46.48920440673828\n",
      "Epoch [142/200] Loss: 45.97934341430664\n",
      "Epoch [143/200] Loss: 45.47642517089844\n",
      "Epoch [144/200] Loss: 44.98031997680664\n",
      "Epoch [145/200] Loss: 44.49100112915039\n",
      "Epoch [146/200] Loss: 44.00837326049805\n",
      "Epoch [147/200] Loss: 43.53238296508789\n",
      "Epoch [148/200] Loss: 43.062923431396484\n",
      "Epoch [149/200] Loss: 42.59992218017578\n",
      "Epoch [150/200] Loss: 42.14333724975586\n",
      "Epoch [151/200] Loss: 41.69306182861328\n",
      "Epoch [152/200] Loss: 41.249061584472656\n",
      "Epoch [153/200] Loss: 40.81123352050781\n",
      "Epoch [154/200] Loss: 40.37952423095703\n",
      "Epoch [155/200] Loss: 39.95384979248047\n",
      "Epoch [156/200] Loss: 39.534149169921875\n",
      "Epoch [157/200] Loss: 39.12034606933594\n",
      "Epoch [158/200] Loss: 38.712379455566406\n",
      "Epoch [159/200] Loss: 38.31020736694336\n",
      "Epoch [160/200] Loss: 37.913726806640625\n",
      "Epoch [161/200] Loss: 37.52288818359375\n",
      "Epoch [162/200] Loss: 37.13760757446289\n",
      "Epoch [163/200] Loss: 36.75785827636719\n",
      "Epoch [164/200] Loss: 36.38353729248047\n",
      "Epoch [165/200] Loss: 36.01459884643555\n",
      "Epoch [166/200] Loss: 35.65098571777344\n",
      "Epoch [167/200] Loss: 35.29264450073242\n",
      "Epoch [168/200] Loss: 34.93946075439453\n",
      "Epoch [169/200] Loss: 34.59143829345703\n",
      "Epoch [170/200] Loss: 34.24849319458008\n",
      "Epoch [171/200] Loss: 33.910552978515625\n",
      "Epoch [172/200] Loss: 33.57756423950195\n",
      "Epoch [173/200] Loss: 33.24948501586914\n",
      "Epoch [174/200] Loss: 32.926231384277344\n",
      "Epoch [175/200] Loss: 32.60775375366211\n",
      "Epoch [176/200] Loss: 32.29399490356445\n",
      "Epoch [177/200] Loss: 31.984901428222656\n",
      "Epoch [178/200] Loss: 31.6804256439209\n",
      "Epoch [179/200] Loss: 31.380504608154297\n",
      "Epoch [180/200] Loss: 31.085073471069336\n",
      "Epoch [181/200] Loss: 30.794090270996094\n",
      "Epoch [182/200] Loss: 30.507484436035156\n",
      "Epoch [183/200] Loss: 30.2252197265625\n",
      "Epoch [184/200] Loss: 29.947227478027344\n",
      "Epoch [185/200] Loss: 29.673471450805664\n",
      "Epoch [186/200] Loss: 29.40389633178711\n",
      "Epoch [187/200] Loss: 29.13843536376953\n",
      "Epoch [188/200] Loss: 28.877059936523438\n",
      "Epoch [189/200] Loss: 28.619701385498047\n",
      "Epoch [190/200] Loss: 28.366310119628906\n",
      "Epoch [191/200] Loss: 28.116844177246094\n",
      "Epoch [192/200] Loss: 27.87125587463379\n",
      "Epoch [193/200] Loss: 27.629467010498047\n",
      "Epoch [194/200] Loss: 27.391468048095703\n",
      "Epoch [195/200] Loss: 27.157209396362305\n",
      "Epoch [196/200] Loss: 26.926603317260742\n",
      "Epoch [197/200] Loss: 26.69965171813965\n",
      "Epoch [198/200] Loss: 26.47627067565918\n",
      "Epoch [199/200] Loss: 26.256431579589844\n",
      "Epoch [200/200] Loss: 26.040077209472656\n",
      "Predicted days_remaining for parent_id 371: 13.375140190124512\n",
      "Training for parent_id 373...\n",
      "Epoch [1/200] Loss: 166.81369018554688\n",
      "Epoch [2/200] Loss: 159.92367553710938\n",
      "Epoch [3/200] Loss: 153.43405151367188\n",
      "Epoch [4/200] Loss: 147.39627075195312\n",
      "Epoch [5/200] Loss: 141.8143310546875\n",
      "Epoch [6/200] Loss: 136.65716552734375\n",
      "Epoch [7/200] Loss: 131.88690185546875\n",
      "Epoch [8/200] Loss: 127.46650695800781\n",
      "Epoch [9/200] Loss: 123.36121368408203\n",
      "Epoch [10/200] Loss: 119.53841400146484\n",
      "Epoch [11/200] Loss: 115.96875\n",
      "Epoch [12/200] Loss: 112.62754821777344\n",
      "Epoch [13/200] Loss: 109.4942855834961\n",
      "Epoch [14/200] Loss: 106.5513687133789\n",
      "Epoch [15/200] Loss: 103.78295135498047\n",
      "Epoch [16/200] Loss: 101.17462921142578\n",
      "Epoch [17/200] Loss: 98.71361541748047\n",
      "Epoch [18/200] Loss: 96.38851928710938\n",
      "Epoch [19/200] Loss: 94.1891860961914\n",
      "Epoch [20/200] Loss: 92.1063461303711\n",
      "Epoch [21/200] Loss: 90.1314926147461\n",
      "Epoch [22/200] Loss: 88.25665283203125\n",
      "Epoch [23/200] Loss: 86.47435760498047\n",
      "Epoch [24/200] Loss: 84.7774887084961\n",
      "Epoch [25/200] Loss: 83.15924835205078\n",
      "Epoch [26/200] Loss: 81.61319732666016\n",
      "Epoch [27/200] Loss: 80.13316345214844\n",
      "Epoch [28/200] Loss: 78.71340942382812\n",
      "Epoch [29/200] Loss: 77.34846496582031\n",
      "Epoch [30/200] Loss: 76.03338623046875\n",
      "Epoch [31/200] Loss: 74.76358795166016\n",
      "Epoch [32/200] Loss: 73.53496551513672\n",
      "Epoch [33/200] Loss: 72.34381103515625\n",
      "Epoch [34/200] Loss: 71.18685913085938\n",
      "Epoch [35/200] Loss: 70.06116485595703\n",
      "Epoch [36/200] Loss: 68.9642562866211\n",
      "Epoch [37/200] Loss: 67.89384460449219\n",
      "Epoch [38/200] Loss: 66.84796142578125\n",
      "Epoch [39/200] Loss: 65.82490539550781\n",
      "Epoch [40/200] Loss: 64.82315063476562\n",
      "Epoch [41/200] Loss: 63.84136199951172\n",
      "Epoch [42/200] Loss: 62.87836456298828\n",
      "Epoch [43/200] Loss: 61.9330940246582\n",
      "Epoch [44/200] Loss: 61.0046272277832\n",
      "Epoch [45/200] Loss: 60.092124938964844\n",
      "Epoch [46/200] Loss: 59.194820404052734\n",
      "Epoch [47/200] Loss: 58.31207275390625\n",
      "Epoch [48/200] Loss: 57.44329833984375\n",
      "Epoch [49/200] Loss: 56.588008880615234\n",
      "Epoch [50/200] Loss: 55.74580764770508\n",
      "Epoch [51/200] Loss: 54.916378021240234\n",
      "Epoch [52/200] Loss: 54.09946060180664\n",
      "Epoch [53/200] Loss: 53.29490280151367\n",
      "Epoch [54/200] Loss: 52.5025520324707\n",
      "Epoch [55/200] Loss: 51.722354888916016\n",
      "Epoch [56/200] Loss: 50.954280853271484\n",
      "Epoch [57/200] Loss: 50.19823455810547\n",
      "Epoch [58/200] Loss: 49.4542121887207\n",
      "Epoch [59/200] Loss: 48.72218322753906\n",
      "Epoch [60/200] Loss: 48.0020866394043\n",
      "Epoch [61/200] Loss: 47.293853759765625\n",
      "Epoch [62/200] Loss: 46.59746551513672\n",
      "Epoch [63/200] Loss: 45.91281509399414\n",
      "Epoch [64/200] Loss: 45.239864349365234\n",
      "Epoch [65/200] Loss: 44.57846450805664\n",
      "Epoch [66/200] Loss: 43.92859649658203\n",
      "Epoch [67/200] Loss: 43.29010009765625\n",
      "Epoch [68/200] Loss: 42.662906646728516\n",
      "Epoch [69/200] Loss: 42.04690170288086\n",
      "Epoch [70/200] Loss: 41.44196319580078\n",
      "Epoch [71/200] Loss: 40.84798049926758\n",
      "Epoch [72/200] Loss: 40.26482391357422\n",
      "Epoch [73/200] Loss: 39.69236373901367\n",
      "Epoch [74/200] Loss: 39.1304931640625\n",
      "Epoch [75/200] Loss: 38.57904052734375\n",
      "Epoch [76/200] Loss: 38.03790283203125\n",
      "Epoch [77/200] Loss: 37.50693130493164\n",
      "Epoch [78/200] Loss: 36.986026763916016\n",
      "Epoch [79/200] Loss: 36.474979400634766\n",
      "Epoch [80/200] Loss: 35.97371292114258\n",
      "Epoch [81/200] Loss: 35.4820556640625\n",
      "Epoch [82/200] Loss: 34.999881744384766\n",
      "Epoch [83/200] Loss: 34.52705001831055\n",
      "Epoch [84/200] Loss: 34.06341552734375\n",
      "Epoch [85/200] Loss: 33.60883712768555\n",
      "Epoch [86/200] Loss: 33.16320037841797\n",
      "Epoch [87/200] Loss: 32.726341247558594\n",
      "Epoch [88/200] Loss: 32.29813003540039\n",
      "Epoch [89/200] Loss: 31.87843894958496\n",
      "Epoch [90/200] Loss: 31.467134475708008\n",
      "Epoch [91/200] Loss: 31.064067840576172\n",
      "Epoch [92/200] Loss: 30.66913604736328\n",
      "Epoch [93/200] Loss: 30.282169342041016\n",
      "Epoch [94/200] Loss: 29.903066635131836\n",
      "Epoch [95/200] Loss: 29.531692504882812\n",
      "Epoch [96/200] Loss: 29.167922973632812\n",
      "Epoch [97/200] Loss: 28.811620712280273\n",
      "Epoch [98/200] Loss: 28.46267318725586\n",
      "Epoch [99/200] Loss: 28.12096405029297\n",
      "Epoch [100/200] Loss: 27.786357879638672\n",
      "Epoch [101/200] Loss: 27.458749771118164\n",
      "Epoch [102/200] Loss: 27.13800621032715\n",
      "Epoch [103/200] Loss: 26.824024200439453\n",
      "Epoch [104/200] Loss: 26.516672134399414\n",
      "Epoch [105/200] Loss: 26.215850830078125\n",
      "Epoch [106/200] Loss: 25.921449661254883\n",
      "Epoch [107/200] Loss: 25.633346557617188\n",
      "Epoch [108/200] Loss: 25.35143280029297\n",
      "Epoch [109/200] Loss: 25.075613021850586\n",
      "Epoch [110/200] Loss: 24.80577278137207\n",
      "Epoch [111/200] Loss: 24.54180335998535\n",
      "Epoch [112/200] Loss: 24.283607482910156\n",
      "Epoch [113/200] Loss: 24.03107452392578\n",
      "Epoch [114/200] Loss: 23.784103393554688\n",
      "Epoch [115/200] Loss: 23.542604446411133\n",
      "Epoch [116/200] Loss: 23.306482315063477\n",
      "Epoch [117/200] Loss: 23.075620651245117\n",
      "Epoch [118/200] Loss: 22.849946975708008\n",
      "Epoch [119/200] Loss: 22.629337310791016\n",
      "Epoch [120/200] Loss: 22.413738250732422\n",
      "Epoch [121/200] Loss: 22.20302963256836\n",
      "Epoch [122/200] Loss: 21.997121810913086\n",
      "Epoch [123/200] Loss: 21.79592514038086\n",
      "Epoch [124/200] Loss: 21.599367141723633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [125/200] Loss: 21.407344818115234\n",
      "Epoch [126/200] Loss: 21.21977996826172\n",
      "Epoch [127/200] Loss: 21.03658676147461\n",
      "Epoch [128/200] Loss: 20.857677459716797\n",
      "Epoch [129/200] Loss: 20.682979583740234\n",
      "Epoch [130/200] Loss: 20.512405395507812\n",
      "Epoch [131/200] Loss: 20.345869064331055\n",
      "Epoch [132/200] Loss: 20.183303833007812\n",
      "Epoch [133/200] Loss: 20.024627685546875\n",
      "Epoch [134/200] Loss: 19.869770050048828\n",
      "Epoch [135/200] Loss: 19.718643188476562\n",
      "Epoch [136/200] Loss: 19.571170806884766\n",
      "Epoch [137/200] Loss: 19.427305221557617\n",
      "Epoch [138/200] Loss: 19.286941528320312\n",
      "Epoch [139/200] Loss: 19.1500301361084\n",
      "Epoch [140/200] Loss: 19.016494750976562\n",
      "Epoch [141/200] Loss: 18.88626480102539\n",
      "Epoch [142/200] Loss: 18.759286880493164\n",
      "Epoch [143/200] Loss: 18.63547134399414\n",
      "Epoch [144/200] Loss: 18.51476287841797\n",
      "Epoch [145/200] Loss: 18.397098541259766\n",
      "Epoch [146/200] Loss: 18.28240966796875\n",
      "Epoch [147/200] Loss: 18.170637130737305\n",
      "Epoch [148/200] Loss: 18.061725616455078\n",
      "Epoch [149/200] Loss: 17.955596923828125\n",
      "Epoch [150/200] Loss: 17.852210998535156\n",
      "Epoch [151/200] Loss: 17.751495361328125\n",
      "Epoch [152/200] Loss: 17.653404235839844\n",
      "Epoch [153/200] Loss: 17.5578556060791\n",
      "Epoch [154/200] Loss: 17.464824676513672\n",
      "Epoch [155/200] Loss: 17.374242782592773\n",
      "Epoch [156/200] Loss: 17.286052703857422\n",
      "Epoch [157/200] Loss: 17.200199127197266\n",
      "Epoch [158/200] Loss: 17.11663818359375\n",
      "Epoch [159/200] Loss: 17.035316467285156\n",
      "Epoch [160/200] Loss: 16.956180572509766\n",
      "Epoch [161/200] Loss: 16.879180908203125\n",
      "Epoch [162/200] Loss: 16.804269790649414\n",
      "Epoch [163/200] Loss: 16.731403350830078\n",
      "Epoch [164/200] Loss: 16.6605224609375\n",
      "Epoch [165/200] Loss: 16.591590881347656\n",
      "Epoch [166/200] Loss: 16.524559020996094\n",
      "Epoch [167/200] Loss: 16.45939064025879\n",
      "Epoch [168/200] Loss: 16.396032333374023\n",
      "Epoch [169/200] Loss: 16.334440231323242\n",
      "Epoch [170/200] Loss: 16.274580001831055\n",
      "Epoch [171/200] Loss: 16.216405868530273\n",
      "Epoch [172/200] Loss: 16.159875869750977\n",
      "Epoch [173/200] Loss: 16.10495376586914\n",
      "Epoch [174/200] Loss: 16.051597595214844\n",
      "Epoch [175/200] Loss: 15.99976921081543\n",
      "Epoch [176/200] Loss: 15.949434280395508\n",
      "Epoch [177/200] Loss: 15.900545120239258\n",
      "Epoch [178/200] Loss: 15.853084564208984\n",
      "Epoch [179/200] Loss: 15.806999206542969\n",
      "Epoch [180/200] Loss: 15.762262344360352\n",
      "Epoch [181/200] Loss: 15.718835830688477\n",
      "Epoch [182/200] Loss: 15.676692962646484\n",
      "Epoch [183/200] Loss: 15.635798454284668\n",
      "Epoch [184/200] Loss: 15.596113204956055\n",
      "Epoch [185/200] Loss: 15.557618141174316\n",
      "Epoch [186/200] Loss: 15.520273208618164\n",
      "Epoch [187/200] Loss: 15.484054565429688\n",
      "Epoch [188/200] Loss: 15.44892692565918\n",
      "Epoch [189/200] Loss: 15.414865493774414\n",
      "Epoch [190/200] Loss: 15.381841659545898\n",
      "Epoch [191/200] Loss: 15.349822998046875\n",
      "Epoch [192/200] Loss: 15.318788528442383\n",
      "Epoch [193/200] Loss: 15.288715362548828\n",
      "Epoch [194/200] Loss: 15.259565353393555\n",
      "Epoch [195/200] Loss: 15.23132038116455\n",
      "Epoch [196/200] Loss: 15.203958511352539\n",
      "Epoch [197/200] Loss: 15.17745304107666\n",
      "Epoch [198/200] Loss: 15.151777267456055\n",
      "Epoch [199/200] Loss: 15.126914024353027\n",
      "Epoch [200/200] Loss: 15.102834701538086\n",
      "Predicted days_remaining for parent_id 373: 11.948737144470215\n",
      "Training for parent_id 376...\n",
      "Epoch [1/200] Loss: 787.3042602539062\n",
      "Epoch [2/200] Loss: 773.4066162109375\n",
      "Epoch [3/200] Loss: 759.83984375\n",
      "Epoch [4/200] Loss: 746.637939453125\n",
      "Epoch [5/200] Loss: 733.814453125\n",
      "Epoch [6/200] Loss: 721.3817138671875\n",
      "Epoch [7/200] Loss: 709.3570556640625\n",
      "Epoch [8/200] Loss: 697.7489013671875\n",
      "Epoch [9/200] Loss: 686.5540161132812\n",
      "Epoch [10/200] Loss: 675.7631225585938\n",
      "Epoch [11/200] Loss: 665.3678588867188\n",
      "Epoch [12/200] Loss: 655.3643188476562\n",
      "Epoch [13/200] Loss: 645.751953125\n",
      "Epoch [14/200] Loss: 636.5318603515625\n",
      "Epoch [15/200] Loss: 627.7025146484375\n",
      "Epoch [16/200] Loss: 619.2581176757812\n",
      "Epoch [17/200] Loss: 611.1869506835938\n",
      "Epoch [18/200] Loss: 603.471923828125\n",
      "Epoch [19/200] Loss: 596.0916748046875\n",
      "Epoch [20/200] Loss: 589.0225830078125\n",
      "Epoch [21/200] Loss: 582.2415771484375\n",
      "Epoch [22/200] Loss: 575.7274780273438\n",
      "Epoch [23/200] Loss: 569.4617919921875\n",
      "Epoch [24/200] Loss: 563.4295654296875\n",
      "Epoch [25/200] Loss: 557.6176147460938\n",
      "Epoch [26/200] Loss: 552.014404296875\n",
      "Epoch [27/200] Loss: 546.6090698242188\n",
      "Epoch [28/200] Loss: 541.390869140625\n",
      "Epoch [29/200] Loss: 536.3488159179688\n",
      "Epoch [30/200] Loss: 531.472412109375\n",
      "Epoch [31/200] Loss: 526.7506103515625\n",
      "Epoch [32/200] Loss: 522.1728515625\n",
      "Epoch [33/200] Loss: 517.7284545898438\n",
      "Epoch [34/200] Loss: 513.4072875976562\n",
      "Epoch [35/200] Loss: 509.1995849609375\n",
      "Epoch [36/200] Loss: 505.0960388183594\n",
      "Epoch [37/200] Loss: 501.08807373046875\n",
      "Epoch [38/200] Loss: 497.167724609375\n",
      "Epoch [39/200] Loss: 493.32806396484375\n",
      "Epoch [40/200] Loss: 489.5623474121094\n",
      "Epoch [41/200] Loss: 485.865234375\n",
      "Epoch [42/200] Loss: 482.23162841796875\n",
      "Epoch [43/200] Loss: 478.6575012207031\n",
      "Epoch [44/200] Loss: 475.1388854980469\n",
      "Epoch [45/200] Loss: 471.6727600097656\n",
      "Epoch [46/200] Loss: 468.25634765625\n",
      "Epoch [47/200] Loss: 464.8870544433594\n",
      "Epoch [48/200] Loss: 461.56280517578125\n",
      "Epoch [49/200] Loss: 458.28143310546875\n",
      "Epoch [50/200] Loss: 455.04095458984375\n",
      "Epoch [51/200] Loss: 451.8395080566406\n",
      "Epoch [52/200] Loss: 448.6750793457031\n",
      "Epoch [53/200] Loss: 445.5460205078125\n",
      "Epoch [54/200] Loss: 442.4502258300781\n",
      "Epoch [55/200] Loss: 439.38629150390625\n",
      "Epoch [56/200] Loss: 436.35247802734375\n",
      "Epoch [57/200] Loss: 433.34716796875\n",
      "Epoch [58/200] Loss: 430.3691711425781\n",
      "Epoch [59/200] Loss: 427.417236328125\n",
      "Epoch [60/200] Loss: 424.4903564453125\n",
      "Epoch [61/200] Loss: 421.58740234375\n",
      "Epoch [62/200] Loss: 418.70770263671875\n",
      "Epoch [63/200] Loss: 415.850341796875\n",
      "Epoch [64/200] Loss: 413.0149841308594\n",
      "Epoch [65/200] Loss: 410.2007751464844\n",
      "Epoch [66/200] Loss: 407.4071960449219\n",
      "Epoch [67/200] Loss: 404.634033203125\n",
      "Epoch [68/200] Loss: 401.8805847167969\n",
      "Epoch [69/200] Loss: 399.1468200683594\n",
      "Epoch [70/200] Loss: 396.43206787109375\n",
      "Epoch [71/200] Loss: 393.7362976074219\n",
      "Epoch [72/200] Loss: 391.05902099609375\n",
      "Epoch [73/200] Loss: 388.4000549316406\n",
      "Epoch [74/200] Loss: 385.7591552734375\n",
      "Epoch [75/200] Loss: 383.1361999511719\n",
      "Epoch [76/200] Loss: 380.53082275390625\n",
      "Epoch [77/200] Loss: 377.94293212890625\n",
      "Epoch [78/200] Loss: 375.3724060058594\n",
      "Epoch [79/200] Loss: 372.8189697265625\n",
      "Epoch [80/200] Loss: 370.2825927734375\n",
      "Epoch [81/200] Loss: 367.7630310058594\n",
      "Epoch [82/200] Loss: 365.26025390625\n",
      "Epoch [83/200] Loss: 362.7740478515625\n",
      "Epoch [84/200] Loss: 360.3042907714844\n",
      "Epoch [85/200] Loss: 357.85089111328125\n",
      "Epoch [86/200] Loss: 355.41375732421875\n",
      "Epoch [87/200] Loss: 352.99261474609375\n",
      "Epoch [88/200] Loss: 350.58758544921875\n",
      "Epoch [89/200] Loss: 348.19830322265625\n",
      "Epoch [90/200] Loss: 345.8247375488281\n",
      "Epoch [91/200] Loss: 343.466796875\n",
      "Epoch [92/200] Loss: 341.1242370605469\n",
      "Epoch [93/200] Loss: 338.7971496582031\n",
      "Epoch [94/200] Loss: 336.4853210449219\n",
      "Epoch [95/200] Loss: 334.1885070800781\n",
      "Epoch [96/200] Loss: 331.9065856933594\n",
      "Epoch [97/200] Loss: 329.63970947265625\n",
      "Epoch [98/200] Loss: 327.3874816894531\n",
      "Epoch [99/200] Loss: 325.1498107910156\n",
      "Epoch [100/200] Loss: 322.9267272949219\n",
      "Epoch [101/200] Loss: 320.7179260253906\n",
      "Epoch [102/200] Loss: 318.5234680175781\n",
      "Epoch [103/200] Loss: 316.3431701660156\n",
      "Epoch [104/200] Loss: 314.1768798828125\n",
      "Epoch [105/200] Loss: 312.02447509765625\n",
      "Epoch [106/200] Loss: 309.8858337402344\n",
      "Epoch [107/200] Loss: 307.7610168457031\n",
      "Epoch [108/200] Loss: 305.6496887207031\n",
      "Epoch [109/200] Loss: 303.55194091796875\n",
      "Epoch [110/200] Loss: 301.46746826171875\n",
      "Epoch [111/200] Loss: 299.3963928222656\n",
      "Epoch [112/200] Loss: 297.3384704589844\n",
      "Epoch [113/200] Loss: 295.29364013671875\n",
      "Epoch [114/200] Loss: 293.2616882324219\n",
      "Epoch [115/200] Loss: 291.24267578125\n",
      "Epoch [116/200] Loss: 289.23651123046875\n",
      "Epoch [117/200] Loss: 287.2430419921875\n",
      "Epoch [118/200] Loss: 285.2621154785156\n",
      "Epoch [119/200] Loss: 283.29376220703125\n",
      "Epoch [120/200] Loss: 281.3378601074219\n",
      "Epoch [121/200] Loss: 279.3942565917969\n",
      "Epoch [122/200] Loss: 277.4629821777344\n",
      "Epoch [123/200] Loss: 275.54376220703125\n",
      "Epoch [124/200] Loss: 273.6367492675781\n",
      "Epoch [125/200] Loss: 271.7417297363281\n",
      "Epoch [126/200] Loss: 269.85858154296875\n",
      "Epoch [127/200] Loss: 267.9873352050781\n",
      "Epoch [128/200] Loss: 266.1278991699219\n",
      "Epoch [129/200] Loss: 264.28009033203125\n",
      "Epoch [130/200] Loss: 262.44390869140625\n",
      "Epoch [131/200] Loss: 260.6192626953125\n",
      "Epoch [132/200] Loss: 258.80609130859375\n",
      "Epoch [133/200] Loss: 257.00433349609375\n",
      "Epoch [134/200] Loss: 255.2139129638672\n",
      "Epoch [135/200] Loss: 253.43467712402344\n",
      "Epoch [136/200] Loss: 251.66664123535156\n",
      "Epoch [137/200] Loss: 249.90980529785156\n",
      "Epoch [138/200] Loss: 248.1639862060547\n",
      "Epoch [139/200] Loss: 246.42906188964844\n",
      "Epoch [140/200] Loss: 244.70513916015625\n",
      "Epoch [141/200] Loss: 242.99200439453125\n",
      "Epoch [142/200] Loss: 241.2896270751953\n",
      "Epoch [143/200] Loss: 239.59799194335938\n",
      "Epoch [144/200] Loss: 237.91697692871094\n",
      "Epoch [145/200] Loss: 236.24661254882812\n",
      "Epoch [146/200] Loss: 234.58673095703125\n",
      "Epoch [147/200] Loss: 232.93728637695312\n",
      "Epoch [148/200] Loss: 231.29824829101562\n",
      "Epoch [149/200] Loss: 229.6696319580078\n",
      "Epoch [150/200] Loss: 228.0512237548828\n",
      "Epoch [151/200] Loss: 226.44300842285156\n",
      "Epoch [152/200] Loss: 224.84495544433594\n",
      "Epoch [153/200] Loss: 223.2570343017578\n",
      "Epoch [154/200] Loss: 221.67918395996094\n",
      "Epoch [155/200] Loss: 220.11129760742188\n",
      "Epoch [156/200] Loss: 218.5533447265625\n",
      "Epoch [157/200] Loss: 217.0052947998047\n",
      "Epoch [158/200] Loss: 215.46697998046875\n",
      "Epoch [159/200] Loss: 213.93850708007812\n",
      "Epoch [160/200] Loss: 212.41976928710938\n",
      "Epoch [161/200] Loss: 210.91062927246094\n",
      "Epoch [162/200] Loss: 209.41111755371094\n",
      "Epoch [163/200] Loss: 207.921142578125\n",
      "Epoch [164/200] Loss: 206.440673828125\n",
      "Epoch [165/200] Loss: 204.96963500976562\n",
      "Epoch [166/200] Loss: 203.50799560546875\n",
      "Epoch [167/200] Loss: 202.0557098388672\n",
      "Epoch [168/200] Loss: 200.61277770996094\n",
      "Epoch [169/200] Loss: 199.17898559570312\n",
      "Epoch [170/200] Loss: 197.7544708251953\n",
      "Epoch [171/200] Loss: 196.33901977539062\n",
      "Epoch [172/200] Loss: 194.9326934814453\n",
      "Epoch [173/200] Loss: 193.5354461669922\n",
      "Epoch [174/200] Loss: 192.14710998535156\n",
      "Epoch [175/200] Loss: 190.76779174804688\n",
      "Epoch [176/200] Loss: 189.3973388671875\n",
      "Epoch [177/200] Loss: 188.0357208251953\n",
      "Epoch [178/200] Loss: 186.6829833984375\n",
      "Epoch [179/200] Loss: 185.33892822265625\n",
      "Epoch [180/200] Loss: 184.0036163330078\n",
      "Epoch [181/200] Loss: 182.67694091796875\n",
      "Epoch [182/200] Loss: 181.35891723632812\n",
      "Epoch [183/200] Loss: 180.04940795898438\n",
      "Epoch [184/200] Loss: 178.74844360351562\n",
      "Epoch [185/200] Loss: 177.4560089111328\n",
      "Epoch [186/200] Loss: 176.17198181152344\n",
      "Epoch [187/200] Loss: 174.89633178710938\n",
      "Epoch [188/200] Loss: 173.6290740966797\n",
      "Epoch [189/200] Loss: 172.3700714111328\n",
      "Epoch [190/200] Loss: 171.11932373046875\n",
      "Epoch [191/200] Loss: 169.8768310546875\n",
      "Epoch [192/200] Loss: 168.6425018310547\n",
      "Epoch [193/200] Loss: 167.416259765625\n",
      "Epoch [194/200] Loss: 166.19813537597656\n",
      "Epoch [195/200] Loss: 164.98802185058594\n",
      "Epoch [196/200] Loss: 163.7859649658203\n",
      "Epoch [197/200] Loss: 162.591796875\n",
      "Epoch [198/200] Loss: 161.4055938720703\n",
      "Epoch [199/200] Loss: 160.2272491455078\n",
      "Epoch [200/200] Loss: 159.05674743652344\n",
      "Predicted days_remaining for parent_id 376: 15.772665977478027\n",
      "Training for parent_id 377...\n",
      "Epoch [1/200] Loss: 114.14857482910156\n",
      "Epoch [2/200] Loss: 109.72171020507812\n",
      "Epoch [3/200] Loss: 105.35821533203125\n",
      "Epoch [4/200] Loss: 101.07955169677734\n",
      "Epoch [5/200] Loss: 96.9171371459961\n",
      "Epoch [6/200] Loss: 92.89424896240234\n",
      "Epoch [7/200] Loss: 89.02191925048828\n",
      "Epoch [8/200] Loss: 85.30520629882812\n",
      "Epoch [9/200] Loss: 81.7458267211914\n",
      "Epoch [10/200] Loss: 78.3437271118164\n",
      "Epoch [11/200] Loss: 75.09883117675781\n",
      "Epoch [12/200] Loss: 72.01166534423828\n",
      "Epoch [13/200] Loss: 69.08282470703125\n",
      "Epoch [14/200] Loss: 66.31245422363281\n",
      "Epoch [15/200] Loss: 63.700042724609375\n",
      "Epoch [16/200] Loss: 61.24419021606445\n",
      "Epoch [17/200] Loss: 58.94230651855469\n",
      "Epoch [18/200] Loss: 56.79022979736328\n",
      "Epoch [19/200] Loss: 54.782142639160156\n",
      "Epoch [20/200] Loss: 52.910770416259766\n",
      "Epoch [21/200] Loss: 51.167701721191406\n",
      "Epoch [22/200] Loss: 49.543907165527344\n",
      "Epoch [23/200] Loss: 48.03004837036133\n",
      "Epoch [24/200] Loss: 46.616973876953125\n",
      "Epoch [25/200] Loss: 45.295814514160156\n",
      "Epoch [26/200] Loss: 44.058231353759766\n",
      "Epoch [27/200] Loss: 42.89643859863281\n",
      "Epoch [28/200] Loss: 41.8032341003418\n",
      "Epoch [29/200] Loss: 40.77202606201172\n",
      "Epoch [30/200] Loss: 39.79680252075195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200] Loss: 38.87211227416992\n",
      "Epoch [32/200] Loss: 37.99311065673828\n",
      "Epoch [33/200] Loss: 37.15547561645508\n",
      "Epoch [34/200] Loss: 36.355350494384766\n",
      "Epoch [35/200] Loss: 35.58941650390625\n",
      "Epoch [36/200] Loss: 34.85470199584961\n",
      "Epoch [37/200] Loss: 34.1486930847168\n",
      "Epoch [38/200] Loss: 33.46915054321289\n",
      "Epoch [39/200] Loss: 32.814144134521484\n",
      "Epoch [40/200] Loss: 32.18203353881836\n",
      "Epoch [41/200] Loss: 31.57136344909668\n",
      "Epoch [42/200] Loss: 30.980867385864258\n",
      "Epoch [43/200] Loss: 30.409465789794922\n",
      "Epoch [44/200] Loss: 29.856201171875\n",
      "Epoch [45/200] Loss: 29.320228576660156\n",
      "Epoch [46/200] Loss: 28.800830841064453\n",
      "Epoch [47/200] Loss: 28.297338485717773\n",
      "Epoch [48/200] Loss: 27.809165954589844\n",
      "Epoch [49/200] Loss: 27.33580207824707\n",
      "Epoch [50/200] Loss: 26.876760482788086\n",
      "Epoch [51/200] Loss: 26.431604385375977\n",
      "Epoch [52/200] Loss: 25.99993133544922\n",
      "Epoch [53/200] Loss: 25.58135414123535\n",
      "Epoch [54/200] Loss: 25.175535202026367\n",
      "Epoch [55/200] Loss: 24.78213119506836\n",
      "Epoch [56/200] Loss: 24.400819778442383\n",
      "Epoch [57/200] Loss: 24.031293869018555\n",
      "Epoch [58/200] Loss: 23.673261642456055\n",
      "Epoch [59/200] Loss: 23.326427459716797\n",
      "Epoch [60/200] Loss: 22.990520477294922\n",
      "Epoch [61/200] Loss: 22.66526985168457\n",
      "Epoch [62/200] Loss: 22.35040283203125\n",
      "Epoch [63/200] Loss: 22.045665740966797\n",
      "Epoch [64/200] Loss: 21.75080108642578\n",
      "Epoch [65/200] Loss: 21.465560913085938\n",
      "Epoch [66/200] Loss: 21.1897029876709\n",
      "Epoch [67/200] Loss: 20.9229736328125\n",
      "Epoch [68/200] Loss: 20.665143966674805\n",
      "Epoch [69/200] Loss: 20.41598892211914\n",
      "Epoch [70/200] Loss: 20.175273895263672\n",
      "Epoch [71/200] Loss: 19.94277000427246\n",
      "Epoch [72/200] Loss: 19.71826171875\n",
      "Epoch [73/200] Loss: 19.50153350830078\n",
      "Epoch [74/200] Loss: 19.292369842529297\n",
      "Epoch [75/200] Loss: 19.090559005737305\n",
      "Epoch [76/200] Loss: 18.895902633666992\n",
      "Epoch [77/200] Loss: 18.70819854736328\n",
      "Epoch [78/200] Loss: 18.52724266052246\n",
      "Epoch [79/200] Loss: 18.352848052978516\n",
      "Epoch [80/200] Loss: 18.18482208251953\n",
      "Epoch [81/200] Loss: 18.02297592163086\n",
      "Epoch [82/200] Loss: 17.86713409423828\n",
      "Epoch [83/200] Loss: 17.717105865478516\n",
      "Epoch [84/200] Loss: 17.572729110717773\n",
      "Epoch [85/200] Loss: 17.433826446533203\n",
      "Epoch [86/200] Loss: 17.30023193359375\n",
      "Epoch [87/200] Loss: 17.171772003173828\n",
      "Epoch [88/200] Loss: 17.048301696777344\n",
      "Epoch [89/200] Loss: 16.929649353027344\n",
      "Epoch [90/200] Loss: 16.815670013427734\n",
      "Epoch [91/200] Loss: 16.706214904785156\n",
      "Epoch [92/200] Loss: 16.60112953186035\n",
      "Epoch [93/200] Loss: 16.500272750854492\n",
      "Epoch [94/200] Loss: 16.40350914001465\n",
      "Epoch [95/200] Loss: 16.310699462890625\n",
      "Epoch [96/200] Loss: 16.22170639038086\n",
      "Epoch [97/200] Loss: 16.136409759521484\n",
      "Epoch [98/200] Loss: 16.054672241210938\n",
      "Epoch [99/200] Loss: 15.976377487182617\n",
      "Epoch [100/200] Loss: 15.90140151977539\n",
      "Epoch [101/200] Loss: 15.82962703704834\n",
      "Epoch [102/200] Loss: 15.760944366455078\n",
      "Epoch [103/200] Loss: 15.695240020751953\n",
      "Epoch [104/200] Loss: 15.63239860534668\n",
      "Epoch [105/200] Loss: 15.572325706481934\n",
      "Epoch [106/200] Loss: 15.514915466308594\n",
      "Epoch [107/200] Loss: 15.460068702697754\n",
      "Epoch [108/200] Loss: 15.407686233520508\n",
      "Epoch [109/200] Loss: 15.35767936706543\n",
      "Epoch [110/200] Loss: 15.309951782226562\n",
      "Epoch [111/200] Loss: 15.264415740966797\n",
      "Epoch [112/200] Loss: 15.220993995666504\n",
      "Epoch [113/200] Loss: 15.179590225219727\n",
      "Epoch [114/200] Loss: 15.140134811401367\n",
      "Epoch [115/200] Loss: 15.102544784545898\n",
      "Epoch [116/200] Loss: 15.066742897033691\n",
      "Epoch [117/200] Loss: 15.032659530639648\n",
      "Epoch [118/200] Loss: 15.000226020812988\n",
      "Epoch [119/200] Loss: 14.969368934631348\n",
      "Epoch [120/200] Loss: 14.940027236938477\n",
      "Epoch [121/200] Loss: 14.912129402160645\n",
      "Epoch [122/200] Loss: 14.8856201171875\n",
      "Epoch [123/200] Loss: 14.86043930053711\n",
      "Epoch [124/200] Loss: 14.836527824401855\n",
      "Epoch [125/200] Loss: 14.813828468322754\n",
      "Epoch [126/200] Loss: 14.792292594909668\n",
      "Epoch [127/200] Loss: 14.77186393737793\n",
      "Epoch [128/200] Loss: 14.752493858337402\n",
      "Epoch [129/200] Loss: 14.734136581420898\n",
      "Epoch [130/200] Loss: 14.716741561889648\n",
      "Epoch [131/200] Loss: 14.700267791748047\n",
      "Epoch [132/200] Loss: 14.684675216674805\n",
      "Epoch [133/200] Loss: 14.669914245605469\n",
      "Epoch [134/200] Loss: 14.655953407287598\n",
      "Epoch [135/200] Loss: 14.642753601074219\n",
      "Epoch [136/200] Loss: 14.630273818969727\n",
      "Epoch [137/200] Loss: 14.61848258972168\n",
      "Epoch [138/200] Loss: 14.607345581054688\n",
      "Epoch [139/200] Loss: 14.596834182739258\n",
      "Epoch [140/200] Loss: 14.586910247802734\n",
      "Epoch [141/200] Loss: 14.577550888061523\n",
      "Epoch [142/200] Loss: 14.568724632263184\n",
      "Epoch [143/200] Loss: 14.560405731201172\n",
      "Epoch [144/200] Loss: 14.552565574645996\n",
      "Epoch [145/200] Loss: 14.545182228088379\n",
      "Epoch [146/200] Loss: 14.538232803344727\n",
      "Epoch [147/200] Loss: 14.53169059753418\n",
      "Epoch [148/200] Loss: 14.525538444519043\n",
      "Epoch [149/200] Loss: 14.519754409790039\n",
      "Epoch [150/200] Loss: 14.51431655883789\n",
      "Epoch [151/200] Loss: 14.509208679199219\n",
      "Epoch [152/200] Loss: 14.504410743713379\n",
      "Epoch [153/200] Loss: 14.499908447265625\n",
      "Epoch [154/200] Loss: 14.495682716369629\n",
      "Epoch [155/200] Loss: 14.491720199584961\n",
      "Epoch [156/200] Loss: 14.488006591796875\n",
      "Epoch [157/200] Loss: 14.484525680541992\n",
      "Epoch [158/200] Loss: 14.481266021728516\n",
      "Epoch [159/200] Loss: 14.478212356567383\n",
      "Epoch [160/200] Loss: 14.475357055664062\n",
      "Epoch [161/200] Loss: 14.472684860229492\n",
      "Epoch [162/200] Loss: 14.470185279846191\n",
      "Epoch [163/200] Loss: 14.467850685119629\n",
      "Epoch [164/200] Loss: 14.465669631958008\n",
      "Epoch [165/200] Loss: 14.463634490966797\n",
      "Epoch [166/200] Loss: 14.461732864379883\n",
      "Epoch [167/200] Loss: 14.459959983825684\n",
      "Epoch [168/200] Loss: 14.458307266235352\n",
      "Epoch [169/200] Loss: 14.456766128540039\n",
      "Epoch [170/200] Loss: 14.455329895019531\n",
      "Epoch [171/200] Loss: 14.453993797302246\n",
      "Epoch [172/200] Loss: 14.452750205993652\n",
      "Epoch [173/200] Loss: 14.451593399047852\n",
      "Epoch [174/200] Loss: 14.450517654418945\n",
      "Epoch [175/200] Loss: 14.449518203735352\n",
      "Epoch [176/200] Loss: 14.448588371276855\n",
      "Epoch [177/200] Loss: 14.44772720336914\n",
      "Epoch [178/200] Loss: 14.446928024291992\n",
      "Epoch [179/200] Loss: 14.446186065673828\n",
      "Epoch [180/200] Loss: 14.445497512817383\n",
      "Epoch [181/200] Loss: 14.444859504699707\n",
      "Epoch [182/200] Loss: 14.444270133972168\n",
      "Epoch [183/200] Loss: 14.443723678588867\n",
      "Epoch [184/200] Loss: 14.443218231201172\n",
      "Epoch [185/200] Loss: 14.44275188446045\n",
      "Epoch [186/200] Loss: 14.442319869995117\n",
      "Epoch [187/200] Loss: 14.44192123413086\n",
      "Epoch [188/200] Loss: 14.441553115844727\n",
      "Epoch [189/200] Loss: 14.441211700439453\n",
      "Epoch [190/200] Loss: 14.440900802612305\n",
      "Epoch [191/200] Loss: 14.440611839294434\n",
      "Epoch [192/200] Loss: 14.440345764160156\n",
      "Epoch [193/200] Loss: 14.44010066986084\n",
      "Epoch [194/200] Loss: 14.439876556396484\n",
      "Epoch [195/200] Loss: 14.439669609069824\n",
      "Epoch [196/200] Loss: 14.43947982788086\n",
      "Epoch [197/200] Loss: 14.43930435180664\n",
      "Epoch [198/200] Loss: 14.439144134521484\n",
      "Epoch [199/200] Loss: 14.438996315002441\n",
      "Epoch [200/200] Loss: 14.438861846923828\n",
      "Predicted days_remaining for parent_id 377: 9.714813232421875\n",
      "Training for parent_id 380...\n",
      "Epoch [1/200] Loss: 119.48128509521484\n",
      "Epoch [2/200] Loss: 115.11760711669922\n",
      "Epoch [3/200] Loss: 110.91316986083984\n",
      "Epoch [4/200] Loss: 106.88861846923828\n",
      "Epoch [5/200] Loss: 103.04808807373047\n",
      "Epoch [6/200] Loss: 99.3779525756836\n",
      "Epoch [7/200] Loss: 95.86711120605469\n",
      "Epoch [8/200] Loss: 92.51367950439453\n",
      "Epoch [9/200] Loss: 89.3207778930664\n",
      "Epoch [10/200] Loss: 86.29074096679688\n",
      "Epoch [11/200] Loss: 83.4227066040039\n",
      "Epoch [12/200] Loss: 80.71202850341797\n",
      "Epoch [13/200] Loss: 78.15101623535156\n",
      "Epoch [14/200] Loss: 75.72982025146484\n",
      "Epoch [15/200] Loss: 73.43760681152344\n",
      "Epoch [16/200] Loss: 71.26375579833984\n",
      "Epoch [17/200] Loss: 69.19895935058594\n",
      "Epoch [18/200] Loss: 67.23571014404297\n",
      "Epoch [19/200] Loss: 65.36830139160156\n",
      "Epoch [20/200] Loss: 63.59248733520508\n",
      "Epoch [21/200] Loss: 61.90460968017578\n",
      "Epoch [22/200] Loss: 60.301063537597656\n",
      "Epoch [23/200] Loss: 58.77772521972656\n",
      "Epoch [24/200] Loss: 57.32989501953125\n",
      "Epoch [25/200] Loss: 55.952327728271484\n",
      "Epoch [26/200] Loss: 54.639522552490234\n",
      "Epoch [27/200] Loss: 53.38592529296875\n",
      "Epoch [28/200] Loss: 52.186248779296875\n",
      "Epoch [29/200] Loss: 51.0356330871582\n",
      "Epoch [30/200] Loss: 49.929752349853516\n",
      "Epoch [31/200] Loss: 48.86482620239258\n",
      "Epoch [32/200] Loss: 47.837562561035156\n",
      "Epoch [33/200] Loss: 46.845149993896484\n",
      "Epoch [34/200] Loss: 45.88507843017578\n",
      "Epoch [35/200] Loss: 44.9552001953125\n",
      "Epoch [36/200] Loss: 44.0536003112793\n",
      "Epoch [37/200] Loss: 43.17857360839844\n",
      "Epoch [38/200] Loss: 42.32861328125\n",
      "Epoch [39/200] Loss: 41.50241470336914\n",
      "Epoch [40/200] Loss: 40.69878387451172\n",
      "Epoch [41/200] Loss: 39.91670608520508\n",
      "Epoch [42/200] Loss: 39.1552619934082\n",
      "Epoch [43/200] Loss: 38.413658142089844\n",
      "Epoch [44/200] Loss: 37.691158294677734\n",
      "Epoch [45/200] Loss: 36.98713302612305\n",
      "Epoch [46/200] Loss: 36.30098342895508\n",
      "Epoch [47/200] Loss: 35.63217544555664\n",
      "Epoch [48/200] Loss: 34.98020553588867\n",
      "Epoch [49/200] Loss: 34.3446044921875\n",
      "Epoch [50/200] Loss: 33.72495651245117\n",
      "Epoch [51/200] Loss: 33.120853424072266\n",
      "Epoch [52/200] Loss: 32.53190994262695\n",
      "Epoch [53/200] Loss: 31.957794189453125\n",
      "Epoch [54/200] Loss: 31.398178100585938\n",
      "Epoch [55/200] Loss: 30.852767944335938\n",
      "Epoch [56/200] Loss: 30.321279525756836\n",
      "Epoch [57/200] Loss: 29.8034725189209\n",
      "Epoch [58/200] Loss: 29.29911231994629\n",
      "Epoch [59/200] Loss: 28.807979583740234\n",
      "Epoch [60/200] Loss: 28.32986068725586\n",
      "Epoch [61/200] Loss: 27.86457633972168\n",
      "Epoch [62/200] Loss: 27.41191864013672\n",
      "Epoch [63/200] Loss: 26.97170066833496\n",
      "Epoch [64/200] Loss: 26.543733596801758\n",
      "Epoch [65/200] Loss: 26.127824783325195\n",
      "Epoch [66/200] Loss: 25.723758697509766\n",
      "Epoch [67/200] Loss: 25.331331253051758\n",
      "Epoch [68/200] Loss: 24.950332641601562\n",
      "Epoch [69/200] Loss: 24.580535888671875\n",
      "Epoch [70/200] Loss: 24.221725463867188\n",
      "Epoch [71/200] Loss: 23.873653411865234\n",
      "Epoch [72/200] Loss: 23.536100387573242\n",
      "Epoch [73/200] Loss: 23.208820343017578\n",
      "Epoch [74/200] Loss: 22.891582489013672\n",
      "Epoch [75/200] Loss: 22.584148406982422\n",
      "Epoch [76/200] Loss: 22.286285400390625\n",
      "Epoch [77/200] Loss: 21.997758865356445\n",
      "Epoch [78/200] Loss: 21.71833610534668\n",
      "Epoch [79/200] Loss: 21.44778060913086\n",
      "Epoch [80/200] Loss: 21.185876846313477\n",
      "Epoch [81/200] Loss: 20.932395935058594\n",
      "Epoch [82/200] Loss: 20.687129974365234\n",
      "Epoch [83/200] Loss: 20.449848175048828\n",
      "Epoch [84/200] Loss: 20.220348358154297\n",
      "Epoch [85/200] Loss: 19.99842071533203\n",
      "Epoch [86/200] Loss: 19.783859252929688\n",
      "Epoch [87/200] Loss: 19.576465606689453\n",
      "Epoch [88/200] Loss: 19.376041412353516\n",
      "Epoch [89/200] Loss: 19.182403564453125\n",
      "Epoch [90/200] Loss: 18.995346069335938\n",
      "Epoch [91/200] Loss: 18.814697265625\n",
      "Epoch [92/200] Loss: 18.64027976989746\n",
      "Epoch [93/200] Loss: 18.471900939941406\n",
      "Epoch [94/200] Loss: 18.309404373168945\n",
      "Epoch [95/200] Loss: 18.152605056762695\n",
      "Epoch [96/200] Loss: 18.001346588134766\n",
      "Epoch [97/200] Loss: 17.855466842651367\n",
      "Epoch [98/200] Loss: 17.714805603027344\n",
      "Epoch [99/200] Loss: 17.579204559326172\n",
      "Epoch [100/200] Loss: 17.44851303100586\n",
      "Epoch [101/200] Loss: 17.32257843017578\n",
      "Epoch [102/200] Loss: 17.20126724243164\n",
      "Epoch [103/200] Loss: 17.08443260192871\n",
      "Epoch [104/200] Loss: 16.97193145751953\n",
      "Epoch [105/200] Loss: 16.863632202148438\n",
      "Epoch [106/200] Loss: 16.759403228759766\n",
      "Epoch [107/200] Loss: 16.65911865234375\n",
      "Epoch [108/200] Loss: 16.562644958496094\n",
      "Epoch [109/200] Loss: 16.469865798950195\n",
      "Epoch [110/200] Loss: 16.380661010742188\n",
      "Epoch [111/200] Loss: 16.29491424560547\n",
      "Epoch [112/200] Loss: 16.21251678466797\n",
      "Epoch [113/200] Loss: 16.133346557617188\n",
      "Epoch [114/200] Loss: 16.05730438232422\n",
      "Epoch [115/200] Loss: 15.984284400939941\n",
      "Epoch [116/200] Loss: 15.91418170928955\n",
      "Epoch [117/200] Loss: 15.846899032592773\n",
      "Epoch [118/200] Loss: 15.782341003417969\n",
      "Epoch [119/200] Loss: 15.72041130065918\n",
      "Epoch [120/200] Loss: 15.661018371582031\n",
      "Epoch [121/200] Loss: 15.604074478149414\n",
      "Epoch [122/200] Loss: 15.549493789672852\n",
      "Epoch [123/200] Loss: 15.49718952178955\n",
      "Epoch [124/200] Loss: 15.447080612182617\n",
      "Epoch [125/200] Loss: 15.399089813232422\n",
      "Epoch [126/200] Loss: 15.353139877319336\n",
      "Epoch [127/200] Loss: 15.309152603149414\n",
      "Epoch [128/200] Loss: 15.267057418823242\n",
      "Epoch [129/200] Loss: 15.226785659790039\n",
      "Epoch [130/200] Loss: 15.188265800476074\n",
      "Epoch [131/200] Loss: 15.151432991027832\n",
      "Epoch [132/200] Loss: 15.116228103637695\n",
      "Epoch [133/200] Loss: 15.08258056640625\n",
      "Epoch [134/200] Loss: 15.050434112548828\n",
      "Epoch [135/200] Loss: 15.019731521606445\n",
      "Epoch [136/200] Loss: 14.990411758422852\n",
      "Epoch [137/200] Loss: 14.962427139282227\n",
      "Epoch [138/200] Loss: 14.935720443725586\n",
      "Epoch [139/200] Loss: 14.910239219665527\n",
      "Epoch [140/200] Loss: 14.885936737060547\n",
      "Epoch [141/200] Loss: 14.862765312194824\n",
      "Epoch [142/200] Loss: 14.840679168701172\n",
      "Epoch [143/200] Loss: 14.819625854492188\n",
      "Epoch [144/200] Loss: 14.799576759338379\n",
      "Epoch [145/200] Loss: 14.780478477478027\n",
      "Epoch [146/200] Loss: 14.76229476928711\n",
      "Epoch [147/200] Loss: 14.744989395141602\n",
      "Epoch [148/200] Loss: 14.72851848602295\n",
      "Epoch [149/200] Loss: 14.712852478027344\n",
      "Epoch [150/200] Loss: 14.697952270507812\n",
      "Epoch [151/200] Loss: 14.68379020690918\n",
      "Epoch [152/200] Loss: 14.670328140258789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [153/200] Loss: 14.657535552978516\n",
      "Epoch [154/200] Loss: 14.645387649536133\n",
      "Epoch [155/200] Loss: 14.63385009765625\n",
      "Epoch [156/200] Loss: 14.62289810180664\n",
      "Epoch [157/200] Loss: 14.612508773803711\n",
      "Epoch [158/200] Loss: 14.60264778137207\n",
      "Epoch [159/200] Loss: 14.593297004699707\n",
      "Epoch [160/200] Loss: 14.584430694580078\n",
      "Epoch [161/200] Loss: 14.576026916503906\n",
      "Epoch [162/200] Loss: 14.568066596984863\n",
      "Epoch [163/200] Loss: 14.560523986816406\n",
      "Epoch [164/200] Loss: 14.553382873535156\n",
      "Epoch [165/200] Loss: 14.546621322631836\n",
      "Epoch [166/200] Loss: 14.540224075317383\n",
      "Epoch [167/200] Loss: 14.534170150756836\n",
      "Epoch [168/200] Loss: 14.528446197509766\n",
      "Epoch [169/200] Loss: 14.523035049438477\n",
      "Epoch [170/200] Loss: 14.517916679382324\n",
      "Epoch [171/200] Loss: 14.513084411621094\n",
      "Epoch [172/200] Loss: 14.508519172668457\n",
      "Epoch [173/200] Loss: 14.504209518432617\n",
      "Epoch [174/200] Loss: 14.500140190124512\n",
      "Epoch [175/200] Loss: 14.49630069732666\n",
      "Epoch [176/200] Loss: 14.49267864227295\n",
      "Epoch [177/200] Loss: 14.489262580871582\n",
      "Epoch [178/200] Loss: 14.486042022705078\n",
      "Epoch [179/200] Loss: 14.483006477355957\n",
      "Epoch [180/200] Loss: 14.480148315429688\n",
      "Epoch [181/200] Loss: 14.477456092834473\n",
      "Epoch [182/200] Loss: 14.474921226501465\n",
      "Epoch [183/200] Loss: 14.472535133361816\n",
      "Epoch [184/200] Loss: 14.470291137695312\n",
      "Epoch [185/200] Loss: 14.468179702758789\n",
      "Epoch [186/200] Loss: 14.466194152832031\n",
      "Epoch [187/200] Loss: 14.46432876586914\n",
      "Epoch [188/200] Loss: 14.462576866149902\n",
      "Epoch [189/200] Loss: 14.460929870605469\n",
      "Epoch [190/200] Loss: 14.45938491821289\n",
      "Epoch [191/200] Loss: 14.45793342590332\n",
      "Epoch [192/200] Loss: 14.456571578979492\n",
      "Epoch [193/200] Loss: 14.45529556274414\n",
      "Epoch [194/200] Loss: 14.454097747802734\n",
      "Epoch [195/200] Loss: 14.45297622680664\n",
      "Epoch [196/200] Loss: 14.451925277709961\n",
      "Epoch [197/200] Loss: 14.45094108581543\n",
      "Epoch [198/200] Loss: 14.450018882751465\n",
      "Epoch [199/200] Loss: 14.449155807495117\n",
      "Epoch [200/200] Loss: 14.448347091674805\n",
      "Predicted days_remaining for parent_id 380: 9.6495361328125\n",
      "Training for parent_id 387...\n",
      "Epoch [1/200] Loss: 421.52410888671875\n",
      "Epoch [2/200] Loss: 411.49365234375\n",
      "Epoch [3/200] Loss: 401.71002197265625\n",
      "Epoch [4/200] Loss: 392.20745849609375\n",
      "Epoch [5/200] Loss: 383.02044677734375\n",
      "Epoch [6/200] Loss: 374.1809387207031\n",
      "Epoch [7/200] Loss: 365.70770263671875\n",
      "Epoch [8/200] Loss: 357.6052551269531\n",
      "Epoch [9/200] Loss: 349.8656311035156\n",
      "Epoch [10/200] Loss: 342.4730529785156\n",
      "Epoch [11/200] Loss: 335.4091796875\n",
      "Epoch [12/200] Loss: 328.6568908691406\n",
      "Epoch [13/200] Loss: 322.2016906738281\n",
      "Epoch [14/200] Loss: 316.0311279296875\n",
      "Epoch [15/200] Loss: 310.1340637207031\n",
      "Epoch [16/200] Loss: 304.4989013671875\n",
      "Epoch [17/200] Loss: 299.1132507324219\n",
      "Epoch [18/200] Loss: 293.9634094238281\n",
      "Epoch [19/200] Loss: 289.0345764160156\n",
      "Epoch [20/200] Loss: 284.3118591308594\n",
      "Epoch [21/200] Loss: 279.7804870605469\n",
      "Epoch [22/200] Loss: 275.42706298828125\n",
      "Epoch [23/200] Loss: 271.23919677734375\n",
      "Epoch [24/200] Loss: 267.20623779296875\n",
      "Epoch [25/200] Loss: 263.318603515625\n",
      "Epoch [26/200] Loss: 259.5677795410156\n",
      "Epoch [27/200] Loss: 255.94610595703125\n",
      "Epoch [28/200] Loss: 252.44613647460938\n",
      "Epoch [29/200] Loss: 249.06068420410156\n",
      "Epoch [30/200] Loss: 245.7826690673828\n",
      "Epoch [31/200] Loss: 242.60508728027344\n",
      "Epoch [32/200] Loss: 239.52099609375\n",
      "Epoch [33/200] Loss: 236.5240020751953\n",
      "Epoch [34/200] Loss: 233.60791015625\n",
      "Epoch [35/200] Loss: 230.7671661376953\n",
      "Epoch [36/200] Loss: 227.9966583251953\n",
      "Epoch [37/200] Loss: 225.29173278808594\n",
      "Epoch [38/200] Loss: 222.6482696533203\n",
      "Epoch [39/200] Loss: 220.06243896484375\n",
      "Epoch [40/200] Loss: 217.5308837890625\n",
      "Epoch [41/200] Loss: 215.05052185058594\n",
      "Epoch [42/200] Loss: 212.6185760498047\n",
      "Epoch [43/200] Loss: 210.2325439453125\n",
      "Epoch [44/200] Loss: 207.8899383544922\n",
      "Epoch [45/200] Loss: 205.5886688232422\n",
      "Epoch [46/200] Loss: 203.32655334472656\n",
      "Epoch [47/200] Loss: 201.10157775878906\n",
      "Epoch [48/200] Loss: 198.91172790527344\n",
      "Epoch [49/200] Loss: 196.7550811767578\n",
      "Epoch [50/200] Loss: 194.6297149658203\n",
      "Epoch [51/200] Loss: 192.533935546875\n",
      "Epoch [52/200] Loss: 190.46597290039062\n",
      "Epoch [53/200] Loss: 188.42437744140625\n",
      "Epoch [54/200] Loss: 186.40769958496094\n",
      "Epoch [55/200] Loss: 184.4148712158203\n",
      "Epoch [56/200] Loss: 182.44497680664062\n",
      "Epoch [57/200] Loss: 180.49734497070312\n",
      "Epoch [58/200] Loss: 178.5714111328125\n",
      "Epoch [59/200] Loss: 176.6669921875\n",
      "Epoch [60/200] Loss: 174.7838592529297\n",
      "Epoch [61/200] Loss: 172.92193603515625\n",
      "Epoch [62/200] Loss: 171.0812225341797\n",
      "Epoch [63/200] Loss: 169.2616424560547\n",
      "Epoch [64/200] Loss: 167.46310424804688\n",
      "Epoch [65/200] Loss: 165.68544006347656\n",
      "Epoch [66/200] Loss: 163.92857360839844\n",
      "Epoch [67/200] Loss: 162.1921844482422\n",
      "Epoch [68/200] Loss: 160.47610473632812\n",
      "Epoch [69/200] Loss: 158.78001403808594\n",
      "Epoch [70/200] Loss: 157.10360717773438\n",
      "Epoch [71/200] Loss: 155.44662475585938\n",
      "Epoch [72/200] Loss: 153.80868530273438\n",
      "Epoch [73/200] Loss: 152.18951416015625\n",
      "Epoch [74/200] Loss: 150.5887451171875\n",
      "Epoch [75/200] Loss: 149.0060577392578\n",
      "Epoch [76/200] Loss: 147.44119262695312\n",
      "Epoch [77/200] Loss: 145.893798828125\n",
      "Epoch [78/200] Loss: 144.363525390625\n",
      "Epoch [79/200] Loss: 142.85009765625\n",
      "Epoch [80/200] Loss: 141.3531951904297\n",
      "Epoch [81/200] Loss: 139.87246704101562\n",
      "Epoch [82/200] Loss: 138.4076385498047\n",
      "Epoch [83/200] Loss: 136.9583740234375\n",
      "Epoch [84/200] Loss: 135.5244140625\n",
      "Epoch [85/200] Loss: 134.10536193847656\n",
      "Epoch [86/200] Loss: 132.70089721679688\n",
      "Epoch [87/200] Loss: 131.31077575683594\n",
      "Epoch [88/200] Loss: 129.9346923828125\n",
      "Epoch [89/200] Loss: 128.57232666015625\n",
      "Epoch [90/200] Loss: 127.2234115600586\n",
      "Epoch [91/200] Loss: 125.88775634765625\n",
      "Epoch [92/200] Loss: 124.56517028808594\n",
      "Epoch [93/200] Loss: 123.2553939819336\n",
      "Epoch [94/200] Loss: 121.95838165283203\n",
      "Epoch [95/200] Loss: 120.6739730834961\n",
      "Epoch [96/200] Loss: 119.402099609375\n",
      "Epoch [97/200] Loss: 118.1427230834961\n",
      "Epoch [98/200] Loss: 116.89579772949219\n",
      "Epoch [99/200] Loss: 115.66128540039062\n",
      "Epoch [100/200] Loss: 114.43917846679688\n",
      "Epoch [101/200] Loss: 113.22940826416016\n",
      "Epoch [102/200] Loss: 112.03199768066406\n",
      "Epoch [103/200] Loss: 110.84683990478516\n",
      "Epoch [104/200] Loss: 109.67395782470703\n",
      "Epoch [105/200] Loss: 108.51323699951172\n",
      "Epoch [106/200] Loss: 107.36463165283203\n",
      "Epoch [107/200] Loss: 106.22809600830078\n",
      "Epoch [108/200] Loss: 105.10347747802734\n",
      "Epoch [109/200] Loss: 103.99078369140625\n",
      "Epoch [110/200] Loss: 102.8898696899414\n",
      "Epoch [111/200] Loss: 101.8006591796875\n",
      "Epoch [112/200] Loss: 100.72308349609375\n",
      "Epoch [113/200] Loss: 99.65699768066406\n",
      "Epoch [114/200] Loss: 98.60232543945312\n",
      "Epoch [115/200] Loss: 97.55899810791016\n",
      "Epoch [116/200] Loss: 96.52684783935547\n",
      "Epoch [117/200] Loss: 95.50585174560547\n",
      "Epoch [118/200] Loss: 94.495849609375\n",
      "Epoch [119/200] Loss: 93.4968032836914\n",
      "Epoch [120/200] Loss: 92.5085678100586\n",
      "Epoch [121/200] Loss: 91.53103637695312\n",
      "Epoch [122/200] Loss: 90.5641098022461\n",
      "Epoch [123/200] Loss: 89.60774230957031\n",
      "Epoch [124/200] Loss: 88.66178894042969\n",
      "Epoch [125/200] Loss: 87.72610473632812\n",
      "Epoch [126/200] Loss: 86.80064392089844\n",
      "Epoch [127/200] Loss: 85.88536834716797\n",
      "Epoch [128/200] Loss: 84.9801025390625\n",
      "Epoch [129/200] Loss: 84.084716796875\n",
      "Epoch [130/200] Loss: 83.19921875\n",
      "Epoch [131/200] Loss: 82.32344818115234\n",
      "Epoch [132/200] Loss: 81.45730590820312\n",
      "Epoch [133/200] Loss: 80.60072326660156\n",
      "Epoch [134/200] Loss: 79.75359344482422\n",
      "Epoch [135/200] Loss: 78.91584014892578\n",
      "Epoch [136/200] Loss: 78.08734893798828\n",
      "Epoch [137/200] Loss: 77.2680435180664\n",
      "Epoch [138/200] Loss: 76.45783996582031\n",
      "Epoch [139/200] Loss: 75.65663146972656\n",
      "Epoch [140/200] Loss: 74.86432647705078\n",
      "Epoch [141/200] Loss: 74.08086395263672\n",
      "Epoch [142/200] Loss: 73.30616760253906\n",
      "Epoch [143/200] Loss: 72.54011535644531\n",
      "Epoch [144/200] Loss: 71.78267669677734\n",
      "Epoch [145/200] Loss: 71.0336685180664\n",
      "Epoch [146/200] Loss: 70.29309844970703\n",
      "Epoch [147/200] Loss: 69.56085205078125\n",
      "Epoch [148/200] Loss: 68.83686828613281\n",
      "Epoch [149/200] Loss: 68.12103271484375\n",
      "Epoch [150/200] Loss: 67.41329193115234\n",
      "Epoch [151/200] Loss: 66.71353912353516\n",
      "Epoch [152/200] Loss: 66.0217514038086\n",
      "Epoch [153/200] Loss: 65.33779907226562\n",
      "Epoch [154/200] Loss: 64.66162872314453\n",
      "Epoch [155/200] Loss: 63.9931640625\n",
      "Epoch [156/200] Loss: 63.332332611083984\n",
      "Epoch [157/200] Loss: 62.67903518676758\n",
      "Epoch [158/200] Loss: 62.03322982788086\n",
      "Epoch [159/200] Loss: 61.394859313964844\n",
      "Epoch [160/200] Loss: 60.76376724243164\n",
      "Epoch [161/200] Loss: 60.139976501464844\n",
      "Epoch [162/200] Loss: 59.52337646484375\n",
      "Epoch [163/200] Loss: 58.91392135620117\n",
      "Epoch [164/200] Loss: 58.31149673461914\n",
      "Epoch [165/200] Loss: 57.716064453125\n",
      "Epoch [166/200] Loss: 57.127559661865234\n",
      "Epoch [167/200] Loss: 56.545894622802734\n",
      "Epoch [168/200] Loss: 55.97101974487305\n",
      "Epoch [169/200] Loss: 55.40288162231445\n",
      "Epoch [170/200] Loss: 54.84138870239258\n",
      "Epoch [171/200] Loss: 54.2864990234375\n",
      "Epoch [172/200] Loss: 53.738101959228516\n",
      "Epoch [173/200] Loss: 53.1961669921875\n",
      "Epoch [174/200] Loss: 52.6606559753418\n",
      "Epoch [175/200] Loss: 52.131465911865234\n",
      "Epoch [176/200] Loss: 51.60857391357422\n",
      "Epoch [177/200] Loss: 51.091880798339844\n",
      "Epoch [178/200] Loss: 50.58132553100586\n",
      "Epoch [179/200] Loss: 50.07687759399414\n",
      "Epoch [180/200] Loss: 49.57849884033203\n",
      "Epoch [181/200] Loss: 49.086036682128906\n",
      "Epoch [182/200] Loss: 48.599491119384766\n",
      "Epoch [183/200] Loss: 48.11882400512695\n",
      "Epoch [184/200] Loss: 47.64393615722656\n",
      "Epoch [185/200] Loss: 47.1748161315918\n",
      "Epoch [186/200] Loss: 46.71136474609375\n",
      "Epoch [187/200] Loss: 46.253536224365234\n",
      "Epoch [188/200] Loss: 45.801273345947266\n",
      "Epoch [189/200] Loss: 45.35454177856445\n",
      "Epoch [190/200] Loss: 44.913265228271484\n",
      "Epoch [191/200] Loss: 44.47739791870117\n",
      "Epoch [192/200] Loss: 44.046878814697266\n",
      "Epoch [193/200] Loss: 43.62165832519531\n",
      "Epoch [194/200] Loss: 43.2016716003418\n",
      "Epoch [195/200] Loss: 42.786888122558594\n",
      "Epoch [196/200] Loss: 42.37724304199219\n",
      "Epoch [197/200] Loss: 41.97269058227539\n",
      "Epoch [198/200] Loss: 41.573184967041016\n",
      "Epoch [199/200] Loss: 41.17863845825195\n",
      "Epoch [200/200] Loss: 40.78904342651367\n",
      "Predicted days_remaining for parent_id 387: 14.654234886169434\n",
      "Training for parent_id 391...\n",
      "Epoch [1/200] Loss: 440.0998229980469\n",
      "Epoch [2/200] Loss: 429.657958984375\n",
      "Epoch [3/200] Loss: 419.54248046875\n",
      "Epoch [4/200] Loss: 409.8260803222656\n",
      "Epoch [5/200] Loss: 400.5237731933594\n",
      "Epoch [6/200] Loss: 391.6255798339844\n",
      "Epoch [7/200] Loss: 383.1171569824219\n",
      "Epoch [8/200] Loss: 374.9812927246094\n",
      "Epoch [9/200] Loss: 367.1997375488281\n",
      "Epoch [10/200] Loss: 359.75634765625\n",
      "Epoch [11/200] Loss: 352.63983154296875\n",
      "Epoch [12/200] Loss: 345.8429260253906\n",
      "Epoch [13/200] Loss: 339.3611145019531\n",
      "Epoch [14/200] Loss: 333.1899719238281\n",
      "Epoch [15/200] Loss: 327.3238220214844\n",
      "Epoch [16/200] Loss: 321.7537536621094\n",
      "Epoch [17/200] Loss: 316.4676208496094\n",
      "Epoch [18/200] Loss: 311.45013427734375\n",
      "Epoch [19/200] Loss: 306.6836242675781\n",
      "Epoch [20/200] Loss: 302.14947509765625\n",
      "Epoch [21/200] Loss: 297.82958984375\n",
      "Epoch [22/200] Loss: 293.7068176269531\n",
      "Epoch [23/200] Loss: 289.7651062011719\n",
      "Epoch [24/200] Loss: 285.98968505859375\n",
      "Epoch [25/200] Loss: 282.3666687011719\n",
      "Epoch [26/200] Loss: 278.88299560546875\n",
      "Epoch [27/200] Loss: 275.5264892578125\n",
      "Epoch [28/200] Loss: 272.2857360839844\n",
      "Epoch [29/200] Loss: 269.1501770019531\n",
      "Epoch [30/200] Loss: 266.1102294921875\n",
      "Epoch [31/200] Loss: 263.1568603515625\n",
      "Epoch [32/200] Loss: 260.2823486328125\n",
      "Epoch [33/200] Loss: 257.4795227050781\n",
      "Epoch [34/200] Loss: 254.7420654296875\n",
      "Epoch [35/200] Loss: 252.06460571289062\n",
      "Epoch [36/200] Loss: 249.44212341308594\n",
      "Epoch [37/200] Loss: 246.8704071044922\n",
      "Epoch [38/200] Loss: 244.34555053710938\n",
      "Epoch [39/200] Loss: 241.8642120361328\n",
      "Epoch [40/200] Loss: 239.42330932617188\n",
      "Epoch [41/200] Loss: 237.02017211914062\n",
      "Epoch [42/200] Loss: 234.65243530273438\n",
      "Epoch [43/200] Loss: 232.31785583496094\n",
      "Epoch [44/200] Loss: 230.01470947265625\n",
      "Epoch [45/200] Loss: 227.74124145507812\n",
      "Epoch [46/200] Loss: 225.49606323242188\n",
      "Epoch [47/200] Loss: 223.27793884277344\n",
      "Epoch [48/200] Loss: 221.08580017089844\n",
      "Epoch [49/200] Loss: 218.91864013671875\n",
      "Epoch [50/200] Loss: 216.77565002441406\n",
      "Epoch [51/200] Loss: 214.65614318847656\n",
      "Epoch [52/200] Loss: 212.55929565429688\n",
      "Epoch [53/200] Loss: 210.484619140625\n",
      "Epoch [54/200] Loss: 208.4315185546875\n",
      "Epoch [55/200] Loss: 206.3994903564453\n",
      "Epoch [56/200] Loss: 204.3880157470703\n",
      "Epoch [57/200] Loss: 202.396728515625\n",
      "Epoch [58/200] Loss: 200.4251708984375\n",
      "Epoch [59/200] Loss: 198.4729766845703\n",
      "Epoch [60/200] Loss: 196.53982543945312\n",
      "Epoch [61/200] Loss: 194.62548828125\n",
      "Epoch [62/200] Loss: 192.72958374023438\n",
      "Epoch [63/200] Loss: 190.85198974609375\n",
      "Epoch [64/200] Loss: 188.9923858642578\n",
      "Epoch [65/200] Loss: 187.1506805419922\n",
      "Epoch [66/200] Loss: 185.32662963867188\n",
      "Epoch [67/200] Loss: 183.52012634277344\n",
      "Epoch [68/200] Loss: 181.7310028076172\n",
      "Epoch [69/200] Loss: 179.9590301513672\n",
      "Epoch [70/200] Loss: 178.2041473388672\n",
      "Epoch [71/200] Loss: 176.4661102294922\n",
      "Epoch [72/200] Loss: 174.74485778808594\n",
      "Epoch [73/200] Loss: 173.04019165039062\n",
      "Epoch [74/200] Loss: 171.35195922851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/200] Loss: 169.67994689941406\n",
      "Epoch [76/200] Loss: 168.02407836914062\n",
      "Epoch [77/200] Loss: 166.38418579101562\n",
      "Epoch [78/200] Loss: 164.76010131835938\n",
      "Epoch [79/200] Loss: 163.1516876220703\n",
      "Epoch [80/200] Loss: 161.55882263183594\n",
      "Epoch [81/200] Loss: 159.9813690185547\n",
      "Epoch [82/200] Loss: 158.41915893554688\n",
      "Epoch [83/200] Loss: 156.87205505371094\n",
      "Epoch [84/200] Loss: 155.33990478515625\n",
      "Epoch [85/200] Loss: 153.82264709472656\n",
      "Epoch [86/200] Loss: 152.32009887695312\n",
      "Epoch [87/200] Loss: 150.83206176757812\n",
      "Epoch [88/200] Loss: 149.35845947265625\n",
      "Epoch [89/200] Loss: 147.89913940429688\n",
      "Epoch [90/200] Loss: 146.4539794921875\n",
      "Epoch [91/200] Loss: 145.0228271484375\n",
      "Epoch [92/200] Loss: 143.6055145263672\n",
      "Epoch [93/200] Loss: 142.20193481445312\n",
      "Epoch [94/200] Loss: 140.81202697753906\n",
      "Epoch [95/200] Loss: 139.435546875\n",
      "Epoch [96/200] Loss: 138.07244873046875\n",
      "Epoch [97/200] Loss: 136.72256469726562\n",
      "Epoch [98/200] Loss: 135.38571166992188\n",
      "Epoch [99/200] Loss: 134.06187438964844\n",
      "Epoch [100/200] Loss: 132.7508544921875\n",
      "Epoch [101/200] Loss: 131.45248413085938\n",
      "Epoch [102/200] Loss: 130.16677856445312\n",
      "Epoch [103/200] Loss: 128.89349365234375\n",
      "Epoch [104/200] Loss: 127.63257598876953\n",
      "Epoch [105/200] Loss: 126.38386535644531\n",
      "Epoch [106/200] Loss: 125.14726257324219\n",
      "Epoch [107/200] Loss: 123.922607421875\n",
      "Epoch [108/200] Loss: 122.70987701416016\n",
      "Epoch [109/200] Loss: 121.50891876220703\n",
      "Epoch [110/200] Loss: 120.31957244873047\n",
      "Epoch [111/200] Loss: 119.14179229736328\n",
      "Epoch [112/200] Loss: 117.97541046142578\n",
      "Epoch [113/200] Loss: 116.82035064697266\n",
      "Epoch [114/200] Loss: 115.67647552490234\n",
      "Epoch [115/200] Loss: 114.54373168945312\n",
      "Epoch [116/200] Loss: 113.42198944091797\n",
      "Epoch [117/200] Loss: 112.31114959716797\n",
      "Epoch [118/200] Loss: 111.2110366821289\n",
      "Epoch [119/200] Loss: 110.1217041015625\n",
      "Epoch [120/200] Loss: 109.04292297363281\n",
      "Epoch [121/200] Loss: 107.9746322631836\n",
      "Epoch [122/200] Loss: 106.91677856445312\n",
      "Epoch [123/200] Loss: 105.86918640136719\n",
      "Epoch [124/200] Loss: 104.83182525634766\n",
      "Epoch [125/200] Loss: 103.80459594726562\n",
      "Epoch [126/200] Loss: 102.78736114501953\n",
      "Epoch [127/200] Loss: 101.7800521850586\n",
      "Epoch [128/200] Loss: 100.78260803222656\n",
      "Epoch [129/200] Loss: 99.794921875\n",
      "Epoch [130/200] Loss: 98.81690216064453\n",
      "Epoch [131/200] Loss: 97.84846496582031\n",
      "Epoch [132/200] Loss: 96.88953399658203\n",
      "Epoch [133/200] Loss: 95.93997955322266\n",
      "Epoch [134/200] Loss: 94.99978637695312\n",
      "Epoch [135/200] Loss: 94.06881713867188\n",
      "Epoch [136/200] Loss: 93.14703369140625\n",
      "Epoch [137/200] Loss: 92.2343521118164\n",
      "Epoch [138/200] Loss: 91.33065795898438\n",
      "Epoch [139/200] Loss: 90.4358901977539\n",
      "Epoch [140/200] Loss: 89.54998779296875\n",
      "Epoch [141/200] Loss: 88.67283630371094\n",
      "Epoch [142/200] Loss: 87.80441284179688\n",
      "Epoch [143/200] Loss: 86.94461059570312\n",
      "Epoch [144/200] Loss: 86.09333801269531\n",
      "Epoch [145/200] Loss: 85.25057983398438\n",
      "Epoch [146/200] Loss: 84.41620635986328\n",
      "Epoch [147/200] Loss: 83.59017181396484\n",
      "Epoch [148/200] Loss: 82.77239227294922\n",
      "Epoch [149/200] Loss: 81.96280670166016\n",
      "Epoch [150/200] Loss: 81.16136169433594\n",
      "Epoch [151/200] Loss: 80.36795043945312\n",
      "Epoch [152/200] Loss: 79.5825424194336\n",
      "Epoch [153/200] Loss: 78.80506134033203\n",
      "Epoch [154/200] Loss: 78.03543090820312\n",
      "Epoch [155/200] Loss: 77.27356719970703\n",
      "Epoch [156/200] Loss: 76.51940155029297\n",
      "Epoch [157/200] Loss: 75.77294921875\n",
      "Epoch [158/200] Loss: 75.03404235839844\n",
      "Epoch [159/200] Loss: 74.30268859863281\n",
      "Epoch [160/200] Loss: 73.57878112792969\n",
      "Epoch [161/200] Loss: 72.86231231689453\n",
      "Epoch [162/200] Loss: 72.15314483642578\n",
      "Epoch [163/200] Loss: 71.45128631591797\n",
      "Epoch [164/200] Loss: 70.75659942626953\n",
      "Epoch [165/200] Loss: 70.06910705566406\n",
      "Epoch [166/200] Loss: 69.38870239257812\n",
      "Epoch [167/200] Loss: 68.71531677246094\n",
      "Epoch [168/200] Loss: 68.04891204833984\n",
      "Epoch [169/200] Loss: 67.38945007324219\n",
      "Epoch [170/200] Loss: 66.7368392944336\n",
      "Epoch [171/200] Loss: 66.09101867675781\n",
      "Epoch [172/200] Loss: 65.45195770263672\n",
      "Epoch [173/200] Loss: 64.81956481933594\n",
      "Epoch [174/200] Loss: 64.19380187988281\n",
      "Epoch [175/200] Loss: 63.57463836669922\n",
      "Epoch [176/200] Loss: 62.96198272705078\n",
      "Epoch [177/200] Loss: 62.355796813964844\n",
      "Epoch [178/200] Loss: 61.756011962890625\n",
      "Epoch [179/200] Loss: 61.162601470947266\n",
      "Epoch [180/200] Loss: 60.575462341308594\n",
      "Epoch [181/200] Loss: 59.994598388671875\n",
      "Epoch [182/200] Loss: 59.41988754272461\n",
      "Epoch [183/200] Loss: 58.851348876953125\n",
      "Epoch [184/200] Loss: 58.28889846801758\n",
      "Epoch [185/200] Loss: 57.73248291015625\n",
      "Epoch [186/200] Loss: 57.18205261230469\n",
      "Epoch [187/200] Loss: 56.63753890991211\n",
      "Epoch [188/200] Loss: 56.09891891479492\n",
      "Epoch [189/200] Loss: 55.566123962402344\n",
      "Epoch [190/200] Loss: 55.03913879394531\n",
      "Epoch [191/200] Loss: 54.51785659790039\n",
      "Epoch [192/200] Loss: 54.00224685668945\n",
      "Epoch [193/200] Loss: 53.4922981262207\n",
      "Epoch [194/200] Loss: 52.987911224365234\n",
      "Epoch [195/200] Loss: 52.48906707763672\n",
      "Epoch [196/200] Loss: 51.99570083618164\n",
      "Epoch [197/200] Loss: 51.507781982421875\n",
      "Epoch [198/200] Loss: 51.025245666503906\n",
      "Epoch [199/200] Loss: 50.54805374145508\n",
      "Epoch [200/200] Loss: 50.07615280151367\n",
      "Predicted days_remaining for parent_id 391: 14.819400787353516\n",
      "Training for parent_id 392...\n",
      "Epoch [1/200] Loss: 140.24591064453125\n",
      "Epoch [2/200] Loss: 135.21817016601562\n",
      "Epoch [3/200] Loss: 130.31907653808594\n",
      "Epoch [4/200] Loss: 125.55738067626953\n",
      "Epoch [5/200] Loss: 120.94577026367188\n",
      "Epoch [6/200] Loss: 116.49547576904297\n",
      "Epoch [7/200] Loss: 112.20830535888672\n",
      "Epoch [8/200] Loss: 108.07942199707031\n",
      "Epoch [9/200] Loss: 104.10342407226562\n",
      "Epoch [10/200] Loss: 100.27760314941406\n",
      "Epoch [11/200] Loss: 96.60160064697266\n",
      "Epoch [12/200] Loss: 93.07584381103516\n",
      "Epoch [13/200] Loss: 89.70044708251953\n",
      "Epoch [14/200] Loss: 86.47486877441406\n",
      "Epoch [15/200] Loss: 83.3979263305664\n",
      "Epoch [16/200] Loss: 80.46795654296875\n",
      "Epoch [17/200] Loss: 77.68277740478516\n",
      "Epoch [18/200] Loss: 75.0397720336914\n",
      "Epoch [19/200] Loss: 72.53569793701172\n",
      "Epoch [20/200] Loss: 70.16651153564453\n",
      "Epoch [21/200] Loss: 67.9273681640625\n",
      "Epoch [22/200] Loss: 65.81253051757812\n",
      "Epoch [23/200] Loss: 63.8156852722168\n",
      "Epoch [24/200] Loss: 61.93000030517578\n",
      "Epoch [25/200] Loss: 60.14848327636719\n",
      "Epoch [26/200] Loss: 58.46405792236328\n",
      "Epoch [27/200] Loss: 56.869747161865234\n",
      "Epoch [28/200] Loss: 55.358882904052734\n",
      "Epoch [29/200] Loss: 53.925132751464844\n",
      "Epoch [30/200] Loss: 52.562591552734375\n",
      "Epoch [31/200] Loss: 51.26588439941406\n",
      "Epoch [32/200] Loss: 50.030029296875\n",
      "Epoch [33/200] Loss: 48.85053253173828\n",
      "Epoch [34/200] Loss: 47.723262786865234\n",
      "Epoch [35/200] Loss: 46.644439697265625\n",
      "Epoch [36/200] Loss: 45.61050796508789\n",
      "Epoch [37/200] Loss: 44.61824035644531\n",
      "Epoch [38/200] Loss: 43.66462326049805\n",
      "Epoch [39/200] Loss: 42.74689865112305\n",
      "Epoch [40/200] Loss: 41.86254119873047\n",
      "Epoch [41/200] Loss: 41.009307861328125\n",
      "Epoch [42/200] Loss: 40.18514633178711\n",
      "Epoch [43/200] Loss: 39.388275146484375\n",
      "Epoch [44/200] Loss: 38.617069244384766\n",
      "Epoch [45/200] Loss: 37.87013244628906\n",
      "Epoch [46/200] Loss: 37.146217346191406\n",
      "Epoch [47/200] Loss: 36.44417953491211\n",
      "Epoch [48/200] Loss: 35.76303482055664\n",
      "Epoch [49/200] Loss: 35.10189437866211\n",
      "Epoch [50/200] Loss: 34.4599494934082\n",
      "Epoch [51/200] Loss: 33.836490631103516\n",
      "Epoch [52/200] Loss: 33.23085021972656\n",
      "Epoch [53/200] Loss: 32.64242935180664\n",
      "Epoch [54/200] Loss: 32.070682525634766\n",
      "Epoch [55/200] Loss: 31.515106201171875\n",
      "Epoch [56/200] Loss: 30.975200653076172\n",
      "Epoch [57/200] Loss: 30.450519561767578\n",
      "Epoch [58/200] Loss: 29.940628051757812\n",
      "Epoch [59/200] Loss: 29.445119857788086\n",
      "Epoch [60/200] Loss: 28.96361541748047\n",
      "Epoch [61/200] Loss: 28.49571418762207\n",
      "Epoch [62/200] Loss: 28.041078567504883\n",
      "Epoch [63/200] Loss: 27.599342346191406\n",
      "Epoch [64/200] Loss: 27.170175552368164\n",
      "Epoch [65/200] Loss: 26.753250122070312\n",
      "Epoch [66/200] Loss: 26.348241806030273\n",
      "Epoch [67/200] Loss: 25.954856872558594\n",
      "Epoch [68/200] Loss: 25.57278823852539\n",
      "Epoch [69/200] Loss: 25.201753616333008\n",
      "Epoch [70/200] Loss: 24.841463088989258\n",
      "Epoch [71/200] Loss: 24.491641998291016\n",
      "Epoch [72/200] Loss: 24.15203285217285\n",
      "Epoch [73/200] Loss: 23.82238006591797\n",
      "Epoch [74/200] Loss: 23.502422332763672\n",
      "Epoch [75/200] Loss: 23.191917419433594\n",
      "Epoch [76/200] Loss: 22.890625\n",
      "Epoch [77/200] Loss: 22.59831428527832\n",
      "Epoch [78/200] Loss: 22.31475257873535\n",
      "Epoch [79/200] Loss: 22.039722442626953\n",
      "Epoch [80/200] Loss: 21.773008346557617\n",
      "Epoch [81/200] Loss: 21.514394760131836\n",
      "Epoch [82/200] Loss: 21.263662338256836\n",
      "Epoch [83/200] Loss: 21.02063751220703\n",
      "Epoch [84/200] Loss: 20.78509521484375\n",
      "Epoch [85/200] Loss: 20.556854248046875\n",
      "Epoch [86/200] Loss: 20.33572006225586\n",
      "Epoch [87/200] Loss: 20.121519088745117\n",
      "Epoch [88/200] Loss: 19.9140567779541\n",
      "Epoch [89/200] Loss: 19.713157653808594\n",
      "Epoch [90/200] Loss: 19.51865005493164\n",
      "Epoch [91/200] Loss: 19.33037567138672\n",
      "Epoch [92/200] Loss: 19.148151397705078\n",
      "Epoch [93/200] Loss: 18.971820831298828\n",
      "Epoch [94/200] Loss: 18.80122947692871\n",
      "Epoch [95/200] Loss: 18.636226654052734\n",
      "Epoch [96/200] Loss: 18.476640701293945\n",
      "Epoch [97/200] Loss: 18.322343826293945\n",
      "Epoch [98/200] Loss: 18.173179626464844\n",
      "Epoch [99/200] Loss: 18.029006958007812\n",
      "Epoch [100/200] Loss: 17.889692306518555\n",
      "Epoch [101/200] Loss: 17.755088806152344\n",
      "Epoch [102/200] Loss: 17.625064849853516\n",
      "Epoch [103/200] Loss: 17.4995059967041\n",
      "Epoch [104/200] Loss: 17.37826919555664\n",
      "Epoch [105/200] Loss: 17.261234283447266\n",
      "Epoch [106/200] Loss: 17.148273468017578\n",
      "Epoch [107/200] Loss: 17.03927993774414\n",
      "Epoch [108/200] Loss: 16.934131622314453\n",
      "Epoch [109/200] Loss: 16.832712173461914\n",
      "Epoch [110/200] Loss: 16.734912872314453\n",
      "Epoch [111/200] Loss: 16.640623092651367\n",
      "Epoch [112/200] Loss: 16.549739837646484\n",
      "Epoch [113/200] Loss: 16.462160110473633\n",
      "Epoch [114/200] Loss: 16.377784729003906\n",
      "Epoch [115/200] Loss: 16.296506881713867\n",
      "Epoch [116/200] Loss: 16.218233108520508\n",
      "Epoch [117/200] Loss: 16.142873764038086\n",
      "Epoch [118/200] Loss: 16.070337295532227\n",
      "Epoch [119/200] Loss: 16.000530242919922\n",
      "Epoch [120/200] Loss: 15.933361053466797\n",
      "Epoch [121/200] Loss: 15.868753433227539\n",
      "Epoch [122/200] Loss: 15.806623458862305\n",
      "Epoch [123/200] Loss: 15.746889114379883\n",
      "Epoch [124/200] Loss: 15.689470291137695\n",
      "Epoch [125/200] Loss: 15.63429069519043\n",
      "Epoch [126/200] Loss: 15.581274032592773\n",
      "Epoch [127/200] Loss: 15.530354499816895\n",
      "Epoch [128/200] Loss: 15.481451034545898\n",
      "Epoch [129/200] Loss: 15.434505462646484\n",
      "Epoch [130/200] Loss: 15.389443397521973\n",
      "Epoch [131/200] Loss: 15.346200942993164\n",
      "Epoch [132/200] Loss: 15.304717063903809\n",
      "Epoch [133/200] Loss: 15.264927864074707\n",
      "Epoch [134/200] Loss: 15.226773262023926\n",
      "Epoch [135/200] Loss: 15.190196990966797\n",
      "Epoch [136/200] Loss: 15.155146598815918\n",
      "Epoch [137/200] Loss: 15.121557235717773\n",
      "Epoch [138/200] Loss: 15.089381217956543\n",
      "Epoch [139/200] Loss: 15.05856704711914\n",
      "Epoch [140/200] Loss: 15.02906608581543\n",
      "Epoch [141/200] Loss: 15.000828742980957\n",
      "Epoch [142/200] Loss: 14.97380542755127\n",
      "Epoch [143/200] Loss: 14.94795036315918\n",
      "Epoch [144/200] Loss: 14.923221588134766\n",
      "Epoch [145/200] Loss: 14.899578094482422\n",
      "Epoch [146/200] Loss: 14.876974105834961\n",
      "Epoch [147/200] Loss: 14.855367660522461\n",
      "Epoch [148/200] Loss: 14.834726333618164\n",
      "Epoch [149/200] Loss: 14.815010070800781\n",
      "Epoch [150/200] Loss: 14.796180725097656\n",
      "Epoch [151/200] Loss: 14.778203964233398\n",
      "Epoch [152/200] Loss: 14.761043548583984\n",
      "Epoch [153/200] Loss: 14.744672775268555\n",
      "Epoch [154/200] Loss: 14.729053497314453\n",
      "Epoch [155/200] Loss: 14.714156150817871\n",
      "Epoch [156/200] Loss: 14.699954986572266\n",
      "Epoch [157/200] Loss: 14.686416625976562\n",
      "Epoch [158/200] Loss: 14.673515319824219\n",
      "Epoch [159/200] Loss: 14.661225318908691\n",
      "Epoch [160/200] Loss: 14.649518966674805\n",
      "Epoch [161/200] Loss: 14.638373374938965\n",
      "Epoch [162/200] Loss: 14.627763748168945\n",
      "Epoch [163/200] Loss: 14.617663383483887\n",
      "Epoch [164/200] Loss: 14.60805892944336\n",
      "Epoch [165/200] Loss: 14.598921775817871\n",
      "Epoch [166/200] Loss: 14.590232849121094\n",
      "Epoch [167/200] Loss: 14.581974029541016\n",
      "Epoch [168/200] Loss: 14.574124336242676\n",
      "Epoch [169/200] Loss: 14.566666603088379\n",
      "Epoch [170/200] Loss: 14.559584617614746\n",
      "Epoch [171/200] Loss: 14.552857398986816\n",
      "Epoch [172/200] Loss: 14.546472549438477\n",
      "Epoch [173/200] Loss: 14.540410995483398\n",
      "Epoch [174/200] Loss: 14.534662246704102\n",
      "Epoch [175/200] Loss: 14.52920913696289\n",
      "Epoch [176/200] Loss: 14.524038314819336\n",
      "Epoch [177/200] Loss: 14.519135475158691\n",
      "Epoch [178/200] Loss: 14.514490127563477\n",
      "Epoch [179/200] Loss: 14.510087966918945\n",
      "Epoch [180/200] Loss: 14.505918502807617\n",
      "Epoch [181/200] Loss: 14.501972198486328\n",
      "Epoch [182/200] Loss: 14.498235702514648\n",
      "Epoch [183/200] Loss: 14.494699478149414\n",
      "Epoch [184/200] Loss: 14.491352081298828\n",
      "Epoch [185/200] Loss: 14.488187789916992\n",
      "Epoch [186/200] Loss: 14.485197067260742\n",
      "Epoch [187/200] Loss: 14.482368469238281\n",
      "Epoch [188/200] Loss: 14.479696273803711\n",
      "Epoch [189/200] Loss: 14.477170944213867\n",
      "Epoch [190/200] Loss: 14.474786758422852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/200] Loss: 14.472535133361816\n",
      "Epoch [192/200] Loss: 14.470409393310547\n",
      "Epoch [193/200] Loss: 14.468404769897461\n",
      "Epoch [194/200] Loss: 14.466512680053711\n",
      "Epoch [195/200] Loss: 14.464729309082031\n",
      "Epoch [196/200] Loss: 14.46304702758789\n",
      "Epoch [197/200] Loss: 14.46146297454834\n",
      "Epoch [198/200] Loss: 14.459969520568848\n",
      "Epoch [199/200] Loss: 14.458562850952148\n",
      "Epoch [200/200] Loss: 14.45723819732666\n",
      "Predicted days_remaining for parent_id 392: 10.614017486572266\n",
      "Training for parent_id 393...\n",
      "Epoch [1/200] Loss: 397.9051208496094\n",
      "Epoch [2/200] Loss: 388.02618408203125\n",
      "Epoch [3/200] Loss: 378.4364318847656\n",
      "Epoch [4/200] Loss: 369.1459045410156\n",
      "Epoch [5/200] Loss: 360.1558837890625\n",
      "Epoch [6/200] Loss: 351.4505615234375\n",
      "Epoch [7/200] Loss: 343.0196228027344\n",
      "Epoch [8/200] Loss: 334.8596496582031\n",
      "Epoch [9/200] Loss: 326.97021484375\n",
      "Epoch [10/200] Loss: 319.35369873046875\n",
      "Epoch [11/200] Loss: 312.0159912109375\n",
      "Epoch [12/200] Loss: 304.9657287597656\n",
      "Epoch [13/200] Loss: 298.2127685546875\n",
      "Epoch [14/200] Loss: 291.7663269042969\n",
      "Epoch [15/200] Loss: 285.6331787109375\n",
      "Epoch [16/200] Loss: 279.8158264160156\n",
      "Epoch [17/200] Loss: 274.3114929199219\n",
      "Epoch [18/200] Loss: 269.1119689941406\n",
      "Epoch [19/200] Loss: 264.2046813964844\n",
      "Epoch [20/200] Loss: 259.5737609863281\n",
      "Epoch [21/200] Loss: 255.2012939453125\n",
      "Epoch [22/200] Loss: 251.0679473876953\n",
      "Epoch [23/200] Loss: 247.15415954589844\n",
      "Epoch [24/200] Loss: 243.44044494628906\n",
      "Epoch [25/200] Loss: 239.90809631347656\n",
      "Epoch [26/200] Loss: 236.53915405273438\n",
      "Epoch [27/200] Loss: 233.31689453125\n",
      "Epoch [28/200] Loss: 230.2260284423828\n",
      "Epoch [29/200] Loss: 227.2526092529297\n",
      "Epoch [30/200] Loss: 224.38438415527344\n",
      "Epoch [31/200] Loss: 221.6104736328125\n",
      "Epoch [32/200] Loss: 218.92169189453125\n",
      "Epoch [33/200] Loss: 216.31011962890625\n",
      "Epoch [34/200] Loss: 213.76910400390625\n",
      "Epoch [35/200] Loss: 211.29302978515625\n",
      "Epoch [36/200] Loss: 208.87693786621094\n",
      "Epoch [37/200] Loss: 206.51638793945312\n",
      "Epoch [38/200] Loss: 204.2069854736328\n",
      "Epoch [39/200] Loss: 201.9447479248047\n",
      "Epoch [40/200] Loss: 199.72557067871094\n",
      "Epoch [41/200] Loss: 197.54576110839844\n",
      "Epoch [42/200] Loss: 195.40203857421875\n",
      "Epoch [43/200] Loss: 193.29153442382812\n",
      "Epoch [44/200] Loss: 191.2119140625\n",
      "Epoch [45/200] Loss: 189.1613006591797\n",
      "Epoch [46/200] Loss: 187.13812255859375\n",
      "Epoch [47/200] Loss: 185.14114379882812\n",
      "Epoch [48/200] Loss: 183.16946411132812\n",
      "Epoch [49/200] Loss: 181.22213745117188\n",
      "Epoch [50/200] Loss: 179.29861450195312\n",
      "Epoch [51/200] Loss: 177.39825439453125\n",
      "Epoch [52/200] Loss: 175.5205841064453\n",
      "Epoch [53/200] Loss: 173.66506958007812\n",
      "Epoch [54/200] Loss: 171.8314208984375\n",
      "Epoch [55/200] Loss: 170.0191650390625\n",
      "Epoch [56/200] Loss: 168.2279815673828\n",
      "Epoch [57/200] Loss: 166.45745849609375\n",
      "Epoch [58/200] Loss: 164.70729064941406\n",
      "Epoch [59/200] Loss: 162.97714233398438\n",
      "Epoch [60/200] Loss: 161.2666473388672\n",
      "Epoch [61/200] Loss: 159.57553100585938\n",
      "Epoch [62/200] Loss: 157.9034423828125\n",
      "Epoch [63/200] Loss: 156.25010681152344\n",
      "Epoch [64/200] Loss: 154.6151885986328\n",
      "Epoch [65/200] Loss: 152.99847412109375\n",
      "Epoch [66/200] Loss: 151.39962768554688\n",
      "Epoch [67/200] Loss: 149.818359375\n",
      "Epoch [68/200] Loss: 148.25448608398438\n",
      "Epoch [69/200] Loss: 146.7077178955078\n",
      "Epoch [70/200] Loss: 145.17779541015625\n",
      "Epoch [71/200] Loss: 143.6644744873047\n",
      "Epoch [72/200] Loss: 142.16751098632812\n",
      "Epoch [73/200] Loss: 140.68670654296875\n",
      "Epoch [74/200] Loss: 139.2218475341797\n",
      "Epoch [75/200] Loss: 137.772705078125\n",
      "Epoch [76/200] Loss: 136.33901977539062\n",
      "Epoch [77/200] Loss: 134.92066955566406\n",
      "Epoch [78/200] Loss: 133.5173797607422\n",
      "Epoch [79/200] Loss: 132.12904357910156\n",
      "Epoch [80/200] Loss: 130.75535583496094\n",
      "Epoch [81/200] Loss: 129.396240234375\n",
      "Epoch [82/200] Loss: 128.0514373779297\n",
      "Epoch [83/200] Loss: 126.72086334228516\n",
      "Epoch [84/200] Loss: 125.40421295166016\n",
      "Epoch [85/200] Loss: 124.10142517089844\n",
      "Epoch [86/200] Loss: 122.81233215332031\n",
      "Epoch [87/200] Loss: 121.53668212890625\n",
      "Epoch [88/200] Loss: 120.27442169189453\n",
      "Epoch [89/200] Loss: 119.02530670166016\n",
      "Epoch [90/200] Loss: 117.78929138183594\n",
      "Epoch [91/200] Loss: 116.56610870361328\n",
      "Epoch [92/200] Loss: 115.355712890625\n",
      "Epoch [93/200] Loss: 114.15792846679688\n",
      "Epoch [94/200] Loss: 112.97260284423828\n",
      "Epoch [95/200] Loss: 111.79957580566406\n",
      "Epoch [96/200] Loss: 110.63880157470703\n",
      "Epoch [97/200] Loss: 109.49008178710938\n",
      "Epoch [98/200] Loss: 108.35330963134766\n",
      "Epoch [99/200] Loss: 107.22834014892578\n",
      "Epoch [100/200] Loss: 106.11505126953125\n",
      "Epoch [101/200] Loss: 105.01336669921875\n",
      "Epoch [102/200] Loss: 103.92311096191406\n",
      "Epoch [103/200] Loss: 102.84420013427734\n",
      "Epoch [104/200] Loss: 101.77649688720703\n",
      "Epoch [105/200] Loss: 100.71992492675781\n",
      "Epoch [106/200] Loss: 99.6743392944336\n",
      "Epoch [107/200] Loss: 98.63961791992188\n",
      "Epoch [108/200] Loss: 97.61573791503906\n",
      "Epoch [109/200] Loss: 96.60247802734375\n",
      "Epoch [110/200] Loss: 95.59982299804688\n",
      "Epoch [111/200] Loss: 94.60758972167969\n",
      "Epoch [112/200] Loss: 93.62577056884766\n",
      "Epoch [113/200] Loss: 92.65419006347656\n",
      "Epoch [114/200] Loss: 91.69280242919922\n",
      "Epoch [115/200] Loss: 90.74147033691406\n",
      "Epoch [116/200] Loss: 89.80012512207031\n",
      "Epoch [117/200] Loss: 88.86865997314453\n",
      "Epoch [118/200] Loss: 87.94701385498047\n",
      "Epoch [119/200] Loss: 87.03501892089844\n",
      "Epoch [120/200] Loss: 86.13265991210938\n",
      "Epoch [121/200] Loss: 85.23982238769531\n",
      "Epoch [122/200] Loss: 84.35639190673828\n",
      "Epoch [123/200] Loss: 83.4823226928711\n",
      "Epoch [124/200] Loss: 82.6175308227539\n",
      "Epoch [125/200] Loss: 81.76188659667969\n",
      "Epoch [126/200] Loss: 80.91534423828125\n",
      "Epoch [127/200] Loss: 80.07779693603516\n",
      "Epoch [128/200] Loss: 79.24921417236328\n",
      "Epoch [129/200] Loss: 78.42942810058594\n",
      "Epoch [130/200] Loss: 77.61843872070312\n",
      "Epoch [131/200] Loss: 76.81612396240234\n",
      "Epoch [132/200] Loss: 76.02241516113281\n",
      "Epoch [133/200] Loss: 75.23723602294922\n",
      "Epoch [134/200] Loss: 74.46051025390625\n",
      "Epoch [135/200] Loss: 73.6921615600586\n",
      "Epoch [136/200] Loss: 72.93209838867188\n",
      "Epoch [137/200] Loss: 72.18026733398438\n",
      "Epoch [138/200] Loss: 71.43660736083984\n",
      "Epoch [139/200] Loss: 70.7010269165039\n",
      "Epoch [140/200] Loss: 69.97343444824219\n",
      "Epoch [141/200] Loss: 69.25379943847656\n",
      "Epoch [142/200] Loss: 68.54199981689453\n",
      "Epoch [143/200] Loss: 67.83802032470703\n",
      "Epoch [144/200] Loss: 67.14175415039062\n",
      "Epoch [145/200] Loss: 66.45317077636719\n",
      "Epoch [146/200] Loss: 65.77212524414062\n",
      "Epoch [147/200] Loss: 65.09861755371094\n",
      "Epoch [148/200] Loss: 64.43256378173828\n",
      "Epoch [149/200] Loss: 63.77389144897461\n",
      "Epoch [150/200] Loss: 63.122554779052734\n",
      "Epoch [151/200] Loss: 62.47845458984375\n",
      "Epoch [152/200] Loss: 61.84152603149414\n",
      "Epoch [153/200] Loss: 61.21173858642578\n",
      "Epoch [154/200] Loss: 60.589019775390625\n",
      "Epoch [155/200] Loss: 59.9732780456543\n",
      "Epoch [156/200] Loss: 59.36448669433594\n",
      "Epoch [157/200] Loss: 58.76256561279297\n",
      "Epoch [158/200] Loss: 58.167423248291016\n",
      "Epoch [159/200] Loss: 57.57905197143555\n",
      "Epoch [160/200] Loss: 56.997352600097656\n",
      "Epoch [161/200] Loss: 56.42228317260742\n",
      "Epoch [162/200] Loss: 55.853759765625\n",
      "Epoch [163/200] Loss: 55.291778564453125\n",
      "Epoch [164/200] Loss: 54.7362060546875\n",
      "Epoch [165/200] Loss: 54.187042236328125\n",
      "Epoch [166/200] Loss: 53.64419937133789\n",
      "Epoch [167/200] Loss: 53.10761642456055\n",
      "Epoch [168/200] Loss: 52.57725524902344\n",
      "Epoch [169/200] Loss: 52.05303955078125\n",
      "Epoch [170/200] Loss: 51.53491973876953\n",
      "Epoch [171/200] Loss: 51.02284240722656\n",
      "Epoch [172/200] Loss: 50.51675033569336\n",
      "Epoch [173/200] Loss: 50.0166015625\n",
      "Epoch [174/200] Loss: 49.52231216430664\n",
      "Epoch [175/200] Loss: 49.03386306762695\n",
      "Epoch [176/200] Loss: 48.551143646240234\n",
      "Epoch [177/200] Loss: 48.07416534423828\n",
      "Epoch [178/200] Loss: 47.60283279418945\n",
      "Epoch [179/200] Loss: 47.137107849121094\n",
      "Epoch [180/200] Loss: 46.67694091796875\n",
      "Epoch [181/200] Loss: 46.22225570678711\n",
      "Epoch [182/200] Loss: 45.77302551269531\n",
      "Epoch [183/200] Loss: 45.32919692993164\n",
      "Epoch [184/200] Loss: 44.89070510864258\n",
      "Epoch [185/200] Loss: 44.45750045776367\n",
      "Epoch [186/200] Loss: 44.02954864501953\n",
      "Epoch [187/200] Loss: 43.60678482055664\n",
      "Epoch [188/200] Loss: 43.18917465209961\n",
      "Epoch [189/200] Loss: 42.77661895751953\n",
      "Epoch [190/200] Loss: 42.36914825439453\n",
      "Epoch [191/200] Loss: 41.96663284301758\n",
      "Epoch [192/200] Loss: 41.569087982177734\n",
      "Epoch [193/200] Loss: 41.176429748535156\n",
      "Epoch [194/200] Loss: 40.78862762451172\n",
      "Epoch [195/200] Loss: 40.40562438964844\n",
      "Epoch [196/200] Loss: 40.027381896972656\n",
      "Epoch [197/200] Loss: 39.65381622314453\n",
      "Epoch [198/200] Loss: 39.28492736816406\n",
      "Epoch [199/200] Loss: 38.920658111572266\n",
      "Epoch [200/200] Loss: 38.560951232910156\n",
      "Predicted days_remaining for parent_id 393: 14.874728202819824\n",
      "Training for parent_id 395...\n",
      "Epoch [1/200] Loss: 107.4838638305664\n",
      "Epoch [2/200] Loss: 103.2157211303711\n",
      "Epoch [3/200] Loss: 99.10619354248047\n",
      "Epoch [4/200] Loss: 95.15435028076172\n",
      "Epoch [5/200] Loss: 91.36019134521484\n",
      "Epoch [6/200] Loss: 87.7233657836914\n",
      "Epoch [7/200] Loss: 84.24434661865234\n",
      "Epoch [8/200] Loss: 80.9251708984375\n",
      "Epoch [9/200] Loss: 77.76812744140625\n",
      "Epoch [10/200] Loss: 74.77365112304688\n",
      "Epoch [11/200] Loss: 71.93961334228516\n",
      "Epoch [12/200] Loss: 69.2616958618164\n",
      "Epoch [13/200] Loss: 66.73438262939453\n",
      "Epoch [14/200] Loss: 64.35151672363281\n",
      "Epoch [15/200] Loss: 62.10696792602539\n",
      "Epoch [16/200] Loss: 59.99459457397461\n",
      "Epoch [17/200] Loss: 58.00810623168945\n",
      "Epoch [18/200] Loss: 56.140899658203125\n",
      "Epoch [19/200] Loss: 54.38589859008789\n",
      "Epoch [20/200] Loss: 52.7355842590332\n",
      "Epoch [21/200] Loss: 51.18224334716797\n",
      "Epoch [22/200] Loss: 49.71818542480469\n",
      "Epoch [23/200] Loss: 48.336002349853516\n",
      "Epoch [24/200] Loss: 47.028778076171875\n",
      "Epoch [25/200] Loss: 45.79017639160156\n",
      "Epoch [26/200] Loss: 44.6145133972168\n",
      "Epoch [27/200] Loss: 43.49668884277344\n",
      "Epoch [28/200] Loss: 42.43217849731445\n",
      "Epoch [29/200] Loss: 41.41693878173828\n",
      "Epoch [30/200] Loss: 40.447288513183594\n",
      "Epoch [31/200] Loss: 39.519935607910156\n",
      "Epoch [32/200] Loss: 38.63181686401367\n",
      "Epoch [33/200] Loss: 37.78016662597656\n",
      "Epoch [34/200] Loss: 36.96242904663086\n",
      "Epoch [35/200] Loss: 36.17626190185547\n",
      "Epoch [36/200] Loss: 35.419559478759766\n",
      "Epoch [37/200] Loss: 34.690391540527344\n",
      "Epoch [38/200] Loss: 33.987037658691406\n",
      "Epoch [39/200] Loss: 33.30793380737305\n",
      "Epoch [40/200] Loss: 32.65172576904297\n",
      "Epoch [41/200] Loss: 32.017173767089844\n",
      "Epoch [42/200] Loss: 31.403215408325195\n",
      "Epoch [43/200] Loss: 30.80891227722168\n",
      "Epoch [44/200] Loss: 30.233417510986328\n",
      "Epoch [45/200] Loss: 29.675987243652344\n",
      "Epoch [46/200] Loss: 29.13596534729004\n",
      "Epoch [47/200] Loss: 28.612762451171875\n",
      "Epoch [48/200] Loss: 28.10582733154297\n",
      "Epoch [49/200] Loss: 27.614654541015625\n",
      "Epoch [50/200] Loss: 27.138776779174805\n",
      "Epoch [51/200] Loss: 26.677753448486328\n",
      "Epoch [52/200] Loss: 26.231149673461914\n",
      "Epoch [53/200] Loss: 25.798568725585938\n",
      "Epoch [54/200] Loss: 25.379608154296875\n",
      "Epoch [55/200] Loss: 24.97389793395996\n",
      "Epoch [56/200] Loss: 24.581066131591797\n",
      "Epoch [57/200] Loss: 24.200756072998047\n",
      "Epoch [58/200] Loss: 23.832618713378906\n",
      "Epoch [59/200] Loss: 23.476322174072266\n",
      "Epoch [60/200] Loss: 23.131528854370117\n",
      "Epoch [61/200] Loss: 22.79792594909668\n",
      "Epoch [62/200] Loss: 22.475204467773438\n",
      "Epoch [63/200] Loss: 22.16305923461914\n",
      "Epoch [64/200] Loss: 21.861202239990234\n",
      "Epoch [65/200] Loss: 21.5693359375\n",
      "Epoch [66/200] Loss: 21.287200927734375\n",
      "Epoch [67/200] Loss: 21.014522552490234\n",
      "Epoch [68/200] Loss: 20.75103187561035\n",
      "Epoch [69/200] Loss: 20.496479034423828\n",
      "Epoch [70/200] Loss: 20.250619888305664\n",
      "Epoch [71/200] Loss: 20.013212203979492\n",
      "Epoch [72/200] Loss: 19.784021377563477\n",
      "Epoch [73/200] Loss: 19.562808990478516\n",
      "Epoch [74/200] Loss: 19.349365234375\n",
      "Epoch [75/200] Loss: 19.14346694946289\n",
      "Epoch [76/200] Loss: 18.94489288330078\n",
      "Epoch [77/200] Loss: 18.753440856933594\n",
      "Epoch [78/200] Loss: 18.568910598754883\n",
      "Epoch [79/200] Loss: 18.391098022460938\n",
      "Epoch [80/200] Loss: 18.21980857849121\n",
      "Epoch [81/200] Loss: 18.054853439331055\n",
      "Epoch [82/200] Loss: 17.896045684814453\n",
      "Epoch [83/200] Loss: 17.743202209472656\n",
      "Epoch [84/200] Loss: 17.596139907836914\n",
      "Epoch [85/200] Loss: 17.454692840576172\n",
      "Epoch [86/200] Loss: 17.318683624267578\n",
      "Epoch [87/200] Loss: 17.187944412231445\n",
      "Epoch [88/200] Loss: 17.06231117248535\n",
      "Epoch [89/200] Loss: 16.941627502441406\n",
      "Epoch [90/200] Loss: 16.825733184814453\n",
      "Epoch [91/200] Loss: 16.714473724365234\n",
      "Epoch [92/200] Loss: 16.607698440551758\n",
      "Epoch [93/200] Loss: 16.505264282226562\n",
      "Epoch [94/200] Loss: 16.407020568847656\n",
      "Epoch [95/200] Loss: 16.312835693359375\n",
      "Epoch [96/200] Loss: 16.222566604614258\n",
      "Epoch [97/200] Loss: 16.13608741760254\n",
      "Epoch [98/200] Loss: 16.05325698852539\n",
      "Epoch [99/200] Loss: 15.973956108093262\n",
      "Epoch [100/200] Loss: 15.898059844970703\n",
      "Epoch [101/200] Loss: 15.825443267822266\n",
      "Epoch [102/200] Loss: 15.755993843078613\n",
      "Epoch [103/200] Loss: 15.689594268798828\n",
      "Epoch [104/200] Loss: 15.626131057739258\n",
      "Epoch [105/200] Loss: 15.565496444702148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/200] Loss: 15.507587432861328\n",
      "Epoch [107/200] Loss: 15.452300071716309\n",
      "Epoch [108/200] Loss: 15.399531364440918\n",
      "Epoch [109/200] Loss: 15.349193572998047\n",
      "Epoch [110/200] Loss: 15.30118179321289\n",
      "Epoch [111/200] Loss: 15.255414962768555\n",
      "Epoch [112/200] Loss: 15.211791038513184\n",
      "Epoch [113/200] Loss: 15.17023754119873\n",
      "Epoch [114/200] Loss: 15.130664825439453\n",
      "Epoch [115/200] Loss: 15.09299373626709\n",
      "Epoch [116/200] Loss: 15.057145118713379\n",
      "Epoch [117/200] Loss: 15.023043632507324\n",
      "Epoch [118/200] Loss: 14.990617752075195\n",
      "Epoch [119/200] Loss: 14.959794998168945\n",
      "Epoch [120/200] Loss: 14.930505752563477\n",
      "Epoch [121/200] Loss: 14.902689933776855\n",
      "Epoch [122/200] Loss: 14.876279830932617\n",
      "Epoch [123/200] Loss: 14.851215362548828\n",
      "Epoch [124/200] Loss: 14.827430725097656\n",
      "Epoch [125/200] Loss: 14.804878234863281\n",
      "Epoch [126/200] Loss: 14.783498764038086\n",
      "Epoch [127/200] Loss: 14.763236999511719\n",
      "Epoch [128/200] Loss: 14.74404525756836\n",
      "Epoch [129/200] Loss: 14.725872039794922\n",
      "Epoch [130/200] Loss: 14.708671569824219\n",
      "Epoch [131/200] Loss: 14.692398071289062\n",
      "Epoch [132/200] Loss: 14.677003860473633\n",
      "Epoch [133/200] Loss: 14.662454605102539\n",
      "Epoch [134/200] Loss: 14.648704528808594\n",
      "Epoch [135/200] Loss: 14.63571548461914\n",
      "Epoch [136/200] Loss: 14.623449325561523\n",
      "Epoch [137/200] Loss: 14.611873626708984\n",
      "Epoch [138/200] Loss: 14.600951194763184\n",
      "Epoch [139/200] Loss: 14.59065055847168\n",
      "Epoch [140/200] Loss: 14.580941200256348\n",
      "Epoch [141/200] Loss: 14.57179069519043\n",
      "Epoch [142/200] Loss: 14.563173294067383\n",
      "Epoch [143/200] Loss: 14.555059432983398\n",
      "Epoch [144/200] Loss: 14.547420501708984\n",
      "Epoch [145/200] Loss: 14.540237426757812\n",
      "Epoch [146/200] Loss: 14.533483505249023\n",
      "Epoch [147/200] Loss: 14.527132987976074\n",
      "Epoch [148/200] Loss: 14.521167755126953\n",
      "Epoch [149/200] Loss: 14.515565872192383\n",
      "Epoch [150/200] Loss: 14.510305404663086\n",
      "Epoch [151/200] Loss: 14.50537109375\n",
      "Epoch [152/200] Loss: 14.50074577331543\n",
      "Epoch [153/200] Loss: 14.496406555175781\n",
      "Epoch [154/200] Loss: 14.492339134216309\n",
      "Epoch [155/200] Loss: 14.488531112670898\n",
      "Epoch [156/200] Loss: 14.484968185424805\n",
      "Epoch [157/200] Loss: 14.481630325317383\n",
      "Epoch [158/200] Loss: 14.478509902954102\n",
      "Epoch [159/200] Loss: 14.475592613220215\n",
      "Epoch [160/200] Loss: 14.47286605834961\n",
      "Epoch [161/200] Loss: 14.470319747924805\n",
      "Epoch [162/200] Loss: 14.467942237854004\n",
      "Epoch [163/200] Loss: 14.465723037719727\n",
      "Epoch [164/200] Loss: 14.463651657104492\n",
      "Epoch [165/200] Loss: 14.461722373962402\n",
      "Epoch [166/200] Loss: 14.459924697875977\n",
      "Epoch [167/200] Loss: 14.458248138427734\n",
      "Epoch [168/200] Loss: 14.45668888092041\n",
      "Epoch [169/200] Loss: 14.45523738861084\n",
      "Epoch [170/200] Loss: 14.453887939453125\n",
      "Epoch [171/200] Loss: 14.452632904052734\n",
      "Epoch [172/200] Loss: 14.451467514038086\n",
      "Epoch [173/200] Loss: 14.450384140014648\n",
      "Epoch [174/200] Loss: 14.449378967285156\n",
      "Epoch [175/200] Loss: 14.448446273803711\n",
      "Epoch [176/200] Loss: 14.447582244873047\n",
      "Epoch [177/200] Loss: 14.446781158447266\n",
      "Epoch [178/200] Loss: 14.446039199829102\n",
      "Epoch [179/200] Loss: 14.445350646972656\n",
      "Epoch [180/200] Loss: 14.44471549987793\n",
      "Epoch [181/200] Loss: 14.44412612915039\n",
      "Epoch [182/200] Loss: 14.443582534790039\n",
      "Epoch [183/200] Loss: 14.443078994750977\n",
      "Epoch [184/200] Loss: 14.442617416381836\n",
      "Epoch [185/200] Loss: 14.442188262939453\n",
      "Epoch [186/200] Loss: 14.441794395446777\n",
      "Epoch [187/200] Loss: 14.44143009185791\n",
      "Epoch [188/200] Loss: 14.441094398498535\n",
      "Epoch [189/200] Loss: 14.44078540802002\n",
      "Epoch [190/200] Loss: 14.440502166748047\n",
      "Epoch [191/200] Loss: 14.440239906311035\n",
      "Epoch [192/200] Loss: 14.440000534057617\n",
      "Epoch [193/200] Loss: 14.439779281616211\n",
      "Epoch [194/200] Loss: 14.439577102661133\n",
      "Epoch [195/200] Loss: 14.43939208984375\n",
      "Epoch [196/200] Loss: 14.43922233581543\n",
      "Epoch [197/200] Loss: 14.439065933227539\n",
      "Epoch [198/200] Loss: 14.438922882080078\n",
      "Epoch [199/200] Loss: 14.438793182373047\n",
      "Epoch [200/200] Loss: 14.438671112060547\n",
      "Predicted days_remaining for parent_id 395: 9.717398643493652\n",
      "Training for parent_id 404...\n",
      "Epoch [1/200] Loss: 160.51126098632812\n",
      "Epoch [2/200] Loss: 154.8740692138672\n",
      "Epoch [3/200] Loss: 149.4537353515625\n",
      "Epoch [4/200] Loss: 144.27752685546875\n",
      "Epoch [5/200] Loss: 139.36141967773438\n",
      "Epoch [6/200] Loss: 134.71214294433594\n",
      "Epoch [7/200] Loss: 130.3323516845703\n",
      "Epoch [8/200] Loss: 126.22154998779297\n",
      "Epoch [9/200] Loss: 122.3731460571289\n",
      "Epoch [10/200] Loss: 118.77361297607422\n",
      "Epoch [11/200] Loss: 115.40483093261719\n",
      "Epoch [12/200] Loss: 112.24686431884766\n",
      "Epoch [13/200] Loss: 109.28020477294922\n",
      "Epoch [14/200] Loss: 106.48654174804688\n",
      "Epoch [15/200] Loss: 103.84848022460938\n",
      "Epoch [16/200] Loss: 101.34953308105469\n",
      "Epoch [17/200] Loss: 98.97471618652344\n",
      "Epoch [18/200] Loss: 96.71133422851562\n",
      "Epoch [19/200] Loss: 94.54911804199219\n",
      "Epoch [20/200] Loss: 92.48014831542969\n",
      "Epoch [21/200] Loss: 90.49834442138672\n",
      "Epoch [22/200] Loss: 88.5987548828125\n",
      "Epoch [23/200] Loss: 86.7770004272461\n",
      "Epoch [24/200] Loss: 85.02854919433594\n",
      "Epoch [25/200] Loss: 83.3485107421875\n",
      "Epoch [26/200] Loss: 81.73179626464844\n",
      "Epoch [27/200] Loss: 80.17317199707031\n",
      "Epoch [28/200] Loss: 78.66769409179688\n",
      "Epoch [29/200] Loss: 77.21087646484375\n",
      "Epoch [30/200] Loss: 75.79872131347656\n",
      "Epoch [31/200] Loss: 74.42784881591797\n",
      "Epoch [32/200] Loss: 73.09527587890625\n",
      "Epoch [33/200] Loss: 71.7984619140625\n",
      "Epoch [34/200] Loss: 70.53514862060547\n",
      "Epoch [35/200] Loss: 69.30337524414062\n",
      "Epoch [36/200] Loss: 68.10137176513672\n",
      "Epoch [37/200] Loss: 66.9276123046875\n",
      "Epoch [38/200] Loss: 65.78067016601562\n",
      "Epoch [39/200] Loss: 64.65931701660156\n",
      "Epoch [40/200] Loss: 63.56242752075195\n",
      "Epoch [41/200] Loss: 62.48902893066406\n",
      "Epoch [42/200] Loss: 61.43820571899414\n",
      "Epoch [43/200] Loss: 60.40918731689453\n",
      "Epoch [44/200] Loss: 59.401214599609375\n",
      "Epoch [45/200] Loss: 58.41366958618164\n",
      "Epoch [46/200] Loss: 57.44597625732422\n",
      "Epoch [47/200] Loss: 56.49755859375\n",
      "Epoch [48/200] Loss: 55.56797409057617\n",
      "Epoch [49/200] Loss: 54.656776428222656\n",
      "Epoch [50/200] Loss: 53.7635383605957\n",
      "Epoch [51/200] Loss: 52.88786697387695\n",
      "Epoch [52/200] Loss: 52.02941131591797\n",
      "Epoch [53/200] Loss: 51.18781661987305\n",
      "Epoch [54/200] Loss: 50.36274719238281\n",
      "Epoch [55/200] Loss: 49.55389404296875\n",
      "Epoch [56/200] Loss: 48.76094436645508\n",
      "Epoch [57/200] Loss: 47.98359298706055\n",
      "Epoch [58/200] Loss: 47.2215690612793\n",
      "Epoch [59/200] Loss: 46.474586486816406\n",
      "Epoch [60/200] Loss: 45.74238204956055\n",
      "Epoch [61/200] Loss: 45.024696350097656\n",
      "Epoch [62/200] Loss: 44.321258544921875\n",
      "Epoch [63/200] Loss: 43.631832122802734\n",
      "Epoch [64/200] Loss: 42.9561653137207\n",
      "Epoch [65/200] Loss: 42.294036865234375\n",
      "Epoch [66/200] Loss: 41.645198822021484\n",
      "Epoch [67/200] Loss: 41.009422302246094\n",
      "Epoch [68/200] Loss: 40.3864860534668\n",
      "Epoch [69/200] Loss: 39.77615737915039\n",
      "Epoch [70/200] Loss: 39.178226470947266\n",
      "Epoch [71/200] Loss: 38.59246826171875\n",
      "Epoch [72/200] Loss: 38.0186882019043\n",
      "Epoch [73/200] Loss: 37.45665740966797\n",
      "Epoch [74/200] Loss: 36.906185150146484\n",
      "Epoch [75/200] Loss: 36.36705017089844\n",
      "Epoch [76/200] Loss: 35.839080810546875\n",
      "Epoch [77/200] Loss: 35.32206344604492\n",
      "Epoch [78/200] Loss: 34.81580352783203\n",
      "Epoch [79/200] Loss: 34.32011413574219\n",
      "Epoch [80/200] Loss: 33.83481216430664\n",
      "Epoch [81/200] Loss: 33.359718322753906\n",
      "Epoch [82/200] Loss: 32.89463806152344\n",
      "Epoch [83/200] Loss: 32.43940353393555\n",
      "Epoch [84/200] Loss: 31.993831634521484\n",
      "Epoch [85/200] Loss: 31.557758331298828\n",
      "Epoch [86/200] Loss: 31.131004333496094\n",
      "Epoch [87/200] Loss: 30.71340560913086\n",
      "Epoch [88/200] Loss: 30.304807662963867\n",
      "Epoch [89/200] Loss: 29.9050235748291\n",
      "Epoch [90/200] Loss: 29.513919830322266\n",
      "Epoch [91/200] Loss: 29.131332397460938\n",
      "Epoch [92/200] Loss: 28.757095336914062\n",
      "Epoch [93/200] Loss: 28.391061782836914\n",
      "Epoch [94/200] Loss: 28.03308868408203\n",
      "Epoch [95/200] Loss: 27.683025360107422\n",
      "Epoch [96/200] Loss: 27.340717315673828\n",
      "Epoch [97/200] Loss: 27.006023406982422\n",
      "Epoch [98/200] Loss: 26.678823471069336\n",
      "Epoch [99/200] Loss: 26.358957290649414\n",
      "Epoch [100/200] Loss: 26.046295166015625\n",
      "Epoch [101/200] Loss: 25.74069595336914\n",
      "Epoch [102/200] Loss: 25.442035675048828\n",
      "Epoch [103/200] Loss: 25.150182723999023\n",
      "Epoch [104/200] Loss: 24.865005493164062\n",
      "Epoch [105/200] Loss: 24.586387634277344\n",
      "Epoch [106/200] Loss: 24.31418228149414\n",
      "Epoch [107/200] Loss: 24.048290252685547\n",
      "Epoch [108/200] Loss: 23.78858184814453\n",
      "Epoch [109/200] Loss: 23.534934997558594\n",
      "Epoch [110/200] Loss: 23.287242889404297\n",
      "Epoch [111/200] Loss: 23.045373916625977\n",
      "Epoch [112/200] Loss: 22.80922508239746\n",
      "Epoch [113/200] Loss: 22.578693389892578\n",
      "Epoch [114/200] Loss: 22.353652954101562\n",
      "Epoch [115/200] Loss: 22.134010314941406\n",
      "Epoch [116/200] Loss: 21.919639587402344\n",
      "Epoch [117/200] Loss: 21.710447311401367\n",
      "Epoch [118/200] Loss: 21.50634002685547\n",
      "Epoch [119/200] Loss: 21.307199478149414\n",
      "Epoch [120/200] Loss: 21.112926483154297\n",
      "Epoch [121/200] Loss: 20.92344093322754\n",
      "Epoch [122/200] Loss: 20.73862075805664\n",
      "Epoch [123/200] Loss: 20.558393478393555\n",
      "Epoch [124/200] Loss: 20.38263511657715\n",
      "Epoch [125/200] Loss: 20.211284637451172\n",
      "Epoch [126/200] Loss: 20.044240951538086\n",
      "Epoch [127/200] Loss: 19.88140869140625\n",
      "Epoch [128/200] Loss: 19.722705841064453\n",
      "Epoch [129/200] Loss: 19.568042755126953\n",
      "Epoch [130/200] Loss: 19.417329788208008\n",
      "Epoch [131/200] Loss: 19.270488739013672\n",
      "Epoch [132/200] Loss: 19.12743377685547\n",
      "Epoch [133/200] Loss: 18.988086700439453\n",
      "Epoch [134/200] Loss: 18.85236930847168\n",
      "Epoch [135/200] Loss: 18.720195770263672\n",
      "Epoch [136/200] Loss: 18.591493606567383\n",
      "Epoch [137/200] Loss: 18.466184616088867\n",
      "Epoch [138/200] Loss: 18.344192504882812\n",
      "Epoch [139/200] Loss: 18.225452423095703\n",
      "Epoch [140/200] Loss: 18.10988426208496\n",
      "Epoch [141/200] Loss: 17.997417449951172\n",
      "Epoch [142/200] Loss: 17.887983322143555\n",
      "Epoch [143/200] Loss: 17.781511306762695\n",
      "Epoch [144/200] Loss: 17.677940368652344\n",
      "Epoch [145/200] Loss: 17.577194213867188\n",
      "Epoch [146/200] Loss: 17.479217529296875\n",
      "Epoch [147/200] Loss: 17.383941650390625\n",
      "Epoch [148/200] Loss: 17.291301727294922\n",
      "Epoch [149/200] Loss: 17.201244354248047\n",
      "Epoch [150/200] Loss: 17.11370086669922\n",
      "Epoch [151/200] Loss: 17.028610229492188\n",
      "Epoch [152/200] Loss: 16.945920944213867\n",
      "Epoch [153/200] Loss: 16.865571975708008\n",
      "Epoch [154/200] Loss: 16.787511825561523\n",
      "Epoch [155/200] Loss: 16.71167755126953\n",
      "Epoch [156/200] Loss: 16.638015747070312\n",
      "Epoch [157/200] Loss: 16.566482543945312\n",
      "Epoch [158/200] Loss: 16.49701499938965\n",
      "Epoch [159/200] Loss: 16.429569244384766\n",
      "Epoch [160/200] Loss: 16.364093780517578\n",
      "Epoch [161/200] Loss: 16.3005313873291\n",
      "Epoch [162/200] Loss: 16.238845825195312\n",
      "Epoch [163/200] Loss: 16.178985595703125\n",
      "Epoch [164/200] Loss: 16.120899200439453\n",
      "Epoch [165/200] Loss: 16.064552307128906\n",
      "Epoch [166/200] Loss: 16.009889602661133\n",
      "Epoch [167/200] Loss: 15.956869125366211\n",
      "Epoch [168/200] Loss: 15.905452728271484\n",
      "Epoch [169/200] Loss: 15.855598449707031\n",
      "Epoch [170/200] Loss: 15.807262420654297\n",
      "Epoch [171/200] Loss: 15.760404586791992\n",
      "Epoch [172/200] Loss: 15.714991569519043\n",
      "Epoch [173/200] Loss: 15.670974731445312\n",
      "Epoch [174/200] Loss: 15.628328323364258\n",
      "Epoch [175/200] Loss: 15.587006568908691\n",
      "Epoch [176/200] Loss: 15.546977043151855\n",
      "Epoch [177/200] Loss: 15.508203506469727\n",
      "Epoch [178/200] Loss: 15.470653533935547\n",
      "Epoch [179/200] Loss: 15.434289932250977\n",
      "Epoch [180/200] Loss: 15.399084091186523\n",
      "Epoch [181/200] Loss: 15.365001678466797\n",
      "Epoch [182/200] Loss: 15.332014083862305\n",
      "Epoch [183/200] Loss: 15.300085067749023\n",
      "Epoch [184/200] Loss: 15.26919174194336\n",
      "Epoch [185/200] Loss: 15.239294052124023\n",
      "Epoch [186/200] Loss: 15.21037769317627\n",
      "Epoch [187/200] Loss: 15.182405471801758\n",
      "Epoch [188/200] Loss: 15.155349731445312\n",
      "Epoch [189/200] Loss: 15.129189491271973\n",
      "Epoch [190/200] Loss: 15.103893280029297\n",
      "Epoch [191/200] Loss: 15.079442024230957\n",
      "Epoch [192/200] Loss: 15.055803298950195\n",
      "Epoch [193/200] Loss: 15.03295612335205\n",
      "Epoch [194/200] Loss: 15.010879516601562\n",
      "Epoch [195/200] Loss: 14.989551544189453\n",
      "Epoch [196/200] Loss: 14.968945503234863\n",
      "Epoch [197/200] Loss: 14.949041366577148\n",
      "Epoch [198/200] Loss: 14.929814338684082\n",
      "Epoch [199/200] Loss: 14.911252975463867\n",
      "Epoch [200/200] Loss: 14.893327713012695\n",
      "Predicted days_remaining for parent_id 404: 11.087787628173828\n",
      "Training for parent_id 406...\n",
      "Epoch [1/200] Loss: 514.1841430664062\n",
      "Epoch [2/200] Loss: 503.49298095703125\n",
      "Epoch [3/200] Loss: 493.21856689453125\n",
      "Epoch [4/200] Loss: 483.38214111328125\n",
      "Epoch [5/200] Loss: 474.0137939453125\n",
      "Epoch [6/200] Loss: 465.1177978515625\n",
      "Epoch [7/200] Loss: 456.678955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] Loss: 448.666748046875\n",
      "Epoch [9/200] Loss: 441.0398254394531\n",
      "Epoch [10/200] Loss: 433.7540588378906\n",
      "Epoch [11/200] Loss: 426.7705993652344\n",
      "Epoch [12/200] Loss: 420.05694580078125\n",
      "Epoch [13/200] Loss: 413.5873718261719\n",
      "Epoch [14/200] Loss: 407.3426513671875\n",
      "Epoch [15/200] Loss: 401.31048583984375\n",
      "Epoch [16/200] Loss: 395.48443603515625\n",
      "Epoch [17/200] Loss: 389.86285400390625\n",
      "Epoch [18/200] Loss: 384.446533203125\n",
      "Epoch [19/200] Loss: 379.2364501953125\n",
      "Epoch [20/200] Loss: 374.23138427734375\n",
      "Epoch [21/200] Loss: 369.42596435546875\n",
      "Epoch [22/200] Loss: 364.81072998046875\n",
      "Epoch [23/200] Loss: 360.3734436035156\n",
      "Epoch [24/200] Loss: 356.1009521484375\n",
      "Epoch [25/200] Loss: 351.9801025390625\n",
      "Epoch [26/200] Loss: 347.998779296875\n",
      "Epoch [27/200] Loss: 344.146484375\n",
      "Epoch [28/200] Loss: 340.41363525390625\n",
      "Epoch [29/200] Loss: 336.7920837402344\n",
      "Epoch [30/200] Loss: 333.2742919921875\n",
      "Epoch [31/200] Loss: 329.853759765625\n",
      "Epoch [32/200] Loss: 326.52398681640625\n",
      "Epoch [33/200] Loss: 323.27880859375\n",
      "Epoch [34/200] Loss: 320.11224365234375\n",
      "Epoch [35/200] Loss: 317.0184020996094\n",
      "Epoch [36/200] Loss: 313.99163818359375\n",
      "Epoch [37/200] Loss: 311.02630615234375\n",
      "Epoch [38/200] Loss: 308.1176452636719\n",
      "Epoch [39/200] Loss: 305.2609558105469\n",
      "Epoch [40/200] Loss: 302.4524841308594\n",
      "Epoch [41/200] Loss: 299.688720703125\n",
      "Epoch [42/200] Loss: 296.9667053222656\n",
      "Epoch [43/200] Loss: 294.28411865234375\n",
      "Epoch [44/200] Loss: 291.638916015625\n",
      "Epoch [45/200] Loss: 289.0291442871094\n",
      "Epoch [46/200] Loss: 286.45343017578125\n",
      "Epoch [47/200] Loss: 283.9104309082031\n",
      "Epoch [48/200] Loss: 281.3988952636719\n",
      "Epoch [49/200] Loss: 278.917724609375\n",
      "Epoch [50/200] Loss: 276.4659729003906\n",
      "Epoch [51/200] Loss: 274.04254150390625\n",
      "Epoch [52/200] Loss: 271.64666748046875\n",
      "Epoch [53/200] Loss: 269.2774353027344\n",
      "Epoch [54/200] Loss: 266.93408203125\n",
      "Epoch [55/200] Loss: 264.6158447265625\n",
      "Epoch [56/200] Loss: 262.32196044921875\n",
      "Epoch [57/200] Loss: 260.0517883300781\n",
      "Epoch [58/200] Loss: 257.8048095703125\n",
      "Epoch [59/200] Loss: 255.5803680419922\n",
      "Epoch [60/200] Loss: 253.37796020507812\n",
      "Epoch [61/200] Loss: 251.19711303710938\n",
      "Epoch [62/200] Loss: 249.037353515625\n",
      "Epoch [63/200] Loss: 246.89822387695312\n",
      "Epoch [64/200] Loss: 244.7793426513672\n",
      "Epoch [65/200] Loss: 242.68028259277344\n",
      "Epoch [66/200] Loss: 240.6007537841797\n",
      "Epoch [67/200] Loss: 238.54037475585938\n",
      "Epoch [68/200] Loss: 236.4988250732422\n",
      "Epoch [69/200] Loss: 234.4757537841797\n",
      "Epoch [70/200] Loss: 232.4709930419922\n",
      "Epoch [71/200] Loss: 230.48411560058594\n",
      "Epoch [72/200] Loss: 228.51495361328125\n",
      "Epoch [73/200] Loss: 226.563232421875\n",
      "Epoch [74/200] Loss: 224.6285858154297\n",
      "Epoch [75/200] Loss: 222.71092224121094\n",
      "Epoch [76/200] Loss: 220.80996704101562\n",
      "Epoch [77/200] Loss: 218.92547607421875\n",
      "Epoch [78/200] Loss: 217.05723571777344\n",
      "Epoch [79/200] Loss: 215.20506286621094\n",
      "Epoch [80/200] Loss: 213.36875915527344\n",
      "Epoch [81/200] Loss: 211.54808044433594\n",
      "Epoch [82/200] Loss: 209.7428741455078\n",
      "Epoch [83/200] Loss: 207.95297241210938\n",
      "Epoch [84/200] Loss: 206.1781768798828\n",
      "Epoch [85/200] Loss: 204.41827392578125\n",
      "Epoch [86/200] Loss: 202.67311096191406\n",
      "Epoch [87/200] Loss: 200.9425506591797\n",
      "Epoch [88/200] Loss: 199.22645568847656\n",
      "Epoch [89/200] Loss: 197.52464294433594\n",
      "Epoch [90/200] Loss: 195.83694458007812\n",
      "Epoch [91/200] Loss: 194.16322326660156\n",
      "Epoch [92/200] Loss: 192.50338745117188\n",
      "Epoch [93/200] Loss: 190.85714721679688\n",
      "Epoch [94/200] Loss: 189.2245330810547\n",
      "Epoch [95/200] Loss: 187.60531616210938\n",
      "Epoch [96/200] Loss: 185.99940490722656\n",
      "Epoch [97/200] Loss: 184.4066162109375\n",
      "Epoch [98/200] Loss: 182.826904296875\n",
      "Epoch [99/200] Loss: 181.26010131835938\n",
      "Epoch [100/200] Loss: 179.70608520507812\n",
      "Epoch [101/200] Loss: 178.16473388671875\n",
      "Epoch [102/200] Loss: 176.63595581054688\n",
      "Epoch [103/200] Loss: 175.11959838867188\n",
      "Epoch [104/200] Loss: 173.61563110351562\n",
      "Epoch [105/200] Loss: 172.1238555908203\n",
      "Epoch [106/200] Loss: 170.64422607421875\n",
      "Epoch [107/200] Loss: 169.1765594482422\n",
      "Epoch [108/200] Loss: 167.7208251953125\n",
      "Epoch [109/200] Loss: 166.27691650390625\n",
      "Epoch [110/200] Loss: 164.84469604492188\n",
      "Epoch [111/200] Loss: 163.42413330078125\n",
      "Epoch [112/200] Loss: 162.0150909423828\n",
      "Epoch [113/200] Loss: 160.61741638183594\n",
      "Epoch [114/200] Loss: 159.2311248779297\n",
      "Epoch [115/200] Loss: 157.85604858398438\n",
      "Epoch [116/200] Loss: 156.4921417236328\n",
      "Epoch [117/200] Loss: 155.13931274414062\n",
      "Epoch [118/200] Loss: 153.79747009277344\n",
      "Epoch [119/200] Loss: 152.46646118164062\n",
      "Epoch [120/200] Loss: 151.14630126953125\n",
      "Epoch [121/200] Loss: 149.83685302734375\n",
      "Epoch [122/200] Loss: 148.53807067871094\n",
      "Epoch [123/200] Loss: 147.2498321533203\n",
      "Epoch [124/200] Loss: 145.9720916748047\n",
      "Epoch [125/200] Loss: 144.7047576904297\n",
      "Epoch [126/200] Loss: 143.4477081298828\n",
      "Epoch [127/200] Loss: 142.20091247558594\n",
      "Epoch [128/200] Loss: 140.9643096923828\n",
      "Epoch [129/200] Loss: 139.73782348632812\n",
      "Epoch [130/200] Loss: 138.52133178710938\n",
      "Epoch [131/200] Loss: 137.31480407714844\n",
      "Epoch [132/200] Loss: 136.11813354492188\n",
      "Epoch [133/200] Loss: 134.93125915527344\n",
      "Epoch [134/200] Loss: 133.75413513183594\n",
      "Epoch [135/200] Loss: 132.58670043945312\n",
      "Epoch [136/200] Loss: 131.42884826660156\n",
      "Epoch [137/200] Loss: 130.28050231933594\n",
      "Epoch [138/200] Loss: 129.14163208007812\n",
      "Epoch [139/200] Loss: 128.01217651367188\n",
      "Epoch [140/200] Loss: 126.8919906616211\n",
      "Epoch [141/200] Loss: 125.7811050415039\n",
      "Epoch [142/200] Loss: 124.67941284179688\n",
      "Epoch [143/200] Loss: 123.58682250976562\n",
      "Epoch [144/200] Loss: 122.50330352783203\n",
      "Epoch [145/200] Loss: 121.42882537841797\n",
      "Epoch [146/200] Loss: 120.36327362060547\n",
      "Epoch [147/200] Loss: 119.30657196044922\n",
      "Epoch [148/200] Loss: 118.25869750976562\n",
      "Epoch [149/200] Loss: 117.21958923339844\n",
      "Epoch [150/200] Loss: 116.1891860961914\n",
      "Epoch [151/200] Loss: 115.16739654541016\n",
      "Epoch [152/200] Loss: 114.15421295166016\n",
      "Epoch [153/200] Loss: 113.14949798583984\n",
      "Epoch [154/200] Loss: 112.15325927734375\n",
      "Epoch [155/200] Loss: 111.16542053222656\n",
      "Epoch [156/200] Loss: 110.18592834472656\n",
      "Epoch [157/200] Loss: 109.2147216796875\n",
      "Epoch [158/200] Loss: 108.251708984375\n",
      "Epoch [159/200] Loss: 107.29690551757812\n",
      "Epoch [160/200] Loss: 106.35018157958984\n",
      "Epoch [161/200] Loss: 105.41154479980469\n",
      "Epoch [162/200] Loss: 104.48088073730469\n",
      "Epoch [163/200] Loss: 103.55818176269531\n",
      "Epoch [164/200] Loss: 102.64338684082031\n",
      "Epoch [165/200] Loss: 101.73641204833984\n",
      "Epoch [166/200] Loss: 100.83726501464844\n",
      "Epoch [167/200] Loss: 99.94578552246094\n",
      "Epoch [168/200] Loss: 99.0619888305664\n",
      "Epoch [169/200] Loss: 98.18585205078125\n",
      "Epoch [170/200] Loss: 97.31726837158203\n",
      "Epoch [171/200] Loss: 96.45620727539062\n",
      "Epoch [172/200] Loss: 95.60265350341797\n",
      "Epoch [173/200] Loss: 94.75648498535156\n",
      "Epoch [174/200] Loss: 93.91767883300781\n",
      "Epoch [175/200] Loss: 93.086181640625\n",
      "Epoch [176/200] Loss: 92.2619857788086\n",
      "Epoch [177/200] Loss: 91.44496154785156\n",
      "Epoch [178/200] Loss: 90.63513946533203\n",
      "Epoch [179/200] Loss: 89.8323974609375\n",
      "Epoch [180/200] Loss: 89.0367660522461\n",
      "Epoch [181/200] Loss: 88.24812316894531\n",
      "Epoch [182/200] Loss: 87.46643829345703\n",
      "Epoch [183/200] Loss: 86.69170379638672\n",
      "Epoch [184/200] Loss: 85.923828125\n",
      "Epoch [185/200] Loss: 85.16278076171875\n",
      "Epoch [186/200] Loss: 84.40852355957031\n",
      "Epoch [187/200] Loss: 83.6609878540039\n",
      "Epoch [188/200] Loss: 82.92012023925781\n",
      "Epoch [189/200] Loss: 82.18590545654297\n",
      "Epoch [190/200] Loss: 81.45825958251953\n",
      "Epoch [191/200] Loss: 80.7371826171875\n",
      "Epoch [192/200] Loss: 80.0226058959961\n",
      "Epoch [193/200] Loss: 79.31443786621094\n",
      "Epoch [194/200] Loss: 78.61273193359375\n",
      "Epoch [195/200] Loss: 77.91734313964844\n",
      "Epoch [196/200] Loss: 77.22828674316406\n",
      "Epoch [197/200] Loss: 76.54551696777344\n",
      "Epoch [198/200] Loss: 75.86897277832031\n",
      "Epoch [199/200] Loss: 75.1985855102539\n",
      "Epoch [200/200] Loss: 74.53434753417969\n",
      "Predicted days_remaining for parent_id 406: 15.040349960327148\n",
      "Training for parent_id 409...\n",
      "Epoch [1/200] Loss: 110.85254669189453\n",
      "Epoch [2/200] Loss: 107.32989501953125\n",
      "Epoch [3/200] Loss: 103.96075439453125\n",
      "Epoch [4/200] Loss: 100.73507690429688\n",
      "Epoch [5/200] Loss: 97.63609313964844\n",
      "Epoch [6/200] Loss: 94.64275360107422\n",
      "Epoch [7/200] Loss: 91.73188781738281\n",
      "Epoch [8/200] Loss: 88.8837890625\n",
      "Epoch [9/200] Loss: 86.08657836914062\n",
      "Epoch [10/200] Loss: 83.33637237548828\n",
      "Epoch [11/200] Loss: 80.63542938232422\n",
      "Epoch [12/200] Loss: 77.98981475830078\n",
      "Epoch [13/200] Loss: 75.40754699707031\n",
      "Epoch [14/200] Loss: 72.8970718383789\n",
      "Epoch [15/200] Loss: 70.46636962890625\n",
      "Epoch [16/200] Loss: 68.12218475341797\n",
      "Epoch [17/200] Loss: 65.86978149414062\n",
      "Epoch [18/200] Loss: 63.71278762817383\n",
      "Epoch [19/200] Loss: 61.65327835083008\n",
      "Epoch [20/200] Loss: 59.69184494018555\n",
      "Epoch [21/200] Loss: 57.827728271484375\n",
      "Epoch [22/200] Loss: 56.058990478515625\n",
      "Epoch [23/200] Loss: 54.38261032104492\n",
      "Epoch [24/200] Loss: 52.7947998046875\n",
      "Epoch [25/200] Loss: 51.291080474853516\n",
      "Epoch [26/200] Loss: 49.86660385131836\n",
      "Epoch [27/200] Loss: 48.516326904296875\n",
      "Epoch [28/200] Loss: 47.235164642333984\n",
      "Epoch [29/200] Loss: 46.01813507080078\n",
      "Epoch [30/200] Loss: 44.86042022705078\n",
      "Epoch [31/200] Loss: 43.75746536254883\n",
      "Epoch [32/200] Loss: 42.70502853393555\n",
      "Epoch [33/200] Loss: 41.699180603027344\n",
      "Epoch [34/200] Loss: 40.73634719848633\n",
      "Epoch [35/200] Loss: 39.81331253051758\n",
      "Epoch [36/200] Loss: 38.927207946777344\n",
      "Epoch [37/200] Loss: 38.075496673583984\n",
      "Epoch [38/200] Loss: 37.25590515136719\n",
      "Epoch [39/200] Loss: 36.466468811035156\n",
      "Epoch [40/200] Loss: 35.705421447753906\n",
      "Epoch [41/200] Loss: 34.971214294433594\n",
      "Epoch [42/200] Loss: 34.262451171875\n",
      "Epoch [43/200] Loss: 33.5778694152832\n",
      "Epoch [44/200] Loss: 32.916343688964844\n",
      "Epoch [45/200] Loss: 32.2768440246582\n",
      "Epoch [46/200] Loss: 31.658382415771484\n",
      "Epoch [47/200] Loss: 31.06007194519043\n",
      "Epoch [48/200] Loss: 30.481063842773438\n",
      "Epoch [49/200] Loss: 29.920581817626953\n",
      "Epoch [50/200] Loss: 29.377893447875977\n",
      "Epoch [51/200] Loss: 28.852296829223633\n",
      "Epoch [52/200] Loss: 28.343154907226562\n",
      "Epoch [53/200] Loss: 27.849882125854492\n",
      "Epoch [54/200] Loss: 27.37191390991211\n",
      "Epoch [55/200] Loss: 26.90874671936035\n",
      "Epoch [56/200] Loss: 26.45989990234375\n",
      "Epoch [57/200] Loss: 26.024953842163086\n",
      "Epoch [58/200] Loss: 25.60348892211914\n",
      "Epoch [59/200] Loss: 25.195112228393555\n",
      "Epoch [60/200] Loss: 24.799480438232422\n",
      "Epoch [61/200] Loss: 24.416236877441406\n",
      "Epoch [62/200] Loss: 24.0450439453125\n",
      "Epoch [63/200] Loss: 23.685588836669922\n",
      "Epoch [64/200] Loss: 23.33755874633789\n",
      "Epoch [65/200] Loss: 23.000635147094727\n",
      "Epoch [66/200] Loss: 22.67453956604004\n",
      "Epoch [67/200] Loss: 22.358964920043945\n",
      "Epoch [68/200] Loss: 22.053632736206055\n",
      "Epoch [69/200] Loss: 21.758258819580078\n",
      "Epoch [70/200] Loss: 21.47256088256836\n",
      "Epoch [71/200] Loss: 21.19628143310547\n",
      "Epoch [72/200] Loss: 20.929147720336914\n",
      "Epoch [73/200] Loss: 20.670909881591797\n",
      "Epoch [74/200] Loss: 20.421314239501953\n",
      "Epoch [75/200] Loss: 20.180112838745117\n",
      "Epoch [76/200] Loss: 19.947080612182617\n",
      "Epoch [77/200] Loss: 19.721973419189453\n",
      "Epoch [78/200] Loss: 19.504575729370117\n",
      "Epoch [79/200] Loss: 19.2946720123291\n",
      "Epoch [80/200] Loss: 19.09204864501953\n",
      "Epoch [81/200] Loss: 18.896503448486328\n",
      "Epoch [82/200] Loss: 18.707839965820312\n",
      "Epoch [83/200] Loss: 18.525861740112305\n",
      "Epoch [84/200] Loss: 18.350391387939453\n",
      "Epoch [85/200] Loss: 18.181232452392578\n",
      "Epoch [86/200] Loss: 18.01821517944336\n",
      "Epoch [87/200] Loss: 17.861169815063477\n",
      "Epoch [88/200] Loss: 17.709918975830078\n",
      "Epoch [89/200] Loss: 17.564302444458008\n",
      "Epoch [90/200] Loss: 17.424152374267578\n",
      "Epoch [91/200] Loss: 17.28931999206543\n",
      "Epoch [92/200] Loss: 17.15964126586914\n",
      "Epoch [93/200] Loss: 17.034963607788086\n",
      "Epoch [94/200] Loss: 16.915142059326172\n",
      "Epoch [95/200] Loss: 16.800020217895508\n",
      "Epoch [96/200] Loss: 16.689464569091797\n",
      "Epoch [97/200] Loss: 16.58332633972168\n",
      "Epoch [98/200] Loss: 16.48147201538086\n",
      "Epoch [99/200] Loss: 16.383758544921875\n",
      "Epoch [100/200] Loss: 16.29006576538086\n",
      "Epoch [101/200] Loss: 16.20024299621582\n",
      "Epoch [102/200] Loss: 16.11417579650879\n",
      "Epoch [103/200] Loss: 16.031734466552734\n",
      "Epoch [104/200] Loss: 15.952796936035156\n",
      "Epoch [105/200] Loss: 15.877249717712402\n",
      "Epoch [106/200] Loss: 15.804967880249023\n",
      "Epoch [107/200] Loss: 15.73583984375\n",
      "Epoch [108/200] Loss: 15.66975212097168\n",
      "Epoch [109/200] Loss: 15.606597900390625\n",
      "Epoch [110/200] Loss: 15.546268463134766\n",
      "Epoch [111/200] Loss: 15.488664627075195\n",
      "Epoch [112/200] Loss: 15.433679580688477\n",
      "Epoch [113/200] Loss: 15.381217956542969\n",
      "Epoch [114/200] Loss: 15.331186294555664\n",
      "Epoch [115/200] Loss: 15.283483505249023\n",
      "Epoch [116/200] Loss: 15.23802661895752\n",
      "Epoch [117/200] Loss: 15.19472885131836\n",
      "Epoch [118/200] Loss: 15.153496742248535\n",
      "Epoch [119/200] Loss: 15.114253044128418\n",
      "Epoch [120/200] Loss: 15.076916694641113\n",
      "Epoch [121/200] Loss: 15.04140567779541\n",
      "Epoch [122/200] Loss: 15.007648468017578\n",
      "Epoch [123/200] Loss: 14.975568771362305\n",
      "Epoch [124/200] Loss: 14.945096969604492\n",
      "Epoch [125/200] Loss: 14.91616439819336\n",
      "Epoch [126/200] Loss: 14.888708114624023\n",
      "Epoch [127/200] Loss: 14.86265754699707\n",
      "Epoch [128/200] Loss: 14.837950706481934\n",
      "Epoch [129/200] Loss: 14.814533233642578\n",
      "Epoch [130/200] Loss: 14.792343139648438\n",
      "Epoch [131/200] Loss: 14.771327018737793\n",
      "Epoch [132/200] Loss: 14.75142765045166\n",
      "Epoch [133/200] Loss: 14.732599258422852\n",
      "Epoch [134/200] Loss: 14.714786529541016\n",
      "Epoch [135/200] Loss: 14.697944641113281\n",
      "Epoch [136/200] Loss: 14.682024955749512\n",
      "Epoch [137/200] Loss: 14.666986465454102\n",
      "Epoch [138/200] Loss: 14.65278148651123\n",
      "Epoch [139/200] Loss: 14.639373779296875\n",
      "Epoch [140/200] Loss: 14.626724243164062\n",
      "Epoch [141/200] Loss: 14.614791870117188\n",
      "Epoch [142/200] Loss: 14.603540420532227\n",
      "Epoch [143/200] Loss: 14.592942237854004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [144/200] Loss: 14.582956314086914\n",
      "Epoch [145/200] Loss: 14.573555946350098\n",
      "Epoch [146/200] Loss: 14.564705848693848\n",
      "Epoch [147/200] Loss: 14.556382179260254\n",
      "Epoch [148/200] Loss: 14.548555374145508\n",
      "Epoch [149/200] Loss: 14.54119873046875\n",
      "Epoch [150/200] Loss: 14.534286499023438\n",
      "Epoch [151/200] Loss: 14.527795791625977\n",
      "Epoch [152/200] Loss: 14.52170181274414\n",
      "Epoch [153/200] Loss: 14.515986442565918\n",
      "Epoch [154/200] Loss: 14.510625839233398\n",
      "Epoch [155/200] Loss: 14.505599021911621\n",
      "Epoch [156/200] Loss: 14.500890731811523\n",
      "Epoch [157/200] Loss: 14.496480941772461\n",
      "Epoch [158/200] Loss: 14.492352485656738\n",
      "Epoch [159/200] Loss: 14.488490104675293\n",
      "Epoch [160/200] Loss: 14.48487663269043\n",
      "Epoch [161/200] Loss: 14.481499671936035\n",
      "Epoch [162/200] Loss: 14.47834300994873\n",
      "Epoch [163/200] Loss: 14.475397109985352\n",
      "Epoch [164/200] Loss: 14.47264289855957\n",
      "Epoch [165/200] Loss: 14.470076560974121\n",
      "Epoch [166/200] Loss: 14.467681884765625\n",
      "Epoch [167/200] Loss: 14.465448379516602\n",
      "Epoch [168/200] Loss: 14.463369369506836\n",
      "Epoch [169/200] Loss: 14.461433410644531\n",
      "Epoch [170/200] Loss: 14.45962905883789\n",
      "Epoch [171/200] Loss: 14.457950592041016\n",
      "Epoch [172/200] Loss: 14.456390380859375\n",
      "Epoch [173/200] Loss: 14.454940795898438\n",
      "Epoch [174/200] Loss: 14.453594207763672\n",
      "Epoch [175/200] Loss: 14.452342987060547\n",
      "Epoch [176/200] Loss: 14.451183319091797\n",
      "Epoch [177/200] Loss: 14.450105667114258\n",
      "Epoch [178/200] Loss: 14.44910717010498\n",
      "Epoch [179/200] Loss: 14.448182106018066\n",
      "Epoch [180/200] Loss: 14.447324752807617\n",
      "Epoch [181/200] Loss: 14.446532249450684\n",
      "Epoch [182/200] Loss: 14.445798873901367\n",
      "Epoch [183/200] Loss: 14.445120811462402\n",
      "Epoch [184/200] Loss: 14.444494247436523\n",
      "Epoch [185/200] Loss: 14.443914413452148\n",
      "Epoch [186/200] Loss: 14.443379402160645\n",
      "Epoch [187/200] Loss: 14.442886352539062\n",
      "Epoch [188/200] Loss: 14.442432403564453\n",
      "Epoch [189/200] Loss: 14.442012786865234\n",
      "Epoch [190/200] Loss: 14.44162654876709\n",
      "Epoch [191/200] Loss: 14.441272735595703\n",
      "Epoch [192/200] Loss: 14.44094467163086\n",
      "Epoch [193/200] Loss: 14.440645217895508\n",
      "Epoch [194/200] Loss: 14.44036865234375\n",
      "Epoch [195/200] Loss: 14.440114974975586\n",
      "Epoch [196/200] Loss: 14.439882278442383\n",
      "Epoch [197/200] Loss: 14.439667701721191\n",
      "Epoch [198/200] Loss: 14.439472198486328\n",
      "Epoch [199/200] Loss: 14.439292907714844\n",
      "Epoch [200/200] Loss: 14.439128875732422\n",
      "Predicted days_remaining for parent_id 409: 9.711544036865234\n",
      "Training for parent_id 412...\n",
      "Epoch [1/200] Loss: 213.1695556640625\n",
      "Epoch [2/200] Loss: 207.01429748535156\n",
      "Epoch [3/200] Loss: 201.1549072265625\n",
      "Epoch [4/200] Loss: 195.59854125976562\n",
      "Epoch [5/200] Loss: 190.33035278320312\n",
      "Epoch [6/200] Loss: 185.32882690429688\n",
      "Epoch [7/200] Loss: 180.56800842285156\n",
      "Epoch [8/200] Loss: 176.0216522216797\n",
      "Epoch [9/200] Loss: 171.66552734375\n",
      "Epoch [10/200] Loss: 167.4779052734375\n",
      "Epoch [11/200] Loss: 163.4398193359375\n",
      "Epoch [12/200] Loss: 159.53565979003906\n",
      "Epoch [13/200] Loss: 155.75332641601562\n",
      "Epoch [14/200] Loss: 152.08392333984375\n",
      "Epoch [15/200] Loss: 148.52099609375\n",
      "Epoch [16/200] Loss: 145.06007385253906\n",
      "Epoch [17/200] Loss: 141.69813537597656\n",
      "Epoch [18/200] Loss: 138.43309020996094\n",
      "Epoch [19/200] Loss: 135.26393127441406\n",
      "Epoch [20/200] Loss: 132.19024658203125\n",
      "Epoch [21/200] Loss: 129.21194458007812\n",
      "Epoch [22/200] Loss: 126.32903289794922\n",
      "Epoch [23/200] Loss: 123.5413818359375\n",
      "Epoch [24/200] Loss: 120.84845733642578\n",
      "Epoch [25/200] Loss: 118.24932098388672\n",
      "Epoch [26/200] Loss: 115.74238586425781\n",
      "Epoch [27/200] Loss: 113.32557678222656\n",
      "Epoch [28/200] Loss: 110.99628448486328\n",
      "Epoch [29/200] Loss: 108.75151062011719\n",
      "Epoch [30/200] Loss: 106.5878677368164\n",
      "Epoch [31/200] Loss: 104.50161743164062\n",
      "Epoch [32/200] Loss: 102.48872375488281\n",
      "Epoch [33/200] Loss: 100.54509735107422\n",
      "Epoch [34/200] Loss: 98.6665267944336\n",
      "Epoch [35/200] Loss: 96.84893035888672\n",
      "Epoch [36/200] Loss: 95.08842468261719\n",
      "Epoch [37/200] Loss: 93.38136291503906\n",
      "Epoch [38/200] Loss: 91.72439575195312\n",
      "Epoch [39/200] Loss: 90.11441040039062\n",
      "Epoch [40/200] Loss: 88.54859161376953\n",
      "Epoch [41/200] Loss: 87.0242691040039\n",
      "Epoch [42/200] Loss: 85.5390396118164\n",
      "Epoch [43/200] Loss: 84.0905990600586\n",
      "Epoch [44/200] Loss: 82.67684936523438\n",
      "Epoch [45/200] Loss: 81.29581451416016\n",
      "Epoch [46/200] Loss: 79.94576263427734\n",
      "Epoch [47/200] Loss: 78.62506866455078\n",
      "Epoch [48/200] Loss: 77.33226013183594\n",
      "Epoch [49/200] Loss: 76.0660171508789\n",
      "Epoch [50/200] Loss: 74.82518005371094\n",
      "Epoch [51/200] Loss: 73.60869598388672\n",
      "Epoch [52/200] Loss: 72.41563415527344\n",
      "Epoch [53/200] Loss: 71.24515533447266\n",
      "Epoch [54/200] Loss: 70.09647369384766\n",
      "Epoch [55/200] Loss: 68.96891784667969\n",
      "Epoch [56/200] Loss: 67.86188507080078\n",
      "Epoch [57/200] Loss: 66.7747573852539\n",
      "Epoch [58/200] Loss: 65.70706176757812\n",
      "Epoch [59/200] Loss: 64.6582260131836\n",
      "Epoch [60/200] Loss: 63.62786102294922\n",
      "Epoch [61/200] Loss: 62.615509033203125\n",
      "Epoch [62/200] Loss: 61.62082290649414\n",
      "Epoch [63/200] Loss: 60.643463134765625\n",
      "Epoch [64/200] Loss: 59.68311309814453\n",
      "Epoch [65/200] Loss: 58.7396240234375\n",
      "Epoch [66/200] Loss: 57.81275177001953\n",
      "Epoch [67/200] Loss: 56.9023551940918\n",
      "Epoch [68/200] Loss: 56.008365631103516\n",
      "Epoch [69/200] Loss: 55.13065719604492\n",
      "Epoch [70/200] Loss: 54.26915740966797\n",
      "Epoch [71/200] Loss: 53.42376708984375\n",
      "Epoch [72/200] Loss: 52.59436798095703\n",
      "Epoch [73/200] Loss: 51.780845642089844\n",
      "Epoch [74/200] Loss: 50.98300552368164\n",
      "Epoch [75/200] Loss: 50.20066833496094\n",
      "Epoch [76/200] Loss: 49.43364715576172\n",
      "Epoch [77/200] Loss: 48.681678771972656\n",
      "Epoch [78/200] Loss: 47.944541931152344\n",
      "Epoch [79/200] Loss: 47.221954345703125\n",
      "Epoch [80/200] Loss: 46.513671875\n",
      "Epoch [81/200] Loss: 45.819427490234375\n",
      "Epoch [82/200] Loss: 45.138938903808594\n",
      "Epoch [83/200] Loss: 44.47193908691406\n",
      "Epoch [84/200] Loss: 43.81817626953125\n",
      "Epoch [85/200] Loss: 43.17736053466797\n",
      "Epoch [86/200] Loss: 42.54928207397461\n",
      "Epoch [87/200] Loss: 41.933650970458984\n",
      "Epoch [88/200] Loss: 41.33024597167969\n",
      "Epoch [89/200] Loss: 40.738800048828125\n",
      "Epoch [90/200] Loss: 40.159095764160156\n",
      "Epoch [91/200] Loss: 39.59089660644531\n",
      "Epoch [92/200] Loss: 39.03398132324219\n",
      "Epoch [93/200] Loss: 38.48814010620117\n",
      "Epoch [94/200] Loss: 37.95315933227539\n",
      "Epoch [95/200] Loss: 37.42881393432617\n",
      "Epoch [96/200] Loss: 36.9149055480957\n",
      "Epoch [97/200] Loss: 36.41127014160156\n",
      "Epoch [98/200] Loss: 35.91765213012695\n",
      "Epoch [99/200] Loss: 35.43391799926758\n",
      "Epoch [100/200] Loss: 34.95985794067383\n",
      "Epoch [101/200] Loss: 34.49528884887695\n",
      "Epoch [102/200] Loss: 34.04002380371094\n",
      "Epoch [103/200] Loss: 33.59393310546875\n",
      "Epoch [104/200] Loss: 33.156795501708984\n",
      "Epoch [105/200] Loss: 32.728485107421875\n",
      "Epoch [106/200] Loss: 32.30881118774414\n",
      "Epoch [107/200] Loss: 31.897634506225586\n",
      "Epoch [108/200] Loss: 31.494796752929688\n",
      "Epoch [109/200] Loss: 31.100130081176758\n",
      "Epoch [110/200] Loss: 30.713499069213867\n",
      "Epoch [111/200] Loss: 30.33475685119629\n",
      "Epoch [112/200] Loss: 29.96375846862793\n",
      "Epoch [113/200] Loss: 29.600351333618164\n",
      "Epoch [114/200] Loss: 29.244417190551758\n",
      "Epoch [115/200] Loss: 28.895816802978516\n",
      "Epoch [116/200] Loss: 28.554412841796875\n",
      "Epoch [117/200] Loss: 28.220062255859375\n",
      "Epoch [118/200] Loss: 27.892663955688477\n",
      "Epoch [119/200] Loss: 27.572063446044922\n",
      "Epoch [120/200] Loss: 27.258153915405273\n",
      "Epoch [121/200] Loss: 26.95081329345703\n",
      "Epoch [122/200] Loss: 26.649932861328125\n",
      "Epoch [123/200] Loss: 26.355361938476562\n",
      "Epoch [124/200] Loss: 26.0670166015625\n",
      "Epoch [125/200] Loss: 25.784780502319336\n",
      "Epoch [126/200] Loss: 25.508525848388672\n",
      "Epoch [127/200] Loss: 25.238162994384766\n",
      "Epoch [128/200] Loss: 24.97356605529785\n",
      "Epoch [129/200] Loss: 24.71465492248535\n",
      "Epoch [130/200] Loss: 24.461292266845703\n",
      "Epoch [131/200] Loss: 24.213401794433594\n",
      "Epoch [132/200] Loss: 23.97088623046875\n",
      "Epoch [133/200] Loss: 23.733627319335938\n",
      "Epoch [134/200] Loss: 23.501523971557617\n",
      "Epoch [135/200] Loss: 23.274505615234375\n",
      "Epoch [136/200] Loss: 23.05245590209961\n",
      "Epoch [137/200] Loss: 22.83529281616211\n",
      "Epoch [138/200] Loss: 22.622936248779297\n",
      "Epoch [139/200] Loss: 22.415273666381836\n",
      "Epoch [140/200] Loss: 22.21223258972168\n",
      "Epoch [141/200] Loss: 22.013717651367188\n",
      "Epoch [142/200] Loss: 21.819644927978516\n",
      "Epoch [143/200] Loss: 21.629940032958984\n",
      "Epoch [144/200] Loss: 21.444515228271484\n",
      "Epoch [145/200] Loss: 21.26327896118164\n",
      "Epoch [146/200] Loss: 21.08615493774414\n",
      "Epoch [147/200] Loss: 20.9130802154541\n",
      "Epoch [148/200] Loss: 20.74395751953125\n",
      "Epoch [149/200] Loss: 20.578710556030273\n",
      "Epoch [150/200] Loss: 20.41727638244629\n",
      "Epoch [151/200] Loss: 20.25958251953125\n",
      "Epoch [152/200] Loss: 20.105541229248047\n",
      "Epoch [153/200] Loss: 19.95508575439453\n",
      "Epoch [154/200] Loss: 19.808156967163086\n",
      "Epoch [155/200] Loss: 19.664670944213867\n",
      "Epoch [156/200] Loss: 19.524564743041992\n",
      "Epoch [157/200] Loss: 19.38776969909668\n",
      "Epoch [158/200] Loss: 19.254230499267578\n",
      "Epoch [159/200] Loss: 19.123865127563477\n",
      "Epoch [160/200] Loss: 18.996618270874023\n",
      "Epoch [161/200] Loss: 18.8724308013916\n",
      "Epoch [162/200] Loss: 18.751220703125\n",
      "Epoch [163/200] Loss: 18.632953643798828\n",
      "Epoch [164/200] Loss: 18.517555236816406\n",
      "Epoch [165/200] Loss: 18.404966354370117\n",
      "Epoch [166/200] Loss: 18.295133590698242\n",
      "Epoch [167/200] Loss: 18.18798828125\n",
      "Epoch [168/200] Loss: 18.0834903717041\n",
      "Epoch [169/200] Loss: 17.98157501220703\n",
      "Epoch [170/200] Loss: 17.8821964263916\n",
      "Epoch [171/200] Loss: 17.7852783203125\n",
      "Epoch [172/200] Loss: 17.690797805786133\n",
      "Epoch [173/200] Loss: 17.598684310913086\n",
      "Epoch [174/200] Loss: 17.508892059326172\n",
      "Epoch [175/200] Loss: 17.421375274658203\n",
      "Epoch [176/200] Loss: 17.33607292175293\n",
      "Epoch [177/200] Loss: 17.252952575683594\n",
      "Epoch [178/200] Loss: 17.171953201293945\n",
      "Epoch [179/200] Loss: 17.093036651611328\n",
      "Epoch [180/200] Loss: 17.016151428222656\n",
      "Epoch [181/200] Loss: 16.941255569458008\n",
      "Epoch [182/200] Loss: 16.86830711364746\n",
      "Epoch [183/200] Loss: 16.797256469726562\n",
      "Epoch [184/200] Loss: 16.728065490722656\n",
      "Epoch [185/200] Loss: 16.660690307617188\n",
      "Epoch [186/200] Loss: 16.595096588134766\n",
      "Epoch [187/200] Loss: 16.531230926513672\n",
      "Epoch [188/200] Loss: 16.469066619873047\n",
      "Epoch [189/200] Loss: 16.408557891845703\n",
      "Epoch [190/200] Loss: 16.34966468811035\n",
      "Epoch [191/200] Loss: 16.29235076904297\n",
      "Epoch [192/200] Loss: 16.236583709716797\n",
      "Epoch [193/200] Loss: 16.182327270507812\n",
      "Epoch [194/200] Loss: 16.12954330444336\n",
      "Epoch [195/200] Loss: 16.078197479248047\n",
      "Epoch [196/200] Loss: 16.02825164794922\n",
      "Epoch [197/200] Loss: 15.979680061340332\n",
      "Epoch [198/200] Loss: 15.932449340820312\n",
      "Epoch [199/200] Loss: 15.886516571044922\n",
      "Epoch [200/200] Loss: 15.841861724853516\n",
      "Predicted days_remaining for parent_id 412: 12.58340072631836\n",
      "Training for parent_id 416...\n",
      "Epoch [1/200] Loss: 528.031005859375\n",
      "Epoch [2/200] Loss: 515.0700073242188\n",
      "Epoch [3/200] Loss: 502.6452941894531\n",
      "Epoch [4/200] Loss: 490.9248352050781\n",
      "Epoch [5/200] Loss: 479.9966735839844\n",
      "Epoch [6/200] Loss: 469.86541748046875\n",
      "Epoch [7/200] Loss: 460.483642578125\n",
      "Epoch [8/200] Loss: 451.7802429199219\n",
      "Epoch [9/200] Loss: 443.6778564453125\n",
      "Epoch [10/200] Loss: 436.1033630371094\n",
      "Epoch [11/200] Loss: 428.99200439453125\n",
      "Epoch [12/200] Loss: 422.2890625\n",
      "Epoch [13/200] Loss: 415.95001220703125\n",
      "Epoch [14/200] Loss: 409.9394836425781\n",
      "Epoch [15/200] Loss: 404.23052978515625\n",
      "Epoch [16/200] Loss: 398.8021240234375\n",
      "Epoch [17/200] Loss: 393.6369934082031\n",
      "Epoch [18/200] Loss: 388.7198486328125\n",
      "Epoch [19/200] Loss: 384.0357360839844\n",
      "Epoch [20/200] Loss: 379.5694885253906\n",
      "Epoch [21/200] Loss: 375.3054504394531\n",
      "Epoch [22/200] Loss: 371.2274169921875\n",
      "Epoch [23/200] Loss: 367.3193054199219\n",
      "Epoch [24/200] Loss: 363.56512451171875\n",
      "Epoch [25/200] Loss: 359.9497375488281\n",
      "Epoch [26/200] Loss: 356.4585876464844\n",
      "Epoch [27/200] Loss: 353.0784912109375\n",
      "Epoch [28/200] Loss: 349.7970275878906\n",
      "Epoch [29/200] Loss: 346.6033020019531\n",
      "Epoch [30/200] Loss: 343.48773193359375\n",
      "Epoch [31/200] Loss: 340.4418640136719\n",
      "Epoch [32/200] Loss: 337.45831298828125\n",
      "Epoch [33/200] Loss: 334.5307312011719\n",
      "Epoch [34/200] Loss: 331.653564453125\n",
      "Epoch [35/200] Loss: 328.8218994140625\n",
      "Epoch [36/200] Loss: 326.03167724609375\n",
      "Epoch [37/200] Loss: 323.2791442871094\n",
      "Epoch [38/200] Loss: 320.5611267089844\n",
      "Epoch [39/200] Loss: 317.8748779296875\n",
      "Epoch [40/200] Loss: 315.2181396484375\n",
      "Epoch [41/200] Loss: 312.5890197753906\n",
      "Epoch [42/200] Loss: 309.9859313964844\n",
      "Epoch [43/200] Loss: 307.40765380859375\n",
      "Epoch [44/200] Loss: 304.85321044921875\n",
      "Epoch [45/200] Loss: 302.32159423828125\n",
      "Epoch [46/200] Loss: 299.8122253417969\n",
      "Epoch [47/200] Loss: 297.32440185546875\n",
      "Epoch [48/200] Loss: 294.8574523925781\n",
      "Epoch [49/200] Loss: 292.4107971191406\n",
      "Epoch [50/200] Loss: 289.98388671875\n",
      "Epoch [51/200] Loss: 287.57635498046875\n",
      "Epoch [52/200] Loss: 285.1878967285156\n",
      "Epoch [53/200] Loss: 282.8183288574219\n",
      "Epoch [54/200] Loss: 280.46771240234375\n",
      "Epoch [55/200] Loss: 278.135986328125\n",
      "Epoch [56/200] Loss: 275.82342529296875\n",
      "Epoch [57/200] Loss: 273.5300598144531\n",
      "Epoch [58/200] Loss: 271.2560119628906\n",
      "Epoch [59/200] Loss: 269.00146484375\n",
      "Epoch [60/200] Loss: 266.766357421875\n",
      "Epoch [61/200] Loss: 264.5506591796875\n",
      "Epoch [62/200] Loss: 262.3542175292969\n",
      "Epoch [63/200] Loss: 260.1768798828125\n",
      "Epoch [64/200] Loss: 258.0185241699219\n",
      "Epoch [65/200] Loss: 255.87881469726562\n",
      "Epoch [66/200] Loss: 253.75759887695312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/200] Loss: 251.6546630859375\n",
      "Epoch [68/200] Loss: 249.5697479248047\n",
      "Epoch [69/200] Loss: 247.5026092529297\n",
      "Epoch [70/200] Loss: 245.45294189453125\n",
      "Epoch [71/200] Loss: 243.4207000732422\n",
      "Epoch [72/200] Loss: 241.40553283691406\n",
      "Epoch [73/200] Loss: 239.4072265625\n",
      "Epoch [74/200] Loss: 237.42568969726562\n",
      "Epoch [75/200] Loss: 235.46054077148438\n",
      "Epoch [76/200] Loss: 233.51168823242188\n",
      "Epoch [77/200] Loss: 231.57904052734375\n",
      "Epoch [78/200] Loss: 229.66221618652344\n",
      "Epoch [79/200] Loss: 227.76119995117188\n",
      "Epoch [80/200] Loss: 225.87570190429688\n",
      "Epoch [81/200] Loss: 224.005615234375\n",
      "Epoch [82/200] Loss: 222.15078735351562\n",
      "Epoch [83/200] Loss: 220.3109893798828\n",
      "Epoch [84/200] Loss: 218.48609924316406\n",
      "Epoch [85/200] Loss: 216.67593383789062\n",
      "Epoch [86/200] Loss: 214.88043212890625\n",
      "Epoch [87/200] Loss: 213.0992889404297\n",
      "Epoch [88/200] Loss: 211.33253479003906\n",
      "Epoch [89/200] Loss: 209.57986450195312\n",
      "Epoch [90/200] Loss: 207.8412322998047\n",
      "Epoch [91/200] Loss: 206.11643981933594\n",
      "Epoch [92/200] Loss: 204.40541076660156\n",
      "Epoch [93/200] Loss: 202.70799255371094\n",
      "Epoch [94/200] Loss: 201.0240478515625\n",
      "Epoch [95/200] Loss: 199.3534393310547\n",
      "Epoch [96/200] Loss: 197.69610595703125\n",
      "Epoch [97/200] Loss: 196.0518341064453\n",
      "Epoch [98/200] Loss: 194.42051696777344\n",
      "Epoch [99/200] Loss: 192.80209350585938\n",
      "Epoch [100/200] Loss: 191.19642639160156\n",
      "Epoch [101/200] Loss: 189.6034393310547\n",
      "Epoch [102/200] Loss: 188.02291870117188\n",
      "Epoch [103/200] Loss: 186.4547882080078\n",
      "Epoch [104/200] Loss: 184.89907836914062\n",
      "Epoch [105/200] Loss: 183.35549926757812\n",
      "Epoch [106/200] Loss: 181.8240203857422\n",
      "Epoch [107/200] Loss: 180.30459594726562\n",
      "Epoch [108/200] Loss: 178.79702758789062\n",
      "Epoch [109/200] Loss: 177.30125427246094\n",
      "Epoch [110/200] Loss: 175.81716918945312\n",
      "Epoch [111/200] Loss: 174.34478759765625\n",
      "Epoch [112/200] Loss: 172.8839111328125\n",
      "Epoch [113/200] Loss: 171.43443298339844\n",
      "Epoch [114/200] Loss: 169.9962921142578\n",
      "Epoch [115/200] Loss: 168.5694122314453\n",
      "Epoch [116/200] Loss: 167.15371704101562\n",
      "Epoch [117/200] Loss: 165.74903869628906\n",
      "Epoch [118/200] Loss: 164.35540771484375\n",
      "Epoch [119/200] Loss: 162.97268676757812\n",
      "Epoch [120/200] Loss: 161.60079956054688\n",
      "Epoch [121/200] Loss: 160.23963928222656\n",
      "Epoch [122/200] Loss: 158.88914489746094\n",
      "Epoch [123/200] Loss: 157.54928588867188\n",
      "Epoch [124/200] Loss: 156.21986389160156\n",
      "Epoch [125/200] Loss: 154.9009246826172\n",
      "Epoch [126/200] Loss: 153.59234619140625\n",
      "Epoch [127/200] Loss: 152.2940216064453\n",
      "Epoch [128/200] Loss: 151.0059051513672\n",
      "Epoch [129/200] Loss: 149.7279510498047\n",
      "Epoch [130/200] Loss: 148.46009826660156\n",
      "Epoch [131/200] Loss: 147.20217895507812\n",
      "Epoch [132/200] Loss: 145.95420837402344\n",
      "Epoch [133/200] Loss: 144.716064453125\n",
      "Epoch [134/200] Loss: 143.48776245117188\n",
      "Epoch [135/200] Loss: 142.26914978027344\n",
      "Epoch [136/200] Loss: 141.06019592285156\n",
      "Epoch [137/200] Loss: 139.86082458496094\n",
      "Epoch [138/200] Loss: 138.6709442138672\n",
      "Epoch [139/200] Loss: 137.4905242919922\n",
      "Epoch [140/200] Loss: 136.31948852539062\n",
      "Epoch [141/200] Loss: 135.15777587890625\n",
      "Epoch [142/200] Loss: 134.00535583496094\n",
      "Epoch [143/200] Loss: 132.86212158203125\n",
      "Epoch [144/200] Loss: 131.7279815673828\n",
      "Epoch [145/200] Loss: 130.60293579101562\n",
      "Epoch [146/200] Loss: 129.48692321777344\n",
      "Epoch [147/200] Loss: 128.3798370361328\n",
      "Epoch [148/200] Loss: 127.28164672851562\n",
      "Epoch [149/200] Loss: 126.19232177734375\n",
      "Epoch [150/200] Loss: 125.11173248291016\n",
      "Epoch [151/200] Loss: 124.03985595703125\n",
      "Epoch [152/200] Loss: 122.97663879394531\n",
      "Epoch [153/200] Loss: 121.92202758789062\n",
      "Epoch [154/200] Loss: 120.8759765625\n",
      "Epoch [155/200] Loss: 119.83838653564453\n",
      "Epoch [156/200] Loss: 118.8092269897461\n",
      "Epoch [157/200] Loss: 117.7884521484375\n",
      "Epoch [158/200] Loss: 116.7759780883789\n",
      "Epoch [159/200] Loss: 115.77178192138672\n",
      "Epoch [160/200] Loss: 114.77576446533203\n",
      "Epoch [161/200] Loss: 113.78795623779297\n",
      "Epoch [162/200] Loss: 112.8082046508789\n",
      "Epoch [163/200] Loss: 111.8365249633789\n",
      "Epoch [164/200] Loss: 110.87281799316406\n",
      "Epoch [165/200] Loss: 109.91703033447266\n",
      "Epoch [166/200] Loss: 108.96916198730469\n",
      "Epoch [167/200] Loss: 108.02913665771484\n",
      "Epoch [168/200] Loss: 107.09689331054688\n",
      "Epoch [169/200] Loss: 106.17235565185547\n",
      "Epoch [170/200] Loss: 105.25552368164062\n",
      "Epoch [171/200] Loss: 104.34630584716797\n",
      "Epoch [172/200] Loss: 103.44469451904297\n",
      "Epoch [173/200] Loss: 102.55060577392578\n",
      "Epoch [174/200] Loss: 101.66398620605469\n",
      "Epoch [175/200] Loss: 100.78480529785156\n",
      "Epoch [176/200] Loss: 99.91300201416016\n",
      "Epoch [177/200] Loss: 99.04850769042969\n",
      "Epoch [178/200] Loss: 98.19137573242188\n",
      "Epoch [179/200] Loss: 97.34140014648438\n",
      "Epoch [180/200] Loss: 96.4986572265625\n",
      "Epoch [181/200] Loss: 95.66307067871094\n",
      "Epoch [182/200] Loss: 94.83451080322266\n",
      "Epoch [183/200] Loss: 94.01306915283203\n",
      "Epoch [184/200] Loss: 93.1986083984375\n",
      "Epoch [185/200] Loss: 92.39109802246094\n",
      "Epoch [186/200] Loss: 91.5904541015625\n",
      "Epoch [187/200] Loss: 90.79670715332031\n",
      "Epoch [188/200] Loss: 90.00979614257812\n",
      "Epoch [189/200] Loss: 89.22960662841797\n",
      "Epoch [190/200] Loss: 88.4561767578125\n",
      "Epoch [191/200] Loss: 87.68940734863281\n",
      "Epoch [192/200] Loss: 86.92928314208984\n",
      "Epoch [193/200] Loss: 86.17571258544922\n",
      "Epoch [194/200] Loss: 85.4287338256836\n",
      "Epoch [195/200] Loss: 84.68824768066406\n",
      "Epoch [196/200] Loss: 83.95418548583984\n",
      "Epoch [197/200] Loss: 83.22657012939453\n",
      "Epoch [198/200] Loss: 82.50530242919922\n",
      "Epoch [199/200] Loss: 81.79036712646484\n",
      "Epoch [200/200] Loss: 81.08171081542969\n",
      "Predicted days_remaining for parent_id 416: 14.629545211791992\n",
      "Training for parent_id 425...\n",
      "Epoch [1/200] Loss: 1523.26318359375\n",
      "Epoch [2/200] Loss: 1503.893310546875\n",
      "Epoch [3/200] Loss: 1484.9688720703125\n",
      "Epoch [4/200] Loss: 1466.781494140625\n",
      "Epoch [5/200] Loss: 1449.5245361328125\n",
      "Epoch [6/200] Loss: 1433.285888671875\n",
      "Epoch [7/200] Loss: 1418.0396728515625\n",
      "Epoch [8/200] Loss: 1403.68798828125\n",
      "Epoch [9/200] Loss: 1390.11181640625\n",
      "Epoch [10/200] Loss: 1377.206787109375\n",
      "Epoch [11/200] Loss: 1364.89013671875\n",
      "Epoch [12/200] Loss: 1353.09521484375\n",
      "Epoch [13/200] Loss: 1341.771728515625\n",
      "Epoch [14/200] Loss: 1330.8798828125\n",
      "Epoch [15/200] Loss: 1320.388916015625\n",
      "Epoch [16/200] Loss: 1310.275634765625\n",
      "Epoch [17/200] Loss: 1300.5198974609375\n",
      "Epoch [18/200] Loss: 1291.102783203125\n",
      "Epoch [19/200] Loss: 1282.00439453125\n",
      "Epoch [20/200] Loss: 1273.2037353515625\n",
      "Epoch [21/200] Loss: 1264.6785888671875\n",
      "Epoch [22/200] Loss: 1256.408447265625\n",
      "Epoch [23/200] Loss: 1248.3736572265625\n",
      "Epoch [24/200] Loss: 1240.5560302734375\n",
      "Epoch [25/200] Loss: 1232.9410400390625\n",
      "Epoch [26/200] Loss: 1225.514892578125\n",
      "Epoch [27/200] Loss: 1218.2664794921875\n",
      "Epoch [28/200] Loss: 1211.1861572265625\n",
      "Epoch [29/200] Loss: 1204.2657470703125\n",
      "Epoch [30/200] Loss: 1197.497314453125\n",
      "Epoch [31/200] Loss: 1190.8743896484375\n",
      "Epoch [32/200] Loss: 1184.389892578125\n",
      "Epoch [33/200] Loss: 1178.037109375\n",
      "Epoch [34/200] Loss: 1171.808837890625\n",
      "Epoch [35/200] Loss: 1165.697998046875\n",
      "Epoch [36/200] Loss: 1159.697509765625\n",
      "Epoch [37/200] Loss: 1153.80078125\n",
      "Epoch [38/200] Loss: 1148.000732421875\n",
      "Epoch [39/200] Loss: 1142.291259765625\n",
      "Epoch [40/200] Loss: 1136.66650390625\n",
      "Epoch [41/200] Loss: 1131.120361328125\n",
      "Epoch [42/200] Loss: 1125.64794921875\n",
      "Epoch [43/200] Loss: 1120.2440185546875\n",
      "Epoch [44/200] Loss: 1114.904052734375\n",
      "Epoch [45/200] Loss: 1109.6236572265625\n",
      "Epoch [46/200] Loss: 1104.3984375\n",
      "Epoch [47/200] Loss: 1099.2254638671875\n",
      "Epoch [48/200] Loss: 1094.100830078125\n",
      "Epoch [49/200] Loss: 1089.0216064453125\n",
      "Epoch [50/200] Loss: 1083.985595703125\n",
      "Epoch [51/200] Loss: 1078.990478515625\n",
      "Epoch [52/200] Loss: 1074.0340576171875\n",
      "Epoch [53/200] Loss: 1069.1148681640625\n",
      "Epoch [54/200] Loss: 1064.231201171875\n",
      "Epoch [55/200] Loss: 1059.3824462890625\n",
      "Epoch [56/200] Loss: 1054.566650390625\n",
      "Epoch [57/200] Loss: 1049.78369140625\n",
      "Epoch [58/200] Loss: 1045.0321044921875\n",
      "Epoch [59/200] Loss: 1040.3114013671875\n",
      "Epoch [60/200] Loss: 1035.62060546875\n",
      "Epoch [61/200] Loss: 1030.959228515625\n",
      "Epoch [62/200] Loss: 1026.3265380859375\n",
      "Epoch [63/200] Loss: 1021.7218627929688\n",
      "Epoch [64/200] Loss: 1017.1446533203125\n",
      "Epoch [65/200] Loss: 1012.5943603515625\n",
      "Epoch [66/200] Loss: 1008.0703125\n",
      "Epoch [67/200] Loss: 1003.5724487304688\n",
      "Epoch [68/200] Loss: 999.099609375\n",
      "Epoch [69/200] Loss: 994.6519775390625\n",
      "Epoch [70/200] Loss: 990.2283325195312\n",
      "Epoch [71/200] Loss: 985.8291625976562\n",
      "Epoch [72/200] Loss: 981.4534912109375\n",
      "Epoch [73/200] Loss: 977.100830078125\n",
      "Epoch [74/200] Loss: 972.7713012695312\n",
      "Epoch [75/200] Loss: 968.4641723632812\n",
      "Epoch [76/200] Loss: 964.1792602539062\n",
      "Epoch [77/200] Loss: 959.9160766601562\n",
      "Epoch [78/200] Loss: 955.6743774414062\n",
      "Epoch [79/200] Loss: 951.4540405273438\n",
      "Epoch [80/200] Loss: 947.2545166015625\n",
      "Epoch [81/200] Loss: 943.075927734375\n",
      "Epoch [82/200] Loss: 938.917236328125\n",
      "Epoch [83/200] Loss: 934.7789306640625\n",
      "Epoch [84/200] Loss: 930.6602783203125\n",
      "Epoch [85/200] Loss: 926.5614013671875\n",
      "Epoch [86/200] Loss: 922.4818725585938\n",
      "Epoch [87/200] Loss: 918.4214477539062\n",
      "Epoch [88/200] Loss: 914.3800048828125\n",
      "Epoch [89/200] Loss: 910.3572998046875\n",
      "Epoch [90/200] Loss: 906.3529663085938\n",
      "Epoch [91/200] Loss: 902.366943359375\n",
      "Epoch [92/200] Loss: 898.3991088867188\n",
      "Epoch [93/200] Loss: 894.44921875\n",
      "Epoch [94/200] Loss: 890.51708984375\n",
      "Epoch [95/200] Loss: 886.6024169921875\n",
      "Epoch [96/200] Loss: 882.705078125\n",
      "Epoch [97/200] Loss: 878.8251342773438\n",
      "Epoch [98/200] Loss: 874.9620971679688\n",
      "Epoch [99/200] Loss: 871.1160888671875\n",
      "Epoch [100/200] Loss: 867.2867431640625\n",
      "Epoch [101/200] Loss: 863.4739990234375\n",
      "Epoch [102/200] Loss: 859.6778564453125\n",
      "Epoch [103/200] Loss: 855.8977661132812\n",
      "Epoch [104/200] Loss: 852.134033203125\n",
      "Epoch [105/200] Loss: 848.3863525390625\n",
      "Epoch [106/200] Loss: 844.6543579101562\n",
      "Epoch [107/200] Loss: 840.9383544921875\n",
      "Epoch [108/200] Loss: 837.2379150390625\n",
      "Epoch [109/200] Loss: 833.5531005859375\n",
      "Epoch [110/200] Loss: 829.8834838867188\n",
      "Epoch [111/200] Loss: 826.2294921875\n",
      "Epoch [112/200] Loss: 822.5905151367188\n",
      "Epoch [113/200] Loss: 818.9666137695312\n",
      "Epoch [114/200] Loss: 815.3576049804688\n",
      "Epoch [115/200] Loss: 811.7635498046875\n",
      "Epoch [116/200] Loss: 808.1842651367188\n",
      "Epoch [117/200] Loss: 804.6196899414062\n",
      "Epoch [118/200] Loss: 801.069580078125\n",
      "Epoch [119/200] Loss: 797.533935546875\n",
      "Epoch [120/200] Loss: 794.0126953125\n",
      "Epoch [121/200] Loss: 790.5057373046875\n",
      "Epoch [122/200] Loss: 787.0130004882812\n",
      "Epoch [123/200] Loss: 783.5343627929688\n",
      "Epoch [124/200] Loss: 780.0697021484375\n",
      "Epoch [125/200] Loss: 776.6188354492188\n",
      "Epoch [126/200] Loss: 773.18212890625\n",
      "Epoch [127/200] Loss: 769.7589721679688\n",
      "Epoch [128/200] Loss: 766.3494873046875\n",
      "Epoch [129/200] Loss: 762.9537353515625\n",
      "Epoch [130/200] Loss: 759.5714721679688\n",
      "Epoch [131/200] Loss: 756.2025756835938\n",
      "Epoch [132/200] Loss: 752.84716796875\n",
      "Epoch [133/200] Loss: 749.505126953125\n",
      "Epoch [134/200] Loss: 746.1760864257812\n",
      "Epoch [135/200] Loss: 742.8602905273438\n",
      "Epoch [136/200] Loss: 739.5576782226562\n",
      "Epoch [137/200] Loss: 736.2680053710938\n",
      "Epoch [138/200] Loss: 732.9913940429688\n",
      "Epoch [139/200] Loss: 729.7276000976562\n",
      "Epoch [140/200] Loss: 726.4766235351562\n",
      "Epoch [141/200] Loss: 723.2384643554688\n",
      "Epoch [142/200] Loss: 720.0128784179688\n",
      "Epoch [143/200] Loss: 716.800048828125\n",
      "Epoch [144/200] Loss: 713.5997924804688\n",
      "Epoch [145/200] Loss: 710.4119262695312\n",
      "Epoch [146/200] Loss: 707.2366943359375\n",
      "Epoch [147/200] Loss: 704.0738525390625\n",
      "Epoch [148/200] Loss: 700.92333984375\n",
      "Epoch [149/200] Loss: 697.7850341796875\n",
      "Epoch [150/200] Loss: 694.658935546875\n",
      "Epoch [151/200] Loss: 691.5450439453125\n",
      "Epoch [152/200] Loss: 688.4432373046875\n",
      "Epoch [153/200] Loss: 685.3536376953125\n",
      "Epoch [154/200] Loss: 682.2760009765625\n",
      "Epoch [155/200] Loss: 679.210205078125\n",
      "Epoch [156/200] Loss: 676.1563110351562\n",
      "Epoch [157/200] Loss: 673.1143798828125\n",
      "Epoch [158/200] Loss: 670.0842895507812\n",
      "Epoch [159/200] Loss: 667.0658569335938\n",
      "Epoch [160/200] Loss: 664.0591430664062\n",
      "Epoch [161/200] Loss: 661.0640258789062\n",
      "Epoch [162/200] Loss: 658.0806274414062\n",
      "Epoch [163/200] Loss: 655.1087646484375\n",
      "Epoch [164/200] Loss: 652.1484375\n",
      "Epoch [165/200] Loss: 649.199462890625\n",
      "Epoch [166/200] Loss: 646.2620239257812\n",
      "Epoch [167/200] Loss: 643.3359375\n",
      "Epoch [168/200] Loss: 640.421142578125\n",
      "Epoch [169/200] Loss: 637.5177001953125\n",
      "Epoch [170/200] Loss: 634.6254272460938\n",
      "Epoch [171/200] Loss: 631.744384765625\n",
      "Epoch [172/200] Loss: 628.8744506835938\n",
      "Epoch [173/200] Loss: 626.0156860351562\n",
      "Epoch [174/200] Loss: 623.16796875\n",
      "Epoch [175/200] Loss: 620.3312377929688\n",
      "Epoch [176/200] Loss: 617.5054321289062\n",
      "Epoch [177/200] Loss: 614.690673828125\n",
      "Epoch [178/200] Loss: 611.88671875\n",
      "Epoch [179/200] Loss: 609.0936889648438\n",
      "Epoch [180/200] Loss: 606.3115234375\n",
      "Epoch [181/200] Loss: 603.5400390625\n",
      "Epoch [182/200] Loss: 600.7794189453125\n",
      "Epoch [183/200] Loss: 598.0293579101562\n",
      "Epoch [184/200] Loss: 595.2900390625\n",
      "Epoch [185/200] Loss: 592.5613403320312\n",
      "Epoch [186/200] Loss: 589.8433227539062\n",
      "Epoch [187/200] Loss: 587.1356811523438\n",
      "Epoch [188/200] Loss: 584.4385375976562\n",
      "Epoch [189/200] Loss: 581.751953125\n",
      "Epoch [190/200] Loss: 579.0758056640625\n",
      "Epoch [191/200] Loss: 576.4100341796875\n",
      "Epoch [192/200] Loss: 573.7544555664062\n",
      "Epoch [193/200] Loss: 571.1094970703125\n",
      "Epoch [194/200] Loss: 568.4746704101562\n",
      "Epoch [195/200] Loss: 565.8501586914062\n",
      "Epoch [196/200] Loss: 563.235595703125\n",
      "Epoch [197/200] Loss: 560.631591796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [198/200] Loss: 558.0374755859375\n",
      "Epoch [199/200] Loss: 555.45361328125\n",
      "Epoch [200/200] Loss: 552.8797607421875\n",
      "Predicted days_remaining for parent_id 425: 15.600950241088867\n",
      "Training for parent_id 430...\n",
      "Epoch [1/200] Loss: 155.83053588867188\n",
      "Epoch [2/200] Loss: 150.44137573242188\n",
      "Epoch [3/200] Loss: 145.3475341796875\n",
      "Epoch [4/200] Loss: 140.52008056640625\n",
      "Epoch [5/200] Loss: 135.93533325195312\n",
      "Epoch [6/200] Loss: 131.5706329345703\n",
      "Epoch [7/200] Loss: 127.40319061279297\n",
      "Epoch [8/200] Loss: 123.41276550292969\n",
      "Epoch [9/200] Loss: 119.58334350585938\n",
      "Epoch [10/200] Loss: 115.9029312133789\n",
      "Epoch [11/200] Loss: 112.36314392089844\n",
      "Epoch [12/200] Loss: 108.95877075195312\n",
      "Epoch [13/200] Loss: 105.68717193603516\n",
      "Epoch [14/200] Loss: 102.54723358154297\n",
      "Epoch [15/200] Loss: 99.53836059570312\n",
      "Epoch [16/200] Loss: 96.65925598144531\n",
      "Epoch [17/200] Loss: 93.90727996826172\n",
      "Epoch [18/200] Loss: 91.27801513671875\n",
      "Epoch [19/200] Loss: 88.7657241821289\n",
      "Epoch [20/200] Loss: 86.36396789550781\n",
      "Epoch [21/200] Loss: 84.06632995605469\n",
      "Epoch [22/200] Loss: 81.8667221069336\n",
      "Epoch [23/200] Loss: 79.75951385498047\n",
      "Epoch [24/200] Loss: 77.73957824707031\n",
      "Epoch [25/200] Loss: 75.80223083496094\n",
      "Epoch [26/200] Loss: 73.94322204589844\n",
      "Epoch [27/200] Loss: 72.15867614746094\n",
      "Epoch [28/200] Loss: 70.44517517089844\n",
      "Epoch [29/200] Loss: 68.79956817626953\n",
      "Epoch [30/200] Loss: 67.21904754638672\n",
      "Epoch [31/200] Loss: 65.7009048461914\n",
      "Epoch [32/200] Loss: 64.24259185791016\n",
      "Epoch [33/200] Loss: 62.841487884521484\n",
      "Epoch [34/200] Loss: 61.494956970214844\n",
      "Epoch [35/200] Loss: 60.20026397705078\n",
      "Epoch [36/200] Loss: 58.95463943481445\n",
      "Epoch [37/200] Loss: 57.75524139404297\n",
      "Epoch [38/200] Loss: 56.59927749633789\n",
      "Epoch [39/200] Loss: 55.48406219482422\n",
      "Epoch [40/200] Loss: 54.406978607177734\n",
      "Epoch [41/200] Loss: 53.36555099487305\n",
      "Epoch [42/200] Loss: 52.35757064819336\n",
      "Epoch [43/200] Loss: 51.380924224853516\n",
      "Epoch [44/200] Loss: 50.433677673339844\n",
      "Epoch [45/200] Loss: 49.514122009277344\n",
      "Epoch [46/200] Loss: 48.620662689208984\n",
      "Epoch [47/200] Loss: 47.751869201660156\n",
      "Epoch [48/200] Loss: 46.90642166137695\n",
      "Epoch [49/200] Loss: 46.08315658569336\n",
      "Epoch [50/200] Loss: 45.280948638916016\n",
      "Epoch [51/200] Loss: 44.49882507324219\n",
      "Epoch [52/200] Loss: 43.735862731933594\n",
      "Epoch [53/200] Loss: 42.99123001098633\n",
      "Epoch [54/200] Loss: 42.2641716003418\n",
      "Epoch [55/200] Loss: 41.55397033691406\n",
      "Epoch [56/200] Loss: 40.8599967956543\n",
      "Epoch [57/200] Loss: 40.18169403076172\n",
      "Epoch [58/200] Loss: 39.51850509643555\n",
      "Epoch [59/200] Loss: 38.86997985839844\n",
      "Epoch [60/200] Loss: 38.235687255859375\n",
      "Epoch [61/200] Loss: 37.61525344848633\n",
      "Epoch [62/200] Loss: 37.00834655761719\n",
      "Epoch [63/200] Loss: 36.41465759277344\n",
      "Epoch [64/200] Loss: 35.83392333984375\n",
      "Epoch [65/200] Loss: 35.26589584350586\n",
      "Epoch [66/200] Loss: 34.71037292480469\n",
      "Epoch [67/200] Loss: 34.167144775390625\n",
      "Epoch [68/200] Loss: 33.63602066040039\n",
      "Epoch [69/200] Loss: 33.11682891845703\n",
      "Epoch [70/200] Loss: 32.60938262939453\n",
      "Epoch [71/200] Loss: 32.113521575927734\n",
      "Epoch [72/200] Loss: 31.629058837890625\n",
      "Epoch [73/200] Loss: 31.15582275390625\n",
      "Epoch [74/200] Loss: 30.69363021850586\n",
      "Epoch [75/200] Loss: 30.2423095703125\n",
      "Epoch [76/200] Loss: 29.80166244506836\n",
      "Epoch [77/200] Loss: 29.371505737304688\n",
      "Epoch [78/200] Loss: 28.9516658782959\n",
      "Epoch [79/200] Loss: 28.541927337646484\n",
      "Epoch [80/200] Loss: 28.14212417602539\n",
      "Epoch [81/200] Loss: 27.752050399780273\n",
      "Epoch [82/200] Loss: 27.371517181396484\n",
      "Epoch [83/200] Loss: 27.0003604888916\n",
      "Epoch [84/200] Loss: 26.638370513916016\n",
      "Epoch [85/200] Loss: 26.285358428955078\n",
      "Epoch [86/200] Loss: 25.941165924072266\n",
      "Epoch [87/200] Loss: 25.605588912963867\n",
      "Epoch [88/200] Loss: 25.27846908569336\n",
      "Epoch [89/200] Loss: 24.95961570739746\n",
      "Epoch [90/200] Loss: 24.648862838745117\n",
      "Epoch [91/200] Loss: 24.346050262451172\n",
      "Epoch [92/200] Loss: 24.05099105834961\n",
      "Epoch [93/200] Loss: 23.76353645324707\n",
      "Epoch [94/200] Loss: 23.483518600463867\n",
      "Epoch [95/200] Loss: 23.21077537536621\n",
      "Epoch [96/200] Loss: 22.945152282714844\n",
      "Epoch [97/200] Loss: 22.68650245666504\n",
      "Epoch [98/200] Loss: 22.43466567993164\n",
      "Epoch [99/200] Loss: 22.189491271972656\n",
      "Epoch [100/200] Loss: 21.950838088989258\n",
      "Epoch [101/200] Loss: 21.71855926513672\n",
      "Epoch [102/200] Loss: 21.49251937866211\n",
      "Epoch [103/200] Loss: 21.272560119628906\n",
      "Epoch [104/200] Loss: 21.058574676513672\n",
      "Epoch [105/200] Loss: 20.850406646728516\n",
      "Epoch [106/200] Loss: 20.647932052612305\n",
      "Epoch [107/200] Loss: 20.45102310180664\n",
      "Epoch [108/200] Loss: 20.25954818725586\n",
      "Epoch [109/200] Loss: 20.073383331298828\n",
      "Epoch [110/200] Loss: 19.89240264892578\n",
      "Epoch [111/200] Loss: 19.716495513916016\n",
      "Epoch [112/200] Loss: 19.5455322265625\n",
      "Epoch [113/200] Loss: 19.379409790039062\n",
      "Epoch [114/200] Loss: 19.21800994873047\n",
      "Epoch [115/200] Loss: 19.061206817626953\n",
      "Epoch [116/200] Loss: 18.90890121459961\n",
      "Epoch [117/200] Loss: 18.760986328125\n",
      "Epoch [118/200] Loss: 18.617361068725586\n",
      "Epoch [119/200] Loss: 18.4779109954834\n",
      "Epoch [120/200] Loss: 18.342544555664062\n",
      "Epoch [121/200] Loss: 18.211153030395508\n",
      "Epoch [122/200] Loss: 18.083642959594727\n",
      "Epoch [123/200] Loss: 17.95991325378418\n",
      "Epoch [124/200] Loss: 17.839876174926758\n",
      "Epoch [125/200] Loss: 17.723438262939453\n",
      "Epoch [126/200] Loss: 17.610504150390625\n",
      "Epoch [127/200] Loss: 17.500991821289062\n",
      "Epoch [128/200] Loss: 17.394807815551758\n",
      "Epoch [129/200] Loss: 17.2918701171875\n",
      "Epoch [130/200] Loss: 17.192096710205078\n",
      "Epoch [131/200] Loss: 17.095399856567383\n",
      "Epoch [132/200] Loss: 17.001705169677734\n",
      "Epoch [133/200] Loss: 16.910930633544922\n",
      "Epoch [134/200] Loss: 16.822999954223633\n",
      "Epoch [135/200] Loss: 16.737834930419922\n",
      "Epoch [136/200] Loss: 16.655363082885742\n",
      "Epoch [137/200] Loss: 16.575519561767578\n",
      "Epoch [138/200] Loss: 16.49822235107422\n",
      "Epoch [139/200] Loss: 16.423404693603516\n",
      "Epoch [140/200] Loss: 16.35100555419922\n",
      "Epoch [141/200] Loss: 16.280954360961914\n",
      "Epoch [142/200] Loss: 16.213184356689453\n",
      "Epoch [143/200] Loss: 16.147628784179688\n",
      "Epoch [144/200] Loss: 16.084232330322266\n",
      "Epoch [145/200] Loss: 16.022930145263672\n",
      "Epoch [146/200] Loss: 15.963674545288086\n",
      "Epoch [147/200] Loss: 15.906387329101562\n",
      "Epoch [148/200] Loss: 15.85102653503418\n",
      "Epoch [149/200] Loss: 15.797531127929688\n",
      "Epoch [150/200] Loss: 15.745847702026367\n",
      "Epoch [151/200] Loss: 15.6959228515625\n",
      "Epoch [152/200] Loss: 15.647704124450684\n",
      "Epoch [153/200] Loss: 15.601146697998047\n",
      "Epoch [154/200] Loss: 15.556196212768555\n",
      "Epoch [155/200] Loss: 15.51280403137207\n",
      "Epoch [156/200] Loss: 15.470925331115723\n",
      "Epoch [157/200] Loss: 15.430510520935059\n",
      "Epoch [158/200] Loss: 15.391523361206055\n",
      "Epoch [159/200] Loss: 15.353910446166992\n",
      "Epoch [160/200] Loss: 15.317638397216797\n",
      "Epoch [161/200] Loss: 15.282658576965332\n",
      "Epoch [162/200] Loss: 15.248934745788574\n",
      "Epoch [163/200] Loss: 15.216423988342285\n",
      "Epoch [164/200] Loss: 15.18509292602539\n",
      "Epoch [165/200] Loss: 15.154901504516602\n",
      "Epoch [166/200] Loss: 15.125814437866211\n",
      "Epoch [167/200] Loss: 15.097793579101562\n",
      "Epoch [168/200] Loss: 15.070807456970215\n",
      "Epoch [169/200] Loss: 15.044820785522461\n",
      "Epoch [170/200] Loss: 15.01980209350586\n",
      "Epoch [171/200] Loss: 14.995718955993652\n",
      "Epoch [172/200] Loss: 14.972541809082031\n",
      "Epoch [173/200] Loss: 14.950237274169922\n",
      "Epoch [174/200] Loss: 14.928781509399414\n",
      "Epoch [175/200] Loss: 14.908140182495117\n",
      "Epoch [176/200] Loss: 14.88829231262207\n",
      "Epoch [177/200] Loss: 14.869205474853516\n",
      "Epoch [178/200] Loss: 14.850854873657227\n",
      "Epoch [179/200] Loss: 14.833216667175293\n",
      "Epoch [180/200] Loss: 14.816266059875488\n",
      "Epoch [181/200] Loss: 14.799980163574219\n",
      "Epoch [182/200] Loss: 14.784331321716309\n",
      "Epoch [183/200] Loss: 14.769303321838379\n",
      "Epoch [184/200] Loss: 14.75486946105957\n",
      "Epoch [185/200] Loss: 14.741010665893555\n",
      "Epoch [186/200] Loss: 14.727707862854004\n",
      "Epoch [187/200] Loss: 14.71493911743164\n",
      "Epoch [188/200] Loss: 14.702686309814453\n",
      "Epoch [189/200] Loss: 14.690929412841797\n",
      "Epoch [190/200] Loss: 14.67965030670166\n",
      "Epoch [191/200] Loss: 14.668834686279297\n",
      "Epoch [192/200] Loss: 14.65846061706543\n",
      "Epoch [193/200] Loss: 14.648515701293945\n",
      "Epoch [194/200] Loss: 14.638984680175781\n",
      "Epoch [195/200] Loss: 14.62984848022461\n",
      "Epoch [196/200] Loss: 14.621092796325684\n",
      "Epoch [197/200] Loss: 14.61270523071289\n",
      "Epoch [198/200] Loss: 14.604671478271484\n",
      "Epoch [199/200] Loss: 14.596979141235352\n",
      "Epoch [200/200] Loss: 14.589611053466797\n",
      "Predicted days_remaining for parent_id 430: 11.369134902954102\n",
      "Training for parent_id 433...\n",
      "Epoch [1/200] Loss: 511.52978515625\n",
      "Epoch [2/200] Loss: 501.4008483886719\n",
      "Epoch [3/200] Loss: 491.5732421875\n",
      "Epoch [4/200] Loss: 482.107666015625\n",
      "Epoch [5/200] Loss: 473.029052734375\n",
      "Epoch [6/200] Loss: 464.3518371582031\n",
      "Epoch [7/200] Loss: 456.0721740722656\n",
      "Epoch [8/200] Loss: 448.167236328125\n",
      "Epoch [9/200] Loss: 440.604736328125\n",
      "Epoch [10/200] Loss: 433.353759765625\n",
      "Epoch [11/200] Loss: 426.3897705078125\n",
      "Epoch [12/200] Loss: 419.694580078125\n",
      "Epoch [13/200] Loss: 413.2547912597656\n",
      "Epoch [14/200] Loss: 407.0589599609375\n",
      "Epoch [15/200] Loss: 401.09710693359375\n",
      "Epoch [16/200] Loss: 395.3591613769531\n",
      "Epoch [17/200] Loss: 389.8360595703125\n",
      "Epoch [18/200] Loss: 384.518798828125\n",
      "Epoch [19/200] Loss: 379.3998107910156\n",
      "Epoch [20/200] Loss: 374.4722595214844\n",
      "Epoch [21/200] Loss: 369.729736328125\n",
      "Epoch [22/200] Loss: 365.1656494140625\n",
      "Epoch [23/200] Loss: 360.7725830078125\n",
      "Epoch [24/200] Loss: 356.54241943359375\n",
      "Epoch [25/200] Loss: 352.46575927734375\n",
      "Epoch [26/200] Loss: 348.53253173828125\n",
      "Epoch [27/200] Loss: 344.7327575683594\n",
      "Epoch [28/200] Loss: 341.0563049316406\n",
      "Epoch [29/200] Loss: 337.4940490722656\n",
      "Epoch [30/200] Loss: 334.0373229980469\n",
      "Epoch [31/200] Loss: 330.678466796875\n",
      "Epoch [32/200] Loss: 327.4105224609375\n",
      "Epoch [33/200] Loss: 324.2271423339844\n",
      "Epoch [34/200] Loss: 321.12261962890625\n",
      "Epoch [35/200] Loss: 318.0916442871094\n",
      "Epoch [36/200] Loss: 315.1291198730469\n",
      "Epoch [37/200] Loss: 312.2301025390625\n",
      "Epoch [38/200] Loss: 309.38983154296875\n",
      "Epoch [39/200] Loss: 306.6036682128906\n",
      "Epoch [40/200] Loss: 303.86724853515625\n",
      "Epoch [41/200] Loss: 301.1766052246094\n",
      "Epoch [42/200] Loss: 298.5281677246094\n",
      "Epoch [43/200] Loss: 295.91888427734375\n",
      "Epoch [44/200] Loss: 293.34625244140625\n",
      "Epoch [45/200] Loss: 290.8081970214844\n",
      "Epoch [46/200] Loss: 288.3028564453125\n",
      "Epoch [47/200] Loss: 285.828857421875\n",
      "Epoch [48/200] Loss: 283.38470458984375\n",
      "Epoch [49/200] Loss: 280.96929931640625\n",
      "Epoch [50/200] Loss: 278.5815734863281\n",
      "Epoch [51/200] Loss: 276.2203369140625\n",
      "Epoch [52/200] Loss: 273.88470458984375\n",
      "Epoch [53/200] Loss: 271.5735168457031\n",
      "Epoch [54/200] Loss: 269.28582763671875\n",
      "Epoch [55/200] Loss: 267.0206298828125\n",
      "Epoch [56/200] Loss: 264.7770690917969\n",
      "Epoch [57/200] Loss: 262.55419921875\n",
      "Epoch [58/200] Loss: 260.3511962890625\n",
      "Epoch [59/200] Loss: 258.1673583984375\n",
      "Epoch [60/200] Loss: 256.00201416015625\n",
      "Epoch [61/200] Loss: 253.85462951660156\n",
      "Epoch [62/200] Loss: 251.72476196289062\n",
      "Epoch [63/200] Loss: 249.61190795898438\n",
      "Epoch [64/200] Loss: 247.51576232910156\n",
      "Epoch [65/200] Loss: 245.4359893798828\n",
      "Epoch [66/200] Loss: 243.3723907470703\n",
      "Epoch [67/200] Loss: 241.32479858398438\n",
      "Epoch [68/200] Loss: 239.29296875\n",
      "Epoch [69/200] Loss: 237.2767791748047\n",
      "Epoch [70/200] Loss: 235.27615356445312\n",
      "Epoch [71/200] Loss: 233.29098510742188\n",
      "Epoch [72/200] Loss: 231.32115173339844\n",
      "Epoch [73/200] Loss: 229.36668395996094\n",
      "Epoch [74/200] Loss: 227.42739868164062\n",
      "Epoch [75/200] Loss: 225.5033721923828\n",
      "Epoch [76/200] Loss: 223.59442138671875\n",
      "Epoch [77/200] Loss: 221.7005615234375\n",
      "Epoch [78/200] Loss: 219.82167053222656\n",
      "Epoch [79/200] Loss: 217.957763671875\n",
      "Epoch [80/200] Loss: 216.10865783691406\n",
      "Epoch [81/200] Loss: 214.27439880371094\n",
      "Epoch [82/200] Loss: 212.4547882080078\n",
      "Epoch [83/200] Loss: 210.64979553222656\n",
      "Epoch [84/200] Loss: 208.85935974121094\n",
      "Epoch [85/200] Loss: 207.08334350585938\n",
      "Epoch [86/200] Loss: 205.32164001464844\n",
      "Epoch [87/200] Loss: 203.57420349121094\n",
      "Epoch [88/200] Loss: 201.84085083007812\n",
      "Epoch [89/200] Loss: 200.12155151367188\n",
      "Epoch [90/200] Loss: 198.41616821289062\n",
      "Epoch [91/200] Loss: 196.72457885742188\n",
      "Epoch [92/200] Loss: 195.04672241210938\n",
      "Epoch [93/200] Loss: 193.38246154785156\n",
      "Epoch [94/200] Loss: 191.73167419433594\n",
      "Epoch [95/200] Loss: 190.0942840576172\n",
      "Epoch [96/200] Loss: 188.47010803222656\n",
      "Epoch [97/200] Loss: 186.859130859375\n",
      "Epoch [98/200] Loss: 185.26116943359375\n",
      "Epoch [99/200] Loss: 183.67620849609375\n",
      "Epoch [100/200] Loss: 182.10401916503906\n",
      "Epoch [101/200] Loss: 180.54458618164062\n",
      "Epoch [102/200] Loss: 178.997802734375\n",
      "Epoch [103/200] Loss: 177.4635009765625\n",
      "Epoch [104/200] Loss: 175.94168090820312\n",
      "Epoch [105/200] Loss: 174.43211364746094\n",
      "Epoch [106/200] Loss: 172.93479919433594\n",
      "Epoch [107/200] Loss: 171.44960021972656\n",
      "Epoch [108/200] Loss: 169.97640991210938\n",
      "Epoch [109/200] Loss: 168.51515197753906\n",
      "Epoch [110/200] Loss: 167.0657196044922\n",
      "Epoch [111/200] Loss: 165.62802124023438\n",
      "Epoch [112/200] Loss: 164.2019805908203\n",
      "Epoch [113/200] Loss: 162.78750610351562\n",
      "Epoch [114/200] Loss: 161.3843994140625\n",
      "Epoch [115/200] Loss: 159.99270629882812\n",
      "Epoch [116/200] Loss: 158.6123504638672\n",
      "Epoch [117/200] Loss: 157.24314880371094\n",
      "Epoch [118/200] Loss: 155.88497924804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/200] Loss: 154.5378875732422\n",
      "Epoch [120/200] Loss: 153.2017364501953\n",
      "Epoch [121/200] Loss: 151.87640380859375\n",
      "Epoch [122/200] Loss: 150.56187438964844\n",
      "Epoch [123/200] Loss: 149.2579803466797\n",
      "Epoch [124/200] Loss: 147.96473693847656\n",
      "Epoch [125/200] Loss: 146.6820068359375\n",
      "Epoch [126/200] Loss: 145.4097137451172\n",
      "Epoch [127/200] Loss: 144.1477508544922\n",
      "Epoch [128/200] Loss: 142.89614868164062\n",
      "Epoch [129/200] Loss: 141.65469360351562\n",
      "Epoch [130/200] Loss: 140.42344665527344\n",
      "Epoch [131/200] Loss: 139.2022247314453\n",
      "Epoch [132/200] Loss: 137.99099731445312\n",
      "Epoch [133/200] Loss: 136.78968811035156\n",
      "Epoch [134/200] Loss: 135.59823608398438\n",
      "Epoch [135/200] Loss: 134.41651916503906\n",
      "Epoch [136/200] Loss: 133.24456787109375\n",
      "Epoch [137/200] Loss: 132.0822296142578\n",
      "Epoch [138/200] Loss: 130.92941284179688\n",
      "Epoch [139/200] Loss: 129.78614807128906\n",
      "Epoch [140/200] Loss: 128.65228271484375\n",
      "Epoch [141/200] Loss: 127.52779388427734\n",
      "Epoch [142/200] Loss: 126.41258239746094\n",
      "Epoch [143/200] Loss: 125.30662536621094\n",
      "Epoch [144/200] Loss: 124.2098617553711\n",
      "Epoch [145/200] Loss: 123.12212371826172\n",
      "Epoch [146/200] Loss: 122.04348754882812\n",
      "Epoch [147/200] Loss: 120.97381591796875\n",
      "Epoch [148/200] Loss: 119.91303253173828\n",
      "Epoch [149/200] Loss: 118.86109161376953\n",
      "Epoch [150/200] Loss: 117.81796264648438\n",
      "Epoch [151/200] Loss: 116.78353881835938\n",
      "Epoch [152/200] Loss: 115.7578125\n",
      "Epoch [153/200] Loss: 114.74061584472656\n",
      "Epoch [154/200] Loss: 113.7320327758789\n",
      "Epoch [155/200] Loss: 112.73193359375\n",
      "Epoch [156/200] Loss: 111.74024963378906\n",
      "Epoch [157/200] Loss: 110.75695037841797\n",
      "Epoch [158/200] Loss: 109.78193664550781\n",
      "Epoch [159/200] Loss: 108.81514739990234\n",
      "Epoch [160/200] Loss: 107.85662841796875\n",
      "Epoch [161/200] Loss: 106.90621185302734\n",
      "Epoch [162/200] Loss: 105.9638671875\n",
      "Epoch [163/200] Loss: 105.0295639038086\n",
      "Epoch [164/200] Loss: 104.10323333740234\n",
      "Epoch [165/200] Loss: 103.18482971191406\n",
      "Epoch [166/200] Loss: 102.27423858642578\n",
      "Epoch [167/200] Loss: 101.37152862548828\n",
      "Epoch [168/200] Loss: 100.47652435302734\n",
      "Epoch [169/200] Loss: 99.5892562866211\n",
      "Epoch [170/200] Loss: 98.70958709716797\n",
      "Epoch [171/200] Loss: 97.83756256103516\n",
      "Epoch [172/200] Loss: 96.97306823730469\n",
      "Epoch [173/200] Loss: 96.11604309082031\n",
      "Epoch [174/200] Loss: 95.2664794921875\n",
      "Epoch [175/200] Loss: 94.42431640625\n",
      "Epoch [176/200] Loss: 93.58946228027344\n",
      "Epoch [177/200] Loss: 92.76193237304688\n",
      "Epoch [178/200] Loss: 91.94159698486328\n",
      "Epoch [179/200] Loss: 91.1284408569336\n",
      "Epoch [180/200] Loss: 90.32246398925781\n",
      "Epoch [181/200] Loss: 89.5235366821289\n",
      "Epoch [182/200] Loss: 88.7316665649414\n",
      "Epoch [183/200] Loss: 87.94679260253906\n",
      "Epoch [184/200] Loss: 87.16883087158203\n",
      "Epoch [185/200] Loss: 86.39778900146484\n",
      "Epoch [186/200] Loss: 85.63359069824219\n",
      "Epoch [187/200] Loss: 84.87616729736328\n",
      "Epoch [188/200] Loss: 84.12549591064453\n",
      "Epoch [189/200] Loss: 83.38152313232422\n",
      "Epoch [190/200] Loss: 82.64421844482422\n",
      "Epoch [191/200] Loss: 81.91351318359375\n",
      "Epoch [192/200] Loss: 81.18936157226562\n",
      "Epoch [193/200] Loss: 80.47171020507812\n",
      "Epoch [194/200] Loss: 79.76056671142578\n",
      "Epoch [195/200] Loss: 79.05582427978516\n",
      "Epoch [196/200] Loss: 78.35746765136719\n",
      "Epoch [197/200] Loss: 77.66539001464844\n",
      "Epoch [198/200] Loss: 76.97966766357422\n",
      "Epoch [199/200] Loss: 76.30014038085938\n",
      "Epoch [200/200] Loss: 75.6268539428711\n",
      "Predicted days_remaining for parent_id 433: 14.970399856567383\n",
      "Training for parent_id 434...\n",
      "Epoch [1/200] Loss: 149.79225158691406\n",
      "Epoch [2/200] Loss: 144.7471160888672\n",
      "Epoch [3/200] Loss: 139.86143493652344\n",
      "Epoch [4/200] Loss: 135.14881896972656\n",
      "Epoch [5/200] Loss: 130.6220245361328\n",
      "Epoch [6/200] Loss: 126.28819274902344\n",
      "Epoch [7/200] Loss: 122.14888000488281\n",
      "Epoch [8/200] Loss: 118.20252990722656\n",
      "Epoch [9/200] Loss: 114.44573211669922\n",
      "Epoch [10/200] Loss: 110.87371063232422\n",
      "Epoch [11/200] Loss: 107.48042297363281\n",
      "Epoch [12/200] Loss: 104.25896453857422\n",
      "Epoch [13/200] Loss: 101.20179748535156\n",
      "Epoch [14/200] Loss: 98.30087280273438\n",
      "Epoch [15/200] Loss: 95.5478286743164\n",
      "Epoch [16/200] Loss: 92.9339370727539\n",
      "Epoch [17/200] Loss: 90.45037078857422\n",
      "Epoch [18/200] Loss: 88.0884017944336\n",
      "Epoch [19/200] Loss: 85.83965301513672\n",
      "Epoch [20/200] Loss: 83.69624328613281\n",
      "Epoch [21/200] Loss: 81.65071105957031\n",
      "Epoch [22/200] Loss: 79.69634246826172\n",
      "Epoch [23/200] Loss: 77.82693481445312\n",
      "Epoch [24/200] Loss: 76.03678131103516\n",
      "Epoch [25/200] Loss: 74.32070922851562\n",
      "Epoch [26/200] Loss: 72.67374420166016\n",
      "Epoch [27/200] Loss: 71.09137725830078\n",
      "Epoch [28/200] Loss: 69.56916046142578\n",
      "Epoch [29/200] Loss: 68.10293579101562\n",
      "Epoch [30/200] Loss: 66.68875885009766\n",
      "Epoch [31/200] Loss: 65.32284545898438\n",
      "Epoch [32/200] Loss: 64.00173950195312\n",
      "Epoch [33/200] Loss: 62.72227478027344\n",
      "Epoch [34/200] Loss: 61.481590270996094\n",
      "Epoch [35/200] Loss: 60.27715301513672\n",
      "Epoch [36/200] Loss: 59.10677719116211\n",
      "Epoch [37/200] Loss: 57.968475341796875\n",
      "Epoch [38/200] Loss: 56.86058807373047\n",
      "Epoch [39/200] Loss: 55.781620025634766\n",
      "Epoch [40/200] Loss: 54.730247497558594\n",
      "Epoch [41/200] Loss: 53.7053108215332\n",
      "Epoch [42/200] Loss: 52.70576095581055\n",
      "Epoch [43/200] Loss: 51.730648040771484\n",
      "Epoch [44/200] Loss: 50.779090881347656\n",
      "Epoch [45/200] Loss: 49.850303649902344\n",
      "Epoch [46/200] Loss: 48.94353103637695\n",
      "Epoch [47/200] Loss: 48.05811309814453\n",
      "Epoch [48/200] Loss: 47.193389892578125\n",
      "Epoch [49/200] Loss: 46.34877014160156\n",
      "Epoch [50/200] Loss: 45.52370834350586\n",
      "Epoch [51/200] Loss: 44.717655181884766\n",
      "Epoch [52/200] Loss: 43.93012619018555\n",
      "Epoch [53/200] Loss: 43.16066360473633\n",
      "Epoch [54/200] Loss: 42.4088134765625\n",
      "Epoch [55/200] Loss: 41.674171447753906\n",
      "Epoch [56/200] Loss: 40.956336975097656\n",
      "Epoch [57/200] Loss: 40.25492858886719\n",
      "Epoch [58/200] Loss: 39.56956481933594\n",
      "Epoch [59/200] Loss: 38.899906158447266\n",
      "Epoch [60/200] Loss: 38.2456169128418\n",
      "Epoch [61/200] Loss: 37.60639572143555\n",
      "Epoch [62/200] Loss: 36.98188018798828\n",
      "Epoch [63/200] Loss: 36.37179946899414\n",
      "Epoch [64/200] Loss: 35.775856018066406\n",
      "Epoch [65/200] Loss: 35.193729400634766\n",
      "Epoch [66/200] Loss: 34.62516784667969\n",
      "Epoch [67/200] Loss: 34.069889068603516\n",
      "Epoch [68/200] Loss: 33.527626037597656\n",
      "Epoch [69/200] Loss: 32.998111724853516\n",
      "Epoch [70/200] Loss: 32.48108673095703\n",
      "Epoch [71/200] Loss: 31.97631072998047\n",
      "Epoch [72/200] Loss: 31.4835205078125\n",
      "Epoch [73/200] Loss: 31.00248908996582\n",
      "Epoch [74/200] Loss: 30.53296661376953\n",
      "Epoch [75/200] Loss: 30.074748992919922\n",
      "Epoch [76/200] Loss: 29.627580642700195\n",
      "Epoch [77/200] Loss: 29.19125747680664\n",
      "Epoch [78/200] Loss: 28.765548706054688\n",
      "Epoch [79/200] Loss: 28.350252151489258\n",
      "Epoch [80/200] Loss: 27.945167541503906\n",
      "Epoch [81/200] Loss: 27.550060272216797\n",
      "Epoch [82/200] Loss: 27.16476821899414\n",
      "Epoch [83/200] Loss: 26.789081573486328\n",
      "Epoch [84/200] Loss: 26.42279052734375\n",
      "Epoch [85/200] Loss: 26.06572723388672\n",
      "Epoch [86/200] Loss: 25.71769905090332\n",
      "Epoch [87/200] Loss: 25.37853240966797\n",
      "Epoch [88/200] Loss: 25.04804229736328\n",
      "Epoch [89/200] Loss: 24.726055145263672\n",
      "Epoch [90/200] Loss: 24.412397384643555\n",
      "Epoch [91/200] Loss: 24.106903076171875\n",
      "Epoch [92/200] Loss: 23.809412002563477\n",
      "Epoch [93/200] Loss: 23.519752502441406\n",
      "Epoch [94/200] Loss: 23.23776626586914\n",
      "Epoch [95/200] Loss: 22.963293075561523\n",
      "Epoch [96/200] Loss: 22.696184158325195\n",
      "Epoch [97/200] Loss: 22.436275482177734\n",
      "Epoch [98/200] Loss: 22.183422088623047\n",
      "Epoch [99/200] Loss: 21.937488555908203\n",
      "Epoch [100/200] Loss: 21.698287963867188\n",
      "Epoch [101/200] Loss: 21.465717315673828\n",
      "Epoch [102/200] Loss: 21.239601135253906\n",
      "Epoch [103/200] Loss: 21.01980972290039\n",
      "Epoch [104/200] Loss: 20.806209564208984\n",
      "Epoch [105/200] Loss: 20.59864044189453\n",
      "Epoch [106/200] Loss: 20.396991729736328\n",
      "Epoch [107/200] Loss: 20.201101303100586\n",
      "Epoch [108/200] Loss: 20.01085662841797\n",
      "Epoch [109/200] Loss: 19.82611846923828\n",
      "Epoch [110/200] Loss: 19.646753311157227\n",
      "Epoch [111/200] Loss: 19.47263526916504\n",
      "Epoch [112/200] Loss: 19.303634643554688\n",
      "Epoch [113/200] Loss: 19.139633178710938\n",
      "Epoch [114/200] Loss: 18.980501174926758\n",
      "Epoch [115/200] Loss: 18.826122283935547\n",
      "Epoch [116/200] Loss: 18.676376342773438\n",
      "Epoch [117/200] Loss: 18.531150817871094\n",
      "Epoch [118/200] Loss: 18.39031982421875\n",
      "Epoch [119/200] Loss: 18.253787994384766\n",
      "Epoch [120/200] Loss: 18.121429443359375\n",
      "Epoch [121/200] Loss: 17.993133544921875\n",
      "Epoch [122/200] Loss: 17.868810653686523\n",
      "Epoch [123/200] Loss: 17.748342514038086\n",
      "Epoch [124/200] Loss: 17.631635665893555\n",
      "Epoch [125/200] Loss: 17.518583297729492\n",
      "Epoch [126/200] Loss: 17.40909194946289\n",
      "Epoch [127/200] Loss: 17.303058624267578\n",
      "Epoch [128/200] Loss: 17.200395584106445\n",
      "Epoch [129/200] Loss: 17.10100746154785\n",
      "Epoch [130/200] Loss: 17.004810333251953\n",
      "Epoch [131/200] Loss: 16.911705017089844\n",
      "Epoch [132/200] Loss: 16.821624755859375\n",
      "Epoch [133/200] Loss: 16.73446273803711\n",
      "Epoch [134/200] Loss: 16.650146484375\n",
      "Epoch [135/200] Loss: 16.5685977935791\n",
      "Epoch [136/200] Loss: 16.4897403717041\n",
      "Epoch [137/200] Loss: 16.41349220275879\n",
      "Epoch [138/200] Loss: 16.33977508544922\n",
      "Epoch [139/200] Loss: 16.268526077270508\n",
      "Epoch [140/200] Loss: 16.199668884277344\n",
      "Epoch [141/200] Loss: 16.13313102722168\n",
      "Epoch [142/200] Loss: 16.06884765625\n",
      "Epoch [143/200] Loss: 16.00674819946289\n",
      "Epoch [144/200] Loss: 15.946776390075684\n",
      "Epoch [145/200] Loss: 15.888860702514648\n",
      "Epoch [146/200] Loss: 15.832945823669434\n",
      "Epoch [147/200] Loss: 15.778964042663574\n",
      "Epoch [148/200] Loss: 15.7268648147583\n",
      "Epoch [149/200] Loss: 15.676587104797363\n",
      "Epoch [150/200] Loss: 15.628076553344727\n",
      "Epoch [151/200] Loss: 15.581274032592773\n",
      "Epoch [152/200] Loss: 15.536134719848633\n",
      "Epoch [153/200] Loss: 15.49260139465332\n",
      "Epoch [154/200] Loss: 15.4506254196167\n",
      "Epoch [155/200] Loss: 15.410158157348633\n",
      "Epoch [156/200] Loss: 15.371152877807617\n",
      "Epoch [157/200] Loss: 15.333562850952148\n",
      "Epoch [158/200] Loss: 15.297341346740723\n",
      "Epoch [159/200] Loss: 15.262446403503418\n",
      "Epoch [160/200] Loss: 15.228836059570312\n",
      "Epoch [161/200] Loss: 15.196462631225586\n",
      "Epoch [162/200] Loss: 15.165292739868164\n",
      "Epoch [163/200] Loss: 15.135284423828125\n",
      "Epoch [164/200] Loss: 15.106402397155762\n",
      "Epoch [165/200] Loss: 15.078604698181152\n",
      "Epoch [166/200] Loss: 15.05185317993164\n",
      "Epoch [167/200] Loss: 15.026121139526367\n",
      "Epoch [168/200] Loss: 15.00136947631836\n",
      "Epoch [169/200] Loss: 14.977566719055176\n",
      "Epoch [170/200] Loss: 14.95467758178711\n",
      "Epoch [171/200] Loss: 14.932672500610352\n",
      "Epoch [172/200] Loss: 14.911521911621094\n",
      "Epoch [173/200] Loss: 14.891196250915527\n",
      "Epoch [174/200] Loss: 14.871665954589844\n",
      "Epoch [175/200] Loss: 14.852904319763184\n",
      "Epoch [176/200] Loss: 14.834882736206055\n",
      "Epoch [177/200] Loss: 14.81757926940918\n",
      "Epoch [178/200] Loss: 14.800962448120117\n",
      "Epoch [179/200] Loss: 14.785011291503906\n",
      "Epoch [180/200] Loss: 14.769702911376953\n",
      "Epoch [181/200] Loss: 14.755010604858398\n",
      "Epoch [182/200] Loss: 14.740915298461914\n",
      "Epoch [183/200] Loss: 14.72739315032959\n",
      "Epoch [184/200] Loss: 14.714425086975098\n",
      "Epoch [185/200] Loss: 14.701990127563477\n",
      "Epoch [186/200] Loss: 14.690068244934082\n",
      "Epoch [187/200] Loss: 14.678640365600586\n",
      "Epoch [188/200] Loss: 14.667688369750977\n",
      "Epoch [189/200] Loss: 14.65719223022461\n",
      "Epoch [190/200] Loss: 14.647139549255371\n",
      "Epoch [191/200] Loss: 14.637510299682617\n",
      "Epoch [192/200] Loss: 14.628288269042969\n",
      "Epoch [193/200] Loss: 14.619458198547363\n",
      "Epoch [194/200] Loss: 14.611005783081055\n",
      "Epoch [195/200] Loss: 14.602914810180664\n",
      "Epoch [196/200] Loss: 14.595172882080078\n",
      "Epoch [197/200] Loss: 14.587764739990234\n",
      "Epoch [198/200] Loss: 14.580678939819336\n",
      "Epoch [199/200] Loss: 14.573904037475586\n",
      "Epoch [200/200] Loss: 14.567424774169922\n",
      "Predicted days_remaining for parent_id 434: 11.398247718811035\n",
      "Training for parent_id 437...\n",
      "Epoch [1/200] Loss: 611.13330078125\n",
      "Epoch [2/200] Loss: 599.2015991210938\n",
      "Epoch [3/200] Loss: 587.5346069335938\n",
      "Epoch [4/200] Loss: 576.218505859375\n",
      "Epoch [5/200] Loss: 565.3060302734375\n",
      "Epoch [6/200] Loss: 554.8203735351562\n",
      "Epoch [7/200] Loss: 544.7612915039062\n",
      "Epoch [8/200] Loss: 535.1145629882812\n",
      "Epoch [9/200] Loss: 525.8607788085938\n",
      "Epoch [10/200] Loss: 516.9824829101562\n",
      "Epoch [11/200] Loss: 508.46527099609375\n",
      "Epoch [12/200] Loss: 500.2964782714844\n",
      "Epoch [13/200] Loss: 492.46392822265625\n",
      "Epoch [14/200] Loss: 484.95458984375\n",
      "Epoch [15/200] Loss: 477.75457763671875\n",
      "Epoch [16/200] Loss: 470.8492126464844\n",
      "Epoch [17/200] Loss: 464.2247314453125\n",
      "Epoch [18/200] Loss: 457.8680419921875\n",
      "Epoch [19/200] Loss: 451.7678527832031\n",
      "Epoch [20/200] Loss: 445.9139404296875\n",
      "Epoch [21/200] Loss: 440.29742431640625\n",
      "Epoch [22/200] Loss: 434.91064453125\n",
      "Epoch [23/200] Loss: 429.7462463378906\n",
      "Epoch [24/200] Loss: 424.7971496582031\n",
      "Epoch [25/200] Loss: 420.0555419921875\n",
      "Epoch [26/200] Loss: 415.512451171875\n",
      "Epoch [27/200] Loss: 411.1573181152344\n",
      "Epoch [28/200] Loss: 406.9784240722656\n",
      "Epoch [29/200] Loss: 402.9625244140625\n",
      "Epoch [30/200] Loss: 399.09613037109375\n",
      "Epoch [31/200] Loss: 395.36572265625\n",
      "Epoch [32/200] Loss: 391.75811767578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/200] Loss: 388.2610168457031\n",
      "Epoch [34/200] Loss: 384.86309814453125\n",
      "Epoch [35/200] Loss: 381.553955078125\n",
      "Epoch [36/200] Loss: 378.3243713378906\n",
      "Epoch [37/200] Loss: 375.166015625\n",
      "Epoch [38/200] Loss: 372.07183837890625\n",
      "Epoch [39/200] Loss: 369.0354919433594\n",
      "Epoch [40/200] Loss: 366.0517883300781\n",
      "Epoch [41/200] Loss: 363.11602783203125\n",
      "Epoch [42/200] Loss: 360.2243957519531\n",
      "Epoch [43/200] Loss: 357.3736877441406\n",
      "Epoch [44/200] Loss: 354.56103515625\n",
      "Epoch [45/200] Loss: 351.783935546875\n",
      "Epoch [46/200] Loss: 349.0403747558594\n",
      "Epoch [47/200] Loss: 346.3284912109375\n",
      "Epoch [48/200] Loss: 343.6467590332031\n",
      "Epoch [49/200] Loss: 340.9935607910156\n",
      "Epoch [50/200] Loss: 338.36773681640625\n",
      "Epoch [51/200] Loss: 335.76812744140625\n",
      "Epoch [52/200] Loss: 333.1937561035156\n",
      "Epoch [53/200] Loss: 330.6436767578125\n",
      "Epoch [54/200] Loss: 328.1172180175781\n",
      "Epoch [55/200] Loss: 325.6135559082031\n",
      "Epoch [56/200] Loss: 323.132080078125\n",
      "Epoch [57/200] Loss: 320.6722412109375\n",
      "Epoch [58/200] Loss: 318.2333679199219\n",
      "Epoch [59/200] Loss: 315.81524658203125\n",
      "Epoch [60/200] Loss: 313.4172058105469\n",
      "Epoch [61/200] Loss: 311.03887939453125\n",
      "Epoch [62/200] Loss: 308.6800842285156\n",
      "Epoch [63/200] Loss: 306.3402404785156\n",
      "Epoch [64/200] Loss: 304.01910400390625\n",
      "Epoch [65/200] Loss: 301.7164001464844\n",
      "Epoch [66/200] Loss: 299.43194580078125\n",
      "Epoch [67/200] Loss: 297.1652526855469\n",
      "Epoch [68/200] Loss: 294.9161376953125\n",
      "Epoch [69/200] Loss: 292.6844787597656\n",
      "Epoch [70/200] Loss: 290.46990966796875\n",
      "Epoch [71/200] Loss: 288.27227783203125\n",
      "Epoch [72/200] Loss: 286.0912780761719\n",
      "Epoch [73/200] Loss: 283.9267578125\n",
      "Epoch [74/200] Loss: 281.7785339355469\n",
      "Epoch [75/200] Loss: 279.6464538574219\n",
      "Epoch [76/200] Loss: 277.5302734375\n",
      "Epoch [77/200] Loss: 275.42974853515625\n",
      "Epoch [78/200] Loss: 273.3446960449219\n",
      "Epoch [79/200] Loss: 271.2751159667969\n",
      "Epoch [80/200] Loss: 269.220703125\n",
      "Epoch [81/200] Loss: 267.18133544921875\n",
      "Epoch [82/200] Loss: 265.15692138671875\n",
      "Epoch [83/200] Loss: 263.14715576171875\n",
      "Epoch [84/200] Loss: 261.1520080566406\n",
      "Epoch [85/200] Loss: 259.1712646484375\n",
      "Epoch [86/200] Loss: 257.2048645019531\n",
      "Epoch [87/200] Loss: 255.2524871826172\n",
      "Epoch [88/200] Loss: 253.3143310546875\n",
      "Epoch [89/200] Loss: 251.38998413085938\n",
      "Epoch [90/200] Loss: 249.47943115234375\n",
      "Epoch [91/200] Loss: 247.5824432373047\n",
      "Epoch [92/200] Loss: 245.6990509033203\n",
      "Epoch [93/200] Loss: 243.8289794921875\n",
      "Epoch [94/200] Loss: 241.97225952148438\n",
      "Epoch [95/200] Loss: 240.12867736816406\n",
      "Epoch [96/200] Loss: 238.29818725585938\n",
      "Epoch [97/200] Loss: 236.48056030273438\n",
      "Epoch [98/200] Loss: 234.6759033203125\n",
      "Epoch [99/200] Loss: 232.88389587402344\n",
      "Epoch [100/200] Loss: 231.10450744628906\n",
      "Epoch [101/200] Loss: 229.33766174316406\n",
      "Epoch [102/200] Loss: 227.5832977294922\n",
      "Epoch [103/200] Loss: 225.8412322998047\n",
      "Epoch [104/200] Loss: 224.1114501953125\n",
      "Epoch [105/200] Loss: 222.39381408691406\n",
      "Epoch [106/200] Loss: 220.68820190429688\n",
      "Epoch [107/200] Loss: 218.99452209472656\n",
      "Epoch [108/200] Loss: 217.31277465820312\n",
      "Epoch [109/200] Loss: 215.64288330078125\n",
      "Epoch [110/200] Loss: 213.98452758789062\n",
      "Epoch [111/200] Loss: 212.3379364013672\n",
      "Epoch [112/200] Loss: 210.70286560058594\n",
      "Epoch [113/200] Loss: 209.0791778564453\n",
      "Epoch [114/200] Loss: 207.46697998046875\n",
      "Epoch [115/200] Loss: 205.865966796875\n",
      "Epoch [116/200] Loss: 204.27621459960938\n",
      "Epoch [117/200] Loss: 202.69764709472656\n",
      "Epoch [118/200] Loss: 201.13011169433594\n",
      "Epoch [119/200] Loss: 199.57362365722656\n",
      "Epoch [120/200] Loss: 198.02796936035156\n",
      "Epoch [121/200] Loss: 196.4932098388672\n",
      "Epoch [122/200] Loss: 194.96920776367188\n",
      "Epoch [123/200] Loss: 193.45596313476562\n",
      "Epoch [124/200] Loss: 191.95330810546875\n",
      "Epoch [125/200] Loss: 190.46124267578125\n",
      "Epoch [126/200] Loss: 188.97962951660156\n",
      "Epoch [127/200] Loss: 187.5084991455078\n",
      "Epoch [128/200] Loss: 186.04776000976562\n",
      "Epoch [129/200] Loss: 184.59722900390625\n",
      "Epoch [130/200] Loss: 183.156982421875\n",
      "Epoch [131/200] Loss: 181.72689819335938\n",
      "Epoch [132/200] Loss: 180.30694580078125\n",
      "Epoch [133/200] Loss: 178.89698791503906\n",
      "Epoch [134/200] Loss: 177.4970703125\n",
      "Epoch [135/200] Loss: 176.10702514648438\n",
      "Epoch [136/200] Loss: 174.7268829345703\n",
      "Epoch [137/200] Loss: 173.3564910888672\n",
      "Epoch [138/200] Loss: 171.995849609375\n",
      "Epoch [139/200] Loss: 170.64491271972656\n",
      "Epoch [140/200] Loss: 169.3035888671875\n",
      "Epoch [141/200] Loss: 167.9718017578125\n",
      "Epoch [142/200] Loss: 166.64952087402344\n",
      "Epoch [143/200] Loss: 165.33668518066406\n",
      "Epoch [144/200] Loss: 164.03326416015625\n",
      "Epoch [145/200] Loss: 162.73916625976562\n",
      "Epoch [146/200] Loss: 161.454345703125\n",
      "Epoch [147/200] Loss: 160.1787872314453\n",
      "Epoch [148/200] Loss: 158.91233825683594\n",
      "Epoch [149/200] Loss: 157.655029296875\n",
      "Epoch [150/200] Loss: 156.40676879882812\n",
      "Epoch [151/200] Loss: 155.1675262451172\n",
      "Epoch [152/200] Loss: 153.93719482421875\n",
      "Epoch [153/200] Loss: 152.71583557128906\n",
      "Epoch [154/200] Loss: 151.50326538085938\n",
      "Epoch [155/200] Loss: 150.299560546875\n",
      "Epoch [156/200] Loss: 149.1045379638672\n",
      "Epoch [157/200] Loss: 147.918212890625\n",
      "Epoch [158/200] Loss: 146.74053955078125\n",
      "Epoch [159/200] Loss: 145.5714569091797\n",
      "Epoch [160/200] Loss: 144.4109344482422\n",
      "Epoch [161/200] Loss: 143.2588653564453\n",
      "Epoch [162/200] Loss: 142.11526489257812\n",
      "Epoch [163/200] Loss: 140.9800262451172\n",
      "Epoch [164/200] Loss: 139.85313415527344\n",
      "Epoch [165/200] Loss: 138.73455810546875\n",
      "Epoch [166/200] Loss: 137.62425231933594\n",
      "Epoch [167/200] Loss: 136.5220947265625\n",
      "Epoch [168/200] Loss: 135.42808532714844\n",
      "Epoch [169/200] Loss: 134.3422088623047\n",
      "Epoch [170/200] Loss: 133.26438903808594\n",
      "Epoch [171/200] Loss: 132.19459533691406\n",
      "Epoch [172/200] Loss: 131.13272094726562\n",
      "Epoch [173/200] Loss: 130.0787811279297\n",
      "Epoch [174/200] Loss: 129.03271484375\n",
      "Epoch [175/200] Loss: 127.99446105957031\n",
      "Epoch [176/200] Loss: 126.96395874023438\n",
      "Epoch [177/200] Loss: 125.94122314453125\n",
      "Epoch [178/200] Loss: 124.92615509033203\n",
      "Epoch [179/200] Loss: 123.91873931884766\n",
      "Epoch [180/200] Loss: 122.9189453125\n",
      "Epoch [181/200] Loss: 121.9266586303711\n",
      "Epoch [182/200] Loss: 120.94189453125\n",
      "Epoch [183/200] Loss: 119.9646224975586\n",
      "Epoch [184/200] Loss: 118.99475860595703\n",
      "Epoch [185/200] Loss: 118.03224182128906\n",
      "Epoch [186/200] Loss: 117.07711029052734\n",
      "Epoch [187/200] Loss: 116.12921142578125\n",
      "Epoch [188/200] Loss: 115.18859100341797\n",
      "Epoch [189/200] Loss: 114.25517272949219\n",
      "Epoch [190/200] Loss: 113.32891082763672\n",
      "Epoch [191/200] Loss: 112.4097671508789\n",
      "Epoch [192/200] Loss: 111.49771118164062\n",
      "Epoch [193/200] Loss: 110.59270477294922\n",
      "Epoch [194/200] Loss: 109.69466400146484\n",
      "Epoch [195/200] Loss: 108.80358123779297\n",
      "Epoch [196/200] Loss: 107.91938018798828\n",
      "Epoch [197/200] Loss: 107.04212188720703\n",
      "Epoch [198/200] Loss: 106.1716079711914\n",
      "Epoch [199/200] Loss: 105.30796813964844\n",
      "Epoch [200/200] Loss: 104.45098876953125\n",
      "Predicted days_remaining for parent_id 437: 15.307368278503418\n",
      "Training for parent_id 445...\n",
      "Epoch [1/200] Loss: 362.8207702636719\n",
      "Epoch [2/200] Loss: 354.52764892578125\n",
      "Epoch [3/200] Loss: 346.2924499511719\n",
      "Epoch [4/200] Loss: 338.1781005859375\n",
      "Epoch [5/200] Loss: 330.24420166015625\n",
      "Epoch [6/200] Loss: 322.5388488769531\n",
      "Epoch [7/200] Loss: 315.09759521484375\n",
      "Epoch [8/200] Loss: 307.9436340332031\n",
      "Epoch [9/200] Loss: 301.0891418457031\n",
      "Epoch [10/200] Loss: 294.5384521484375\n",
      "Epoch [11/200] Loss: 288.2903137207031\n",
      "Epoch [12/200] Loss: 282.3396301269531\n",
      "Epoch [13/200] Loss: 276.67767333984375\n",
      "Epoch [14/200] Loss: 271.29290771484375\n",
      "Epoch [15/200] Loss: 266.17193603515625\n",
      "Epoch [16/200] Loss: 261.30059814453125\n",
      "Epoch [17/200] Loss: 256.6642761230469\n",
      "Epoch [18/200] Loss: 252.2482147216797\n",
      "Epoch [19/200] Loss: 248.0375213623047\n",
      "Epoch [20/200] Loss: 244.0175323486328\n",
      "Epoch [21/200] Loss: 240.17388916015625\n",
      "Epoch [22/200] Loss: 236.49327087402344\n",
      "Epoch [23/200] Loss: 232.96327209472656\n",
      "Epoch [24/200] Loss: 229.57281494140625\n",
      "Epoch [25/200] Loss: 226.3118896484375\n",
      "Epoch [26/200] Loss: 223.1716766357422\n",
      "Epoch [27/200] Loss: 220.1440887451172\n",
      "Epoch [28/200] Loss: 217.221923828125\n",
      "Epoch [29/200] Loss: 214.3983612060547\n",
      "Epoch [30/200] Loss: 211.66668701171875\n",
      "Epoch [31/200] Loss: 209.02056884765625\n",
      "Epoch [32/200] Loss: 206.45362854003906\n",
      "Epoch [33/200] Loss: 203.9596710205078\n",
      "Epoch [34/200] Loss: 201.53256225585938\n",
      "Epoch [35/200] Loss: 199.1666717529297\n",
      "Epoch [36/200] Loss: 196.8567657470703\n",
      "Epoch [37/200] Loss: 194.59800720214844\n",
      "Epoch [38/200] Loss: 192.3861846923828\n",
      "Epoch [39/200] Loss: 190.21755981445312\n",
      "Epoch [40/200] Loss: 188.08888244628906\n",
      "Epoch [41/200] Loss: 185.99732971191406\n",
      "Epoch [42/200] Loss: 183.94044494628906\n",
      "Epoch [43/200] Loss: 181.9162139892578\n",
      "Epoch [44/200] Loss: 179.9228515625\n",
      "Epoch [45/200] Loss: 177.95883178710938\n",
      "Epoch [46/200] Loss: 176.02284240722656\n",
      "Epoch [47/200] Loss: 174.11380004882812\n",
      "Epoch [48/200] Loss: 172.23068237304688\n",
      "Epoch [49/200] Loss: 170.37266540527344\n",
      "Epoch [50/200] Loss: 168.53895568847656\n",
      "Epoch [51/200] Loss: 166.72882080078125\n",
      "Epoch [52/200] Loss: 164.941650390625\n",
      "Epoch [53/200] Loss: 163.17686462402344\n",
      "Epoch [54/200] Loss: 161.4339141845703\n",
      "Epoch [55/200] Loss: 159.71224975585938\n",
      "Epoch [56/200] Loss: 158.01145935058594\n",
      "Epoch [57/200] Loss: 156.3309783935547\n",
      "Epoch [58/200] Loss: 154.67042541503906\n",
      "Epoch [59/200] Loss: 153.02944946289062\n",
      "Epoch [60/200] Loss: 151.4075927734375\n",
      "Epoch [61/200] Loss: 149.80445861816406\n",
      "Epoch [62/200] Loss: 148.21978759765625\n",
      "Epoch [63/200] Loss: 146.65310668945312\n",
      "Epoch [64/200] Loss: 145.1042022705078\n",
      "Epoch [65/200] Loss: 143.57275390625\n",
      "Epoch [66/200] Loss: 142.05844116210938\n",
      "Epoch [67/200] Loss: 140.56092834472656\n",
      "Epoch [68/200] Loss: 139.080078125\n",
      "Epoch [69/200] Loss: 137.61550903320312\n",
      "Epoch [70/200] Loss: 136.16705322265625\n",
      "Epoch [71/200] Loss: 134.734375\n",
      "Epoch [72/200] Loss: 133.31732177734375\n",
      "Epoch [73/200] Loss: 131.9156951904297\n",
      "Epoch [74/200] Loss: 130.52914428710938\n",
      "Epoch [75/200] Loss: 129.15756225585938\n",
      "Epoch [76/200] Loss: 127.80077362060547\n",
      "Epoch [77/200] Loss: 126.45850372314453\n",
      "Epoch [78/200] Loss: 125.13060760498047\n",
      "Epoch [79/200] Loss: 123.81686401367188\n",
      "Epoch [80/200] Loss: 122.51709747314453\n",
      "Epoch [81/200] Loss: 121.2311782836914\n",
      "Epoch [82/200] Loss: 119.95887756347656\n",
      "Epoch [83/200] Loss: 118.7000732421875\n",
      "Epoch [84/200] Loss: 117.45458984375\n",
      "Epoch [85/200] Loss: 116.22225952148438\n",
      "Epoch [86/200] Loss: 115.00292205810547\n",
      "Epoch [87/200] Loss: 113.79644775390625\n",
      "Epoch [88/200] Loss: 112.60266876220703\n",
      "Epoch [89/200] Loss: 111.42147064208984\n",
      "Epoch [90/200] Loss: 110.25267791748047\n",
      "Epoch [91/200] Loss: 109.09618377685547\n",
      "Epoch [92/200] Loss: 107.95182037353516\n",
      "Epoch [93/200] Loss: 106.81950378417969\n",
      "Epoch [94/200] Loss: 105.6990737915039\n",
      "Epoch [95/200] Loss: 104.59037017822266\n",
      "Epoch [96/200] Loss: 103.49333953857422\n",
      "Epoch [97/200] Loss: 102.40780639648438\n",
      "Epoch [98/200] Loss: 101.33369445800781\n",
      "Epoch [99/200] Loss: 100.27084350585938\n",
      "Epoch [100/200] Loss: 99.21916961669922\n",
      "Epoch [101/200] Loss: 98.17855072021484\n",
      "Epoch [102/200] Loss: 97.14884948730469\n",
      "Epoch [103/200] Loss: 96.12998962402344\n",
      "Epoch [104/200] Loss: 95.12187957763672\n",
      "Epoch [105/200] Loss: 94.1243667602539\n",
      "Epoch [106/200] Loss: 93.13739013671875\n",
      "Epoch [107/200] Loss: 92.16080474853516\n",
      "Epoch [108/200] Loss: 91.19454956054688\n",
      "Epoch [109/200] Loss: 90.23847961425781\n",
      "Epoch [110/200] Loss: 89.29258728027344\n",
      "Epoch [111/200] Loss: 88.35664367675781\n",
      "Epoch [112/200] Loss: 87.4306640625\n",
      "Epoch [113/200] Loss: 86.5145034790039\n",
      "Epoch [114/200] Loss: 85.60809326171875\n",
      "Epoch [115/200] Loss: 84.7113037109375\n",
      "Epoch [116/200] Loss: 83.82412719726562\n",
      "Epoch [117/200] Loss: 82.94639587402344\n",
      "Epoch [118/200] Loss: 82.0780258178711\n",
      "Epoch [119/200] Loss: 81.21895599365234\n",
      "Epoch [120/200] Loss: 80.36911010742188\n",
      "Epoch [121/200] Loss: 79.52839660644531\n",
      "Epoch [122/200] Loss: 78.69672393798828\n",
      "Epoch [123/200] Loss: 77.87399291992188\n",
      "Epoch [124/200] Loss: 77.06011199951172\n",
      "Epoch [125/200] Loss: 76.25508880615234\n",
      "Epoch [126/200] Loss: 75.45874786376953\n",
      "Epoch [127/200] Loss: 74.67103576660156\n",
      "Epoch [128/200] Loss: 73.8919448852539\n",
      "Epoch [129/200] Loss: 73.12126922607422\n",
      "Epoch [130/200] Loss: 72.3590316772461\n",
      "Epoch [131/200] Loss: 71.60508728027344\n",
      "Epoch [132/200] Loss: 70.85944366455078\n",
      "Epoch [133/200] Loss: 70.12196350097656\n",
      "Epoch [134/200] Loss: 69.39258575439453\n",
      "Epoch [135/200] Loss: 68.67124938964844\n",
      "Epoch [136/200] Loss: 67.95787048339844\n",
      "Epoch [137/200] Loss: 67.25237274169922\n",
      "Epoch [138/200] Loss: 66.5547103881836\n",
      "Epoch [139/200] Loss: 65.86479187011719\n",
      "Epoch [140/200] Loss: 65.18256378173828\n",
      "Epoch [141/200] Loss: 64.50792694091797\n",
      "Epoch [142/200] Loss: 63.84083557128906\n",
      "Epoch [143/200] Loss: 63.18123245239258\n",
      "Epoch [144/200] Loss: 62.529014587402344\n",
      "Epoch [145/200] Loss: 61.88414764404297\n",
      "Epoch [146/200] Loss: 61.24656295776367\n",
      "Epoch [147/200] Loss: 60.61616134643555\n",
      "Epoch [148/200] Loss: 59.9929313659668\n",
      "Epoch [149/200] Loss: 59.376766204833984\n",
      "Epoch [150/200] Loss: 58.767635345458984\n",
      "Epoch [151/200] Loss: 58.16542434692383\n",
      "Epoch [152/200] Loss: 57.570098876953125\n",
      "Epoch [153/200] Loss: 56.98160171508789\n",
      "Epoch [154/200] Loss: 56.39988708496094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/200] Loss: 55.82484817504883\n",
      "Epoch [156/200] Loss: 55.2564811706543\n",
      "Epoch [157/200] Loss: 54.69466781616211\n",
      "Epoch [158/200] Loss: 54.13936233520508\n",
      "Epoch [159/200] Loss: 53.590518951416016\n",
      "Epoch [160/200] Loss: 53.048099517822266\n",
      "Epoch [161/200] Loss: 52.51201248168945\n",
      "Epoch [162/200] Loss: 51.982181549072266\n",
      "Epoch [163/200] Loss: 51.45859909057617\n",
      "Epoch [164/200] Loss: 50.94117736816406\n",
      "Epoch [165/200] Loss: 50.42985916137695\n",
      "Epoch [166/200] Loss: 49.92459487915039\n",
      "Epoch [167/200] Loss: 49.42533493041992\n",
      "Epoch [168/200] Loss: 48.93199157714844\n",
      "Epoch [169/200] Loss: 48.44453048706055\n",
      "Epoch [170/200] Loss: 47.9629020690918\n",
      "Epoch [171/200] Loss: 47.48704147338867\n",
      "Epoch [172/200] Loss: 47.01690673828125\n",
      "Epoch [173/200] Loss: 46.55242919921875\n",
      "Epoch [174/200] Loss: 46.09356689453125\n",
      "Epoch [175/200] Loss: 45.64025115966797\n",
      "Epoch [176/200] Loss: 45.192447662353516\n",
      "Epoch [177/200] Loss: 44.750099182128906\n",
      "Epoch [178/200] Loss: 44.31313705444336\n",
      "Epoch [179/200] Loss: 43.88151931762695\n",
      "Epoch [180/200] Loss: 43.45520782470703\n",
      "Epoch [181/200] Loss: 43.03411102294922\n",
      "Epoch [182/200] Loss: 42.61823272705078\n",
      "Epoch [183/200] Loss: 42.207481384277344\n",
      "Epoch [184/200] Loss: 41.80182647705078\n",
      "Epoch [185/200] Loss: 41.401222229003906\n",
      "Epoch [186/200] Loss: 41.00558090209961\n",
      "Epoch [187/200] Loss: 40.614898681640625\n",
      "Epoch [188/200] Loss: 40.22911834716797\n",
      "Epoch [189/200] Loss: 39.8481559753418\n",
      "Epoch [190/200] Loss: 39.47201156616211\n",
      "Epoch [191/200] Loss: 39.10061264038086\n",
      "Epoch [192/200] Loss: 38.73391342163086\n",
      "Epoch [193/200] Loss: 38.371864318847656\n",
      "Epoch [194/200] Loss: 38.01442337036133\n",
      "Epoch [195/200] Loss: 37.66154098510742\n",
      "Epoch [196/200] Loss: 37.31317138671875\n",
      "Epoch [197/200] Loss: 36.969276428222656\n",
      "Epoch [198/200] Loss: 36.629783630371094\n",
      "Epoch [199/200] Loss: 36.29469680786133\n",
      "Epoch [200/200] Loss: 35.963924407958984\n",
      "Predicted days_remaining for parent_id 445: 14.145660400390625\n",
      "Training for parent_id 454...\n",
      "Epoch [1/200] Loss: 481.6788330078125\n",
      "Epoch [2/200] Loss: 472.90704345703125\n",
      "Epoch [3/200] Loss: 464.2947082519531\n",
      "Epoch [4/200] Loss: 455.8601989746094\n",
      "Epoch [5/200] Loss: 447.6258544921875\n",
      "Epoch [6/200] Loss: 439.6021423339844\n",
      "Epoch [7/200] Loss: 431.785888671875\n",
      "Epoch [8/200] Loss: 424.1678161621094\n",
      "Epoch [9/200] Loss: 416.7400817871094\n",
      "Epoch [10/200] Loss: 409.4964904785156\n",
      "Epoch [11/200] Loss: 402.4331359863281\n",
      "Epoch [12/200] Loss: 395.5487365722656\n",
      "Epoch [13/200] Loss: 388.8440856933594\n",
      "Epoch [14/200] Loss: 382.3217468261719\n",
      "Epoch [15/200] Loss: 375.98565673828125\n",
      "Epoch [16/200] Loss: 369.84039306640625\n",
      "Epoch [17/200] Loss: 363.890625\n",
      "Epoch [18/200] Loss: 358.1399230957031\n",
      "Epoch [19/200] Loss: 352.5908203125\n",
      "Epoch [20/200] Loss: 347.243896484375\n",
      "Epoch [21/200] Loss: 342.09722900390625\n",
      "Epoch [22/200] Loss: 337.1473388671875\n",
      "Epoch [23/200] Loss: 332.3883361816406\n",
      "Epoch [24/200] Loss: 327.8128967285156\n",
      "Epoch [25/200] Loss: 323.41229248046875\n",
      "Epoch [26/200] Loss: 319.17706298828125\n",
      "Epoch [27/200] Loss: 315.09710693359375\n",
      "Epoch [28/200] Loss: 311.1619567871094\n",
      "Epoch [29/200] Loss: 307.3611145019531\n",
      "Epoch [30/200] Loss: 303.68438720703125\n",
      "Epoch [31/200] Loss: 300.1216125488281\n",
      "Epoch [32/200] Loss: 296.66357421875\n",
      "Epoch [33/200] Loss: 293.3013000488281\n",
      "Epoch [34/200] Loss: 290.02685546875\n",
      "Epoch [35/200] Loss: 286.8327331542969\n",
      "Epoch [36/200] Loss: 283.7124328613281\n",
      "Epoch [37/200] Loss: 280.6600341796875\n",
      "Epoch [38/200] Loss: 277.6702880859375\n",
      "Epoch [39/200] Loss: 274.7384948730469\n",
      "Epoch [40/200] Loss: 271.8606262207031\n",
      "Epoch [41/200] Loss: 269.03277587890625\n",
      "Epoch [42/200] Loss: 266.2518615722656\n",
      "Epoch [43/200] Loss: 263.5149230957031\n",
      "Epoch [44/200] Loss: 260.8193664550781\n",
      "Epoch [45/200] Loss: 258.1628723144531\n",
      "Epoch [46/200] Loss: 255.54344177246094\n",
      "Epoch [47/200] Loss: 252.95921325683594\n",
      "Epoch [48/200] Loss: 250.4086456298828\n",
      "Epoch [49/200] Loss: 247.89028930664062\n",
      "Epoch [50/200] Loss: 245.40284729003906\n",
      "Epoch [51/200] Loss: 242.9451904296875\n",
      "Epoch [52/200] Loss: 240.5162353515625\n",
      "Epoch [53/200] Loss: 238.1151885986328\n",
      "Epoch [54/200] Loss: 235.7412872314453\n",
      "Epoch [55/200] Loss: 233.39352416992188\n",
      "Epoch [56/200] Loss: 231.0714569091797\n",
      "Epoch [57/200] Loss: 228.77439880371094\n",
      "Epoch [58/200] Loss: 226.50180053710938\n",
      "Epoch [59/200] Loss: 224.25318908691406\n",
      "Epoch [60/200] Loss: 222.02818298339844\n",
      "Epoch [61/200] Loss: 219.8263702392578\n",
      "Epoch [62/200] Loss: 217.6473846435547\n",
      "Epoch [63/200] Loss: 215.4910430908203\n",
      "Epoch [64/200] Loss: 213.35693359375\n",
      "Epoch [65/200] Loss: 211.24497985839844\n",
      "Epoch [66/200] Loss: 209.15493774414062\n",
      "Epoch [67/200] Loss: 207.08653259277344\n",
      "Epoch [68/200] Loss: 205.0395965576172\n",
      "Epoch [69/200] Loss: 203.0139617919922\n",
      "Epoch [70/200] Loss: 201.0093536376953\n",
      "Epoch [71/200] Loss: 199.02561950683594\n",
      "Epoch [72/200] Loss: 197.06251525878906\n",
      "Epoch [73/200] Loss: 195.119873046875\n",
      "Epoch [74/200] Loss: 193.197509765625\n",
      "Epoch [75/200] Loss: 191.29518127441406\n",
      "Epoch [76/200] Loss: 189.4126739501953\n",
      "Epoch [77/200] Loss: 187.54991149902344\n",
      "Epoch [78/200] Loss: 185.70655822753906\n",
      "Epoch [79/200] Loss: 183.8824920654297\n",
      "Epoch [80/200] Loss: 182.07754516601562\n",
      "Epoch [81/200] Loss: 180.29144287109375\n",
      "Epoch [82/200] Loss: 178.52403259277344\n",
      "Epoch [83/200] Loss: 176.7750701904297\n",
      "Epoch [84/200] Loss: 175.04437255859375\n",
      "Epoch [85/200] Loss: 173.33169555664062\n",
      "Epoch [86/200] Loss: 171.6368408203125\n",
      "Epoch [87/200] Loss: 169.9595947265625\n",
      "Epoch [88/200] Loss: 168.29969787597656\n",
      "Epoch [89/200] Loss: 166.65699768066406\n",
      "Epoch [90/200] Loss: 165.03126525878906\n",
      "Epoch [91/200] Loss: 163.4222412109375\n",
      "Epoch [92/200] Loss: 161.8297576904297\n",
      "Epoch [93/200] Loss: 160.25363159179688\n",
      "Epoch [94/200] Loss: 158.69357299804688\n",
      "Epoch [95/200] Loss: 157.14942932128906\n",
      "Epoch [96/200] Loss: 155.6210174560547\n",
      "Epoch [97/200] Loss: 154.1081085205078\n",
      "Epoch [98/200] Loss: 152.61048889160156\n",
      "Epoch [99/200] Loss: 151.1280517578125\n",
      "Epoch [100/200] Loss: 149.66050720214844\n",
      "Epoch [101/200] Loss: 148.20773315429688\n",
      "Epoch [102/200] Loss: 146.76951599121094\n",
      "Epoch [103/200] Loss: 145.34567260742188\n",
      "Epoch [104/200] Loss: 143.93603515625\n",
      "Epoch [105/200] Loss: 142.5404510498047\n",
      "Epoch [106/200] Loss: 141.15872192382812\n",
      "Epoch [107/200] Loss: 139.7907257080078\n",
      "Epoch [108/200] Loss: 138.43626403808594\n",
      "Epoch [109/200] Loss: 137.09518432617188\n",
      "Epoch [110/200] Loss: 135.7672882080078\n",
      "Epoch [111/200] Loss: 134.45254516601562\n",
      "Epoch [112/200] Loss: 133.150634765625\n",
      "Epoch [113/200] Loss: 131.86155700683594\n",
      "Epoch [114/200] Loss: 130.58509826660156\n",
      "Epoch [115/200] Loss: 129.3211212158203\n",
      "Epoch [116/200] Loss: 128.0695343017578\n",
      "Epoch [117/200] Loss: 126.83013153076172\n",
      "Epoch [118/200] Loss: 125.60283660888672\n",
      "Epoch [119/200] Loss: 124.38748168945312\n",
      "Epoch [120/200] Loss: 123.1839828491211\n",
      "Epoch [121/200] Loss: 121.99217987060547\n",
      "Epoch [122/200] Loss: 120.81192016601562\n",
      "Epoch [123/200] Loss: 119.64315795898438\n",
      "Epoch [124/200] Loss: 118.48576354980469\n",
      "Epoch [125/200] Loss: 117.33956909179688\n",
      "Epoch [126/200] Loss: 116.20449829101562\n",
      "Epoch [127/200] Loss: 115.0804443359375\n",
      "Epoch [128/200] Loss: 113.9672622680664\n",
      "Epoch [129/200] Loss: 112.86491394042969\n",
      "Epoch [130/200] Loss: 111.77318572998047\n",
      "Epoch [131/200] Loss: 110.69210052490234\n",
      "Epoch [132/200] Loss: 109.6214599609375\n",
      "Epoch [133/200] Loss: 108.56120300292969\n",
      "Epoch [134/200] Loss: 107.5112075805664\n",
      "Epoch [135/200] Loss: 106.47142791748047\n",
      "Epoch [136/200] Loss: 105.44171142578125\n",
      "Epoch [137/200] Loss: 104.42201232910156\n",
      "Epoch [138/200] Loss: 103.41218566894531\n",
      "Epoch [139/200] Loss: 102.41215515136719\n",
      "Epoch [140/200] Loss: 101.42184448242188\n",
      "Epoch [141/200] Loss: 100.44119262695312\n",
      "Epoch [142/200] Loss: 99.47004699707031\n",
      "Epoch [143/200] Loss: 98.50839233398438\n",
      "Epoch [144/200] Loss: 97.55609130859375\n",
      "Epoch [145/200] Loss: 96.61307525634766\n",
      "Epoch [146/200] Loss: 95.67927551269531\n",
      "Epoch [147/200] Loss: 94.75458526611328\n",
      "Epoch [148/200] Loss: 93.83892059326172\n",
      "Epoch [149/200] Loss: 92.93222045898438\n",
      "Epoch [150/200] Loss: 92.03443145751953\n",
      "Epoch [151/200] Loss: 91.14543151855469\n",
      "Epoch [152/200] Loss: 90.26515197753906\n",
      "Epoch [153/200] Loss: 89.39353942871094\n",
      "Epoch [154/200] Loss: 88.53047943115234\n",
      "Epoch [155/200] Loss: 87.67591857910156\n",
      "Epoch [156/200] Loss: 86.82981872558594\n",
      "Epoch [157/200] Loss: 85.99203491210938\n",
      "Epoch [158/200] Loss: 85.16258239746094\n",
      "Epoch [159/200] Loss: 84.34130096435547\n",
      "Epoch [160/200] Loss: 83.52818298339844\n",
      "Epoch [161/200] Loss: 82.72315979003906\n",
      "Epoch [162/200] Loss: 81.92610931396484\n",
      "Epoch [163/200] Loss: 81.13697814941406\n",
      "Epoch [164/200] Loss: 80.35574340820312\n",
      "Epoch [165/200] Loss: 79.58232116699219\n",
      "Epoch [166/200] Loss: 78.8166275024414\n",
      "Epoch [167/200] Loss: 78.05860137939453\n",
      "Epoch [168/200] Loss: 77.30818176269531\n",
      "Epoch [169/200] Loss: 76.56527709960938\n",
      "Epoch [170/200] Loss: 75.82989501953125\n",
      "Epoch [171/200] Loss: 75.10186767578125\n",
      "Epoch [172/200] Loss: 74.38124084472656\n",
      "Epoch [173/200] Loss: 73.6678695678711\n",
      "Epoch [174/200] Loss: 72.96174621582031\n",
      "Epoch [175/200] Loss: 72.2627944946289\n",
      "Epoch [176/200] Loss: 71.5709457397461\n",
      "Epoch [177/200] Loss: 70.88611602783203\n",
      "Epoch [178/200] Loss: 70.20830535888672\n",
      "Epoch [179/200] Loss: 69.53739929199219\n",
      "Epoch [180/200] Loss: 68.8733901977539\n",
      "Epoch [181/200] Loss: 68.21617126464844\n",
      "Epoch [182/200] Loss: 67.56570434570312\n",
      "Epoch [183/200] Loss: 66.92191314697266\n",
      "Epoch [184/200] Loss: 66.28480529785156\n",
      "Epoch [185/200] Loss: 65.65422821044922\n",
      "Epoch [186/200] Loss: 65.03022003173828\n",
      "Epoch [187/200] Loss: 64.41270446777344\n",
      "Epoch [188/200] Loss: 63.80156326293945\n",
      "Epoch [189/200] Loss: 63.1967887878418\n",
      "Epoch [190/200] Loss: 62.59831619262695\n",
      "Epoch [191/200] Loss: 62.006107330322266\n",
      "Epoch [192/200] Loss: 61.420127868652344\n",
      "Epoch [193/200] Loss: 60.84026336669922\n",
      "Epoch [194/200] Loss: 60.26650619506836\n",
      "Epoch [195/200] Loss: 59.698768615722656\n",
      "Epoch [196/200] Loss: 59.13706970214844\n",
      "Epoch [197/200] Loss: 58.581302642822266\n",
      "Epoch [198/200] Loss: 58.03139114379883\n",
      "Epoch [199/200] Loss: 57.48734664916992\n",
      "Epoch [200/200] Loss: 56.9490852355957\n",
      "Predicted days_remaining for parent_id 454: 15.27087688446045\n",
      "Training for parent_id 458...\n",
      "Epoch [1/200] Loss: 1161.5498046875\n",
      "Epoch [2/200] Loss: 1143.666015625\n",
      "Epoch [3/200] Loss: 1125.921875\n",
      "Epoch [4/200] Loss: 1108.5728759765625\n",
      "Epoch [5/200] Loss: 1091.873046875\n",
      "Epoch [6/200] Loss: 1076.0379638671875\n",
      "Epoch [7/200] Loss: 1061.2178955078125\n",
      "Epoch [8/200] Loss: 1047.4793701171875\n",
      "Epoch [9/200] Loss: 1034.81103515625\n",
      "Epoch [10/200] Loss: 1023.1502075195312\n",
      "Epoch [11/200] Loss: 1012.4129028320312\n",
      "Epoch [12/200] Loss: 1002.5105590820312\n",
      "Epoch [13/200] Loss: 993.3546142578125\n",
      "Epoch [14/200] Loss: 984.8573608398438\n",
      "Epoch [15/200] Loss: 976.9330444335938\n",
      "Epoch [16/200] Loss: 969.5010375976562\n",
      "Epoch [17/200] Loss: 962.4896240234375\n",
      "Epoch [18/200] Loss: 955.8370361328125\n",
      "Epoch [19/200] Loss: 949.49169921875\n",
      "Epoch [20/200] Loss: 943.4098510742188\n",
      "Epoch [21/200] Loss: 937.5537109375\n",
      "Epoch [22/200] Loss: 931.8887939453125\n",
      "Epoch [23/200] Loss: 926.3828735351562\n",
      "Epoch [24/200] Loss: 921.0067749023438\n",
      "Epoch [25/200] Loss: 915.735107421875\n",
      "Epoch [26/200] Loss: 910.548828125\n",
      "Epoch [27/200] Loss: 905.4369506835938\n",
      "Epoch [28/200] Loss: 900.39697265625\n",
      "Epoch [29/200] Loss: 895.43310546875\n",
      "Epoch [30/200] Loss: 890.5534057617188\n",
      "Epoch [31/200] Loss: 885.7646484375\n",
      "Epoch [32/200] Loss: 881.0691528320312\n",
      "Epoch [33/200] Loss: 876.4630126953125\n",
      "Epoch [34/200] Loss: 871.9383544921875\n",
      "Epoch [35/200] Loss: 867.484130859375\n",
      "Epoch [36/200] Loss: 863.08984375\n",
      "Epoch [37/200] Loss: 858.7462768554688\n",
      "Epoch [38/200] Loss: 854.4454345703125\n",
      "Epoch [39/200] Loss: 850.1818237304688\n",
      "Epoch [40/200] Loss: 845.9508666992188\n",
      "Epoch [41/200] Loss: 841.7493896484375\n",
      "Epoch [42/200] Loss: 837.5748901367188\n",
      "Epoch [43/200] Loss: 833.4253540039062\n",
      "Epoch [44/200] Loss: 829.2994995117188\n",
      "Epoch [45/200] Loss: 825.1962890625\n",
      "Epoch [46/200] Loss: 821.1146240234375\n",
      "Epoch [47/200] Loss: 817.0541381835938\n",
      "Epoch [48/200] Loss: 813.0142822265625\n",
      "Epoch [49/200] Loss: 808.99462890625\n",
      "Epoch [50/200] Loss: 804.9951171875\n",
      "Epoch [51/200] Loss: 801.0155639648438\n",
      "Epoch [52/200] Loss: 797.0559692382812\n",
      "Epoch [53/200] Loss: 793.116455078125\n",
      "Epoch [54/200] Loss: 789.1969604492188\n",
      "Epoch [55/200] Loss: 785.2972412109375\n",
      "Epoch [56/200] Loss: 781.4176025390625\n",
      "Epoch [57/200] Loss: 777.5578002929688\n",
      "Epoch [58/200] Loss: 773.718017578125\n",
      "Epoch [59/200] Loss: 769.8980712890625\n",
      "Epoch [60/200] Loss: 766.097900390625\n",
      "Epoch [61/200] Loss: 762.3173828125\n",
      "Epoch [62/200] Loss: 758.5563354492188\n",
      "Epoch [63/200] Loss: 754.814697265625\n",
      "Epoch [64/200] Loss: 751.0923461914062\n",
      "Epoch [65/200] Loss: 747.3889770507812\n",
      "Epoch [66/200] Loss: 743.7045288085938\n",
      "Epoch [67/200] Loss: 740.0387573242188\n",
      "Epoch [68/200] Loss: 736.3917236328125\n",
      "Epoch [69/200] Loss: 732.762939453125\n",
      "Epoch [70/200] Loss: 729.15234375\n",
      "Epoch [71/200] Loss: 725.5597534179688\n",
      "Epoch [72/200] Loss: 721.9850463867188\n",
      "Epoch [73/200] Loss: 718.4281005859375\n",
      "Epoch [74/200] Loss: 714.8887329101562\n",
      "Epoch [75/200] Loss: 711.36669921875\n",
      "Epoch [76/200] Loss: 707.86181640625\n",
      "Epoch [77/200] Loss: 704.3742065429688\n",
      "Epoch [78/200] Loss: 700.9033203125\n",
      "Epoch [79/200] Loss: 697.44921875\n",
      "Epoch [80/200] Loss: 694.0117797851562\n",
      "Epoch [81/200] Loss: 690.5908203125\n",
      "Epoch [82/200] Loss: 687.186279296875\n",
      "Epoch [83/200] Loss: 683.7977294921875\n",
      "Epoch [84/200] Loss: 680.425537109375\n",
      "Epoch [85/200] Loss: 677.0690307617188\n",
      "Epoch [86/200] Loss: 673.7283935546875\n",
      "Epoch [87/200] Loss: 670.4035034179688\n",
      "Epoch [88/200] Loss: 667.0940551757812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/200] Loss: 663.8001708984375\n",
      "Epoch [90/200] Loss: 660.521484375\n",
      "Epoch [91/200] Loss: 657.2581176757812\n",
      "Epoch [92/200] Loss: 654.0097045898438\n",
      "Epoch [93/200] Loss: 650.7763671875\n",
      "Epoch [94/200] Loss: 647.5576782226562\n",
      "Epoch [95/200] Loss: 644.35400390625\n",
      "Epoch [96/200] Loss: 641.164794921875\n",
      "Epoch [97/200] Loss: 637.990234375\n",
      "Epoch [98/200] Loss: 634.830078125\n",
      "Epoch [99/200] Loss: 631.6842651367188\n",
      "Epoch [100/200] Loss: 628.5525512695312\n",
      "Epoch [101/200] Loss: 625.4352416992188\n",
      "Epoch [102/200] Loss: 622.3318481445312\n",
      "Epoch [103/200] Loss: 619.2423706054688\n",
      "Epoch [104/200] Loss: 616.166748046875\n",
      "Epoch [105/200] Loss: 613.10498046875\n",
      "Epoch [106/200] Loss: 610.056884765625\n",
      "Epoch [107/200] Loss: 607.0223999023438\n",
      "Epoch [108/200] Loss: 604.0014038085938\n",
      "Epoch [109/200] Loss: 600.993896484375\n",
      "Epoch [110/200] Loss: 597.9996948242188\n",
      "Epoch [111/200] Loss: 595.0188598632812\n",
      "Epoch [112/200] Loss: 592.05126953125\n",
      "Epoch [113/200] Loss: 589.0965576171875\n",
      "Epoch [114/200] Loss: 586.155029296875\n",
      "Epoch [115/200] Loss: 583.2266235351562\n",
      "Epoch [116/200] Loss: 580.3109130859375\n",
      "Epoch [117/200] Loss: 577.4080810546875\n",
      "Epoch [118/200] Loss: 574.5181274414062\n",
      "Epoch [119/200] Loss: 571.6408081054688\n",
      "Epoch [120/200] Loss: 568.7760620117188\n",
      "Epoch [121/200] Loss: 565.9239501953125\n",
      "Epoch [122/200] Loss: 563.0842895507812\n",
      "Epoch [123/200] Loss: 560.257080078125\n",
      "Epoch [124/200] Loss: 557.4422607421875\n",
      "Epoch [125/200] Loss: 554.6396484375\n",
      "Epoch [126/200] Loss: 551.849365234375\n",
      "Epoch [127/200] Loss: 549.0712890625\n",
      "Epoch [128/200] Loss: 546.3052978515625\n",
      "Epoch [129/200] Loss: 543.5513305664062\n",
      "Epoch [130/200] Loss: 540.8094482421875\n",
      "Epoch [131/200] Loss: 538.0794677734375\n",
      "Epoch [132/200] Loss: 535.3613891601562\n",
      "Epoch [133/200] Loss: 532.6550903320312\n",
      "Epoch [134/200] Loss: 529.9606323242188\n",
      "Epoch [135/200] Loss: 527.2779541015625\n",
      "Epoch [136/200] Loss: 524.6068115234375\n",
      "Epoch [137/200] Loss: 521.9473266601562\n",
      "Epoch [138/200] Loss: 519.2994995117188\n",
      "Epoch [139/200] Loss: 516.6631469726562\n",
      "Epoch [140/200] Loss: 514.0382080078125\n",
      "Epoch [141/200] Loss: 511.4246826171875\n",
      "Epoch [142/200] Loss: 508.8226013183594\n",
      "Epoch [143/200] Loss: 506.231689453125\n",
      "Epoch [144/200] Loss: 503.6521911621094\n",
      "Epoch [145/200] Loss: 501.08380126953125\n",
      "Epoch [146/200] Loss: 498.5266418457031\n",
      "Epoch [147/200] Loss: 495.9805908203125\n",
      "Epoch [148/200] Loss: 493.4456481933594\n",
      "Epoch [149/200] Loss: 490.9217224121094\n",
      "Epoch [150/200] Loss: 488.40875244140625\n",
      "Epoch [151/200] Loss: 485.9066467285156\n",
      "Epoch [152/200] Loss: 483.41558837890625\n",
      "Epoch [153/200] Loss: 480.9352111816406\n",
      "Epoch [154/200] Loss: 478.46575927734375\n",
      "Epoch [155/200] Loss: 476.0071105957031\n",
      "Epoch [156/200] Loss: 473.5589599609375\n",
      "Epoch [157/200] Loss: 471.1216735839844\n",
      "Epoch [158/200] Loss: 468.6949157714844\n",
      "Epoch [159/200] Loss: 466.27886962890625\n",
      "Epoch [160/200] Loss: 463.873291015625\n",
      "Epoch [161/200] Loss: 461.47821044921875\n",
      "Epoch [162/200] Loss: 459.09368896484375\n",
      "Epoch [163/200] Loss: 456.7195739746094\n",
      "Epoch [164/200] Loss: 454.3557434082031\n",
      "Epoch [165/200] Loss: 452.0021667480469\n",
      "Epoch [166/200] Loss: 449.65911865234375\n",
      "Epoch [167/200] Loss: 447.326171875\n",
      "Epoch [168/200] Loss: 445.0035095214844\n",
      "Epoch [169/200] Loss: 442.6910400390625\n",
      "Epoch [170/200] Loss: 440.3886413574219\n",
      "Epoch [171/200] Loss: 438.0965270996094\n",
      "Epoch [172/200] Loss: 435.8143615722656\n",
      "Epoch [173/200] Loss: 433.5422058105469\n",
      "Epoch [174/200] Loss: 431.2801818847656\n",
      "Epoch [175/200] Loss: 429.0279541015625\n",
      "Epoch [176/200] Loss: 426.7856750488281\n",
      "Epoch [177/200] Loss: 424.55328369140625\n",
      "Epoch [178/200] Loss: 422.330810546875\n",
      "Epoch [179/200] Loss: 420.1180725097656\n",
      "Epoch [180/200] Loss: 417.9151306152344\n",
      "Epoch [181/200] Loss: 415.72198486328125\n",
      "Epoch [182/200] Loss: 413.53851318359375\n",
      "Epoch [183/200] Loss: 411.36468505859375\n",
      "Epoch [184/200] Loss: 409.2005310058594\n",
      "Epoch [185/200] Loss: 407.0459899902344\n",
      "Epoch [186/200] Loss: 404.90093994140625\n",
      "Epoch [187/200] Loss: 402.765380859375\n",
      "Epoch [188/200] Loss: 400.6394348144531\n",
      "Epoch [189/200] Loss: 398.5228271484375\n",
      "Epoch [190/200] Loss: 396.41571044921875\n",
      "Epoch [191/200] Loss: 394.31805419921875\n",
      "Epoch [192/200] Loss: 392.2296447753906\n",
      "Epoch [193/200] Loss: 390.15057373046875\n",
      "Epoch [194/200] Loss: 388.08087158203125\n",
      "Epoch [195/200] Loss: 386.0203552246094\n",
      "Epoch [196/200] Loss: 383.968994140625\n",
      "Epoch [197/200] Loss: 381.9269714355469\n",
      "Epoch [198/200] Loss: 379.8940734863281\n",
      "Epoch [199/200] Loss: 377.8702087402344\n",
      "Epoch [200/200] Loss: 375.8555603027344\n",
      "Predicted days_remaining for parent_id 458: 14.791824340820312\n",
      "Training for parent_id 460...\n",
      "Epoch [1/200] Loss: 395.035888671875\n",
      "Epoch [2/200] Loss: 386.5906677246094\n",
      "Epoch [3/200] Loss: 378.17724609375\n",
      "Epoch [4/200] Loss: 369.84027099609375\n",
      "Epoch [5/200] Loss: 361.6361389160156\n",
      "Epoch [6/200] Loss: 353.6033935546875\n",
      "Epoch [7/200] Loss: 345.7605285644531\n",
      "Epoch [8/200] Loss: 338.1150207519531\n",
      "Epoch [9/200] Loss: 330.6701965332031\n",
      "Epoch [10/200] Loss: 323.4275207519531\n",
      "Epoch [11/200] Loss: 316.3878173828125\n",
      "Epoch [12/200] Loss: 309.5530700683594\n",
      "Epoch [13/200] Loss: 302.9266052246094\n",
      "Epoch [14/200] Loss: 296.513916015625\n",
      "Epoch [15/200] Loss: 290.3232421875\n",
      "Epoch [16/200] Loss: 284.3651123046875\n",
      "Epoch [17/200] Loss: 278.65234375\n",
      "Epoch [18/200] Loss: 273.19830322265625\n",
      "Epoch [19/200] Loss: 268.0155944824219\n",
      "Epoch [20/200] Loss: 263.113037109375\n",
      "Epoch [21/200] Loss: 258.4940185546875\n",
      "Epoch [22/200] Loss: 254.15530395507812\n",
      "Epoch [23/200] Loss: 250.0869140625\n",
      "Epoch [24/200] Loss: 246.2728729248047\n",
      "Epoch [25/200] Loss: 242.69300842285156\n",
      "Epoch [26/200] Loss: 239.32444763183594\n",
      "Epoch [27/200] Loss: 236.14373779296875\n",
      "Epoch [28/200] Loss: 233.12803649902344\n",
      "Epoch [29/200] Loss: 230.2564239501953\n",
      "Epoch [30/200] Loss: 227.51023864746094\n",
      "Epoch [31/200] Loss: 224.87338256835938\n",
      "Epoch [32/200] Loss: 222.33193969726562\n",
      "Epoch [33/200] Loss: 219.874267578125\n",
      "Epoch [34/200] Loss: 217.49053955078125\n",
      "Epoch [35/200] Loss: 215.17239379882812\n",
      "Epoch [36/200] Loss: 212.9127197265625\n",
      "Epoch [37/200] Loss: 210.7055206298828\n",
      "Epoch [38/200] Loss: 208.5455322265625\n",
      "Epoch [39/200] Loss: 206.42828369140625\n",
      "Epoch [40/200] Loss: 204.3497314453125\n",
      "Epoch [41/200] Loss: 202.3064422607422\n",
      "Epoch [42/200] Loss: 200.2954864501953\n",
      "Epoch [43/200] Loss: 198.31431579589844\n",
      "Epoch [44/200] Loss: 196.3606414794922\n",
      "Epoch [45/200] Loss: 194.43260192871094\n",
      "Epoch [46/200] Loss: 192.528564453125\n",
      "Epoch [47/200] Loss: 190.64695739746094\n",
      "Epoch [48/200] Loss: 188.78668212890625\n",
      "Epoch [49/200] Loss: 186.94659423828125\n",
      "Epoch [50/200] Loss: 185.12570190429688\n",
      "Epoch [51/200] Loss: 183.3231201171875\n",
      "Epoch [52/200] Loss: 181.53810119628906\n",
      "Epoch [53/200] Loss: 179.76992797851562\n",
      "Epoch [54/200] Loss: 178.0179443359375\n",
      "Epoch [55/200] Loss: 176.28179931640625\n",
      "Epoch [56/200] Loss: 174.5609588623047\n",
      "Epoch [57/200] Loss: 172.8552703857422\n",
      "Epoch [58/200] Loss: 171.16445922851562\n",
      "Epoch [59/200] Loss: 169.48855590820312\n",
      "Epoch [60/200] Loss: 167.82745361328125\n",
      "Epoch [61/200] Loss: 166.18115234375\n",
      "Epoch [62/200] Loss: 164.54971313476562\n",
      "Epoch [63/200] Loss: 162.93328857421875\n",
      "Epoch [64/200] Loss: 161.33169555664062\n",
      "Epoch [65/200] Loss: 159.7450408935547\n",
      "Epoch [66/200] Loss: 158.17332458496094\n",
      "Epoch [67/200] Loss: 156.61639404296875\n",
      "Epoch [68/200] Loss: 155.07424926757812\n",
      "Epoch [69/200] Loss: 153.5468292236328\n",
      "Epoch [70/200] Loss: 152.03395080566406\n",
      "Epoch [71/200] Loss: 150.5355224609375\n",
      "Epoch [72/200] Loss: 149.05148315429688\n",
      "Epoch [73/200] Loss: 147.5816192626953\n",
      "Epoch [74/200] Loss: 146.12582397460938\n",
      "Epoch [75/200] Loss: 144.68405151367188\n",
      "Epoch [76/200] Loss: 143.25601196289062\n",
      "Epoch [77/200] Loss: 141.8417510986328\n",
      "Epoch [78/200] Loss: 140.44100952148438\n",
      "Epoch [79/200] Loss: 139.0537109375\n",
      "Epoch [80/200] Loss: 137.67970275878906\n",
      "Epoch [81/200] Loss: 136.31886291503906\n",
      "Epoch [82/200] Loss: 134.97109985351562\n",
      "Epoch [83/200] Loss: 133.6362762451172\n",
      "Epoch [84/200] Loss: 132.3142547607422\n",
      "Epoch [85/200] Loss: 131.0048828125\n",
      "Epoch [86/200] Loss: 129.7080841064453\n",
      "Epoch [87/200] Loss: 128.4237518310547\n",
      "Epoch [88/200] Loss: 127.15170288085938\n",
      "Epoch [89/200] Loss: 125.89187622070312\n",
      "Epoch [90/200] Loss: 124.6441650390625\n",
      "Epoch [91/200] Loss: 123.40843200683594\n",
      "Epoch [92/200] Loss: 122.18456268310547\n",
      "Epoch [93/200] Loss: 120.97245025634766\n",
      "Epoch [94/200] Loss: 119.77198791503906\n",
      "Epoch [95/200] Loss: 118.58308410644531\n",
      "Epoch [96/200] Loss: 117.40558624267578\n",
      "Epoch [97/200] Loss: 116.23946380615234\n",
      "Epoch [98/200] Loss: 115.08457946777344\n",
      "Epoch [99/200] Loss: 113.9408187866211\n",
      "Epoch [100/200] Loss: 112.80806732177734\n",
      "Epoch [101/200] Loss: 111.686279296875\n",
      "Epoch [102/200] Loss: 110.5752944946289\n",
      "Epoch [103/200] Loss: 109.47509765625\n",
      "Epoch [104/200] Loss: 108.38553619384766\n",
      "Epoch [105/200] Loss: 107.3065185546875\n",
      "Epoch [106/200] Loss: 106.23799896240234\n",
      "Epoch [107/200] Loss: 105.17980194091797\n",
      "Epoch [108/200] Loss: 104.13189697265625\n",
      "Epoch [109/200] Loss: 103.09419250488281\n",
      "Epoch [110/200] Loss: 102.06660461425781\n",
      "Epoch [111/200] Loss: 101.04903411865234\n",
      "Epoch [112/200] Loss: 100.04141998291016\n",
      "Epoch [113/200] Loss: 99.04362487792969\n",
      "Epoch [114/200] Loss: 98.05559539794922\n",
      "Epoch [115/200] Loss: 97.07723999023438\n",
      "Epoch [116/200] Loss: 96.10848999023438\n",
      "Epoch [117/200] Loss: 95.1493148803711\n",
      "Epoch [118/200] Loss: 94.19952392578125\n",
      "Epoch [119/200] Loss: 93.25914001464844\n",
      "Epoch [120/200] Loss: 92.32801818847656\n",
      "Epoch [121/200] Loss: 91.40614318847656\n",
      "Epoch [122/200] Loss: 90.49333190917969\n",
      "Epoch [123/200] Loss: 89.58963012695312\n",
      "Epoch [124/200] Loss: 88.69488525390625\n",
      "Epoch [125/200] Loss: 87.80906677246094\n",
      "Epoch [126/200] Loss: 86.93207550048828\n",
      "Epoch [127/200] Loss: 86.0638427734375\n",
      "Epoch [128/200] Loss: 85.20430755615234\n",
      "Epoch [129/200] Loss: 84.3533935546875\n",
      "Epoch [130/200] Loss: 83.51102447509766\n",
      "Epoch [131/200] Loss: 82.67713928222656\n",
      "Epoch [132/200] Loss: 81.85164642333984\n",
      "Epoch [133/200] Loss: 81.03450775146484\n",
      "Epoch [134/200] Loss: 80.22564697265625\n",
      "Epoch [135/200] Loss: 79.42498779296875\n",
      "Epoch [136/200] Loss: 78.63245391845703\n",
      "Epoch [137/200] Loss: 77.8479995727539\n",
      "Epoch [138/200] Loss: 77.0715560913086\n",
      "Epoch [139/200] Loss: 76.30303955078125\n",
      "Epoch [140/200] Loss: 75.54239654541016\n",
      "Epoch [141/200] Loss: 74.78958129882812\n",
      "Epoch [142/200] Loss: 74.04450225830078\n",
      "Epoch [143/200] Loss: 73.30709838867188\n",
      "Epoch [144/200] Loss: 72.57733917236328\n",
      "Epoch [145/200] Loss: 71.85511016845703\n",
      "Epoch [146/200] Loss: 71.1404037475586\n",
      "Epoch [147/200] Loss: 70.43310546875\n",
      "Epoch [148/200] Loss: 69.73320007324219\n",
      "Epoch [149/200] Loss: 69.04059600830078\n",
      "Epoch [150/200] Loss: 68.35525512695312\n",
      "Epoch [151/200] Loss: 67.6771011352539\n",
      "Epoch [152/200] Loss: 67.00606536865234\n",
      "Epoch [153/200] Loss: 66.34213256835938\n",
      "Epoch [154/200] Loss: 65.68519592285156\n",
      "Epoch [155/200] Loss: 65.03520965576172\n",
      "Epoch [156/200] Loss: 64.39215850830078\n",
      "Epoch [157/200] Loss: 63.75593185424805\n",
      "Epoch [158/200] Loss: 63.12646484375\n",
      "Epoch [159/200] Loss: 62.50375747680664\n",
      "Epoch [160/200] Loss: 61.8877067565918\n",
      "Epoch [161/200] Loss: 61.27829360961914\n",
      "Epoch [162/200] Loss: 60.6754150390625\n",
      "Epoch [163/200] Loss: 60.07905578613281\n",
      "Epoch [164/200] Loss: 59.489131927490234\n",
      "Epoch [165/200] Loss: 58.90564727783203\n",
      "Epoch [166/200] Loss: 58.32846450805664\n",
      "Epoch [167/200] Loss: 57.7575798034668\n",
      "Epoch [168/200] Loss: 57.19294738769531\n",
      "Epoch [169/200] Loss: 56.63447570800781\n",
      "Epoch [170/200] Loss: 56.082157135009766\n",
      "Epoch [171/200] Loss: 55.535892486572266\n",
      "Epoch [172/200] Loss: 54.99566650390625\n",
      "Epoch [173/200] Loss: 54.461387634277344\n",
      "Epoch [174/200] Loss: 53.93305969238281\n",
      "Epoch [175/200] Loss: 53.410587310791016\n",
      "Epoch [176/200] Loss: 52.8939323425293\n",
      "Epoch [177/200] Loss: 52.38306427001953\n",
      "Epoch [178/200] Loss: 51.87790298461914\n",
      "Epoch [179/200] Loss: 51.37840270996094\n",
      "Epoch [180/200] Loss: 50.884521484375\n",
      "Epoch [181/200] Loss: 50.39623260498047\n",
      "Epoch [182/200] Loss: 49.913448333740234\n",
      "Epoch [183/200] Loss: 49.43614196777344\n",
      "Epoch [184/200] Loss: 48.9642448425293\n",
      "Epoch [185/200] Loss: 48.49773406982422\n",
      "Epoch [186/200] Loss: 48.03654098510742\n",
      "Epoch [187/200] Loss: 47.58064651489258\n",
      "Epoch [188/200] Loss: 47.12998580932617\n",
      "Epoch [189/200] Loss: 46.68450164794922\n",
      "Epoch [190/200] Loss: 46.24415969848633\n",
      "Epoch [191/200] Loss: 45.808895111083984\n",
      "Epoch [192/200] Loss: 45.37868118286133\n",
      "Epoch [193/200] Loss: 44.9534912109375\n",
      "Epoch [194/200] Loss: 44.53322219848633\n",
      "Epoch [195/200] Loss: 44.117881774902344\n",
      "Epoch [196/200] Loss: 43.70740509033203\n",
      "Epoch [197/200] Loss: 43.30173873901367\n",
      "Epoch [198/200] Loss: 42.9008674621582\n",
      "Epoch [199/200] Loss: 42.504722595214844\n",
      "Epoch [200/200] Loss: 42.113250732421875\n",
      "Predicted days_remaining for parent_id 460: 14.526118278503418\n",
      "Training for parent_id 464...\n",
      "Epoch [1/200] Loss: 837.4356689453125\n",
      "Epoch [2/200] Loss: 824.9102783203125\n",
      "Epoch [3/200] Loss: 812.4449462890625\n",
      "Epoch [4/200] Loss: 800.1126098632812\n",
      "Epoch [5/200] Loss: 787.9867553710938\n",
      "Epoch [6/200] Loss: 776.1126708984375\n",
      "Epoch [7/200] Loss: 764.5126953125\n",
      "Epoch [8/200] Loss: 753.1968383789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/200] Loss: 742.1692504882812\n",
      "Epoch [10/200] Loss: 731.4337158203125\n",
      "Epoch [11/200] Loss: 720.9981689453125\n",
      "Epoch [12/200] Loss: 710.8745727539062\n",
      "Epoch [13/200] Loss: 701.0791625976562\n",
      "Epoch [14/200] Loss: 691.627685546875\n",
      "Epoch [15/200] Loss: 682.5332641601562\n",
      "Epoch [16/200] Loss: 673.8049926757812\n",
      "Epoch [17/200] Loss: 665.4464721679688\n",
      "Epoch [18/200] Loss: 657.4554443359375\n",
      "Epoch [19/200] Loss: 649.8240966796875\n",
      "Epoch [20/200] Loss: 642.539306640625\n",
      "Epoch [21/200] Loss: 635.5841674804688\n",
      "Epoch [22/200] Loss: 628.9375610351562\n",
      "Epoch [23/200] Loss: 622.5771484375\n",
      "Epoch [24/200] Loss: 616.48046875\n",
      "Epoch [25/200] Loss: 610.6256713867188\n",
      "Epoch [26/200] Loss: 604.9925537109375\n",
      "Epoch [27/200] Loss: 599.5620727539062\n",
      "Epoch [28/200] Loss: 594.3165283203125\n",
      "Epoch [29/200] Loss: 589.239501953125\n",
      "Epoch [30/200] Loss: 584.3162841796875\n",
      "Epoch [31/200] Loss: 579.5336303710938\n",
      "Epoch [32/200] Loss: 574.8798217773438\n",
      "Epoch [33/200] Loss: 570.3444213867188\n",
      "Epoch [34/200] Loss: 565.9182739257812\n",
      "Epoch [35/200] Loss: 561.5932006835938\n",
      "Epoch [36/200] Loss: 557.361572265625\n",
      "Epoch [37/200] Loss: 553.2169189453125\n",
      "Epoch [38/200] Loss: 549.1533813476562\n",
      "Epoch [39/200] Loss: 545.1651611328125\n",
      "Epoch [40/200] Loss: 541.247802734375\n",
      "Epoch [41/200] Loss: 537.3969116210938\n",
      "Epoch [42/200] Loss: 533.6084594726562\n",
      "Epoch [43/200] Loss: 529.8790893554688\n",
      "Epoch [44/200] Loss: 526.2052001953125\n",
      "Epoch [45/200] Loss: 522.583740234375\n",
      "Epoch [46/200] Loss: 519.0117797851562\n",
      "Epoch [47/200] Loss: 515.4864501953125\n",
      "Epoch [48/200] Loss: 512.0050659179688\n",
      "Epoch [49/200] Loss: 508.5650939941406\n",
      "Epoch [50/200] Loss: 505.1642761230469\n",
      "Epoch [51/200] Loss: 501.80023193359375\n",
      "Epoch [52/200] Loss: 498.4711608886719\n",
      "Epoch [53/200] Loss: 495.175048828125\n",
      "Epoch [54/200] Loss: 491.9101257324219\n",
      "Epoch [55/200] Loss: 488.67498779296875\n",
      "Epoch [56/200] Loss: 485.4681396484375\n",
      "Epoch [57/200] Loss: 482.2881774902344\n",
      "Epoch [58/200] Loss: 479.13397216796875\n",
      "Epoch [59/200] Loss: 476.00445556640625\n",
      "Epoch [60/200] Loss: 472.89862060546875\n",
      "Epoch [61/200] Loss: 469.8155822753906\n",
      "Epoch [62/200] Loss: 466.7546081542969\n",
      "Epoch [63/200] Loss: 463.7149353027344\n",
      "Epoch [64/200] Loss: 460.6961669921875\n",
      "Epoch [65/200] Loss: 457.69781494140625\n",
      "Epoch [66/200] Loss: 454.7192077636719\n",
      "Epoch [67/200] Loss: 451.7603454589844\n",
      "Epoch [68/200] Loss: 448.82086181640625\n",
      "Epoch [69/200] Loss: 445.9006042480469\n",
      "Epoch [70/200] Loss: 442.9993896484375\n",
      "Epoch [71/200] Loss: 440.1171875\n",
      "Epoch [72/200] Loss: 437.25372314453125\n",
      "Epoch [73/200] Loss: 434.4092102050781\n",
      "Epoch [74/200] Loss: 431.58343505859375\n",
      "Epoch [75/200] Loss: 428.7762145996094\n",
      "Epoch [76/200] Loss: 425.9876708984375\n",
      "Epoch [77/200] Loss: 423.2176208496094\n",
      "Epoch [78/200] Loss: 420.4659118652344\n",
      "Epoch [79/200] Loss: 417.73236083984375\n",
      "Epoch [80/200] Loss: 415.01708984375\n",
      "Epoch [81/200] Loss: 412.3197326660156\n",
      "Epoch [82/200] Loss: 409.6402587890625\n",
      "Epoch [83/200] Loss: 406.9784851074219\n",
      "Epoch [84/200] Loss: 404.334228515625\n",
      "Epoch [85/200] Loss: 401.7073974609375\n",
      "Epoch [86/200] Loss: 399.09771728515625\n",
      "Epoch [87/200] Loss: 396.5049743652344\n",
      "Epoch [88/200] Loss: 393.9293212890625\n",
      "Epoch [89/200] Loss: 391.3702697753906\n",
      "Epoch [90/200] Loss: 388.8277587890625\n",
      "Epoch [91/200] Loss: 386.3016357421875\n",
      "Epoch [92/200] Loss: 383.791748046875\n",
      "Epoch [93/200] Loss: 381.2979736328125\n",
      "Epoch [94/200] Loss: 378.82000732421875\n",
      "Epoch [95/200] Loss: 376.35784912109375\n",
      "Epoch [96/200] Loss: 373.9112548828125\n",
      "Epoch [97/200] Loss: 371.4802551269531\n",
      "Epoch [98/200] Loss: 369.06439208984375\n",
      "Epoch [99/200] Loss: 366.6637878417969\n",
      "Epoch [100/200] Loss: 364.2781677246094\n",
      "Epoch [101/200] Loss: 361.9075622558594\n",
      "Epoch [102/200] Loss: 359.55169677734375\n",
      "Epoch [103/200] Loss: 357.21038818359375\n",
      "Epoch [104/200] Loss: 354.88360595703125\n",
      "Epoch [105/200] Loss: 352.5712890625\n",
      "Epoch [106/200] Loss: 350.27313232421875\n",
      "Epoch [107/200] Loss: 347.9891662597656\n",
      "Epoch [108/200] Loss: 345.71917724609375\n",
      "Epoch [109/200] Loss: 343.4631652832031\n",
      "Epoch [110/200] Loss: 341.220947265625\n",
      "Epoch [111/200] Loss: 338.99237060546875\n",
      "Epoch [112/200] Loss: 336.77740478515625\n",
      "Epoch [113/200] Loss: 334.57586669921875\n",
      "Epoch [114/200] Loss: 332.38775634765625\n",
      "Epoch [115/200] Loss: 330.2128601074219\n",
      "Epoch [116/200] Loss: 328.0511474609375\n",
      "Epoch [117/200] Loss: 325.9025573730469\n",
      "Epoch [118/200] Loss: 323.766845703125\n",
      "Epoch [119/200] Loss: 321.6440734863281\n",
      "Epoch [120/200] Loss: 319.5340270996094\n",
      "Epoch [121/200] Loss: 317.4366760253906\n",
      "Epoch [122/200] Loss: 315.3519592285156\n",
      "Epoch [123/200] Loss: 313.2797546386719\n",
      "Epoch [124/200] Loss: 311.2199401855469\n",
      "Epoch [125/200] Loss: 309.1724548339844\n",
      "Epoch [126/200] Loss: 307.13726806640625\n",
      "Epoch [127/200] Loss: 305.1142883300781\n",
      "Epoch [128/200] Loss: 303.1033020019531\n",
      "Epoch [129/200] Loss: 301.1043701171875\n",
      "Epoch [130/200] Loss: 299.11737060546875\n",
      "Epoch [131/200] Loss: 297.1423034667969\n",
      "Epoch [132/200] Loss: 295.1789245605469\n",
      "Epoch [133/200] Loss: 293.2272644042969\n",
      "Epoch [134/200] Loss: 291.2872619628906\n",
      "Epoch [135/200] Loss: 289.3587646484375\n",
      "Epoch [136/200] Loss: 287.4416809082031\n",
      "Epoch [137/200] Loss: 285.5362548828125\n",
      "Epoch [138/200] Loss: 283.6419372558594\n",
      "Epoch [139/200] Loss: 281.7590026855469\n",
      "Epoch [140/200] Loss: 279.88726806640625\n",
      "Epoch [141/200] Loss: 278.0266418457031\n",
      "Epoch [142/200] Loss: 276.17706298828125\n",
      "Epoch [143/200] Loss: 274.3385314941406\n",
      "Epoch [144/200] Loss: 272.5110168457031\n",
      "Epoch [145/200] Loss: 270.69427490234375\n",
      "Epoch [146/200] Loss: 268.8883361816406\n",
      "Epoch [147/200] Loss: 267.09320068359375\n",
      "Epoch [148/200] Loss: 265.3087158203125\n",
      "Epoch [149/200] Loss: 263.5349426269531\n",
      "Epoch [150/200] Loss: 261.7716369628906\n",
      "Epoch [151/200] Loss: 260.01885986328125\n",
      "Epoch [152/200] Loss: 258.2765808105469\n",
      "Epoch [153/200] Loss: 256.5447082519531\n",
      "Epoch [154/200] Loss: 254.82315063476562\n",
      "Epoch [155/200] Loss: 253.1118621826172\n",
      "Epoch [156/200] Loss: 251.41078186035156\n",
      "Epoch [157/200] Loss: 249.7198486328125\n",
      "Epoch [158/200] Loss: 248.03909301757812\n",
      "Epoch [159/200] Loss: 246.36831665039062\n",
      "Epoch [160/200] Loss: 244.7075653076172\n",
      "Epoch [161/200] Loss: 243.05677795410156\n",
      "Epoch [162/200] Loss: 241.41592407226562\n",
      "Epoch [163/200] Loss: 239.78485107421875\n",
      "Epoch [164/200] Loss: 238.16358947753906\n",
      "Epoch [165/200] Loss: 236.55201721191406\n",
      "Epoch [166/200] Loss: 234.9501953125\n",
      "Epoch [167/200] Loss: 233.35792541503906\n",
      "Epoch [168/200] Loss: 231.77532958984375\n",
      "Epoch [169/200] Loss: 230.20223999023438\n",
      "Epoch [170/200] Loss: 228.6385955810547\n",
      "Epoch [171/200] Loss: 227.08438110351562\n",
      "Epoch [172/200] Loss: 225.5396270751953\n",
      "Epoch [173/200] Loss: 224.00416564941406\n",
      "Epoch [174/200] Loss: 222.4779815673828\n",
      "Epoch [175/200] Loss: 220.96102905273438\n",
      "Epoch [176/200] Loss: 219.45330810546875\n",
      "Epoch [177/200] Loss: 217.9547119140625\n",
      "Epoch [178/200] Loss: 216.46517944335938\n",
      "Epoch [179/200] Loss: 214.98480224609375\n",
      "Epoch [180/200] Loss: 213.5133056640625\n",
      "Epoch [181/200] Loss: 212.0508575439453\n",
      "Epoch [182/200] Loss: 210.59730529785156\n",
      "Epoch [183/200] Loss: 209.1526336669922\n",
      "Epoch [184/200] Loss: 207.71669006347656\n",
      "Epoch [185/200] Loss: 206.28964233398438\n",
      "Epoch [186/200] Loss: 204.87132263183594\n",
      "Epoch [187/200] Loss: 203.46165466308594\n",
      "Epoch [188/200] Loss: 202.06063842773438\n",
      "Epoch [189/200] Loss: 200.6682891845703\n",
      "Epoch [190/200] Loss: 199.284423828125\n",
      "Epoch [191/200] Loss: 197.90907287597656\n",
      "Epoch [192/200] Loss: 196.54220581054688\n",
      "Epoch [193/200] Loss: 195.18382263183594\n",
      "Epoch [194/200] Loss: 193.8337860107422\n",
      "Epoch [195/200] Loss: 192.4921417236328\n",
      "Epoch [196/200] Loss: 191.1587371826172\n",
      "Epoch [197/200] Loss: 189.8336639404297\n",
      "Epoch [198/200] Loss: 188.51678466796875\n",
      "Epoch [199/200] Loss: 187.20809936523438\n",
      "Epoch [200/200] Loss: 185.9075469970703\n",
      "Predicted days_remaining for parent_id 464: 15.704785346984863\n",
      "Training for parent_id 466...\n",
      "Epoch [1/200] Loss: 688.91015625\n",
      "Epoch [2/200] Loss: 675.3175659179688\n",
      "Epoch [3/200] Loss: 662.1414794921875\n",
      "Epoch [4/200] Loss: 649.5319213867188\n",
      "Epoch [5/200] Loss: 637.5474243164062\n",
      "Epoch [6/200] Loss: 626.1765747070312\n",
      "Epoch [7/200] Loss: 615.3749389648438\n",
      "Epoch [8/200] Loss: 605.0940551757812\n",
      "Epoch [9/200] Loss: 595.2959594726562\n",
      "Epoch [10/200] Loss: 585.9579467773438\n",
      "Epoch [11/200] Loss: 577.0689086914062\n",
      "Epoch [12/200] Loss: 568.6227416992188\n",
      "Epoch [13/200] Loss: 560.612548828125\n",
      "Epoch [14/200] Loss: 553.0267944335938\n",
      "Epoch [15/200] Loss: 545.8468627929688\n",
      "Epoch [16/200] Loss: 539.0469360351562\n",
      "Epoch [17/200] Loss: 532.596435546875\n",
      "Epoch [18/200] Loss: 526.4635009765625\n",
      "Epoch [19/200] Loss: 520.6170043945312\n",
      "Epoch [20/200] Loss: 515.0282592773438\n",
      "Epoch [21/200] Loss: 509.67095947265625\n",
      "Epoch [22/200] Loss: 504.5221252441406\n",
      "Epoch [23/200] Loss: 499.5618896484375\n",
      "Epoch [24/200] Loss: 494.7729187011719\n",
      "Epoch [25/200] Loss: 490.14019775390625\n",
      "Epoch [26/200] Loss: 485.6507568359375\n",
      "Epoch [27/200] Loss: 481.2930603027344\n",
      "Epoch [28/200] Loss: 477.05633544921875\n",
      "Epoch [29/200] Loss: 472.93096923828125\n",
      "Epoch [30/200] Loss: 468.9078369140625\n",
      "Epoch [31/200] Loss: 464.9785461425781\n",
      "Epoch [32/200] Loss: 461.1357421875\n",
      "Epoch [33/200] Loss: 457.37249755859375\n",
      "Epoch [34/200] Loss: 453.6827697753906\n",
      "Epoch [35/200] Loss: 450.0615539550781\n",
      "Epoch [36/200] Loss: 446.5043029785156\n",
      "Epoch [37/200] Loss: 443.0071105957031\n",
      "Epoch [38/200] Loss: 439.5665588378906\n",
      "Epoch [39/200] Loss: 436.1796569824219\n",
      "Epoch [40/200] Loss: 432.8438415527344\n",
      "Epoch [41/200] Loss: 429.5563659667969\n",
      "Epoch [42/200] Loss: 426.3149108886719\n",
      "Epoch [43/200] Loss: 423.1171875\n",
      "Epoch [44/200] Loss: 419.961181640625\n",
      "Epoch [45/200] Loss: 416.8445129394531\n",
      "Epoch [46/200] Loss: 413.765380859375\n",
      "Epoch [47/200] Loss: 410.7218322753906\n",
      "Epoch [48/200] Loss: 407.712158203125\n",
      "Epoch [49/200] Loss: 404.7346496582031\n",
      "Epoch [50/200] Loss: 401.7878723144531\n",
      "Epoch [51/200] Loss: 398.8706970214844\n",
      "Epoch [52/200] Loss: 395.9818115234375\n",
      "Epoch [53/200] Loss: 393.1202392578125\n",
      "Epoch [54/200] Loss: 390.28485107421875\n",
      "Epoch [55/200] Loss: 387.4749450683594\n",
      "Epoch [56/200] Loss: 384.68951416015625\n",
      "Epoch [57/200] Loss: 381.9274597167969\n",
      "Epoch [58/200] Loss: 379.18798828125\n",
      "Epoch [59/200] Loss: 376.46966552734375\n",
      "Epoch [60/200] Loss: 373.77154541015625\n",
      "Epoch [61/200] Loss: 371.09197998046875\n",
      "Epoch [62/200] Loss: 368.42974853515625\n",
      "Epoch [63/200] Loss: 365.7833557128906\n",
      "Epoch [64/200] Loss: 363.15179443359375\n",
      "Epoch [65/200] Loss: 360.5340270996094\n",
      "Epoch [66/200] Loss: 357.9296569824219\n",
      "Epoch [67/200] Loss: 355.33856201171875\n",
      "Epoch [68/200] Loss: 352.7608337402344\n",
      "Epoch [69/200] Loss: 350.197265625\n",
      "Epoch [70/200] Loss: 347.6482849121094\n",
      "Epoch [71/200] Loss: 345.1148681640625\n",
      "Epoch [72/200] Loss: 342.5971374511719\n",
      "Epoch [73/200] Loss: 340.09539794921875\n",
      "Epoch [74/200] Loss: 337.609375\n",
      "Epoch [75/200] Loss: 335.1389465332031\n",
      "Epoch [76/200] Loss: 332.6832580566406\n",
      "Epoch [77/200] Loss: 330.2420654296875\n",
      "Epoch [78/200] Loss: 327.8146057128906\n",
      "Epoch [79/200] Loss: 325.4006652832031\n",
      "Epoch [80/200] Loss: 322.999755859375\n",
      "Epoch [81/200] Loss: 320.612060546875\n",
      "Epoch [82/200] Loss: 318.23748779296875\n",
      "Epoch [83/200] Loss: 315.87640380859375\n",
      "Epoch [84/200] Loss: 313.5291442871094\n",
      "Epoch [85/200] Loss: 311.1958923339844\n",
      "Epoch [86/200] Loss: 308.8769226074219\n",
      "Epoch [87/200] Loss: 306.572509765625\n",
      "Epoch [88/200] Loss: 304.28271484375\n",
      "Epoch [89/200] Loss: 302.0077819824219\n",
      "Epoch [90/200] Loss: 299.7475891113281\n",
      "Epoch [91/200] Loss: 297.5023498535156\n",
      "Epoch [92/200] Loss: 295.2720031738281\n",
      "Epoch [93/200] Loss: 293.05670166015625\n",
      "Epoch [94/200] Loss: 290.8564147949219\n",
      "Epoch [95/200] Loss: 288.6714172363281\n",
      "Epoch [96/200] Loss: 286.50146484375\n",
      "Epoch [97/200] Loss: 284.34698486328125\n",
      "Epoch [98/200] Loss: 282.20782470703125\n",
      "Epoch [99/200] Loss: 280.0840759277344\n",
      "Epoch [100/200] Loss: 277.9755859375\n",
      "Epoch [101/200] Loss: 275.8824157714844\n",
      "Epoch [102/200] Loss: 273.8045349121094\n",
      "Epoch [103/200] Loss: 271.7417297363281\n",
      "Epoch [104/200] Loss: 269.6940002441406\n",
      "Epoch [105/200] Loss: 267.661376953125\n",
      "Epoch [106/200] Loss: 265.64361572265625\n",
      "Epoch [107/200] Loss: 263.6408386230469\n",
      "Epoch [108/200] Loss: 261.6526184082031\n",
      "Epoch [109/200] Loss: 259.67919921875\n",
      "Epoch [110/200] Loss: 257.7204284667969\n",
      "Epoch [111/200] Loss: 255.77609252929688\n",
      "Epoch [112/200] Loss: 253.8461456298828\n",
      "Epoch [113/200] Loss: 251.93052673339844\n",
      "Epoch [114/200] Loss: 250.0291290283203\n",
      "Epoch [115/200] Loss: 248.1417236328125\n",
      "Epoch [116/200] Loss: 246.26828002929688\n",
      "Epoch [117/200] Loss: 244.4087371826172\n",
      "Epoch [118/200] Loss: 242.56283569335938\n",
      "Epoch [119/200] Loss: 240.7305908203125\n",
      "Epoch [120/200] Loss: 238.9118194580078\n",
      "Epoch [121/200] Loss: 237.10638427734375\n",
      "Epoch [122/200] Loss: 235.31431579589844\n",
      "Epoch [123/200] Loss: 233.53533935546875\n",
      "Epoch [124/200] Loss: 231.76939392089844\n",
      "Epoch [125/200] Loss: 230.0164031982422\n",
      "Epoch [126/200] Loss: 228.27613830566406\n",
      "Epoch [127/200] Loss: 226.54867553710938\n",
      "Epoch [128/200] Loss: 224.83375549316406\n",
      "Epoch [129/200] Loss: 223.13125610351562\n",
      "Epoch [130/200] Loss: 221.4412078857422\n",
      "Epoch [131/200] Loss: 219.76339721679688\n",
      "Epoch [132/200] Loss: 218.0977325439453\n",
      "Epoch [133/200] Loss: 216.44418334960938\n",
      "Epoch [134/200] Loss: 214.80255126953125\n",
      "Epoch [135/200] Loss: 213.1727294921875\n",
      "Epoch [136/200] Loss: 211.5547332763672\n",
      "Epoch [137/200] Loss: 209.94837951660156\n",
      "Epoch [138/200] Loss: 208.35360717773438\n",
      "Epoch [139/200] Loss: 206.77027893066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/200] Loss: 205.19833374023438\n",
      "Epoch [141/200] Loss: 203.63766479492188\n",
      "Epoch [142/200] Loss: 202.0882110595703\n",
      "Epoch [143/200] Loss: 200.54986572265625\n",
      "Epoch [144/200] Loss: 199.02255249023438\n",
      "Epoch [145/200] Loss: 197.5061798095703\n",
      "Epoch [146/200] Loss: 196.00059509277344\n",
      "Epoch [147/200] Loss: 194.50582885742188\n",
      "Epoch [148/200] Loss: 193.0217742919922\n",
      "Epoch [149/200] Loss: 191.5482635498047\n",
      "Epoch [150/200] Loss: 190.08526611328125\n",
      "Epoch [151/200] Loss: 188.63278198242188\n",
      "Epoch [152/200] Loss: 187.19065856933594\n",
      "Epoch [153/200] Loss: 185.7587890625\n",
      "Epoch [154/200] Loss: 184.33712768554688\n",
      "Epoch [155/200] Loss: 182.9256591796875\n",
      "Epoch [156/200] Loss: 181.52423095703125\n",
      "Epoch [157/200] Loss: 180.13287353515625\n",
      "Epoch [158/200] Loss: 178.75132751464844\n",
      "Epoch [159/200] Loss: 177.37973022460938\n",
      "Epoch [160/200] Loss: 176.01792907714844\n",
      "Epoch [161/200] Loss: 174.6658172607422\n",
      "Epoch [162/200] Loss: 173.32339477539062\n",
      "Epoch [163/200] Loss: 171.9905242919922\n",
      "Epoch [164/200] Loss: 170.6671905517578\n",
      "Epoch [165/200] Loss: 169.35333251953125\n",
      "Epoch [166/200] Loss: 168.0489044189453\n",
      "Epoch [167/200] Loss: 166.75376892089844\n",
      "Epoch [168/200] Loss: 165.46791076660156\n",
      "Epoch [169/200] Loss: 164.19125366210938\n",
      "Epoch [170/200] Loss: 162.92379760742188\n",
      "Epoch [171/200] Loss: 161.66542053222656\n",
      "Epoch [172/200] Loss: 160.416015625\n",
      "Epoch [173/200] Loss: 159.1756591796875\n",
      "Epoch [174/200] Loss: 157.94422912597656\n",
      "Epoch [175/200] Loss: 156.72164916992188\n",
      "Epoch [176/200] Loss: 155.50782775878906\n",
      "Epoch [177/200] Loss: 154.302734375\n",
      "Epoch [178/200] Loss: 153.1063690185547\n",
      "Epoch [179/200] Loss: 151.91867065429688\n",
      "Epoch [180/200] Loss: 150.73953247070312\n",
      "Epoch [181/200] Loss: 149.56890869140625\n",
      "Epoch [182/200] Loss: 148.40676879882812\n",
      "Epoch [183/200] Loss: 147.25303649902344\n",
      "Epoch [184/200] Loss: 146.107666015625\n",
      "Epoch [185/200] Loss: 144.9706268310547\n",
      "Epoch [186/200] Loss: 143.84182739257812\n",
      "Epoch [187/200] Loss: 142.72128295898438\n",
      "Epoch [188/200] Loss: 141.60887145996094\n",
      "Epoch [189/200] Loss: 140.50454711914062\n",
      "Epoch [190/200] Loss: 139.40835571289062\n",
      "Epoch [191/200] Loss: 138.32009887695312\n",
      "Epoch [192/200] Loss: 137.23983764648438\n",
      "Epoch [193/200] Loss: 136.16749572753906\n",
      "Epoch [194/200] Loss: 135.10302734375\n",
      "Epoch [195/200] Loss: 134.04635620117188\n",
      "Epoch [196/200] Loss: 132.9974365234375\n",
      "Epoch [197/200] Loss: 131.95628356933594\n",
      "Epoch [198/200] Loss: 130.9227752685547\n",
      "Epoch [199/200] Loss: 129.8968963623047\n",
      "Epoch [200/200] Loss: 128.8786163330078\n",
      "Predicted days_remaining for parent_id 466: 15.099630355834961\n",
      "Training for parent_id 481...\n",
      "Epoch [1/200] Loss: 986.2728881835938\n",
      "Epoch [2/200] Loss: 968.7471313476562\n",
      "Epoch [3/200] Loss: 951.5504150390625\n",
      "Epoch [4/200] Loss: 934.7274169921875\n",
      "Epoch [5/200] Loss: 918.3658447265625\n",
      "Epoch [6/200] Loss: 902.54931640625\n",
      "Epoch [7/200] Loss: 887.3607177734375\n",
      "Epoch [8/200] Loss: 872.8937377929688\n",
      "Epoch [9/200] Loss: 859.2460327148438\n",
      "Epoch [10/200] Loss: 846.5022583007812\n",
      "Epoch [11/200] Loss: 834.7081909179688\n",
      "Epoch [12/200] Loss: 823.8541259765625\n",
      "Epoch [13/200] Loss: 813.875244140625\n",
      "Epoch [14/200] Loss: 804.6708984375\n",
      "Epoch [15/200] Loss: 796.1289672851562\n",
      "Epoch [16/200] Loss: 788.1466674804688\n",
      "Epoch [17/200] Loss: 780.6452026367188\n",
      "Epoch [18/200] Loss: 773.5736083984375\n",
      "Epoch [19/200] Loss: 766.905517578125\n",
      "Epoch [20/200] Loss: 760.6273193359375\n",
      "Epoch [21/200] Loss: 754.7235107421875\n",
      "Epoch [22/200] Loss: 749.1668701171875\n",
      "Epoch [23/200] Loss: 743.9166259765625\n",
      "Epoch [24/200] Loss: 738.92529296875\n",
      "Epoch [25/200] Loss: 734.1463012695312\n",
      "Epoch [26/200] Loss: 729.5390625\n",
      "Epoch [27/200] Loss: 725.0707397460938\n",
      "Epoch [28/200] Loss: 720.7162475585938\n",
      "Epoch [29/200] Loss: 716.4556884765625\n",
      "Epoch [30/200] Loss: 712.274169921875\n",
      "Epoch [31/200] Loss: 708.159912109375\n",
      "Epoch [32/200] Loss: 704.1036987304688\n",
      "Epoch [33/200] Loss: 700.097900390625\n",
      "Epoch [34/200] Loss: 696.1361694335938\n",
      "Epoch [35/200] Loss: 692.2135009765625\n",
      "Epoch [36/200] Loss: 688.3250122070312\n",
      "Epoch [37/200] Loss: 684.4672241210938\n",
      "Epoch [38/200] Loss: 680.6365966796875\n",
      "Epoch [39/200] Loss: 676.8305053710938\n",
      "Epoch [40/200] Loss: 673.046630859375\n",
      "Epoch [41/200] Loss: 669.2831420898438\n",
      "Epoch [42/200] Loss: 665.5387573242188\n",
      "Epoch [43/200] Loss: 661.8126220703125\n",
      "Epoch [44/200] Loss: 658.1043090820312\n",
      "Epoch [45/200] Loss: 654.4139404296875\n",
      "Epoch [46/200] Loss: 650.7415771484375\n",
      "Epoch [47/200] Loss: 647.087646484375\n",
      "Epoch [48/200] Loss: 643.4522094726562\n",
      "Epoch [49/200] Loss: 639.8355712890625\n",
      "Epoch [50/200] Loss: 636.23779296875\n",
      "Epoch [51/200] Loss: 632.658935546875\n",
      "Epoch [52/200] Loss: 629.0987548828125\n",
      "Epoch [53/200] Loss: 625.5571899414062\n",
      "Epoch [54/200] Loss: 622.034423828125\n",
      "Epoch [55/200] Loss: 618.5301513671875\n",
      "Epoch [56/200] Loss: 615.0445556640625\n",
      "Epoch [57/200] Loss: 611.577392578125\n",
      "Epoch [58/200] Loss: 608.12890625\n",
      "Epoch [59/200] Loss: 604.6994018554688\n",
      "Epoch [60/200] Loss: 601.2883911132812\n",
      "Epoch [61/200] Loss: 597.896240234375\n",
      "Epoch [62/200] Loss: 594.5231323242188\n",
      "Epoch [63/200] Loss: 591.168701171875\n",
      "Epoch [64/200] Loss: 587.8331909179688\n",
      "Epoch [65/200] Loss: 584.5164794921875\n",
      "Epoch [66/200] Loss: 581.2185668945312\n",
      "Epoch [67/200] Loss: 577.939208984375\n",
      "Epoch [68/200] Loss: 574.6784057617188\n",
      "Epoch [69/200] Loss: 571.4361572265625\n",
      "Epoch [70/200] Loss: 568.2122802734375\n",
      "Epoch [71/200] Loss: 565.0066528320312\n",
      "Epoch [72/200] Loss: 561.819091796875\n",
      "Epoch [73/200] Loss: 558.6497192382812\n",
      "Epoch [74/200] Loss: 555.4981079101562\n",
      "Epoch [75/200] Loss: 552.3643188476562\n",
      "Epoch [76/200] Loss: 549.2479858398438\n",
      "Epoch [77/200] Loss: 546.1494750976562\n",
      "Epoch [78/200] Loss: 543.0679931640625\n",
      "Epoch [79/200] Loss: 540.00390625\n",
      "Epoch [80/200] Loss: 536.9569091796875\n",
      "Epoch [81/200] Loss: 533.9267578125\n",
      "Epoch [82/200] Loss: 530.9134521484375\n",
      "Epoch [83/200] Loss: 527.9168090820312\n",
      "Epoch [84/200] Loss: 524.9367065429688\n",
      "Epoch [85/200] Loss: 521.972900390625\n",
      "Epoch [86/200] Loss: 519.025390625\n",
      "Epoch [87/200] Loss: 516.093994140625\n",
      "Epoch [88/200] Loss: 513.1785888671875\n",
      "Epoch [89/200] Loss: 510.2791442871094\n",
      "Epoch [90/200] Loss: 507.395263671875\n",
      "Epoch [91/200] Loss: 504.52703857421875\n",
      "Epoch [92/200] Loss: 501.6741943359375\n",
      "Epoch [93/200] Loss: 498.83673095703125\n",
      "Epoch [94/200] Loss: 496.0144958496094\n",
      "Epoch [95/200] Loss: 493.2074279785156\n",
      "Epoch [96/200] Loss: 490.41522216796875\n",
      "Epoch [97/200] Loss: 487.6380310058594\n",
      "Epoch [98/200] Loss: 484.8753662109375\n",
      "Epoch [99/200] Loss: 482.12744140625\n",
      "Epoch [100/200] Loss: 479.39410400390625\n",
      "Epoch [101/200] Loss: 476.67510986328125\n",
      "Epoch [102/200] Loss: 473.970458984375\n",
      "Epoch [103/200] Loss: 471.280029296875\n",
      "Epoch [104/200] Loss: 468.6037292480469\n",
      "Epoch [105/200] Loss: 465.9413146972656\n",
      "Epoch [106/200] Loss: 463.2928466796875\n",
      "Epoch [107/200] Loss: 460.65814208984375\n",
      "Epoch [108/200] Loss: 458.0372314453125\n",
      "Epoch [109/200] Loss: 455.4299621582031\n",
      "Epoch [110/200] Loss: 452.8360900878906\n",
      "Epoch [111/200] Loss: 450.2557678222656\n",
      "Epoch [112/200] Loss: 447.688720703125\n",
      "Epoch [113/200] Loss: 445.1348876953125\n",
      "Epoch [114/200] Loss: 442.5942077636719\n",
      "Epoch [115/200] Loss: 440.06671142578125\n",
      "Epoch [116/200] Loss: 437.55218505859375\n",
      "Epoch [117/200] Loss: 435.05059814453125\n",
      "Epoch [118/200] Loss: 432.561767578125\n",
      "Epoch [119/200] Loss: 430.0857849121094\n",
      "Epoch [120/200] Loss: 427.6224060058594\n",
      "Epoch [121/200] Loss: 425.171630859375\n",
      "Epoch [122/200] Loss: 422.7333984375\n",
      "Epoch [123/200] Loss: 420.3075866699219\n",
      "Epoch [124/200] Loss: 417.8941955566406\n",
      "Epoch [125/200] Loss: 415.4931335449219\n",
      "Epoch [126/200] Loss: 413.1042175292969\n",
      "Epoch [127/200] Loss: 410.7275695800781\n",
      "Epoch [128/200] Loss: 408.3629455566406\n",
      "Epoch [129/200] Loss: 406.0103759765625\n",
      "Epoch [130/200] Loss: 403.66973876953125\n",
      "Epoch [131/200] Loss: 401.34100341796875\n",
      "Epoch [132/200] Loss: 399.0242004394531\n",
      "Epoch [133/200] Loss: 396.7190246582031\n",
      "Epoch [134/200] Loss: 394.425537109375\n",
      "Epoch [135/200] Loss: 392.1437072753906\n",
      "Epoch [136/200] Loss: 389.8735656738281\n",
      "Epoch [137/200] Loss: 387.61492919921875\n",
      "Epoch [138/200] Loss: 385.3677062988281\n",
      "Epoch [139/200] Loss: 383.13177490234375\n",
      "Epoch [140/200] Loss: 380.9072570800781\n",
      "Epoch [141/200] Loss: 378.6941223144531\n",
      "Epoch [142/200] Loss: 376.4920959472656\n",
      "Epoch [143/200] Loss: 374.30126953125\n",
      "Epoch [144/200] Loss: 372.1215515136719\n",
      "Epoch [145/200] Loss: 369.95294189453125\n",
      "Epoch [146/200] Loss: 367.79522705078125\n",
      "Epoch [147/200] Loss: 365.6485290527344\n",
      "Epoch [148/200] Loss: 363.51275634765625\n",
      "Epoch [149/200] Loss: 361.3878173828125\n",
      "Epoch [150/200] Loss: 359.2735595703125\n",
      "Epoch [151/200] Loss: 357.17010498046875\n",
      "Epoch [152/200] Loss: 355.0774230957031\n",
      "Epoch [153/200] Loss: 352.9952392578125\n",
      "Epoch [154/200] Loss: 350.9236755371094\n",
      "Epoch [155/200] Loss: 348.8627014160156\n",
      "Epoch [156/200] Loss: 346.8121643066406\n",
      "Epoch [157/200] Loss: 344.7720642089844\n",
      "Epoch [158/200] Loss: 342.7422790527344\n",
      "Epoch [159/200] Loss: 340.7229919433594\n",
      "Epoch [160/200] Loss: 338.71392822265625\n",
      "Epoch [161/200] Loss: 336.715087890625\n",
      "Epoch [162/200] Loss: 334.7264404296875\n",
      "Epoch [163/200] Loss: 332.7480163574219\n",
      "Epoch [164/200] Loss: 330.77960205078125\n",
      "Epoch [165/200] Loss: 328.8212585449219\n",
      "Epoch [166/200] Loss: 326.8730163574219\n",
      "Epoch [167/200] Loss: 324.9346618652344\n",
      "Epoch [168/200] Loss: 323.00628662109375\n",
      "Epoch [169/200] Loss: 321.0877990722656\n",
      "Epoch [170/200] Loss: 319.1791076660156\n",
      "Epoch [171/200] Loss: 317.28021240234375\n",
      "Epoch [172/200] Loss: 315.39111328125\n",
      "Epoch [173/200] Loss: 313.5116882324219\n",
      "Epoch [174/200] Loss: 311.6418762207031\n",
      "Epoch [175/200] Loss: 309.78179931640625\n",
      "Epoch [176/200] Loss: 307.9313049316406\n",
      "Epoch [177/200] Loss: 306.0902404785156\n",
      "Epoch [178/200] Loss: 304.2586975097656\n",
      "Epoch [179/200] Loss: 302.4366760253906\n",
      "Epoch [180/200] Loss: 300.6240539550781\n",
      "Epoch [181/200] Loss: 298.82073974609375\n",
      "Epoch [182/200] Loss: 297.02685546875\n",
      "Epoch [183/200] Loss: 295.24224853515625\n",
      "Epoch [184/200] Loss: 293.466796875\n",
      "Epoch [185/200] Loss: 291.70068359375\n",
      "Epoch [186/200] Loss: 289.9436950683594\n",
      "Epoch [187/200] Loss: 288.19586181640625\n",
      "Epoch [188/200] Loss: 286.4571533203125\n",
      "Epoch [189/200] Loss: 284.7275085449219\n",
      "Epoch [190/200] Loss: 283.0068359375\n",
      "Epoch [191/200] Loss: 281.29510498046875\n",
      "Epoch [192/200] Loss: 279.59246826171875\n",
      "Epoch [193/200] Loss: 277.8986511230469\n",
      "Epoch [194/200] Loss: 276.2137451171875\n",
      "Epoch [195/200] Loss: 274.5375061035156\n",
      "Epoch [196/200] Loss: 272.8702697753906\n",
      "Epoch [197/200] Loss: 271.2117614746094\n",
      "Epoch [198/200] Loss: 269.5619201660156\n",
      "Epoch [199/200] Loss: 267.9207763671875\n",
      "Epoch [200/200] Loss: 266.2882995605469\n",
      "Predicted days_remaining for parent_id 481: 14.931439399719238\n",
      "Training for parent_id 486...\n",
      "Epoch [1/200] Loss: 452.00408935546875\n",
      "Epoch [2/200] Loss: 442.3216857910156\n",
      "Epoch [3/200] Loss: 432.6116943359375\n",
      "Epoch [4/200] Loss: 423.0708923339844\n",
      "Epoch [5/200] Loss: 413.85797119140625\n",
      "Epoch [6/200] Loss: 405.0633850097656\n",
      "Epoch [7/200] Loss: 396.72528076171875\n",
      "Epoch [8/200] Loss: 388.84393310546875\n",
      "Epoch [9/200] Loss: 381.3970642089844\n",
      "Epoch [10/200] Loss: 374.35247802734375\n",
      "Epoch [11/200] Loss: 367.6761779785156\n",
      "Epoch [12/200] Loss: 361.3367919921875\n",
      "Epoch [13/200] Loss: 355.30731201171875\n",
      "Epoch [14/200] Loss: 349.5650329589844\n",
      "Epoch [15/200] Loss: 344.0902099609375\n",
      "Epoch [16/200] Loss: 338.86541748046875\n",
      "Epoch [17/200] Loss: 333.8746032714844\n",
      "Epoch [18/200] Loss: 329.1032409667969\n",
      "Epoch [19/200] Loss: 324.537841796875\n",
      "Epoch [20/200] Loss: 320.165283203125\n",
      "Epoch [21/200] Loss: 315.972900390625\n",
      "Epoch [22/200] Loss: 311.94854736328125\n",
      "Epoch [23/200] Loss: 308.0801086425781\n",
      "Epoch [24/200] Loss: 304.3559265136719\n",
      "Epoch [25/200] Loss: 300.7645568847656\n",
      "Epoch [26/200] Loss: 297.2952575683594\n",
      "Epoch [27/200] Loss: 293.93798828125\n",
      "Epoch [28/200] Loss: 290.683349609375\n",
      "Epoch [29/200] Loss: 287.5230712890625\n",
      "Epoch [30/200] Loss: 284.44952392578125\n",
      "Epoch [31/200] Loss: 281.4562072753906\n",
      "Epoch [32/200] Loss: 278.5368347167969\n",
      "Epoch [33/200] Loss: 275.6858825683594\n",
      "Epoch [34/200] Loss: 272.8979187011719\n",
      "Epoch [35/200] Loss: 270.1678161621094\n",
      "Epoch [36/200] Loss: 267.4903869628906\n",
      "Epoch [37/200] Loss: 264.8610534667969\n",
      "Epoch [38/200] Loss: 262.27520751953125\n",
      "Epoch [39/200] Loss: 259.7291259765625\n",
      "Epoch [40/200] Loss: 257.2193908691406\n",
      "Epoch [41/200] Loss: 254.74327087402344\n",
      "Epoch [42/200] Loss: 252.2987823486328\n",
      "Epoch [43/200] Loss: 249.88421630859375\n",
      "Epoch [44/200] Loss: 247.49842834472656\n",
      "Epoch [45/200] Loss: 245.14053344726562\n",
      "Epoch [46/200] Loss: 242.80978393554688\n",
      "Epoch [47/200] Loss: 240.5055389404297\n",
      "Epoch [48/200] Loss: 238.2272186279297\n",
      "Epoch [49/200] Loss: 235.97427368164062\n",
      "Epoch [50/200] Loss: 233.7460174560547\n",
      "Epoch [51/200] Loss: 231.54177856445312\n",
      "Epoch [52/200] Loss: 229.36082458496094\n",
      "Epoch [53/200] Loss: 227.20237731933594\n",
      "Epoch [54/200] Loss: 225.0657501220703\n",
      "Epoch [55/200] Loss: 222.95013427734375\n",
      "Epoch [56/200] Loss: 220.85479736328125\n",
      "Epoch [57/200] Loss: 218.77906799316406\n",
      "Epoch [58/200] Loss: 216.72230529785156\n",
      "Epoch [59/200] Loss: 214.6838836669922\n",
      "Epoch [60/200] Loss: 212.66339111328125\n",
      "Epoch [61/200] Loss: 210.6603240966797\n",
      "Epoch [62/200] Loss: 208.6742706298828\n",
      "Epoch [63/200] Loss: 206.70501708984375\n",
      "Epoch [64/200] Loss: 204.75225830078125\n",
      "Epoch [65/200] Loss: 202.8158721923828\n",
      "Epoch [66/200] Loss: 200.8956298828125\n",
      "Epoch [67/200] Loss: 198.99143981933594\n",
      "Epoch [68/200] Loss: 197.10325622558594\n",
      "Epoch [69/200] Loss: 195.2310791015625\n",
      "Epoch [70/200] Loss: 193.37477111816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/200] Loss: 191.53445434570312\n",
      "Epoch [72/200] Loss: 189.71005249023438\n",
      "Epoch [73/200] Loss: 187.90158081054688\n",
      "Epoch [74/200] Loss: 186.10903930664062\n",
      "Epoch [75/200] Loss: 184.33241271972656\n",
      "Epoch [76/200] Loss: 182.5717010498047\n",
      "Epoch [77/200] Loss: 180.82687377929688\n",
      "Epoch [78/200] Loss: 179.0980224609375\n",
      "Epoch [79/200] Loss: 177.385009765625\n",
      "Epoch [80/200] Loss: 175.68780517578125\n",
      "Epoch [81/200] Loss: 174.00633239746094\n",
      "Epoch [82/200] Loss: 172.34066772460938\n",
      "Epoch [83/200] Loss: 170.69064331054688\n",
      "Epoch [84/200] Loss: 169.05625915527344\n",
      "Epoch [85/200] Loss: 167.4373321533203\n",
      "Epoch [86/200] Loss: 165.83383178710938\n",
      "Epoch [87/200] Loss: 164.24563598632812\n",
      "Epoch [88/200] Loss: 162.67266845703125\n",
      "Epoch [89/200] Loss: 161.11477661132812\n",
      "Epoch [90/200] Loss: 159.5718231201172\n",
      "Epoch [91/200] Loss: 158.04371643066406\n",
      "Epoch [92/200] Loss: 156.5303192138672\n",
      "Epoch [93/200] Loss: 155.0314483642578\n",
      "Epoch [94/200] Loss: 153.54708862304688\n",
      "Epoch [95/200] Loss: 152.0769500732422\n",
      "Epoch [96/200] Loss: 150.62100219726562\n",
      "Epoch [97/200] Loss: 149.1790008544922\n",
      "Epoch [98/200] Loss: 147.7509002685547\n",
      "Epoch [99/200] Loss: 146.3364715576172\n",
      "Epoch [100/200] Loss: 144.9356689453125\n",
      "Epoch [101/200] Loss: 143.54827880859375\n",
      "Epoch [102/200] Loss: 142.17417907714844\n",
      "Epoch [103/200] Loss: 140.8132781982422\n",
      "Epoch [104/200] Loss: 139.4653778076172\n",
      "Epoch [105/200] Loss: 138.1303253173828\n",
      "Epoch [106/200] Loss: 136.8080596923828\n",
      "Epoch [107/200] Loss: 135.49838256835938\n",
      "Epoch [108/200] Loss: 134.20120239257812\n",
      "Epoch [109/200] Loss: 132.91641235351562\n",
      "Epoch [110/200] Loss: 131.64382934570312\n",
      "Epoch [111/200] Loss: 130.38330078125\n",
      "Epoch [112/200] Loss: 129.13485717773438\n",
      "Epoch [113/200] Loss: 127.89823913574219\n",
      "Epoch [114/200] Loss: 126.67338562011719\n",
      "Epoch [115/200] Loss: 125.46013641357422\n",
      "Epoch [116/200] Loss: 124.25841522216797\n",
      "Epoch [117/200] Loss: 123.06809997558594\n",
      "Epoch [118/200] Loss: 121.88908386230469\n",
      "Epoch [119/200] Loss: 120.72122955322266\n",
      "Epoch [120/200] Loss: 119.56446075439453\n",
      "Epoch [121/200] Loss: 118.41865539550781\n",
      "Epoch [122/200] Loss: 117.28372192382812\n",
      "Epoch [123/200] Loss: 116.15955352783203\n",
      "Epoch [124/200] Loss: 115.04606628417969\n",
      "Epoch [125/200] Loss: 113.94310760498047\n",
      "Epoch [126/200] Loss: 112.85065460205078\n",
      "Epoch [127/200] Loss: 111.76851654052734\n",
      "Epoch [128/200] Loss: 110.69670104980469\n",
      "Epoch [129/200] Loss: 109.63504028320312\n",
      "Epoch [130/200] Loss: 108.58353424072266\n",
      "Epoch [131/200] Loss: 107.54193115234375\n",
      "Epoch [132/200] Loss: 106.51026916503906\n",
      "Epoch [133/200] Loss: 105.48844909667969\n",
      "Epoch [134/200] Loss: 104.47632598876953\n",
      "Epoch [135/200] Loss: 103.47389221191406\n",
      "Epoch [136/200] Loss: 102.48101043701172\n",
      "Epoch [137/200] Loss: 101.49759674072266\n",
      "Epoch [138/200] Loss: 100.52359008789062\n",
      "Epoch [139/200] Loss: 99.55889129638672\n",
      "Epoch [140/200] Loss: 98.60343170166016\n",
      "Epoch [141/200] Loss: 97.65711975097656\n",
      "Epoch [142/200] Loss: 96.71988677978516\n",
      "Epoch [143/200] Loss: 95.79163360595703\n",
      "Epoch [144/200] Loss: 94.87236785888672\n",
      "Epoch [145/200] Loss: 93.96188354492188\n",
      "Epoch [146/200] Loss: 93.06022644042969\n",
      "Epoch [147/200] Loss: 92.16722106933594\n",
      "Epoch [148/200] Loss: 91.28285217285156\n",
      "Epoch [149/200] Loss: 90.40705108642578\n",
      "Epoch [150/200] Loss: 89.53971862792969\n",
      "Epoch [151/200] Loss: 88.68080139160156\n",
      "Epoch [152/200] Loss: 87.83023071289062\n",
      "Epoch [153/200] Loss: 86.98793029785156\n",
      "Epoch [154/200] Loss: 86.15382385253906\n",
      "Epoch [155/200] Loss: 85.32785034179688\n",
      "Epoch [156/200] Loss: 84.50992584228516\n",
      "Epoch [157/200] Loss: 83.70005798339844\n",
      "Epoch [158/200] Loss: 82.8980941772461\n",
      "Epoch [159/200] Loss: 82.10397338867188\n",
      "Epoch [160/200] Loss: 81.31768798828125\n",
      "Epoch [161/200] Loss: 80.53913116455078\n",
      "Epoch [162/200] Loss: 79.76827239990234\n",
      "Epoch [163/200] Loss: 79.00498962402344\n",
      "Epoch [164/200] Loss: 78.24927520751953\n",
      "Epoch [165/200] Loss: 77.50105285644531\n",
      "Epoch [166/200] Loss: 76.76023864746094\n",
      "Epoch [167/200] Loss: 76.02680206298828\n",
      "Epoch [168/200] Loss: 75.30066680908203\n",
      "Epoch [169/200] Loss: 74.5818099975586\n",
      "Epoch [170/200] Loss: 73.8700942993164\n",
      "Epoch [171/200] Loss: 73.16551971435547\n",
      "Epoch [172/200] Loss: 72.46802520751953\n",
      "Epoch [173/200] Loss: 71.77752685546875\n",
      "Epoch [174/200] Loss: 71.09398651123047\n",
      "Epoch [175/200] Loss: 70.41736602783203\n",
      "Epoch [176/200] Loss: 69.74756622314453\n",
      "Epoch [177/200] Loss: 69.08455657958984\n",
      "Epoch [178/200] Loss: 68.42826080322266\n",
      "Epoch [179/200] Loss: 67.77864074707031\n",
      "Epoch [180/200] Loss: 67.13563537597656\n",
      "Epoch [181/200] Loss: 66.49920654296875\n",
      "Epoch [182/200] Loss: 65.86927795410156\n",
      "Epoch [183/200] Loss: 65.24581146240234\n",
      "Epoch [184/200] Loss: 64.62872314453125\n",
      "Epoch [185/200] Loss: 64.01800537109375\n",
      "Epoch [186/200] Loss: 63.41356658935547\n",
      "Epoch [187/200] Loss: 62.81538772583008\n",
      "Epoch [188/200] Loss: 62.22338104248047\n",
      "Epoch [189/200] Loss: 61.63753890991211\n",
      "Epoch [190/200] Loss: 61.05778503417969\n",
      "Epoch [191/200] Loss: 60.484066009521484\n",
      "Epoch [192/200] Loss: 59.91632843017578\n",
      "Epoch [193/200] Loss: 59.35451889038086\n",
      "Epoch [194/200] Loss: 58.798606872558594\n",
      "Epoch [195/200] Loss: 58.2485237121582\n",
      "Epoch [196/200] Loss: 57.70423126220703\n",
      "Epoch [197/200] Loss: 57.16571044921875\n",
      "Epoch [198/200] Loss: 56.63286209106445\n",
      "Epoch [199/200] Loss: 56.105655670166016\n",
      "Epoch [200/200] Loss: 55.58403015136719\n",
      "Predicted days_remaining for parent_id 486: 14.3757963180542\n",
      "Training for parent_id 511...\n",
      "Epoch [1/200] Loss: 183.7837677001953\n",
      "Epoch [2/200] Loss: 176.03607177734375\n",
      "Epoch [3/200] Loss: 168.57199096679688\n",
      "Epoch [4/200] Loss: 161.51406860351562\n",
      "Epoch [5/200] Loss: 154.93727111816406\n",
      "Epoch [6/200] Loss: 148.8612823486328\n",
      "Epoch [7/200] Loss: 143.26895141601562\n",
      "Epoch [8/200] Loss: 138.12355041503906\n",
      "Epoch [9/200] Loss: 133.38363647460938\n",
      "Epoch [10/200] Loss: 129.01023864746094\n",
      "Epoch [11/200] Loss: 124.96551513671875\n",
      "Epoch [12/200] Loss: 121.21134948730469\n",
      "Epoch [13/200] Loss: 117.71024322509766\n",
      "Epoch [14/200] Loss: 114.42766571044922\n",
      "Epoch [15/200] Loss: 111.33381652832031\n",
      "Epoch [16/200] Loss: 108.40472412109375\n",
      "Epoch [17/200] Loss: 105.62187957763672\n",
      "Epoch [18/200] Loss: 102.97129821777344\n",
      "Epoch [19/200] Loss: 100.44246673583984\n",
      "Epoch [20/200] Loss: 98.02716064453125\n",
      "Epoch [21/200] Loss: 95.7188949584961\n",
      "Epoch [22/200] Loss: 93.5121841430664\n",
      "Epoch [23/200] Loss: 91.4022445678711\n",
      "Epoch [24/200] Loss: 89.3846435546875\n",
      "Epoch [25/200] Loss: 87.4551773071289\n",
      "Epoch [26/200] Loss: 85.60979461669922\n",
      "Epoch [27/200] Loss: 83.84454345703125\n",
      "Epoch [28/200] Loss: 82.15567016601562\n",
      "Epoch [29/200] Loss: 80.53953552246094\n",
      "Epoch [30/200] Loss: 78.99251556396484\n",
      "Epoch [31/200] Loss: 77.51087188720703\n",
      "Epoch [32/200] Loss: 76.09065246582031\n",
      "Epoch [33/200] Loss: 74.72765350341797\n",
      "Epoch [34/200] Loss: 73.4175033569336\n",
      "Epoch [35/200] Loss: 72.15571594238281\n",
      "Epoch [36/200] Loss: 70.93800354003906\n",
      "Epoch [37/200] Loss: 69.76030731201172\n",
      "Epoch [38/200] Loss: 68.61891174316406\n",
      "Epoch [39/200] Loss: 67.5105209350586\n",
      "Epoch [40/200] Loss: 66.43225860595703\n",
      "Epoch [41/200] Loss: 65.38162231445312\n",
      "Epoch [42/200] Loss: 64.35652923583984\n",
      "Epoch [43/200] Loss: 63.355133056640625\n",
      "Epoch [44/200] Loss: 62.37593078613281\n",
      "Epoch [45/200] Loss: 61.41763687133789\n",
      "Epoch [46/200] Loss: 60.479129791259766\n",
      "Epoch [47/200] Loss: 59.5594596862793\n",
      "Epoch [48/200] Loss: 58.6578483581543\n",
      "Epoch [49/200] Loss: 57.7735595703125\n",
      "Epoch [50/200] Loss: 56.90597915649414\n",
      "Epoch [51/200] Loss: 56.0545768737793\n",
      "Epoch [52/200] Loss: 55.21884536743164\n",
      "Epoch [53/200] Loss: 54.39837646484375\n",
      "Epoch [54/200] Loss: 53.5927619934082\n",
      "Epoch [55/200] Loss: 52.80164337158203\n",
      "Epoch [56/200] Loss: 52.024681091308594\n",
      "Epoch [57/200] Loss: 51.2615852355957\n",
      "Epoch [58/200] Loss: 50.512054443359375\n",
      "Epoch [59/200] Loss: 49.77582550048828\n",
      "Epoch [60/200] Loss: 49.05263900756836\n",
      "Epoch [61/200] Loss: 48.34226608276367\n",
      "Epoch [62/200] Loss: 47.644466400146484\n",
      "Epoch [63/200] Loss: 46.959014892578125\n",
      "Epoch [64/200] Loss: 46.28569793701172\n",
      "Epoch [65/200] Loss: 45.62432861328125\n",
      "Epoch [66/200] Loss: 44.97468185424805\n",
      "Epoch [67/200] Loss: 44.336578369140625\n",
      "Epoch [68/200] Loss: 43.7098503112793\n",
      "Epoch [69/200] Loss: 43.09428024291992\n",
      "Epoch [70/200] Loss: 42.48970413208008\n",
      "Epoch [71/200] Loss: 41.89598083496094\n",
      "Epoch [72/200] Loss: 41.31287384033203\n",
      "Epoch [73/200] Loss: 40.74027633666992\n",
      "Epoch [74/200] Loss: 40.17799377441406\n",
      "Epoch [75/200] Loss: 39.62589645385742\n",
      "Epoch [76/200] Loss: 39.083805084228516\n",
      "Epoch [77/200] Loss: 38.551570892333984\n",
      "Epoch [78/200] Loss: 38.02904510498047\n",
      "Epoch [79/200] Loss: 37.516075134277344\n",
      "Epoch [80/200] Loss: 37.01252365112305\n",
      "Epoch [81/200] Loss: 36.51824188232422\n",
      "Epoch [82/200] Loss: 36.0330924987793\n",
      "Epoch [83/200] Loss: 35.55693435668945\n",
      "Epoch [84/200] Loss: 35.089637756347656\n",
      "Epoch [85/200] Loss: 34.63106155395508\n",
      "Epoch [86/200] Loss: 34.181068420410156\n",
      "Epoch [87/200] Loss: 33.739532470703125\n",
      "Epoch [88/200] Loss: 33.30633544921875\n",
      "Epoch [89/200] Loss: 32.88132858276367\n",
      "Epoch [90/200] Loss: 32.46439743041992\n",
      "Epoch [91/200] Loss: 32.05542755126953\n",
      "Epoch [92/200] Loss: 31.6542911529541\n",
      "Epoch [93/200] Loss: 31.260854721069336\n",
      "Epoch [94/200] Loss: 30.875015258789062\n",
      "Epoch [95/200] Loss: 30.49664306640625\n",
      "Epoch [96/200] Loss: 30.125648498535156\n",
      "Epoch [97/200] Loss: 29.761886596679688\n",
      "Epoch [98/200] Loss: 29.405254364013672\n",
      "Epoch [99/200] Loss: 29.055652618408203\n",
      "Epoch [100/200] Loss: 28.71295166015625\n",
      "Epoch [101/200] Loss: 28.377065658569336\n",
      "Epoch [102/200] Loss: 28.047853469848633\n",
      "Epoch [103/200] Loss: 27.725238800048828\n",
      "Epoch [104/200] Loss: 27.40911102294922\n",
      "Epoch [105/200] Loss: 27.09935188293457\n",
      "Epoch [106/200] Loss: 26.795879364013672\n",
      "Epoch [107/200] Loss: 26.498577117919922\n",
      "Epoch [108/200] Loss: 26.207355499267578\n",
      "Epoch [109/200] Loss: 25.922115325927734\n",
      "Epoch [110/200] Loss: 25.642749786376953\n",
      "Epoch [111/200] Loss: 25.369163513183594\n",
      "Epoch [112/200] Loss: 25.101272583007812\n",
      "Epoch [113/200] Loss: 24.83897590637207\n",
      "Epoch [114/200] Loss: 24.582183837890625\n",
      "Epoch [115/200] Loss: 24.33078956604004\n",
      "Epoch [116/200] Loss: 24.0847225189209\n",
      "Epoch [117/200] Loss: 23.84387969970703\n",
      "Epoch [118/200] Loss: 23.60817527770996\n",
      "Epoch [119/200] Loss: 23.37752914428711\n",
      "Epoch [120/200] Loss: 23.151840209960938\n",
      "Epoch [121/200] Loss: 22.931041717529297\n",
      "Epoch [122/200] Loss: 22.71504020690918\n",
      "Epoch [123/200] Loss: 22.503742218017578\n",
      "Epoch [124/200] Loss: 22.297077178955078\n",
      "Epoch [125/200] Loss: 22.094968795776367\n",
      "Epoch [126/200] Loss: 21.89731788635254\n",
      "Epoch [127/200] Loss: 21.704063415527344\n",
      "Epoch [128/200] Loss: 21.51511573791504\n",
      "Epoch [129/200] Loss: 21.33040428161621\n",
      "Epoch [130/200] Loss: 21.149843215942383\n",
      "Epoch [131/200] Loss: 20.973369598388672\n",
      "Epoch [132/200] Loss: 20.800905227661133\n",
      "Epoch [133/200] Loss: 20.632362365722656\n",
      "Epoch [134/200] Loss: 20.46768569946289\n",
      "Epoch [135/200] Loss: 20.306798934936523\n",
      "Epoch [136/200] Loss: 20.149633407592773\n",
      "Epoch [137/200] Loss: 19.996116638183594\n",
      "Epoch [138/200] Loss: 19.846174240112305\n",
      "Epoch [139/200] Loss: 19.699745178222656\n",
      "Epoch [140/200] Loss: 19.556758880615234\n",
      "Epoch [141/200] Loss: 19.417156219482422\n",
      "Epoch [142/200] Loss: 19.28085708618164\n",
      "Epoch [143/200] Loss: 19.147808074951172\n",
      "Epoch [144/200] Loss: 19.017946243286133\n",
      "Epoch [145/200] Loss: 18.891212463378906\n",
      "Epoch [146/200] Loss: 18.767534255981445\n",
      "Epoch [147/200] Loss: 18.64685821533203\n",
      "Epoch [148/200] Loss: 18.52912139892578\n",
      "Epoch [149/200] Loss: 18.414260864257812\n",
      "Epoch [150/200] Loss: 18.302227020263672\n",
      "Epoch [151/200] Loss: 18.19295883178711\n",
      "Epoch [152/200] Loss: 18.08639907836914\n",
      "Epoch [153/200] Loss: 17.982494354248047\n",
      "Epoch [154/200] Loss: 17.88118553161621\n",
      "Epoch [155/200] Loss: 17.782421112060547\n",
      "Epoch [156/200] Loss: 17.686147689819336\n",
      "Epoch [157/200] Loss: 17.592317581176758\n",
      "Epoch [158/200] Loss: 17.50086784362793\n",
      "Epoch [159/200] Loss: 17.411758422851562\n",
      "Epoch [160/200] Loss: 17.324932098388672\n",
      "Epoch [161/200] Loss: 17.240346908569336\n",
      "Epoch [162/200] Loss: 17.1579532623291\n",
      "Epoch [163/200] Loss: 17.077699661254883\n",
      "Epoch [164/200] Loss: 16.999536514282227\n",
      "Epoch [165/200] Loss: 16.92342758178711\n",
      "Epoch [166/200] Loss: 16.849315643310547\n",
      "Epoch [167/200] Loss: 16.777164459228516\n",
      "Epoch [168/200] Loss: 16.706933975219727\n",
      "Epoch [169/200] Loss: 16.63857078552246\n",
      "Epoch [170/200] Loss: 16.57204246520996\n",
      "Epoch [171/200] Loss: 16.507293701171875\n",
      "Epoch [172/200] Loss: 16.444299697875977\n",
      "Epoch [173/200] Loss: 16.383010864257812\n",
      "Epoch [174/200] Loss: 16.32339096069336\n",
      "Epoch [175/200] Loss: 16.265403747558594\n",
      "Epoch [176/200] Loss: 16.209001541137695\n",
      "Epoch [177/200] Loss: 16.154151916503906\n",
      "Epoch [178/200] Loss: 16.100826263427734\n",
      "Epoch [179/200] Loss: 16.048980712890625\n",
      "Epoch [180/200] Loss: 15.998575210571289\n",
      "Epoch [181/200] Loss: 15.949586868286133\n",
      "Epoch [182/200] Loss: 15.9019775390625\n",
      "Epoch [183/200] Loss: 15.855708122253418\n",
      "Epoch [184/200] Loss: 15.810751914978027\n",
      "Epoch [185/200] Loss: 15.76707649230957\n",
      "Epoch [186/200] Loss: 15.724651336669922\n",
      "Epoch [187/200] Loss: 15.68343734741211\n",
      "Epoch [188/200] Loss: 15.643412590026855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/200] Loss: 15.604547500610352\n",
      "Epoch [190/200] Loss: 15.566810607910156\n",
      "Epoch [191/200] Loss: 15.530168533325195\n",
      "Epoch [192/200] Loss: 15.49460506439209\n",
      "Epoch [193/200] Loss: 15.460081100463867\n",
      "Epoch [194/200] Loss: 15.4265775680542\n",
      "Epoch [195/200] Loss: 15.39406681060791\n",
      "Epoch [196/200] Loss: 15.362520217895508\n",
      "Epoch [197/200] Loss: 15.331918716430664\n",
      "Epoch [198/200] Loss: 15.302230834960938\n",
      "Epoch [199/200] Loss: 15.273438453674316\n",
      "Epoch [200/200] Loss: 15.245511054992676\n",
      "Predicted days_remaining for parent_id 511: 11.866296768188477\n",
      "Total training time: 12.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialize the RNN model parameters\n",
    "input_size = 65  # Number of input features\n",
    "hidden_size = 64\n",
    "output_size = 1  # We are predicting a single value (e.g., days_remaining)\n",
    "num_epochs = 200  # Number of epochs\n",
    "\n",
    "# Define the loss criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Save the models\n",
    "models = []\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each parent_id and train the model\n",
    "unique_parent_ids = df['parent_id'].unique()\n",
    "\n",
    "for parent_id in unique_parent_ids:\n",
    "    print(f\"Training for parent_id {parent_id}...\")\n",
    "    \n",
    "    # Get the data for the current parent_id\n",
    "    features, target = get_data_for_parent(df, parent_id)\n",
    "    \n",
    "    # Initialize a new model and optimizer for each parent_id\n",
    "    model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop for each parent_id\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item()}\")\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = f\"Models/model_parent_{parent_id}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    models.append(model)  # Add model to the list\n",
    "\n",
    "    # Optionally, evaluate the model or predict on test data after training\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "        print(f\"Predicted days_remaining for parent_id {parent_id}: {predictions.item()}\")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total training time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total training time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6dc41",
   "metadata": {},
   "source": [
    "### Function for prediction through ensemble method as it takes the average of the output of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7eb879e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each model and create the ensemble prediction function\n",
    "def ensemble_prediction(models, input_data):\n",
    "    predictions = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model in models:\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            output = model(input_data)\n",
    "            predictions.append(output.item())  # Store the prediction\n",
    "        \n",
    "    # Average the predictions to form the final prediction\n",
    "    return sum(predictions) / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47dacda",
   "metadata": {},
   "source": [
    "### Testing in random 25% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c664a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model MSE on test data: 689.4647915017564\n",
      "Total evaluation time: 0.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preparing the test data\n",
    "test_data = df.drop(\"parent_id\", axis=1)\n",
    "X = test_data.drop(\"days_remaining\", axis=1)\n",
    "y = test_data.days_remaining\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize lists for true and predicted values\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate ensemble model on the test set\n",
    "for i in range(len(X_test)):\n",
    "    # Prepare each test sample with the correct dimensions\n",
    "    test_sample = torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0)  # Shape (1, 1, input_size)\n",
    "    true_value = y_test.iloc[i]  # Actual value for the test sample\n",
    "    \n",
    "    # Get the ensemble prediction\n",
    "    prediction = ensemble_prediction(models, test_sample)\n",
    "    \n",
    "    # Append to lists for MSE calculation\n",
    "    y_true.append(true_value)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Ensemble Model MSE on test data: {mse}\")\n",
    "\n",
    "# Calculate and print the total evaluation time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "383ee162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 8,\n",
       " 17,\n",
       " 11,\n",
       " 15,\n",
       " 13,\n",
       " 38,\n",
       " 15,\n",
       " 23,\n",
       " 15,\n",
       " 14,\n",
       " 20,\n",
       " 53,\n",
       " 13,\n",
       " 23,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 11,\n",
       " 11,\n",
       " 15,\n",
       " 24,\n",
       " 16,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 27,\n",
       " 37,\n",
       " 23,\n",
       " 3,\n",
       " 10,\n",
       " 12,\n",
       " 37,\n",
       " 9,\n",
       " 18,\n",
       " 40,\n",
       " 21,\n",
       " 15,\n",
       " 8,\n",
       " 8,\n",
       " 46,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 14,\n",
       " 14,\n",
       " 80,\n",
       " 21,\n",
       " 17,\n",
       " 24,\n",
       " 17,\n",
       " 14,\n",
       " 36,\n",
       " 12,\n",
       " 21,\n",
       " 9,\n",
       " 20,\n",
       " 7,\n",
       " 32,\n",
       " 28,\n",
       " 9,\n",
       " 65,\n",
       " 27,\n",
       " 15,\n",
       " 58,\n",
       " 11,\n",
       " 15,\n",
       " 10,\n",
       " 52,\n",
       " 55,\n",
       " 21,\n",
       " 14,\n",
       " 20,\n",
       " 11,\n",
       " 34,\n",
       " 5,\n",
       " 9,\n",
       " 75,\n",
       " 20,\n",
       " 21,\n",
       " 31,\n",
       " 25,\n",
       " 16,\n",
       " 15,\n",
       " 34,\n",
       " 28,\n",
       " 54,\n",
       " 28,\n",
       " 20,\n",
       " 1,\n",
       " 33,\n",
       " 17,\n",
       " 14,\n",
       " 30,\n",
       " 14,\n",
       " 19,\n",
       " 32,\n",
       " 36,\n",
       " 19,\n",
       " 66,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 8,\n",
       " 15,\n",
       " 43,\n",
       " 17,\n",
       " 15,\n",
       " 31,\n",
       " 7,\n",
       " 22,\n",
       " 20,\n",
       " 25,\n",
       " 5,\n",
       " 18,\n",
       " 13,\n",
       " 23,\n",
       " 30,\n",
       " 22,\n",
       " 22,\n",
       " 58,\n",
       " 18,\n",
       " 13,\n",
       " 24,\n",
       " 39,\n",
       " 43,\n",
       " 34,\n",
       " 27,\n",
       " 25,\n",
       " 27,\n",
       " 30,\n",
       " 15,\n",
       " 28,\n",
       " 18,\n",
       " 33,\n",
       " 11,\n",
       " 39,\n",
       " 30,\n",
       " 32,\n",
       " 29,\n",
       " 22,\n",
       " 13,\n",
       " 31,\n",
       " 28,\n",
       " 26,\n",
       " 37,\n",
       " 18,\n",
       " 48,\n",
       " 5,\n",
       " 42,\n",
       " 9,\n",
       " 39,\n",
       " 9,\n",
       " 33,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 33,\n",
       " 11,\n",
       " 14,\n",
       " 24,\n",
       " 56,\n",
       " 19,\n",
       " 40,\n",
       " 50,\n",
       " 23,\n",
       " 26,\n",
       " 40,\n",
       " 26,\n",
       " 33,\n",
       " 19,\n",
       " 28,\n",
       " 42,\n",
       " 12,\n",
       " 41,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 17,\n",
       " 12,\n",
       " 15,\n",
       " 10,\n",
       " 39,\n",
       " 22,\n",
       " 9,\n",
       " 12,\n",
       " 36,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 16,\n",
       " 48,\n",
       " 36,\n",
       " 17,\n",
       " 3,\n",
       " 20]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b45cc7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.13185802524035073,\n",
       " 2.0282793961921515,\n",
       " -0.2022667840427282,\n",
       " -2.410910342154758,\n",
       " 1.7636443002871713,\n",
       " -0.4671705573303055,\n",
       " 0.2032238860063407,\n",
       " 1.094239984543956,\n",
       " -1.6728887531374181,\n",
       " 0.6553400960199687,\n",
       " 2.2925445119641266,\n",
       " -0.40621846997920347,\n",
       " 0.721347966630544,\n",
       " 0.7624812117986837,\n",
       " 1.5131583982432375,\n",
       " 2.111413592624725,\n",
       " 2.130959064680703,\n",
       " 1.3697353292788779,\n",
       " 2.409967783344339,\n",
       " 1.6830194083859726,\n",
       " 1.1615173786080308,\n",
       " -0.07296764090353129,\n",
       " 1.7075479589972873,\n",
       " 1.8583166660094748,\n",
       " 1.04803083983383,\n",
       " 0.008919557594523137,\n",
       " -0.3023595061077147,\n",
       " 1.838167242477743,\n",
       " 1.477879485153422,\n",
       " 1.6193178690475774,\n",
       " 1.791738170028037,\n",
       " 1.1853421006199656,\n",
       " -0.3500823441664783,\n",
       " 2.4162441260200374,\n",
       " 1.646983121426738,\n",
       " 1.8541254625674717,\n",
       " 0.03546348590479822,\n",
       " 2.0104839426788446,\n",
       " 1.8543693803402843,\n",
       " 1.4291005909290848,\n",
       " 1.6393294780686194,\n",
       " 0.9172725901767915,\n",
       " 0.6132920407975206,\n",
       " 2.480506772365497,\n",
       " 0.5661859580942866,\n",
       " 0.5611791121868455,\n",
       " 0.9284347135345546,\n",
       " 2.0967732994836203,\n",
       " 1.6284167879090017,\n",
       " 2.191852105040179,\n",
       " 1.2199216904688854,\n",
       " 1.744443622094636,\n",
       " 0.126607194352819,\n",
       " 1.9533685629389115,\n",
       " 2.2394815654474862,\n",
       " 0.201248720820461,\n",
       " -1.3937329381932408,\n",
       " 1.1121176718449106,\n",
       " 1.4922156271581748,\n",
       " -0.9075934747804184,\n",
       " 1.7692341333901396,\n",
       " -0.1802716655846761,\n",
       " 1.1214087759429703,\n",
       " 1.6267816900294654,\n",
       " -0.9215058095905245,\n",
       " 0.808736602072509,\n",
       " 1.5960022047618214,\n",
       " 2.0175941081869664,\n",
       " -0.8228987228809571,\n",
       " -0.28583487883514286,\n",
       " 1.9360129535198212,\n",
       " 2.003873290912229,\n",
       " 1.5210179583910777,\n",
       " 2.035349621836628,\n",
       " -0.02514844357359166,\n",
       " 1.4700480403606684,\n",
       " 2.054372626641879,\n",
       " 0.534010248995215,\n",
       " 2.43543072736689,\n",
       " 1.452453883630889,\n",
       " 0.5363757615547855,\n",
       " 0.21678586045698245,\n",
       " 1.9200160167351061,\n",
       " 1.9693270636608406,\n",
       " 0.83269658426241,\n",
       " 0.8640035545187337,\n",
       " 0.9065126470119065,\n",
       " 2.0163917435827305,\n",
       " -0.35802048840084855,\n",
       " 0.20243473495451772,\n",
       " 0.1213020948427064,\n",
       " 2.621700943428643,\n",
       " 0.7038347321201344,\n",
       " 0.7275094980830137,\n",
       " 1.2928948151517887,\n",
       " 1.9385765570462967,\n",
       " 0.9148086184749797,\n",
       " 1.9632601570718142,\n",
       " 1.458737780054917,\n",
       " -0.2661810048228624,\n",
       " 2.3843326177828166,\n",
       " -0.27515321703893797,\n",
       " 1.33885088353893,\n",
       " 0.2761345473403225,\n",
       " 1.4912020259973955,\n",
       " 0.9506086101945566,\n",
       " 1.0094804888667197,\n",
       " 2.1066893380971585,\n",
       " 1.0838965740130873,\n",
       " 2.0064163442168916,\n",
       " 1.1570336506987104,\n",
       " 2.166863204569233,\n",
       " 2.5647787434258023,\n",
       " 2.288699963719261,\n",
       " 1.6682128150548254,\n",
       " -0.8056976205992455,\n",
       " 0.2631032548901834,\n",
       " 1.1158116323759362,\n",
       " 1.0864198334439068,\n",
       " 1.9218335010643517,\n",
       " 0.16120930502609332,\n",
       " 0.6728206276893616,\n",
       " 1.6629608721301263,\n",
       " 0.9973421563223308,\n",
       " 1.5413348907889912,\n",
       " 0.17276344417917486,\n",
       " 1.3891122944044823,\n",
       " 2.008037191872694,\n",
       " 1.0144894326067702,\n",
       " 0.49501687961117347,\n",
       " 0.09597110892741048,\n",
       " 1.161269427638273,\n",
       " -0.25828039334440717,\n",
       " -0.4505902948428173,\n",
       " 2.5248630544436828,\n",
       " 2.2529933169788245,\n",
       " 0.12064724214070914,\n",
       " -1.8337329490756502,\n",
       " 2.0433340375216638,\n",
       " -0.7259960901372287,\n",
       " 0.10523408111564968,\n",
       " 0.30942329338618685,\n",
       " -1.1164123839223568,\n",
       " 1.261109992614662,\n",
       " 2.019781250430613,\n",
       " -0.6115062100698754,\n",
       " 1.4113488374468015,\n",
       " 0.27422130145892804,\n",
       " 0.5443639508923706,\n",
       " 0.40815094599918444,\n",
       " 2.1441882316555296,\n",
       " -0.3206985297099668,\n",
       " 1.5977694594814462,\n",
       " 2.230502611159214,\n",
       " 2.0749404735255,\n",
       " 0.4854880116727887,\n",
       " 1.718333750507053,\n",
       " -1.5435439817303298,\n",
       " 1.341584368932004,\n",
       " 2.1159936536331565,\n",
       " 1.163056531883016,\n",
       " -0.6811738003577504,\n",
       " -0.15609368738927404,\n",
       " 0.7517369226837645,\n",
       " 1.8441175051337602,\n",
       " 2.3370048393096243,\n",
       " 1.7248406584317586,\n",
       " 1.6250882467293009,\n",
       " 0.8448842601204405,\n",
       " 0.19106861029048355,\n",
       " 1.5220775400877606,\n",
       " 0.9077239539474249,\n",
       " 1.6702204867057047,\n",
       " 1.5029452935393368,\n",
       " 0.3521278296046111,\n",
       " 0.3578812157621189,\n",
       " 0.41193465934116014,\n",
       " 1.8128940091297334,\n",
       " 1.5862215304253053,\n",
       " 1.4472248575041944,\n",
       " -0.7366315189901055,\n",
       " 1.8445950197626133,\n",
       " 2.0022885531795267,\n",
       " 2.5065282783946214,\n",
       " 2.1137578834076316,\n",
       " 1.185915512393932,\n",
       " 1.815732618071595,\n",
       " 1.005044246624623,\n",
       " -0.4472594747072732,\n",
       " 0.8881456977402677,\n",
       " -0.3138089882475989,\n",
       " 1.8718413209960776,\n",
       " 1.2614348867566,\n",
       " 1.8897148319805155,\n",
       " 1.3644531944728628,\n",
       " 1.319948565366925]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29dfcac",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc553d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "hidden_size = 64\n",
    "batch_size = 1  # Single sequence in the batch\n",
    "\n",
    "# Random sequence data of shape [1, 8, 80] (1 sequence, 8 time steps, 80 features)\n",
    "sequence_data = torch.randn(batch_size, seq_len, 80)\n",
    "\n",
    "# RNN model (hidden size 64)\n",
    "rnn = nn.RNN(input_size=80, hidden_size=hidden_size, batch_first=True)\n",
    "\n",
    "# Forward pass through the RNN\n",
    "out, h_n = rnn(sequence_data)\n",
    "print(out.shape)  # Output shape: [1, 8, 64] (1 batch, 8 time steps, 64 hidden units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cda5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_output = out[:, -1, :]  # Shape: [1, 64], which is the hidden state of the last time step\n",
    "last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_n[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8079bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 65)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[df['parent_id'] == 3]\n",
    "features = data.drop(columns=['parent_id', 'days_remaining'])\n",
    "target = data['days_remaining'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b069d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
