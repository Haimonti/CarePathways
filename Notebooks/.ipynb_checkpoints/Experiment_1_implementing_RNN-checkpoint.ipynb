{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ba83a5",
   "metadata": {},
   "source": [
    "### Implementing RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4592fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9508f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/df_over_14.csv\")\n",
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799c48b",
   "metadata": {},
   "source": [
    "### A bit of preprecessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24d3a4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>highest_mean_arterial_pressure</th>\n",
       "      <th>lowest_mean_arterial_pressure</th>\n",
       "      <th>highest_heart_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Bilateral consolidationinfiltration</th>\n",
       "      <th>Emphysematous or Bronchiectasis changes</th>\n",
       "      <th>Emphysematous or Bronchiectatic changes</th>\n",
       "      <th>Pulmonary Embolism</th>\n",
       "      <th>Scarring or Fibrosis</th>\n",
       "      <th>Unilateral Ground Glass Opacities</th>\n",
       "      <th>Unilateral consolidationinfiltration</th>\n",
       "      <th>Subarachnoid Hemorrhage</th>\n",
       "      <th>Subdural Hemorrhage</th>\n",
       "      <th>days_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>79.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>121.333333</td>\n",
       "      <td>61.833333</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  systolic_blood_pressure  diastolic_blood_pressure  heart_rate  \\\n",
       "0           6               127.000000                 76.000000        68.0   \n",
       "1           6                97.000000                 60.000000        68.0   \n",
       "2           6               140.000000                 68.000000        72.0   \n",
       "3           6               108.000000                 63.000000        98.0   \n",
       "4           6               126.000000                 77.000000        68.0   \n",
       "5           6               120.000000                 82.000000        76.0   \n",
       "6           6               128.000000                 62.000000        73.0   \n",
       "7           6               116.000000                 74.000000        82.0   \n",
       "8          14               107.000000                 64.000000        60.0   \n",
       "9          14               153.000000                 72.000000        56.0   \n",
       "10         14               168.000000                 66.000000        58.0   \n",
       "11         14               160.000000                 60.000000        58.0   \n",
       "12         14               188.000000                 75.000000        60.0   \n",
       "13         14               172.000000                 65.000000        68.0   \n",
       "14         14               153.000000                 60.000000        62.0   \n",
       "15         14               132.000000                 79.000000        83.0   \n",
       "16         15               157.000000                100.000000        89.0   \n",
       "17         15               168.000000                 93.000000        85.0   \n",
       "18         15               161.000000                100.000000        88.0   \n",
       "19         15               139.000000                 87.000000        91.0   \n",
       "20         15               139.000000                 89.000000        67.0   \n",
       "21         15               131.000000                 89.000000        73.0   \n",
       "22         15               133.000000                 85.000000        67.0   \n",
       "23         15               103.000000                 66.000000        66.0   \n",
       "24         25               105.000000                 56.000000        84.0   \n",
       "25         25               118.000000                 66.000000        60.0   \n",
       "26         25               115.000000                 61.000000        79.0   \n",
       "27         25               121.333333                 61.833333        90.0   \n",
       "28         25               133.000000                 71.000000        91.0   \n",
       "29         25               116.000000                 59.000000        92.0   \n",
       "\n",
       "    respiratory_rate  oxygen_saturation  temperature  \\\n",
       "0               19.0               95.0         36.6   \n",
       "1               22.0               98.0         36.4   \n",
       "2               22.0               99.0         36.5   \n",
       "3               22.0               95.0         36.5   \n",
       "4               24.0               98.0         36.5   \n",
       "5               24.0               93.0         36.6   \n",
       "6               26.0               93.0         36.8   \n",
       "7               20.0               96.0         37.6   \n",
       "8               20.0               94.0         36.9   \n",
       "9               18.0               93.0         36.7   \n",
       "10              18.0               94.0         36.3   \n",
       "11              18.0               93.0         36.4   \n",
       "12              20.0               93.0         36.6   \n",
       "13              18.0               99.0         36.5   \n",
       "14              18.0               95.0         36.8   \n",
       "15              20.0               97.0         37.0   \n",
       "16              18.0               95.0         36.8   \n",
       "17              20.0               93.0         37.6   \n",
       "18              18.0               93.0         37.1   \n",
       "19              17.0               84.0         36.9   \n",
       "20              18.0               92.0         36.8   \n",
       "21              19.0               93.0         36.5   \n",
       "22              19.0               93.0         36.9   \n",
       "23              20.0               95.0         37.3   \n",
       "24              22.0               88.0         37.7   \n",
       "25              30.0               97.0         36.7   \n",
       "26              34.0               95.0         36.3   \n",
       "27              30.0               96.0         37.3   \n",
       "28              30.0               98.0         37.0   \n",
       "29              30.0               96.0         37.2   \n",
       "\n",
       "    highest_mean_arterial_pressure  lowest_mean_arterial_pressure  \\\n",
       "0                        92.000000                      80.000000   \n",
       "1                        71.000000                      67.000000   \n",
       "2                        84.000000                      84.000000   \n",
       "3                        77.000000                      77.000000   \n",
       "4                        92.000000                      92.000000   \n",
       "5                        93.000000                      93.000000   \n",
       "6                        93.000000                      81.000000   \n",
       "7                        86.000000                      66.000000   \n",
       "8                         0.000000                       0.000000   \n",
       "9                         0.000000                       0.000000   \n",
       "10                        0.000000                       0.000000   \n",
       "11                        0.000000                       0.000000   \n",
       "12                        0.000000                       0.000000   \n",
       "13                        0.000000                       0.000000   \n",
       "14                        0.000000                       0.000000   \n",
       "15                        0.000000                       0.000000   \n",
       "16                        0.000000                       0.000000   \n",
       "17                        0.000000                       0.000000   \n",
       "18                        0.000000                       0.000000   \n",
       "19                        0.000000                       0.000000   \n",
       "20                        0.000000                       0.000000   \n",
       "21                        0.000000                       0.000000   \n",
       "22                        0.000000                       0.000000   \n",
       "23                        0.000000                       0.000000   \n",
       "24                       78.000000                      71.000000   \n",
       "25                      107.000000                      72.000000   \n",
       "26                      122.000000                      71.000000   \n",
       "27                       93.166667                      72.333333   \n",
       "28                       91.000000                      75.000000   \n",
       "29                       82.000000                      76.000000   \n",
       "\n",
       "    highest_heart_rate  ...  Bilateral consolidationinfiltration  \\\n",
       "0                 77.0  ...                                    0   \n",
       "1                 77.0  ...                                    0   \n",
       "2                 97.0  ...                                    0   \n",
       "3                107.0  ...                                    0   \n",
       "4                 77.0  ...                                    1   \n",
       "5                 82.0  ...                                    0   \n",
       "6                 78.0  ...                                    0   \n",
       "7                 94.0  ...                                    0   \n",
       "8                 64.0  ...                                    0   \n",
       "9                 83.0  ...                                    0   \n",
       "10                58.0  ...                                    0   \n",
       "11                60.0  ...                                    0   \n",
       "12                81.0  ...                                    0   \n",
       "13                70.0  ...                                    0   \n",
       "14                73.0  ...                                    0   \n",
       "15                83.0  ...                                    0   \n",
       "16                89.0  ...                                    0   \n",
       "17                94.0  ...                                    0   \n",
       "18                92.0  ...                                    0   \n",
       "19                91.0  ...                                    0   \n",
       "20                75.0  ...                                    0   \n",
       "21                73.0  ...                                    0   \n",
       "22                73.0  ...                                    0   \n",
       "23                78.0  ...                                    0   \n",
       "24                92.0  ...                                    0   \n",
       "25               114.0  ...                                    0   \n",
       "26                90.0  ...                                    0   \n",
       "27                98.0  ...                                    0   \n",
       "28               101.0  ...                                    0   \n",
       "29                99.0  ...                                    0   \n",
       "\n",
       "    Emphysematous or Bronchiectasis changes  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "5                                         0   \n",
       "6                                         0   \n",
       "7                                         0   \n",
       "8                                         0   \n",
       "9                                         0   \n",
       "10                                        0   \n",
       "11                                        0   \n",
       "12                                        0   \n",
       "13                                        0   \n",
       "14                                        0   \n",
       "15                                        0   \n",
       "16                                        0   \n",
       "17                                        0   \n",
       "18                                        0   \n",
       "19                                        0   \n",
       "20                                        0   \n",
       "21                                        0   \n",
       "22                                        0   \n",
       "23                                        0   \n",
       "24                                        0   \n",
       "25                                        0   \n",
       "26                                        0   \n",
       "27                                        0   \n",
       "28                                        0   \n",
       "29                                        0   \n",
       "\n",
       "    Emphysematous or Bronchiectatic changes  Pulmonary Embolism  \\\n",
       "0                                         0                   0   \n",
       "1                                         0                   0   \n",
       "2                                         0                   0   \n",
       "3                                         0                   0   \n",
       "4                                         0                   0   \n",
       "5                                         0                   0   \n",
       "6                                         0                   0   \n",
       "7                                         0                   0   \n",
       "8                                         0                   0   \n",
       "9                                         0                   0   \n",
       "10                                        0                   0   \n",
       "11                                        0                   0   \n",
       "12                                        0                   0   \n",
       "13                                        0                   0   \n",
       "14                                        0                   0   \n",
       "15                                        0                   0   \n",
       "16                                        0                   0   \n",
       "17                                        1                   0   \n",
       "18                                        0                   0   \n",
       "19                                        0                   0   \n",
       "20                                        0                   0   \n",
       "21                                        0                   0   \n",
       "22                                        0                   0   \n",
       "23                                        0                   0   \n",
       "24                                        0                   0   \n",
       "25                                        0                   0   \n",
       "26                                        0                   0   \n",
       "27                                        0                   0   \n",
       "28                                        0                   0   \n",
       "29                                        0                   0   \n",
       "\n",
       "    Scarring or Fibrosis  Unilateral Ground Glass Opacities  \\\n",
       "0                      0                                  0   \n",
       "1                      0                                  0   \n",
       "2                      0                                  0   \n",
       "3                      0                                  0   \n",
       "4                      0                                  0   \n",
       "5                      0                                  0   \n",
       "6                      0                                  0   \n",
       "7                      0                                  0   \n",
       "8                      0                                  0   \n",
       "9                      0                                  0   \n",
       "10                     0                                  0   \n",
       "11                     0                                  0   \n",
       "12                     0                                  0   \n",
       "13                     0                                  0   \n",
       "14                     0                                  0   \n",
       "15                     0                                  0   \n",
       "16                     0                                  0   \n",
       "17                     1                                  0   \n",
       "18                     0                                  0   \n",
       "19                     0                                  0   \n",
       "20                     0                                  0   \n",
       "21                     0                                  0   \n",
       "22                     0                                  0   \n",
       "23                     0                                  0   \n",
       "24                     0                                  0   \n",
       "25                     0                                  0   \n",
       "26                     0                                  0   \n",
       "27                     0                                  0   \n",
       "28                     0                                  0   \n",
       "29                     0                                  0   \n",
       "\n",
       "    Unilateral consolidationinfiltration  Subarachnoid Hemorrhage  \\\n",
       "0                                      0                        0   \n",
       "1                                      0                        0   \n",
       "2                                      0                        0   \n",
       "3                                      0                        0   \n",
       "4                                      0                        0   \n",
       "5                                      0                        0   \n",
       "6                                      0                        0   \n",
       "7                                      0                        0   \n",
       "8                                      0                        0   \n",
       "9                                      0                        0   \n",
       "10                                     0                        0   \n",
       "11                                     0                        0   \n",
       "12                                     0                        0   \n",
       "13                                     0                        0   \n",
       "14                                     0                        0   \n",
       "15                                     0                        0   \n",
       "16                                     0                        0   \n",
       "17                                     0                        0   \n",
       "18                                     0                        0   \n",
       "19                                     0                        0   \n",
       "20                                     0                        0   \n",
       "21                                     0                        0   \n",
       "22                                     0                        0   \n",
       "23                                     0                        0   \n",
       "24                                     0                        0   \n",
       "25                                     0                        0   \n",
       "26                                     0                        0   \n",
       "27                                     0                        0   \n",
       "28                                     0                        0   \n",
       "29                                     0                        0   \n",
       "\n",
       "    Subdural Hemorrhage  days_remaining  \n",
       "0                     0              31  \n",
       "1                     0              30  \n",
       "2                     0              29  \n",
       "3                     0              28  \n",
       "4                     0              27  \n",
       "5                     0              26  \n",
       "6                     0              25  \n",
       "7                     0              18  \n",
       "8                     0              32  \n",
       "9                     0              31  \n",
       "10                    0              30  \n",
       "11                    0              29  \n",
       "12                    0              28  \n",
       "13                    0              27  \n",
       "14                    0              26  \n",
       "15                    0              19  \n",
       "16                    0              33  \n",
       "17                    0              32  \n",
       "18                    0              31  \n",
       "19                    0              30  \n",
       "20                    0              29  \n",
       "21                    0              28  \n",
       "22                    0              27  \n",
       "23                    0              20  \n",
       "24                    0              71  \n",
       "25                    0              70  \n",
       "26                    0              69  \n",
       "27                    0              68  \n",
       "28                    0              67  \n",
       "29                    0              66  \n",
       "\n",
       "[30 rows x 82 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['days_remaining'] = df['hospital_length_of_stay']-df['day']\n",
    "df = df.drop(['hospital_length_of_stay','day'],axis=1)\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f0df23d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>highest_mean_arterial_pressure</th>\n",
       "      <th>lowest_mean_arterial_pressure</th>\n",
       "      <th>highest_heart_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Bilateral consolidationinfiltration</th>\n",
       "      <th>Emphysematous or Bronchiectasis changes</th>\n",
       "      <th>Emphysematous or Bronchiectatic changes</th>\n",
       "      <th>Pulmonary Embolism</th>\n",
       "      <th>Scarring or Fibrosis</th>\n",
       "      <th>Unilateral Ground Glass Opacities</th>\n",
       "      <th>Unilateral consolidationinfiltration</th>\n",
       "      <th>Subarachnoid Hemorrhage</th>\n",
       "      <th>Subdural Hemorrhage</th>\n",
       "      <th>days_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.471299</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>-0.426718</td>\n",
       "      <td>0.274956</td>\n",
       "      <td>-0.415526</td>\n",
       "      <td>0.775369</td>\n",
       "      <td>1.002965</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>-1.416503</td>\n",
       "      <td>-0.741557</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>1.190958</td>\n",
       "      <td>-0.847667</td>\n",
       "      <td>0.355284</td>\n",
       "      <td>0.668960</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.615945</td>\n",
       "      <td>-0.135129</td>\n",
       "      <td>-0.352470</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>1.496292</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.615336</td>\n",
       "      <td>1.105736</td>\n",
       "      <td>0.443514</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.896574</td>\n",
       "      <td>-0.514146</td>\n",
       "      <td>1.254138</td>\n",
       "      <td>0.216288</td>\n",
       "      <td>0.274956</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.475308</td>\n",
       "      <td>0.925887</td>\n",
       "      <td>1.055964</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.045782</td>\n",
       "      <td>0.547103</td>\n",
       "      <td>-0.599641</td>\n",
       "      <td>0.644958</td>\n",
       "      <td>1.190958</td>\n",
       "      <td>-0.631597</td>\n",
       "      <td>0.775369</td>\n",
       "      <td>1.311277</td>\n",
       "      <td>-0.781388</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parent_id  systolic_blood_pressure  diastolic_blood_pressure  heart_rate  \\\n",
       "0          6                 0.001484                  0.471299   -0.599641   \n",
       "1          6                -1.416503                 -0.741557   -0.599641   \n",
       "2          6                 0.615945                 -0.135129   -0.352470   \n",
       "3          6                -0.896574                 -0.514146    1.254138   \n",
       "4          6                -0.045782                  0.547103   -0.599641   \n",
       "\n",
       "   respiratory_rate  oxygen_saturation  temperature  \\\n",
       "0         -0.426718           0.274956    -0.415526   \n",
       "1          0.216288           1.190958    -0.847667   \n",
       "2          0.216288           1.496292    -0.631597   \n",
       "3          0.216288           0.274956    -0.631597   \n",
       "4          0.644958           1.190958    -0.631597   \n",
       "\n",
       "   highest_mean_arterial_pressure  lowest_mean_arterial_pressure  \\\n",
       "0                        0.775369                       1.002965   \n",
       "1                        0.355284                       0.668960   \n",
       "2                        0.615336                       1.105736   \n",
       "3                        0.475308                       0.925887   \n",
       "4                        0.775369                       1.311277   \n",
       "\n",
       "   highest_heart_rate  ...  Bilateral consolidationinfiltration  \\\n",
       "0           -0.781388  ...                                    0   \n",
       "1           -0.781388  ...                                    0   \n",
       "2            0.443514  ...                                    0   \n",
       "3            1.055964  ...                                    0   \n",
       "4           -0.781388  ...                                    1   \n",
       "\n",
       "   Emphysematous or Bronchiectasis changes  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "\n",
       "   Emphysematous or Bronchiectatic changes  Pulmonary Embolism  \\\n",
       "0                                        0                   0   \n",
       "1                                        0                   0   \n",
       "2                                        0                   0   \n",
       "3                                        0                   0   \n",
       "4                                        0                   0   \n",
       "\n",
       "   Scarring or Fibrosis  Unilateral Ground Glass Opacities  \\\n",
       "0                     0                                  0   \n",
       "1                     0                                  0   \n",
       "2                     0                                  0   \n",
       "3                     0                                  0   \n",
       "4                     0                                  0   \n",
       "\n",
       "   Unilateral consolidationinfiltration  Subarachnoid Hemorrhage  \\\n",
       "0                                     0                        0   \n",
       "1                                     0                        0   \n",
       "2                                     0                        0   \n",
       "3                                     0                        0   \n",
       "4                                     0                        0   \n",
       "\n",
       "   Subdural Hemorrhage  days_remaining  \n",
       "0                    0              31  \n",
       "1                    0              30  \n",
       "2                    0              29  \n",
       "3                    0              28  \n",
       "4                    0              27  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_binary = [\n",
    "    'intubated', 'cardiac_arrest', 'arrested_time', 'major_cardiac_events', \n",
    "    'clinically_diagnosed_infections', 'mechanical_ventilation', 'antiarrhythmic_therapies', \n",
    "    'renal_replacement_therapy_dialysis', 'cardiovascular_mechanical_support', 'echocardiogram', \n",
    "    'chest_x_ray', 'chest_ct', 'head_ct', 'antimicrobial', 'anticoagulation', 'steroid',\n",
    "    'Bilateral Consolidation', 'Bilateral Ground Glass', 'Cardiomegaly', 'Edema', 'Effusion', \n",
    "    'Pneumothorax', 'Unilateral Consolidation', 'Unilateral Ground Glass', 'Bilateral Ground Glass Opacities',\n",
    "    'Bilateral consolidationinfiltration', 'Subarachnoid Hemorrhage', 'Subdural Hemorrhage',\n",
    "    'Emphysematous or Bronchiectasis changes', 'Emphysematous or Bronchiectatic changes', \n",
    "    'Pulmonary Embolism', 'Scarring or Fibrosis', 'Unilateral Ground Glass Opacities', \n",
    "    'Unilateral consolidationinfiltration'\n",
    "]\n",
    "\n",
    "# Define your columns to exclude from scaling\n",
    "columns_to_exclude = ['parent_id', 'days_remaining'] + columns_binary\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the columns that need scaling\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Check the scaled data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec91f59",
   "metadata": {},
   "source": [
    "### Define a simple RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "664a1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the RNN model as before\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # Take the output of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5fced31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for parent_id 6...\n",
      "Epoch [1/100] Loss: 760.8948974609375\n",
      "Epoch [2/100] Loss: 744.7484741210938\n",
      "Epoch [3/100] Loss: 729.0761108398438\n",
      "Epoch [4/100] Loss: 714.0304565429688\n",
      "Epoch [5/100] Loss: 699.6372680664062\n",
      "Epoch [6/100] Loss: 685.8488159179688\n",
      "Epoch [7/100] Loss: 672.6060180664062\n",
      "Epoch [8/100] Loss: 659.8720092773438\n",
      "Epoch [9/100] Loss: 647.6497192382812\n",
      "Epoch [10/100] Loss: 635.9739379882812\n",
      "Epoch [11/100] Loss: 624.8880615234375\n",
      "Epoch [12/100] Loss: 614.4259643554688\n",
      "Epoch [13/100] Loss: 604.6004638671875\n",
      "Epoch [14/100] Loss: 595.4034423828125\n",
      "Epoch [15/100] Loss: 586.8095092773438\n",
      "Epoch [16/100] Loss: 578.78369140625\n",
      "Epoch [17/100] Loss: 571.2865600585938\n",
      "Epoch [18/100] Loss: 564.2778930664062\n",
      "Epoch [19/100] Loss: 557.718017578125\n",
      "Epoch [20/100] Loss: 551.568115234375\n",
      "Epoch [21/100] Loss: 545.790283203125\n",
      "Epoch [22/100] Loss: 540.3469848632812\n",
      "Epoch [23/100] Loss: 535.201416015625\n",
      "Epoch [24/100] Loss: 530.3180541992188\n",
      "Epoch [25/100] Loss: 525.6640014648438\n",
      "Epoch [26/100] Loss: 521.2088012695312\n",
      "Epoch [27/100] Loss: 516.9253540039062\n",
      "Epoch [28/100] Loss: 512.7905883789062\n",
      "Epoch [29/100] Loss: 508.784423828125\n",
      "Epoch [30/100] Loss: 504.889892578125\n",
      "Epoch [31/100] Loss: 501.0926818847656\n",
      "Epoch [32/100] Loss: 497.3810729980469\n",
      "Epoch [33/100] Loss: 493.7448425292969\n",
      "Epoch [34/100] Loss: 490.17572021484375\n",
      "Epoch [35/100] Loss: 486.66668701171875\n",
      "Epoch [36/100] Loss: 483.211669921875\n",
      "Epoch [37/100] Loss: 479.8058166503906\n",
      "Epoch [38/100] Loss: 476.4448547363281\n",
      "Epoch [39/100] Loss: 473.125244140625\n",
      "Epoch [40/100] Loss: 469.8440246582031\n",
      "Epoch [41/100] Loss: 466.5986633300781\n",
      "Epoch [42/100] Loss: 463.3871154785156\n",
      "Epoch [43/100] Loss: 460.2073059082031\n",
      "Epoch [44/100] Loss: 457.0579833984375\n",
      "Epoch [45/100] Loss: 453.9376220703125\n",
      "Epoch [46/100] Loss: 450.84521484375\n",
      "Epoch [47/100] Loss: 447.7796630859375\n",
      "Epoch [48/100] Loss: 444.7400817871094\n",
      "Epoch [49/100] Loss: 441.7257385253906\n",
      "Epoch [50/100] Loss: 438.7359619140625\n",
      "Epoch [51/100] Loss: 435.77032470703125\n",
      "Epoch [52/100] Loss: 432.82794189453125\n",
      "Epoch [53/100] Loss: 429.90863037109375\n",
      "Epoch [54/100] Loss: 427.01190185546875\n",
      "Epoch [55/100] Loss: 424.1371765136719\n",
      "Epoch [56/100] Loss: 421.2843017578125\n",
      "Epoch [57/100] Loss: 418.4529724121094\n",
      "Epoch [58/100] Loss: 415.6428527832031\n",
      "Epoch [59/100] Loss: 412.8536682128906\n",
      "Epoch [60/100] Loss: 410.0852355957031\n",
      "Epoch [61/100] Loss: 407.33758544921875\n",
      "Epoch [62/100] Loss: 404.6103210449219\n",
      "Epoch [63/100] Loss: 401.90362548828125\n",
      "Epoch [64/100] Loss: 399.2171936035156\n",
      "Epoch [65/100] Loss: 396.55133056640625\n",
      "Epoch [66/100] Loss: 393.9058837890625\n",
      "Epoch [67/100] Loss: 391.2809753417969\n",
      "Epoch [68/100] Loss: 388.6765441894531\n",
      "Epoch [69/100] Loss: 386.0926208496094\n",
      "Epoch [70/100] Loss: 383.52899169921875\n",
      "Epoch [71/100] Loss: 380.98565673828125\n",
      "Epoch [72/100] Loss: 378.4620666503906\n",
      "Epoch [73/100] Loss: 375.9578552246094\n",
      "Epoch [74/100] Loss: 373.4724426269531\n",
      "Epoch [75/100] Loss: 371.00518798828125\n",
      "Epoch [76/100] Loss: 368.5553894042969\n",
      "Epoch [77/100] Loss: 366.122314453125\n",
      "Epoch [78/100] Loss: 363.7053527832031\n",
      "Epoch [79/100] Loss: 361.3038024902344\n",
      "Epoch [80/100] Loss: 358.9173889160156\n",
      "Epoch [81/100] Loss: 356.54541015625\n",
      "Epoch [82/100] Loss: 354.1876220703125\n",
      "Epoch [83/100] Loss: 351.84393310546875\n",
      "Epoch [84/100] Loss: 349.5138854980469\n",
      "Epoch [85/100] Loss: 347.197509765625\n",
      "Epoch [86/100] Loss: 344.89471435546875\n",
      "Epoch [87/100] Loss: 342.60528564453125\n",
      "Epoch [88/100] Loss: 340.3292541503906\n",
      "Epoch [89/100] Loss: 338.0665283203125\n",
      "Epoch [90/100] Loss: 335.81707763671875\n",
      "Epoch [91/100] Loss: 333.58074951171875\n",
      "Epoch [92/100] Loss: 331.3574523925781\n",
      "Epoch [93/100] Loss: 329.14703369140625\n",
      "Epoch [94/100] Loss: 326.9494934082031\n",
      "Epoch [95/100] Loss: 324.7646484375\n",
      "Epoch [96/100] Loss: 322.59234619140625\n",
      "Epoch [97/100] Loss: 320.43267822265625\n",
      "Epoch [98/100] Loss: 318.28521728515625\n",
      "Epoch [99/100] Loss: 316.1499938964844\n",
      "Epoch [100/100] Loss: 314.02716064453125\n",
      "Predicted days_remaining for parent_id 6: 9.502427101135254\n",
      "Training for parent_id 14...\n",
      "Epoch [1/100] Loss: 783.8323974609375\n",
      "Epoch [2/100] Loss: 769.8873901367188\n",
      "Epoch [3/100] Loss: 756.002685546875\n",
      "Epoch [4/100] Loss: 742.3824462890625\n",
      "Epoch [5/100] Loss: 729.1731567382812\n",
      "Epoch [6/100] Loss: 716.4423828125\n",
      "Epoch [7/100] Loss: 704.2071533203125\n",
      "Epoch [8/100] Loss: 692.4680786132812\n",
      "Epoch [9/100] Loss: 681.218994140625\n",
      "Epoch [10/100] Loss: 670.4466552734375\n",
      "Epoch [11/100] Loss: 660.1327514648438\n",
      "Epoch [12/100] Loss: 650.2557983398438\n",
      "Epoch [13/100] Loss: 640.7945556640625\n",
      "Epoch [14/100] Loss: 631.7314453125\n",
      "Epoch [15/100] Loss: 623.053466796875\n",
      "Epoch [16/100] Loss: 614.7517700195312\n",
      "Epoch [17/100] Loss: 606.81982421875\n",
      "Epoch [18/100] Loss: 599.2506103515625\n",
      "Epoch [19/100] Loss: 592.0343627929688\n",
      "Epoch [20/100] Loss: 585.1582641601562\n",
      "Epoch [21/100] Loss: 578.6058349609375\n",
      "Epoch [22/100] Loss: 572.3580322265625\n",
      "Epoch [23/100] Loss: 566.3944091796875\n",
      "Epoch [24/100] Loss: 560.69287109375\n",
      "Epoch [25/100] Loss: 555.231689453125\n",
      "Epoch [26/100] Loss: 549.9895629882812\n",
      "Epoch [27/100] Loss: 544.9464111328125\n",
      "Epoch [28/100] Loss: 540.0833740234375\n",
      "Epoch [29/100] Loss: 535.3838500976562\n",
      "Epoch [30/100] Loss: 530.8323974609375\n",
      "Epoch [31/100] Loss: 526.4151611328125\n",
      "Epoch [32/100] Loss: 522.1199951171875\n",
      "Epoch [33/100] Loss: 517.93603515625\n",
      "Epoch [34/100] Loss: 513.8530883789062\n",
      "Epoch [35/100] Loss: 509.86248779296875\n",
      "Epoch [36/100] Loss: 505.955810546875\n",
      "Epoch [37/100] Loss: 502.125732421875\n",
      "Epoch [38/100] Loss: 498.3657531738281\n",
      "Epoch [39/100] Loss: 494.66986083984375\n",
      "Epoch [40/100] Loss: 491.032958984375\n",
      "Epoch [41/100] Loss: 487.4502868652344\n",
      "Epoch [42/100] Loss: 483.9181213378906\n",
      "Epoch [43/100] Loss: 480.43304443359375\n",
      "Epoch [44/100] Loss: 476.9920349121094\n",
      "Epoch [45/100] Loss: 473.59271240234375\n",
      "Epoch [46/100] Loss: 470.2330017089844\n",
      "Epoch [47/100] Loss: 466.9109802246094\n",
      "Epoch [48/100] Loss: 463.6250915527344\n",
      "Epoch [49/100] Loss: 460.3739013671875\n",
      "Epoch [50/100] Loss: 457.15606689453125\n",
      "Epoch [51/100] Loss: 453.9705810546875\n",
      "Epoch [52/100] Loss: 450.8161926269531\n",
      "Epoch [53/100] Loss: 447.692138671875\n",
      "Epoch [54/100] Loss: 444.5974426269531\n",
      "Epoch [55/100] Loss: 441.5311279296875\n",
      "Epoch [56/100] Loss: 438.4925842285156\n",
      "Epoch [57/100] Loss: 435.48101806640625\n",
      "Epoch [58/100] Loss: 432.49566650390625\n",
      "Epoch [59/100] Loss: 429.5358581542969\n",
      "Epoch [60/100] Loss: 426.6010437011719\n",
      "Epoch [61/100] Loss: 423.6907043457031\n",
      "Epoch [62/100] Loss: 420.80413818359375\n",
      "Epoch [63/100] Loss: 417.94091796875\n",
      "Epoch [64/100] Loss: 415.1004638671875\n",
      "Epoch [65/100] Loss: 412.28240966796875\n",
      "Epoch [66/100] Loss: 409.48626708984375\n",
      "Epoch [67/100] Loss: 406.71160888671875\n",
      "Epoch [68/100] Loss: 403.95819091796875\n",
      "Epoch [69/100] Loss: 401.22540283203125\n",
      "Epoch [70/100] Loss: 398.512939453125\n",
      "Epoch [71/100] Loss: 395.82049560546875\n",
      "Epoch [72/100] Loss: 393.1479187011719\n",
      "Epoch [73/100] Loss: 390.49456787109375\n",
      "Epoch [74/100] Loss: 387.86041259765625\n",
      "Epoch [75/100] Loss: 385.24505615234375\n",
      "Epoch [76/100] Loss: 382.6482238769531\n",
      "Epoch [77/100] Loss: 380.0697021484375\n",
      "Epoch [78/100] Loss: 377.5091552734375\n",
      "Epoch [79/100] Loss: 374.9664306640625\n",
      "Epoch [80/100] Loss: 372.4413146972656\n",
      "Epoch [81/100] Loss: 369.9334716796875\n",
      "Epoch [82/100] Loss: 367.44268798828125\n",
      "Epoch [83/100] Loss: 364.96893310546875\n",
      "Epoch [84/100] Loss: 362.51171875\n",
      "Epoch [85/100] Loss: 360.0711364746094\n",
      "Epoch [86/100] Loss: 357.64678955078125\n",
      "Epoch [87/100] Loss: 355.2386169433594\n",
      "Epoch [88/100] Loss: 352.84637451171875\n",
      "Epoch [89/100] Loss: 350.4699401855469\n",
      "Epoch [90/100] Loss: 348.1091613769531\n",
      "Epoch [91/100] Loss: 345.7637634277344\n",
      "Epoch [92/100] Loss: 343.43365478515625\n",
      "Epoch [93/100] Loss: 341.1187744140625\n",
      "Epoch [94/100] Loss: 338.81884765625\n",
      "Epoch [95/100] Loss: 336.53375244140625\n",
      "Epoch [96/100] Loss: 334.2633361816406\n",
      "Epoch [97/100] Loss: 332.007568359375\n",
      "Epoch [98/100] Loss: 329.7662048339844\n",
      "Epoch [99/100] Loss: 327.5391845703125\n",
      "Epoch [100/100] Loss: 325.3262634277344\n",
      "Predicted days_remaining for parent_id 14: 10.180424690246582\n",
      "Training for parent_id 15...\n",
      "Epoch [1/100] Loss: 844.8304443359375\n",
      "Epoch [2/100] Loss: 830.1392822265625\n",
      "Epoch [3/100] Loss: 816.0345458984375\n",
      "Epoch [4/100] Loss: 802.6666870117188\n",
      "Epoch [5/100] Loss: 790.0477905273438\n",
      "Epoch [6/100] Loss: 778.1218872070312\n",
      "Epoch [7/100] Loss: 766.816162109375\n",
      "Epoch [8/100] Loss: 756.059326171875\n",
      "Epoch [9/100] Loss: 745.7892456054688\n",
      "Epoch [10/100] Loss: 735.9540405273438\n",
      "Epoch [11/100] Loss: 726.5120849609375\n",
      "Epoch [12/100] Loss: 717.432373046875\n",
      "Epoch [13/100] Loss: 708.6919555664062\n",
      "Epoch [14/100] Loss: 700.274169921875\n",
      "Epoch [15/100] Loss: 692.1646728515625\n",
      "Epoch [16/100] Loss: 684.3499755859375\n",
      "Epoch [17/100] Loss: 676.8163452148438\n",
      "Epoch [18/100] Loss: 669.548828125\n",
      "Epoch [19/100] Loss: 662.53271484375\n",
      "Epoch [20/100] Loss: 655.7537841796875\n",
      "Epoch [21/100] Loss: 649.1988525390625\n",
      "Epoch [22/100] Loss: 642.8576049804688\n",
      "Epoch [23/100] Loss: 636.72119140625\n",
      "Epoch [24/100] Loss: 630.7823486328125\n",
      "Epoch [25/100] Loss: 625.034423828125\n",
      "Epoch [26/100] Loss: 619.4696655273438\n",
      "Epoch [27/100] Loss: 614.0791625976562\n",
      "Epoch [28/100] Loss: 608.8526000976562\n",
      "Epoch [29/100] Loss: 603.7783813476562\n",
      "Epoch [30/100] Loss: 598.8441162109375\n",
      "Epoch [31/100] Loss: 594.03759765625\n",
      "Epoch [32/100] Loss: 589.347412109375\n",
      "Epoch [33/100] Loss: 584.7627563476562\n",
      "Epoch [34/100] Loss: 580.2740478515625\n",
      "Epoch [35/100] Loss: 575.8731689453125\n",
      "Epoch [36/100] Loss: 571.55322265625\n",
      "Epoch [37/100] Loss: 567.308349609375\n",
      "Epoch [38/100] Loss: 563.1337890625\n",
      "Epoch [39/100] Loss: 559.0252075195312\n",
      "Epoch [40/100] Loss: 554.979248046875\n",
      "Epoch [41/100] Loss: 550.9928588867188\n",
      "Epoch [42/100] Loss: 547.0633544921875\n",
      "Epoch [43/100] Loss: 543.188232421875\n",
      "Epoch [44/100] Loss: 539.3651733398438\n",
      "Epoch [45/100] Loss: 535.5922241210938\n",
      "Epoch [46/100] Loss: 531.8670043945312\n",
      "Epoch [47/100] Loss: 528.187744140625\n",
      "Epoch [48/100] Loss: 524.5524291992188\n",
      "Epoch [49/100] Loss: 520.9591674804688\n",
      "Epoch [50/100] Loss: 517.4063720703125\n",
      "Epoch [51/100] Loss: 513.8923950195312\n",
      "Epoch [52/100] Loss: 510.4155578613281\n",
      "Epoch [53/100] Loss: 506.9745788574219\n",
      "Epoch [54/100] Loss: 503.5680847167969\n",
      "Epoch [55/100] Loss: 500.1949157714844\n",
      "Epoch [56/100] Loss: 496.85382080078125\n",
      "Epoch [57/100] Loss: 493.5437927246094\n",
      "Epoch [58/100] Loss: 490.2638854980469\n",
      "Epoch [59/100] Loss: 487.0132751464844\n",
      "Epoch [60/100] Loss: 483.7911071777344\n",
      "Epoch [61/100] Loss: 480.59649658203125\n",
      "Epoch [62/100] Loss: 477.4288330078125\n",
      "Epoch [63/100] Loss: 474.2874755859375\n",
      "Epoch [64/100] Loss: 471.1717529296875\n",
      "Epoch [65/100] Loss: 468.0811462402344\n",
      "Epoch [66/100] Loss: 465.01513671875\n",
      "Epoch [67/100] Loss: 461.97308349609375\n",
      "Epoch [68/100] Loss: 458.9546813964844\n",
      "Epoch [69/100] Loss: 455.9593200683594\n",
      "Epoch [70/100] Loss: 452.9866027832031\n",
      "Epoch [71/100] Loss: 450.0362548828125\n",
      "Epoch [72/100] Loss: 447.1076965332031\n",
      "Epoch [73/100] Loss: 444.2006530761719\n",
      "Epoch [74/100] Loss: 441.3148193359375\n",
      "Epoch [75/100] Loss: 438.4497985839844\n",
      "Epoch [76/100] Loss: 435.6051940917969\n",
      "Epoch [77/100] Loss: 432.7807922363281\n",
      "Epoch [78/100] Loss: 429.9763488769531\n",
      "Epoch [79/100] Loss: 427.1913757324219\n",
      "Epoch [80/100] Loss: 424.4258117675781\n",
      "Epoch [81/100] Loss: 421.6791687011719\n",
      "Epoch [82/100] Loss: 418.9513244628906\n",
      "Epoch [83/100] Loss: 416.2422180175781\n",
      "Epoch [84/100] Loss: 413.5511474609375\n",
      "Epoch [85/100] Loss: 410.8782653808594\n",
      "Epoch [86/100] Loss: 408.22314453125\n",
      "Epoch [87/100] Loss: 405.58563232421875\n",
      "Epoch [88/100] Loss: 402.96563720703125\n",
      "Epoch [89/100] Loss: 400.36273193359375\n",
      "Epoch [90/100] Loss: 397.7768249511719\n",
      "Epoch [91/100] Loss: 395.2077941894531\n",
      "Epoch [92/100] Loss: 392.6553039550781\n",
      "Epoch [93/100] Loss: 390.1192626953125\n",
      "Epoch [94/100] Loss: 387.5995178222656\n",
      "Epoch [95/100] Loss: 385.09576416015625\n",
      "Epoch [96/100] Loss: 382.6080627441406\n",
      "Epoch [97/100] Loss: 380.1360168457031\n",
      "Epoch [98/100] Loss: 377.6795654296875\n",
      "Epoch [99/100] Loss: 375.2385559082031\n",
      "Epoch [100/100] Loss: 372.81292724609375\n",
      "Predicted days_remaining for parent_id 15: 9.88296890258789\n",
      "Training for parent_id 25...\n",
      "Epoch [1/100] Loss: 4454.59033203125\n",
      "Epoch [2/100] Loss: 4415.43359375\n",
      "Epoch [3/100] Loss: 4377.2373046875\n",
      "Epoch [4/100] Loss: 4340.2421875\n",
      "Epoch [5/100] Loss: 4304.736328125\n",
      "Epoch [6/100] Loss: 4270.998046875\n",
      "Epoch [7/100] Loss: 4239.17529296875\n",
      "Epoch [8/100] Loss: 4209.2685546875\n",
      "Epoch [9/100] Loss: 4181.189453125\n",
      "Epoch [10/100] Loss: 4154.80712890625\n",
      "Epoch [11/100] Loss: 4129.982421875\n",
      "Epoch [12/100] Loss: 4106.572265625\n",
      "Epoch [13/100] Loss: 4084.442626953125\n",
      "Epoch [14/100] Loss: 4063.472412109375\n",
      "Epoch [15/100] Loss: 4043.5634765625\n",
      "Epoch [16/100] Loss: 4024.6357421875\n",
      "Epoch [17/100] Loss: 4006.619384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] Loss: 3989.45263671875\n",
      "Epoch [19/100] Loss: 3973.06787109375\n",
      "Epoch [20/100] Loss: 3957.398193359375\n",
      "Epoch [21/100] Loss: 3942.378173828125\n",
      "Epoch [22/100] Loss: 3927.952880859375\n",
      "Epoch [23/100] Loss: 3914.074951171875\n",
      "Epoch [24/100] Loss: 3900.70703125\n",
      "Epoch [25/100] Loss: 3887.8154296875\n",
      "Epoch [26/100] Loss: 3875.37060546875\n",
      "Epoch [27/100] Loss: 3863.336181640625\n",
      "Epoch [28/100] Loss: 3851.67578125\n",
      "Epoch [29/100] Loss: 3840.3466796875\n",
      "Epoch [30/100] Loss: 3829.30859375\n",
      "Epoch [31/100] Loss: 3818.522216796875\n",
      "Epoch [32/100] Loss: 3807.95361328125\n",
      "Epoch [33/100] Loss: 3797.57421875\n",
      "Epoch [34/100] Loss: 3787.360107421875\n",
      "Epoch [35/100] Loss: 3777.291015625\n",
      "Epoch [36/100] Loss: 3767.350830078125\n",
      "Epoch [37/100] Loss: 3757.52685546875\n",
      "Epoch [38/100] Loss: 3747.80859375\n",
      "Epoch [39/100] Loss: 3738.186767578125\n",
      "Epoch [40/100] Loss: 3728.655029296875\n",
      "Epoch [41/100] Loss: 3719.20703125\n",
      "Epoch [42/100] Loss: 3709.836669921875\n",
      "Epoch [43/100] Loss: 3700.5400390625\n",
      "Epoch [44/100] Loss: 3691.310546875\n",
      "Epoch [45/100] Loss: 3682.144775390625\n",
      "Epoch [46/100] Loss: 3673.038330078125\n",
      "Epoch [47/100] Loss: 3663.98681640625\n",
      "Epoch [48/100] Loss: 3654.986083984375\n",
      "Epoch [49/100] Loss: 3646.032470703125\n",
      "Epoch [50/100] Loss: 3637.1220703125\n",
      "Epoch [51/100] Loss: 3628.253662109375\n",
      "Epoch [52/100] Loss: 3619.42333984375\n",
      "Epoch [53/100] Loss: 3610.6298828125\n",
      "Epoch [54/100] Loss: 3601.871337890625\n",
      "Epoch [55/100] Loss: 3593.146484375\n",
      "Epoch [56/100] Loss: 3584.454345703125\n",
      "Epoch [57/100] Loss: 3575.79443359375\n",
      "Epoch [58/100] Loss: 3567.165283203125\n",
      "Epoch [59/100] Loss: 3558.56689453125\n",
      "Epoch [60/100] Loss: 3549.9990234375\n",
      "Epoch [61/100] Loss: 3541.46044921875\n",
      "Epoch [62/100] Loss: 3532.950927734375\n",
      "Epoch [63/100] Loss: 3524.47119140625\n",
      "Epoch [64/100] Loss: 3516.01904296875\n",
      "Epoch [65/100] Loss: 3507.596435546875\n",
      "Epoch [66/100] Loss: 3499.201416015625\n",
      "Epoch [67/100] Loss: 3490.833984375\n",
      "Epoch [68/100] Loss: 3482.494140625\n",
      "Epoch [69/100] Loss: 3474.18115234375\n",
      "Epoch [70/100] Loss: 3465.89501953125\n",
      "Epoch [71/100] Loss: 3457.635986328125\n",
      "Epoch [72/100] Loss: 3449.4033203125\n",
      "Epoch [73/100] Loss: 3441.196044921875\n",
      "Epoch [74/100] Loss: 3433.01513671875\n",
      "Epoch [75/100] Loss: 3424.859619140625\n",
      "Epoch [76/100] Loss: 3416.728515625\n",
      "Epoch [77/100] Loss: 3408.623291015625\n",
      "Epoch [78/100] Loss: 3400.541748046875\n",
      "Epoch [79/100] Loss: 3392.48486328125\n",
      "Epoch [80/100] Loss: 3384.452392578125\n",
      "Epoch [81/100] Loss: 3376.44287109375\n",
      "Epoch [82/100] Loss: 3368.457763671875\n",
      "Epoch [83/100] Loss: 3360.494873046875\n",
      "Epoch [84/100] Loss: 3352.555908203125\n",
      "Epoch [85/100] Loss: 3344.6396484375\n",
      "Epoch [86/100] Loss: 3336.74462890625\n",
      "Epoch [87/100] Loss: 3328.87255859375\n",
      "Epoch [88/100] Loss: 3321.0224609375\n",
      "Epoch [89/100] Loss: 3313.194580078125\n",
      "Epoch [90/100] Loss: 3305.38818359375\n",
      "Epoch [91/100] Loss: 3297.602783203125\n",
      "Epoch [92/100] Loss: 3289.8388671875\n",
      "Epoch [93/100] Loss: 3282.095458984375\n",
      "Epoch [94/100] Loss: 3274.37255859375\n",
      "Epoch [95/100] Loss: 3266.6708984375\n",
      "Epoch [96/100] Loss: 3258.988525390625\n",
      "Epoch [97/100] Loss: 3251.3271484375\n",
      "Epoch [98/100] Loss: 3243.685302734375\n",
      "Epoch [99/100] Loss: 3236.0634765625\n",
      "Epoch [100/100] Loss: 3228.46044921875\n",
      "Predicted days_remaining for parent_id 25: 10.124561309814453\n",
      "Training for parent_id 37...\n",
      "Epoch [1/100] Loss: 140.94317626953125\n",
      "Epoch [2/100] Loss: 136.2098388671875\n",
      "Epoch [3/100] Loss: 131.60641479492188\n",
      "Epoch [4/100] Loss: 127.1833724975586\n",
      "Epoch [5/100] Loss: 122.97596740722656\n",
      "Epoch [6/100] Loss: 119.00051879882812\n",
      "Epoch [7/100] Loss: 115.25267028808594\n",
      "Epoch [8/100] Loss: 111.71533966064453\n",
      "Epoch [9/100] Loss: 108.36702728271484\n",
      "Epoch [10/100] Loss: 105.1876449584961\n",
      "Epoch [11/100] Loss: 102.16083526611328\n",
      "Epoch [12/100] Loss: 99.27485656738281\n",
      "Epoch [13/100] Loss: 96.52186584472656\n",
      "Epoch [14/100] Loss: 93.8971939086914\n",
      "Epoch [15/100] Loss: 91.39800262451172\n",
      "Epoch [16/100] Loss: 89.02224731445312\n",
      "Epoch [17/100] Loss: 86.76771545410156\n",
      "Epoch [18/100] Loss: 84.63145446777344\n",
      "Epoch [19/100] Loss: 82.60948944091797\n",
      "Epoch [20/100] Loss: 80.69664001464844\n",
      "Epoch [21/100] Loss: 78.88683319091797\n",
      "Epoch [22/100] Loss: 77.17318725585938\n",
      "Epoch [23/100] Loss: 75.54838562011719\n",
      "Epoch [24/100] Loss: 74.00498962402344\n",
      "Epoch [25/100] Loss: 72.5357666015625\n",
      "Epoch [26/100] Loss: 71.13387298583984\n",
      "Epoch [27/100] Loss: 69.79300689697266\n",
      "Epoch [28/100] Loss: 68.50746154785156\n",
      "Epoch [29/100] Loss: 67.27212524414062\n",
      "Epoch [30/100] Loss: 66.08240509033203\n",
      "Epoch [31/100] Loss: 64.9342041015625\n",
      "Epoch [32/100] Loss: 63.823814392089844\n",
      "Epoch [33/100] Loss: 62.74788284301758\n",
      "Epoch [34/100] Loss: 61.703392028808594\n",
      "Epoch [35/100] Loss: 60.68755340576172\n",
      "Epoch [36/100] Loss: 59.69784927368164\n",
      "Epoch [37/100] Loss: 58.73194885253906\n",
      "Epoch [38/100] Loss: 57.78778839111328\n",
      "Epoch [39/100] Loss: 56.863487243652344\n",
      "Epoch [40/100] Loss: 55.95738220214844\n",
      "Epoch [41/100] Loss: 55.067989349365234\n",
      "Epoch [42/100] Loss: 54.19403839111328\n",
      "Epoch [43/100] Loss: 53.334503173828125\n",
      "Epoch [44/100] Loss: 52.488502502441406\n",
      "Epoch [45/100] Loss: 51.65536880493164\n",
      "Epoch [46/100] Loss: 50.834598541259766\n",
      "Epoch [47/100] Loss: 50.025917053222656\n",
      "Epoch [48/100] Loss: 49.22913360595703\n",
      "Epoch [49/100] Loss: 48.444190979003906\n",
      "Epoch [50/100] Loss: 47.6711311340332\n",
      "Epoch [51/100] Loss: 46.91007995605469\n",
      "Epoch [52/100] Loss: 46.16117477416992\n",
      "Epoch [53/100] Loss: 45.424591064453125\n",
      "Epoch [54/100] Loss: 44.700531005859375\n",
      "Epoch [55/100] Loss: 43.98914337158203\n",
      "Epoch [56/100] Loss: 43.29060745239258\n",
      "Epoch [57/100] Loss: 42.605037689208984\n",
      "Epoch [58/100] Loss: 41.932533264160156\n",
      "Epoch [59/100] Loss: 41.273128509521484\n",
      "Epoch [60/100] Loss: 40.6268310546875\n",
      "Epoch [61/100] Loss: 39.99357223510742\n",
      "Epoch [62/100] Loss: 39.373252868652344\n",
      "Epoch [63/100] Loss: 38.76572799682617\n",
      "Epoch [64/100] Loss: 38.17082977294922\n",
      "Epoch [65/100] Loss: 37.58833312988281\n",
      "Epoch [66/100] Loss: 37.01805114746094\n",
      "Epoch [67/100] Loss: 36.459716796875\n",
      "Epoch [68/100] Loss: 35.913116455078125\n",
      "Epoch [69/100] Loss: 35.37801742553711\n",
      "Epoch [70/100] Loss: 34.85417556762695\n",
      "Epoch [71/100] Loss: 34.34135818481445\n",
      "Epoch [72/100] Loss: 33.839378356933594\n",
      "Epoch [73/100] Loss: 33.34800338745117\n",
      "Epoch [74/100] Loss: 32.86705017089844\n",
      "Epoch [75/100] Loss: 32.396324157714844\n",
      "Epoch [76/100] Loss: 31.93565559387207\n",
      "Epoch [77/100] Loss: 31.48486328125\n",
      "Epoch [78/100] Loss: 31.04378890991211\n",
      "Epoch [79/100] Loss: 30.61228370666504\n",
      "Epoch [80/100] Loss: 30.190187454223633\n",
      "Epoch [81/100] Loss: 29.777368545532227\n",
      "Epoch [82/100] Loss: 29.373640060424805\n",
      "Epoch [83/100] Loss: 28.978893280029297\n",
      "Epoch [84/100] Loss: 28.59296989440918\n",
      "Epoch [85/100] Loss: 28.215726852416992\n",
      "Epoch [86/100] Loss: 27.847023010253906\n",
      "Epoch [87/100] Loss: 27.4867000579834\n",
      "Epoch [88/100] Loss: 27.134634017944336\n",
      "Epoch [89/100] Loss: 26.79066276550293\n",
      "Epoch [90/100] Loss: 26.454639434814453\n",
      "Epoch [91/100] Loss: 26.126445770263672\n",
      "Epoch [92/100] Loss: 25.80590057373047\n",
      "Epoch [93/100] Loss: 25.492897033691406\n",
      "Epoch [94/100] Loss: 25.1872615814209\n",
      "Epoch [95/100] Loss: 24.88886833190918\n",
      "Epoch [96/100] Loss: 24.59758949279785\n",
      "Epoch [97/100] Loss: 24.313274383544922\n",
      "Epoch [98/100] Loss: 24.03579330444336\n",
      "Epoch [99/100] Loss: 23.7650146484375\n",
      "Epoch [100/100] Loss: 23.50080108642578\n",
      "Predicted days_remaining for parent_id 37: 8.782590866088867\n",
      "Training for parent_id 40...\n",
      "Epoch [1/100] Loss: 241.81886291503906\n",
      "Epoch [2/100] Loss: 233.93312072753906\n",
      "Epoch [3/100] Loss: 226.27041625976562\n",
      "Epoch [4/100] Loss: 218.92030334472656\n",
      "Epoch [5/100] Loss: 211.9310760498047\n",
      "Epoch [6/100] Loss: 205.31956481933594\n",
      "Epoch [7/100] Loss: 199.0827178955078\n",
      "Epoch [8/100] Loss: 193.20513916015625\n",
      "Epoch [9/100] Loss: 187.66632080078125\n",
      "Epoch [10/100] Loss: 182.44447326660156\n",
      "Epoch [11/100] Loss: 177.51748657226562\n",
      "Epoch [12/100] Loss: 172.8643035888672\n",
      "Epoch [13/100] Loss: 168.46621704101562\n",
      "Epoch [14/100] Loss: 164.3074951171875\n",
      "Epoch [15/100] Loss: 160.3748016357422\n",
      "Epoch [16/100] Loss: 156.6562042236328\n",
      "Epoch [17/100] Loss: 153.14035034179688\n",
      "Epoch [18/100] Loss: 149.81568908691406\n",
      "Epoch [19/100] Loss: 146.67019653320312\n",
      "Epoch [20/100] Loss: 143.69131469726562\n",
      "Epoch [21/100] Loss: 140.8660888671875\n",
      "Epoch [22/100] Loss: 138.1815643310547\n",
      "Epoch [23/100] Loss: 135.6248779296875\n",
      "Epoch [24/100] Loss: 133.183837890625\n",
      "Epoch [25/100] Loss: 130.84707641601562\n",
      "Epoch [26/100] Loss: 128.60435485839844\n",
      "Epoch [27/100] Loss: 126.44650268554688\n",
      "Epoch [28/100] Loss: 124.3655776977539\n",
      "Epoch [29/100] Loss: 122.3546142578125\n",
      "Epoch [30/100] Loss: 120.40764617919922\n",
      "Epoch [31/100] Loss: 118.51946258544922\n",
      "Epoch [32/100] Loss: 116.6855239868164\n",
      "Epoch [33/100] Loss: 114.90190887451172\n",
      "Epoch [34/100] Loss: 113.1650161743164\n",
      "Epoch [35/100] Loss: 111.47177124023438\n",
      "Epoch [36/100] Loss: 109.8193588256836\n",
      "Epoch [37/100] Loss: 108.2051773071289\n",
      "Epoch [38/100] Loss: 106.62701416015625\n",
      "Epoch [39/100] Loss: 105.08272552490234\n",
      "Epoch [40/100] Loss: 103.57044982910156\n",
      "Epoch [41/100] Loss: 102.08843231201172\n",
      "Epoch [42/100] Loss: 100.63511657714844\n",
      "Epoch [43/100] Loss: 99.2091064453125\n",
      "Epoch [44/100] Loss: 97.80912017822266\n",
      "Epoch [45/100] Loss: 96.43400573730469\n",
      "Epoch [46/100] Loss: 95.08279418945312\n",
      "Epoch [47/100] Loss: 93.75455474853516\n",
      "Epoch [48/100] Loss: 92.4485092163086\n",
      "Epoch [49/100] Loss: 91.1639404296875\n",
      "Epoch [50/100] Loss: 89.90021514892578\n",
      "Epoch [51/100] Loss: 88.65677642822266\n",
      "Epoch [52/100] Loss: 87.43314361572266\n",
      "Epoch [53/100] Loss: 86.22882843017578\n",
      "Epoch [54/100] Loss: 85.04341888427734\n",
      "Epoch [55/100] Loss: 83.87654113769531\n",
      "Epoch [56/100] Loss: 82.72783660888672\n",
      "Epoch [57/100] Loss: 81.596923828125\n",
      "Epoch [58/100] Loss: 80.48348999023438\n",
      "Epoch [59/100] Loss: 79.38722229003906\n",
      "Epoch [60/100] Loss: 78.30780029296875\n",
      "Epoch [61/100] Loss: 77.24494171142578\n",
      "Epoch [62/100] Loss: 76.1983413696289\n",
      "Epoch [63/100] Loss: 75.1677017211914\n",
      "Epoch [64/100] Loss: 74.15276336669922\n",
      "Epoch [65/100] Loss: 73.15326690673828\n",
      "Epoch [66/100] Loss: 72.1689224243164\n",
      "Epoch [67/100] Loss: 71.19950103759766\n",
      "Epoch [68/100] Loss: 70.24471282958984\n",
      "Epoch [69/100] Loss: 69.30433654785156\n",
      "Epoch [70/100] Loss: 68.378173828125\n",
      "Epoch [71/100] Loss: 67.46595001220703\n",
      "Epoch [72/100] Loss: 66.56744384765625\n",
      "Epoch [73/100] Loss: 65.68246459960938\n",
      "Epoch [74/100] Loss: 64.8107681274414\n",
      "Epoch [75/100] Loss: 63.95218276977539\n",
      "Epoch [76/100] Loss: 63.10649490356445\n",
      "Epoch [77/100] Loss: 62.27349853515625\n",
      "Epoch [78/100] Loss: 61.4530029296875\n",
      "Epoch [79/100] Loss: 60.64483642578125\n",
      "Epoch [80/100] Loss: 59.84882354736328\n",
      "Epoch [81/100] Loss: 59.064754486083984\n",
      "Epoch [82/100] Loss: 58.29249954223633\n",
      "Epoch [83/100] Loss: 57.53183364868164\n",
      "Epoch [84/100] Loss: 56.78264617919922\n",
      "Epoch [85/100] Loss: 56.044742584228516\n",
      "Epoch [86/100] Loss: 55.31795883178711\n",
      "Epoch [87/100] Loss: 54.60216522216797\n",
      "Epoch [88/100] Loss: 53.89719772338867\n",
      "Epoch [89/100] Loss: 53.20288848876953\n",
      "Epoch [90/100] Loss: 52.51911163330078\n",
      "Epoch [91/100] Loss: 51.845726013183594\n",
      "Epoch [92/100] Loss: 51.182559967041016\n",
      "Epoch [93/100] Loss: 50.52949142456055\n",
      "Epoch [94/100] Loss: 49.88638687133789\n",
      "Epoch [95/100] Loss: 49.25312423706055\n",
      "Epoch [96/100] Loss: 48.62955093383789\n",
      "Epoch [97/100] Loss: 48.01553726196289\n",
      "Epoch [98/100] Loss: 47.410972595214844\n",
      "Epoch [99/100] Loss: 46.81570053100586\n",
      "Epoch [100/100] Loss: 46.2296257019043\n",
      "Predicted days_remaining for parent_id 40: 9.162952423095703\n",
      "Training for parent_id 41...\n",
      "Epoch [1/100] Loss: 1072.82421875\n",
      "Epoch [2/100] Loss: 1055.326904296875\n",
      "Epoch [3/100] Loss: 1038.755615234375\n",
      "Epoch [4/100] Loss: 1023.2086181640625\n",
      "Epoch [5/100] Loss: 1008.6754150390625\n",
      "Epoch [6/100] Loss: 995.0482177734375\n",
      "Epoch [7/100] Loss: 982.1868286132812\n",
      "Epoch [8/100] Loss: 969.9788208007812\n",
      "Epoch [9/100] Loss: 958.3529663085938\n",
      "Epoch [10/100] Loss: 947.26416015625\n",
      "Epoch [11/100] Loss: 936.6784057617188\n",
      "Epoch [12/100] Loss: 926.566162109375\n",
      "Epoch [13/100] Loss: 916.900390625\n",
      "Epoch [14/100] Loss: 907.656005859375\n",
      "Epoch [15/100] Loss: 898.8101196289062\n",
      "Epoch [16/100] Loss: 890.3402099609375\n",
      "Epoch [17/100] Loss: 882.2249755859375\n",
      "Epoch [18/100] Loss: 874.4425659179688\n",
      "Epoch [19/100] Loss: 866.9715576171875\n",
      "Epoch [20/100] Loss: 859.7900390625\n",
      "Epoch [21/100] Loss: 852.8765258789062\n",
      "Epoch [22/100] Loss: 846.2106323242188\n",
      "Epoch [23/100] Loss: 839.7725830078125\n",
      "Epoch [24/100] Loss: 833.5438232421875\n",
      "Epoch [25/100] Loss: 827.5068969726562\n",
      "Epoch [26/100] Loss: 821.64599609375\n",
      "Epoch [27/100] Loss: 815.9462280273438\n",
      "Epoch [28/100] Loss: 810.3943481445312\n",
      "Epoch [29/100] Loss: 804.9783935546875\n",
      "Epoch [30/100] Loss: 799.6871948242188\n",
      "Epoch [31/100] Loss: 794.5112915039062\n",
      "Epoch [32/100] Loss: 789.4423217773438\n",
      "Epoch [33/100] Loss: 784.4722900390625\n",
      "Epoch [34/100] Loss: 779.5943603515625\n",
      "Epoch [35/100] Loss: 774.8025512695312\n",
      "Epoch [36/100] Loss: 770.09130859375\n",
      "Epoch [37/100] Loss: 765.4552001953125\n",
      "Epoch [38/100] Loss: 760.8900146484375\n",
      "Epoch [39/100] Loss: 756.3911743164062\n",
      "Epoch [40/100] Loss: 751.9547729492188\n",
      "Epoch [41/100] Loss: 747.5770874023438\n",
      "Epoch [42/100] Loss: 743.2547607421875\n",
      "Epoch [43/100] Loss: 738.9844970703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100] Loss: 734.76318359375\n",
      "Epoch [45/100] Loss: 730.5882568359375\n",
      "Epoch [46/100] Loss: 726.4566650390625\n",
      "Epoch [47/100] Loss: 722.3663940429688\n",
      "Epoch [48/100] Loss: 718.3150024414062\n",
      "Epoch [49/100] Loss: 714.3003540039062\n",
      "Epoch [50/100] Loss: 710.320556640625\n",
      "Epoch [51/100] Loss: 706.3736572265625\n",
      "Epoch [52/100] Loss: 702.4583129882812\n",
      "Epoch [53/100] Loss: 698.5726928710938\n",
      "Epoch [54/100] Loss: 694.7152709960938\n",
      "Epoch [55/100] Loss: 690.8851318359375\n",
      "Epoch [56/100] Loss: 687.0806274414062\n",
      "Epoch [57/100] Loss: 683.301025390625\n",
      "Epoch [58/100] Loss: 679.5452880859375\n",
      "Epoch [59/100] Loss: 675.8126831054688\n",
      "Epoch [60/100] Loss: 672.1024780273438\n",
      "Epoch [61/100] Loss: 668.4141235351562\n",
      "Epoch [62/100] Loss: 664.7474365234375\n",
      "Epoch [63/100] Loss: 661.1017456054688\n",
      "Epoch [64/100] Loss: 657.4769287109375\n",
      "Epoch [65/100] Loss: 653.872802734375\n",
      "Epoch [66/100] Loss: 650.2894897460938\n",
      "Epoch [67/100] Loss: 646.7264404296875\n",
      "Epoch [68/100] Loss: 643.1838989257812\n",
      "Epoch [69/100] Loss: 639.6618041992188\n",
      "Epoch [70/100] Loss: 636.159912109375\n",
      "Epoch [71/100] Loss: 632.6782836914062\n",
      "Epoch [72/100] Loss: 629.2167358398438\n",
      "Epoch [73/100] Loss: 625.7752685546875\n",
      "Epoch [74/100] Loss: 622.3536987304688\n",
      "Epoch [75/100] Loss: 618.9517822265625\n",
      "Epoch [76/100] Loss: 615.569580078125\n",
      "Epoch [77/100] Loss: 612.206787109375\n",
      "Epoch [78/100] Loss: 608.86328125\n",
      "Epoch [79/100] Loss: 605.5390014648438\n",
      "Epoch [80/100] Loss: 602.2337646484375\n",
      "Epoch [81/100] Loss: 598.9472045898438\n",
      "Epoch [82/100] Loss: 595.6793212890625\n",
      "Epoch [83/100] Loss: 592.4298095703125\n",
      "Epoch [84/100] Loss: 589.1986083984375\n",
      "Epoch [85/100] Loss: 585.9856567382812\n",
      "Epoch [86/100] Loss: 582.7904052734375\n",
      "Epoch [87/100] Loss: 579.6129150390625\n",
      "Epoch [88/100] Loss: 576.4531860351562\n",
      "Epoch [89/100] Loss: 573.3106689453125\n",
      "Epoch [90/100] Loss: 570.1856079101562\n",
      "Epoch [91/100] Loss: 567.0773315429688\n",
      "Epoch [92/100] Loss: 563.9862060546875\n",
      "Epoch [93/100] Loss: 560.9117431640625\n",
      "Epoch [94/100] Loss: 557.853759765625\n",
      "Epoch [95/100] Loss: 554.8123168945312\n",
      "Epoch [96/100] Loss: 551.7872314453125\n",
      "Epoch [97/100] Loss: 548.7781982421875\n",
      "Epoch [98/100] Loss: 545.78515625\n",
      "Epoch [99/100] Loss: 542.8079833984375\n",
      "Epoch [100/100] Loss: 539.8464965820312\n",
      "Predicted days_remaining for parent_id 41: 9.892548561096191\n",
      "Training for parent_id 50...\n",
      "Epoch [1/100] Loss: 1061.318359375\n",
      "Epoch [2/100] Loss: 1046.221923828125\n",
      "Epoch [3/100] Loss: 1031.4046630859375\n",
      "Epoch [4/100] Loss: 1016.9235229492188\n",
      "Epoch [5/100] Loss: 1002.8154907226562\n",
      "Epoch [6/100] Loss: 989.0985717773438\n",
      "Epoch [7/100] Loss: 975.7906494140625\n",
      "Epoch [8/100] Loss: 962.9179077148438\n",
      "Epoch [9/100] Loss: 950.510009765625\n",
      "Epoch [10/100] Loss: 938.5921630859375\n",
      "Epoch [11/100] Loss: 927.179931640625\n",
      "Epoch [12/100] Loss: 916.2770385742188\n",
      "Epoch [13/100] Loss: 905.8762817382812\n",
      "Epoch [14/100] Loss: 895.9609375\n",
      "Epoch [15/100] Loss: 886.5082397460938\n",
      "Epoch [16/100] Loss: 877.4910278320312\n",
      "Epoch [17/100] Loss: 868.8803100585938\n",
      "Epoch [18/100] Loss: 860.6463623046875\n",
      "Epoch [19/100] Loss: 852.7598876953125\n",
      "Epoch [20/100] Loss: 845.1930541992188\n",
      "Epoch [21/100] Loss: 837.91943359375\n",
      "Epoch [22/100] Loss: 830.9147338867188\n",
      "Epoch [23/100] Loss: 824.1574096679688\n",
      "Epoch [24/100] Loss: 817.627197265625\n",
      "Epoch [25/100] Loss: 811.3070678710938\n",
      "Epoch [26/100] Loss: 805.1814575195312\n",
      "Epoch [27/100] Loss: 799.236572265625\n",
      "Epoch [28/100] Loss: 793.4605712890625\n",
      "Epoch [29/100] Loss: 787.841796875\n",
      "Epoch [30/100] Loss: 782.3697509765625\n",
      "Epoch [31/100] Loss: 777.0343627929688\n",
      "Epoch [32/100] Loss: 771.825927734375\n",
      "Epoch [33/100] Loss: 766.7349243164062\n",
      "Epoch [34/100] Loss: 761.7525634765625\n",
      "Epoch [35/100] Loss: 756.869873046875\n",
      "Epoch [36/100] Loss: 752.0785522460938\n",
      "Epoch [37/100] Loss: 747.370849609375\n",
      "Epoch [38/100] Loss: 742.739501953125\n",
      "Epoch [39/100] Loss: 738.1780395507812\n",
      "Epoch [40/100] Loss: 733.6803588867188\n",
      "Epoch [41/100] Loss: 729.2418212890625\n",
      "Epoch [42/100] Loss: 724.8582763671875\n",
      "Epoch [43/100] Loss: 720.5262451171875\n",
      "Epoch [44/100] Loss: 716.2427978515625\n",
      "Epoch [45/100] Loss: 712.0060424804688\n",
      "Epoch [46/100] Loss: 707.8139038085938\n",
      "Epoch [47/100] Loss: 703.6646728515625\n",
      "Epoch [48/100] Loss: 699.5567016601562\n",
      "Epoch [49/100] Loss: 695.4888916015625\n",
      "Epoch [50/100] Loss: 691.4598388671875\n",
      "Epoch [51/100] Loss: 687.468017578125\n",
      "Epoch [52/100] Loss: 683.5126953125\n",
      "Epoch [53/100] Loss: 679.5921630859375\n",
      "Epoch [54/100] Loss: 675.7053833007812\n",
      "Epoch [55/100] Loss: 671.8513793945312\n",
      "Epoch [56/100] Loss: 668.0289916992188\n",
      "Epoch [57/100] Loss: 664.2372436523438\n",
      "Epoch [58/100] Loss: 660.475341796875\n",
      "Epoch [59/100] Loss: 656.7422485351562\n",
      "Epoch [60/100] Loss: 653.0374145507812\n",
      "Epoch [61/100] Loss: 649.35986328125\n",
      "Epoch [62/100] Loss: 645.7089233398438\n",
      "Epoch [63/100] Loss: 642.0840454101562\n",
      "Epoch [64/100] Loss: 638.4846801757812\n",
      "Epoch [65/100] Loss: 634.9103393554688\n",
      "Epoch [66/100] Loss: 631.3604125976562\n",
      "Epoch [67/100] Loss: 627.8343505859375\n",
      "Epoch [68/100] Loss: 624.3319702148438\n",
      "Epoch [69/100] Loss: 620.8524169921875\n",
      "Epoch [70/100] Loss: 617.395751953125\n",
      "Epoch [71/100] Loss: 613.9613037109375\n",
      "Epoch [72/100] Loss: 610.5486450195312\n",
      "Epoch [73/100] Loss: 607.1577758789062\n",
      "Epoch [74/100] Loss: 603.7880249023438\n",
      "Epoch [75/100] Loss: 600.4391479492188\n",
      "Epoch [76/100] Loss: 597.1109008789062\n",
      "Epoch [77/100] Loss: 593.8029174804688\n",
      "Epoch [78/100] Loss: 590.5150146484375\n",
      "Epoch [79/100] Loss: 587.2466430664062\n",
      "Epoch [80/100] Loss: 583.997802734375\n",
      "Epoch [81/100] Loss: 580.7681884765625\n",
      "Epoch [82/100] Loss: 577.557373046875\n",
      "Epoch [83/100] Loss: 574.3654174804688\n",
      "Epoch [84/100] Loss: 571.19189453125\n",
      "Epoch [85/100] Loss: 568.0365600585938\n",
      "Epoch [86/100] Loss: 564.8992309570312\n",
      "Epoch [87/100] Loss: 561.7798461914062\n",
      "Epoch [88/100] Loss: 558.6778564453125\n",
      "Epoch [89/100] Loss: 555.59326171875\n",
      "Epoch [90/100] Loss: 552.52587890625\n",
      "Epoch [91/100] Loss: 549.4755859375\n",
      "Epoch [92/100] Loss: 546.442138671875\n",
      "Epoch [93/100] Loss: 543.4253540039062\n",
      "Epoch [94/100] Loss: 540.4249267578125\n",
      "Epoch [95/100] Loss: 537.4407958984375\n",
      "Epoch [96/100] Loss: 534.4730224609375\n",
      "Epoch [97/100] Loss: 531.5210571289062\n",
      "Epoch [98/100] Loss: 528.5851440429688\n",
      "Epoch [99/100] Loss: 525.664794921875\n",
      "Epoch [100/100] Loss: 522.7601318359375\n",
      "Predicted days_remaining for parent_id 50: 10.268157005310059\n",
      "Training for parent_id 51...\n",
      "Epoch [1/100] Loss: 172.7074737548828\n",
      "Epoch [2/100] Loss: 166.76840209960938\n",
      "Epoch [3/100] Loss: 161.0166473388672\n",
      "Epoch [4/100] Loss: 155.49807739257812\n",
      "Epoch [5/100] Loss: 150.23252868652344\n",
      "Epoch [6/100] Loss: 145.22079467773438\n",
      "Epoch [7/100] Loss: 140.45364379882812\n",
      "Epoch [8/100] Loss: 135.91909790039062\n",
      "Epoch [9/100] Loss: 131.60696411132812\n",
      "Epoch [10/100] Loss: 127.50997924804688\n",
      "Epoch [11/100] Loss: 123.62259674072266\n",
      "Epoch [12/100] Loss: 119.93934631347656\n",
      "Epoch [13/100] Loss: 116.45407104492188\n",
      "Epoch [14/100] Loss: 113.1597900390625\n",
      "Epoch [15/100] Loss: 110.04855346679688\n",
      "Epoch [16/100] Loss: 107.1116714477539\n",
      "Epoch [17/100] Loss: 104.3397216796875\n",
      "Epoch [18/100] Loss: 101.72266387939453\n",
      "Epoch [19/100] Loss: 99.25010681152344\n",
      "Epoch [20/100] Loss: 96.91151428222656\n",
      "Epoch [21/100] Loss: 94.69635772705078\n",
      "Epoch [22/100] Loss: 92.5946044921875\n",
      "Epoch [23/100] Loss: 90.59662628173828\n",
      "Epoch [24/100] Loss: 88.69340515136719\n",
      "Epoch [25/100] Loss: 86.87665557861328\n",
      "Epoch [26/100] Loss: 85.13874816894531\n",
      "Epoch [27/100] Loss: 83.4727554321289\n",
      "Epoch [28/100] Loss: 81.8724365234375\n",
      "Epoch [29/100] Loss: 80.33222198486328\n",
      "Epoch [30/100] Loss: 78.84709167480469\n",
      "Epoch [31/100] Loss: 77.41263580322266\n",
      "Epoch [32/100] Loss: 76.02491760253906\n",
      "Epoch [33/100] Loss: 74.680419921875\n",
      "Epoch [34/100] Loss: 73.37605285644531\n",
      "Epoch [35/100] Loss: 72.10909271240234\n",
      "Epoch [36/100] Loss: 70.87708282470703\n",
      "Epoch [37/100] Loss: 69.6778335571289\n",
      "Epoch [38/100] Loss: 68.50945281982422\n",
      "Epoch [39/100] Loss: 67.37016296386719\n",
      "Epoch [40/100] Loss: 66.2584228515625\n",
      "Epoch [41/100] Loss: 65.1728515625\n",
      "Epoch [42/100] Loss: 64.1121826171875\n",
      "Epoch [43/100] Loss: 63.075252532958984\n",
      "Epoch [44/100] Loss: 62.061073303222656\n",
      "Epoch [45/100] Loss: 61.06867980957031\n",
      "Epoch [46/100] Loss: 60.09722900390625\n",
      "Epoch [47/100] Loss: 59.14595413208008\n",
      "Epoch [48/100] Loss: 58.214141845703125\n",
      "Epoch [49/100] Loss: 57.3011474609375\n",
      "Epoch [50/100] Loss: 56.40639114379883\n",
      "Epoch [51/100] Loss: 55.529293060302734\n",
      "Epoch [52/100] Loss: 54.66937255859375\n",
      "Epoch [53/100] Loss: 53.826168060302734\n",
      "Epoch [54/100] Loss: 52.99922180175781\n",
      "Epoch [55/100] Loss: 52.18814468383789\n",
      "Epoch [56/100] Loss: 51.39253616333008\n",
      "Epoch [57/100] Loss: 50.61205291748047\n",
      "Epoch [58/100] Loss: 49.84635543823242\n",
      "Epoch [59/100] Loss: 49.095115661621094\n",
      "Epoch [60/100] Loss: 48.35804748535156\n",
      "Epoch [61/100] Loss: 47.63483428955078\n",
      "Epoch [62/100] Loss: 46.925209045410156\n",
      "Epoch [63/100] Loss: 46.22892761230469\n",
      "Epoch [64/100] Loss: 45.54570388793945\n",
      "Epoch [65/100] Loss: 44.87529754638672\n",
      "Epoch [66/100] Loss: 44.21747970581055\n",
      "Epoch [67/100] Loss: 43.57202911376953\n",
      "Epoch [68/100] Loss: 42.93869400024414\n",
      "Epoch [69/100] Loss: 42.31728744506836\n",
      "Epoch [70/100] Loss: 41.70758819580078\n",
      "Epoch [71/100] Loss: 41.109378814697266\n",
      "Epoch [72/100] Loss: 40.52247619628906\n",
      "Epoch [73/100] Loss: 39.94669723510742\n",
      "Epoch [74/100] Loss: 39.381813049316406\n",
      "Epoch [75/100] Loss: 38.827693939208984\n",
      "Epoch [76/100] Loss: 38.28410339355469\n",
      "Epoch [77/100] Loss: 37.750892639160156\n",
      "Epoch [78/100] Loss: 37.227882385253906\n",
      "Epoch [79/100] Loss: 36.714908599853516\n",
      "Epoch [80/100] Loss: 36.21180725097656\n",
      "Epoch [81/100] Loss: 35.71840286254883\n",
      "Epoch [82/100] Loss: 35.23453140258789\n",
      "Epoch [83/100] Loss: 34.760066986083984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100] Loss: 34.29480743408203\n",
      "Epoch [85/100] Loss: 33.83864974975586\n",
      "Epoch [86/100] Loss: 33.39140319824219\n",
      "Epoch [87/100] Loss: 32.952938079833984\n",
      "Epoch [88/100] Loss: 32.52311706542969\n",
      "Epoch [89/100] Loss: 32.10179901123047\n",
      "Epoch [90/100] Loss: 31.688827514648438\n",
      "Epoch [91/100] Loss: 31.284076690673828\n",
      "Epoch [92/100] Loss: 30.88741111755371\n",
      "Epoch [93/100] Loss: 30.498708724975586\n",
      "Epoch [94/100] Loss: 30.117809295654297\n",
      "Epoch [95/100] Loss: 29.744598388671875\n",
      "Epoch [96/100] Loss: 29.37895965576172\n",
      "Epoch [97/100] Loss: 29.020763397216797\n",
      "Epoch [98/100] Loss: 28.669876098632812\n",
      "Epoch [99/100] Loss: 28.326175689697266\n",
      "Epoch [100/100] Loss: 27.98955726623535\n",
      "Predicted days_remaining for parent_id 51: 9.113739967346191\n",
      "Training for parent_id 61...\n",
      "Epoch [1/100] Loss: 336.3182678222656\n",
      "Epoch [2/100] Loss: 329.7677001953125\n",
      "Epoch [3/100] Loss: 323.3105163574219\n",
      "Epoch [4/100] Loss: 316.95306396484375\n",
      "Epoch [5/100] Loss: 310.70587158203125\n",
      "Epoch [6/100] Loss: 304.57098388671875\n",
      "Epoch [7/100] Loss: 298.54486083984375\n",
      "Epoch [8/100] Loss: 292.624267578125\n",
      "Epoch [9/100] Loss: 286.8099670410156\n",
      "Epoch [10/100] Loss: 281.1059875488281\n",
      "Epoch [11/100] Loss: 275.5193786621094\n",
      "Epoch [12/100] Loss: 270.0596618652344\n",
      "Epoch [13/100] Loss: 264.7372741699219\n",
      "Epoch [14/100] Loss: 259.5625305175781\n",
      "Epoch [15/100] Loss: 254.54379272460938\n",
      "Epoch [16/100] Loss: 249.68698120117188\n",
      "Epoch [17/100] Loss: 244.9952850341797\n",
      "Epoch [18/100] Loss: 240.46954345703125\n",
      "Epoch [19/100] Loss: 236.10870361328125\n",
      "Epoch [20/100] Loss: 231.91036987304688\n",
      "Epoch [21/100] Loss: 227.87118530273438\n",
      "Epoch [22/100] Loss: 223.9869384765625\n",
      "Epoch [23/100] Loss: 220.25271606445312\n",
      "Epoch [24/100] Loss: 216.6631622314453\n",
      "Epoch [25/100] Loss: 213.21214294433594\n",
      "Epoch [26/100] Loss: 209.89320373535156\n",
      "Epoch [27/100] Loss: 206.699462890625\n",
      "Epoch [28/100] Loss: 203.62400817871094\n",
      "Epoch [29/100] Loss: 200.65977478027344\n",
      "Epoch [30/100] Loss: 197.79981994628906\n",
      "Epoch [31/100] Loss: 195.0372772216797\n",
      "Epoch [32/100] Loss: 192.36572265625\n",
      "Epoch [33/100] Loss: 189.77880859375\n",
      "Epoch [34/100] Loss: 187.27069091796875\n",
      "Epoch [35/100] Loss: 184.83583068847656\n",
      "Epoch [36/100] Loss: 182.46888732910156\n",
      "Epoch [37/100] Loss: 180.16477966308594\n",
      "Epoch [38/100] Loss: 177.91879272460938\n",
      "Epoch [39/100] Loss: 175.72622680664062\n",
      "Epoch [40/100] Loss: 173.58282470703125\n",
      "Epoch [41/100] Loss: 171.4844207763672\n",
      "Epoch [42/100] Loss: 169.42726135253906\n",
      "Epoch [43/100] Loss: 167.40794372558594\n",
      "Epoch [44/100] Loss: 165.42352294921875\n",
      "Epoch [45/100] Loss: 163.47146606445312\n",
      "Epoch [46/100] Loss: 161.54962158203125\n",
      "Epoch [47/100] Loss: 159.65635681152344\n",
      "Epoch [48/100] Loss: 157.7902374267578\n",
      "Epoch [49/100] Loss: 155.9501953125\n",
      "Epoch [50/100] Loss: 154.1353759765625\n",
      "Epoch [51/100] Loss: 152.3450469970703\n",
      "Epoch [52/100] Loss: 150.57862854003906\n",
      "Epoch [53/100] Loss: 148.83567810058594\n",
      "Epoch [54/100] Loss: 147.11569213867188\n",
      "Epoch [55/100] Loss: 145.41824340820312\n",
      "Epoch [56/100] Loss: 143.74288940429688\n",
      "Epoch [57/100] Loss: 142.08921813964844\n",
      "Epoch [58/100] Loss: 140.4567108154297\n",
      "Epoch [59/100] Loss: 138.84495544433594\n",
      "Epoch [60/100] Loss: 137.25341796875\n",
      "Epoch [61/100] Loss: 135.68167114257812\n",
      "Epoch [62/100] Loss: 134.1291961669922\n",
      "Epoch [63/100] Loss: 132.59559631347656\n",
      "Epoch [64/100] Loss: 131.080322265625\n",
      "Epoch [65/100] Loss: 129.5830535888672\n",
      "Epoch [66/100] Loss: 128.1033935546875\n",
      "Epoch [67/100] Loss: 126.64104461669922\n",
      "Epoch [68/100] Loss: 125.19566345214844\n",
      "Epoch [69/100] Loss: 123.76697540283203\n",
      "Epoch [70/100] Loss: 122.35487365722656\n",
      "Epoch [71/100] Loss: 120.95906829833984\n",
      "Epoch [72/100] Loss: 119.57942962646484\n",
      "Epoch [73/100] Loss: 118.21581268310547\n",
      "Epoch [74/100] Loss: 116.86805725097656\n",
      "Epoch [75/100] Loss: 115.53601837158203\n",
      "Epoch [76/100] Loss: 114.21952819824219\n",
      "Epoch [77/100] Loss: 112.91845703125\n",
      "Epoch [78/100] Loss: 111.6326904296875\n",
      "Epoch [79/100] Loss: 110.36196899414062\n",
      "Epoch [80/100] Loss: 109.10613250732422\n",
      "Epoch [81/100] Loss: 107.86506652832031\n",
      "Epoch [82/100] Loss: 106.63847351074219\n",
      "Epoch [83/100] Loss: 105.42622375488281\n",
      "Epoch [84/100] Loss: 104.22813415527344\n",
      "Epoch [85/100] Loss: 103.04397583007812\n",
      "Epoch [86/100] Loss: 101.87356567382812\n",
      "Epoch [87/100] Loss: 100.71665954589844\n",
      "Epoch [88/100] Loss: 99.5730972290039\n",
      "Epoch [89/100] Loss: 98.4427261352539\n",
      "Epoch [90/100] Loss: 97.32522583007812\n",
      "Epoch [91/100] Loss: 96.22053527832031\n",
      "Epoch [92/100] Loss: 95.12838745117188\n",
      "Epoch [93/100] Loss: 94.04867553710938\n",
      "Epoch [94/100] Loss: 92.98115539550781\n",
      "Epoch [95/100] Loss: 91.92573547363281\n",
      "Epoch [96/100] Loss: 90.88218688964844\n",
      "Epoch [97/100] Loss: 89.85039520263672\n",
      "Epoch [98/100] Loss: 88.83024597167969\n",
      "Epoch [99/100] Loss: 87.82154846191406\n",
      "Epoch [100/100] Loss: 86.82420349121094\n",
      "Predicted days_remaining for parent_id 61: 9.300113677978516\n",
      "Training for parent_id 70...\n",
      "Epoch [1/100] Loss: 206.64239501953125\n",
      "Epoch [2/100] Loss: 199.85568237304688\n",
      "Epoch [3/100] Loss: 193.3898162841797\n",
      "Epoch [4/100] Loss: 187.2606964111328\n",
      "Epoch [5/100] Loss: 181.44700622558594\n",
      "Epoch [6/100] Loss: 175.91668701171875\n",
      "Epoch [7/100] Loss: 170.64065551757812\n",
      "Epoch [8/100] Loss: 165.59434509277344\n",
      "Epoch [9/100] Loss: 160.75814819335938\n",
      "Epoch [10/100] Loss: 156.11790466308594\n",
      "Epoch [11/100] Loss: 151.66452026367188\n",
      "Epoch [12/100] Loss: 147.392578125\n",
      "Epoch [13/100] Loss: 143.29908752441406\n",
      "Epoch [14/100] Loss: 139.3822021484375\n",
      "Epoch [15/100] Loss: 135.6402587890625\n",
      "Epoch [16/100] Loss: 132.07041931152344\n",
      "Epoch [17/100] Loss: 128.66824340820312\n",
      "Epoch [18/100] Loss: 125.42745208740234\n",
      "Epoch [19/100] Loss: 122.3404312133789\n",
      "Epoch [20/100] Loss: 119.39883422851562\n",
      "Epoch [21/100] Loss: 116.59412384033203\n",
      "Epoch [22/100] Loss: 113.91796875\n",
      "Epoch [23/100] Loss: 111.36249542236328\n",
      "Epoch [24/100] Loss: 108.9201889038086\n",
      "Epoch [25/100] Loss: 106.5840072631836\n",
      "Epoch [26/100] Loss: 104.34725952148438\n",
      "Epoch [27/100] Loss: 102.20341491699219\n",
      "Epoch [28/100] Loss: 100.14629364013672\n",
      "Epoch [29/100] Loss: 98.1698226928711\n",
      "Epoch [30/100] Loss: 96.26822662353516\n",
      "Epoch [31/100] Loss: 94.43594360351562\n",
      "Epoch [32/100] Loss: 92.66785430908203\n",
      "Epoch [33/100] Loss: 90.95909881591797\n",
      "Epoch [34/100] Loss: 89.30533599853516\n",
      "Epoch [35/100] Loss: 87.7026138305664\n",
      "Epoch [36/100] Loss: 86.14746856689453\n",
      "Epoch [37/100] Loss: 84.63678741455078\n",
      "Epoch [38/100] Loss: 83.16793060302734\n",
      "Epoch [39/100] Loss: 81.73844909667969\n",
      "Epoch [40/100] Loss: 80.34629821777344\n",
      "Epoch [41/100] Loss: 78.98966217041016\n",
      "Epoch [42/100] Loss: 77.66686248779297\n",
      "Epoch [43/100] Loss: 76.37635803222656\n",
      "Epoch [44/100] Loss: 75.1168441772461\n",
      "Epoch [45/100] Loss: 73.88707733154297\n",
      "Epoch [46/100] Loss: 72.68585968017578\n",
      "Epoch [47/100] Loss: 71.51214599609375\n",
      "Epoch [48/100] Loss: 70.3648681640625\n",
      "Epoch [49/100] Loss: 69.24312591552734\n",
      "Epoch [50/100] Loss: 68.14596557617188\n",
      "Epoch [51/100] Loss: 67.07255554199219\n",
      "Epoch [52/100] Loss: 66.02207946777344\n",
      "Epoch [53/100] Loss: 64.9937973022461\n",
      "Epoch [54/100] Loss: 63.98698043823242\n",
      "Epoch [55/100] Loss: 63.0009765625\n",
      "Epoch [56/100] Loss: 62.03518295288086\n",
      "Epoch [57/100] Loss: 61.08900451660156\n",
      "Epoch [58/100] Loss: 60.16192626953125\n",
      "Epoch [59/100] Loss: 59.253421783447266\n",
      "Epoch [60/100] Loss: 58.363033294677734\n",
      "Epoch [61/100] Loss: 57.490318298339844\n",
      "Epoch [62/100] Loss: 56.63482666015625\n",
      "Epoch [63/100] Loss: 55.7961540222168\n",
      "Epoch [64/100] Loss: 54.973876953125\n",
      "Epoch [65/100] Loss: 54.167564392089844\n",
      "Epoch [66/100] Loss: 53.37682342529297\n",
      "Epoch [67/100] Loss: 52.60121536254883\n",
      "Epoch [68/100] Loss: 51.84035110473633\n",
      "Epoch [69/100] Loss: 51.093807220458984\n",
      "Epoch [70/100] Loss: 50.36121368408203\n",
      "Epoch [71/100] Loss: 49.64214324951172\n",
      "Epoch [72/100] Loss: 48.93626403808594\n",
      "Epoch [73/100] Loss: 48.24325180053711\n",
      "Epoch [74/100] Loss: 47.562740325927734\n",
      "Epoch [75/100] Loss: 46.89447021484375\n",
      "Epoch [76/100] Loss: 46.23814010620117\n",
      "Epoch [77/100] Loss: 45.59351348876953\n",
      "Epoch [78/100] Loss: 44.96030807495117\n",
      "Epoch [79/100] Loss: 44.338348388671875\n",
      "Epoch [80/100] Loss: 43.72737503051758\n",
      "Epoch [81/100] Loss: 43.1271858215332\n",
      "Epoch [82/100] Loss: 42.53763961791992\n",
      "Epoch [83/100] Loss: 41.95849609375\n",
      "Epoch [84/100] Loss: 41.38962173461914\n",
      "Epoch [85/100] Loss: 40.8307991027832\n",
      "Epoch [86/100] Loss: 40.281890869140625\n",
      "Epoch [87/100] Loss: 39.74272537231445\n",
      "Epoch [88/100] Loss: 39.213138580322266\n",
      "Epoch [89/100] Loss: 38.69300079345703\n",
      "Epoch [90/100] Loss: 38.182132720947266\n",
      "Epoch [91/100] Loss: 37.68040084838867\n",
      "Epoch [92/100] Loss: 37.187652587890625\n",
      "Epoch [93/100] Loss: 36.703765869140625\n",
      "Epoch [94/100] Loss: 36.228572845458984\n",
      "Epoch [95/100] Loss: 35.76195526123047\n",
      "Epoch [96/100] Loss: 35.303775787353516\n",
      "Epoch [97/100] Loss: 34.85389709472656\n",
      "Epoch [98/100] Loss: 34.41220474243164\n",
      "Epoch [99/100] Loss: 33.97856903076172\n",
      "Epoch [100/100] Loss: 33.55286407470703\n",
      "Predicted days_remaining for parent_id 70: 9.425943374633789\n",
      "Training for parent_id 74...\n",
      "Epoch [1/100] Loss: 105.51995086669922\n",
      "Epoch [2/100] Loss: 101.68017578125\n",
      "Epoch [3/100] Loss: 97.94471740722656\n",
      "Epoch [4/100] Loss: 94.32303619384766\n",
      "Epoch [5/100] Loss: 90.8145523071289\n",
      "Epoch [6/100] Loss: 87.41206359863281\n",
      "Epoch [7/100] Loss: 84.1075210571289\n",
      "Epoch [8/100] Loss: 80.89544677734375\n",
      "Epoch [9/100] Loss: 77.77355194091797\n",
      "Epoch [10/100] Loss: 74.74244689941406\n",
      "Epoch [11/100] Loss: 71.8055648803711\n",
      "Epoch [12/100] Loss: 68.96894836425781\n",
      "Epoch [13/100] Loss: 66.24034118652344\n",
      "Epoch [14/100] Loss: 63.62810134887695\n",
      "Epoch [15/100] Loss: 61.13982391357422\n",
      "Epoch [16/100] Loss: 58.78132247924805\n",
      "Epoch [17/100] Loss: 56.55592727661133\n",
      "Epoch [18/100] Loss: 54.464073181152344\n",
      "Epoch [19/100] Loss: 52.50344467163086\n",
      "Epoch [20/100] Loss: 50.66932678222656\n",
      "Epoch [21/100] Loss: 48.95515441894531\n",
      "Epoch [22/100] Loss: 47.353271484375\n",
      "Epoch [23/100] Loss: 45.85544204711914\n",
      "Epoch [24/100] Loss: 44.45338821411133\n",
      "Epoch [25/100] Loss: 43.13905715942383\n",
      "Epoch [26/100] Loss: 41.9048957824707\n",
      "Epoch [27/100] Loss: 40.74394607543945\n",
      "Epoch [28/100] Loss: 39.64979934692383\n",
      "Epoch [29/100] Loss: 38.61669921875\n",
      "Epoch [30/100] Loss: 37.639404296875\n",
      "Epoch [31/100] Loss: 36.71322250366211\n",
      "Epoch [32/100] Loss: 35.83391571044922\n",
      "Epoch [33/100] Loss: 34.997650146484375\n",
      "Epoch [34/100] Loss: 34.20098114013672\n",
      "Epoch [35/100] Loss: 33.44080352783203\n",
      "Epoch [36/100] Loss: 32.714332580566406\n",
      "Epoch [37/100] Loss: 32.01907730102539\n",
      "Epoch [38/100] Loss: 31.352792739868164\n",
      "Epoch [39/100] Loss: 30.713502883911133\n",
      "Epoch [40/100] Loss: 30.09942626953125\n",
      "Epoch [41/100] Loss: 29.50899887084961\n",
      "Epoch [42/100] Loss: 28.9407958984375\n",
      "Epoch [43/100] Loss: 28.393571853637695\n",
      "Epoch [44/100] Loss: 27.8662109375\n",
      "Epoch [45/100] Loss: 27.357707977294922\n",
      "Epoch [46/100] Loss: 26.86715316772461\n",
      "Epoch [47/100] Loss: 26.39373207092285\n",
      "Epoch [48/100] Loss: 25.93670654296875\n",
      "Epoch [49/100] Loss: 25.49539566040039\n",
      "Epoch [50/100] Loss: 25.069177627563477\n",
      "Epoch [51/100] Loss: 24.65748405456543\n",
      "Epoch [52/100] Loss: 24.259796142578125\n",
      "Epoch [53/100] Loss: 23.875619888305664\n",
      "Epoch [54/100] Loss: 23.504497528076172\n",
      "Epoch [55/100] Loss: 23.145999908447266\n",
      "Epoch [56/100] Loss: 22.799724578857422\n",
      "Epoch [57/100] Loss: 22.465290069580078\n",
      "Epoch [58/100] Loss: 22.142333984375\n",
      "Epoch [59/100] Loss: 21.830509185791016\n",
      "Epoch [60/100] Loss: 21.529476165771484\n",
      "Epoch [61/100] Loss: 21.238924026489258\n",
      "Epoch [62/100] Loss: 20.958541870117188\n",
      "Epoch [63/100] Loss: 20.688026428222656\n",
      "Epoch [64/100] Loss: 20.427085876464844\n",
      "Epoch [65/100] Loss: 20.17544937133789\n",
      "Epoch [66/100] Loss: 19.932842254638672\n",
      "Epoch [67/100] Loss: 19.69899559020996\n",
      "Epoch [68/100] Loss: 19.47364616394043\n",
      "Epoch [69/100] Loss: 19.25655174255371\n",
      "Epoch [70/100] Loss: 19.047454833984375\n",
      "Epoch [71/100] Loss: 18.84612274169922\n",
      "Epoch [72/100] Loss: 18.65231704711914\n",
      "Epoch [73/100] Loss: 18.46581268310547\n",
      "Epoch [74/100] Loss: 18.2863826751709\n",
      "Epoch [75/100] Loss: 18.11380958557129\n",
      "Epoch [76/100] Loss: 17.947879791259766\n",
      "Epoch [77/100] Loss: 17.788389205932617\n",
      "Epoch [78/100] Loss: 17.6351318359375\n",
      "Epoch [79/100] Loss: 17.48790740966797\n",
      "Epoch [80/100] Loss: 17.346525192260742\n",
      "Epoch [81/100] Loss: 17.210796356201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100] Loss: 17.08053207397461\n",
      "Epoch [83/100] Loss: 16.955556869506836\n",
      "Epoch [84/100] Loss: 16.835693359375\n",
      "Epoch [85/100] Loss: 16.720767974853516\n",
      "Epoch [86/100] Loss: 16.610618591308594\n",
      "Epoch [87/100] Loss: 16.505077362060547\n",
      "Epoch [88/100] Loss: 16.40399169921875\n",
      "Epoch [89/100] Loss: 16.30720329284668\n",
      "Epoch [90/100] Loss: 16.21455955505371\n",
      "Epoch [91/100] Loss: 16.125913619995117\n",
      "Epoch [92/100] Loss: 16.041126251220703\n",
      "Epoch [93/100] Loss: 15.960057258605957\n",
      "Epoch [94/100] Loss: 15.882570266723633\n",
      "Epoch [95/100] Loss: 15.808528900146484\n",
      "Epoch [96/100] Loss: 15.737817764282227\n",
      "Epoch [97/100] Loss: 15.670302391052246\n",
      "Epoch [98/100] Loss: 15.60586166381836\n",
      "Epoch [99/100] Loss: 15.544380187988281\n",
      "Epoch [100/100] Loss: 15.485747337341309\n",
      "Predicted days_remaining for parent_id 74: 8.753832817077637\n",
      "Training for parent_id 76...\n",
      "Epoch [1/100] Loss: 149.03305053710938\n",
      "Epoch [2/100] Loss: 144.18194580078125\n",
      "Epoch [3/100] Loss: 139.501220703125\n",
      "Epoch [4/100] Loss: 134.98228454589844\n",
      "Epoch [5/100] Loss: 130.6215362548828\n",
      "Epoch [6/100] Loss: 126.41838073730469\n",
      "Epoch [7/100] Loss: 122.37321472167969\n",
      "Epoch [8/100] Loss: 118.48715209960938\n",
      "Epoch [9/100] Loss: 114.7614974975586\n",
      "Epoch [10/100] Loss: 111.19715118408203\n",
      "Epoch [11/100] Loss: 107.79405975341797\n",
      "Epoch [12/100] Loss: 104.55126953125\n",
      "Epoch [13/100] Loss: 101.46686553955078\n",
      "Epoch [14/100] Loss: 98.53791809082031\n",
      "Epoch [15/100] Loss: 95.7603759765625\n",
      "Epoch [16/100] Loss: 93.12898254394531\n",
      "Epoch [17/100] Loss: 90.63729858398438\n",
      "Epoch [18/100] Loss: 88.27787780761719\n",
      "Epoch [19/100] Loss: 86.04261779785156\n",
      "Epoch [20/100] Loss: 83.9231185913086\n",
      "Epoch [21/100] Loss: 81.9109115600586\n",
      "Epoch [22/100] Loss: 79.99775695800781\n",
      "Epoch [23/100] Loss: 78.17572784423828\n",
      "Epoch [24/100] Loss: 76.4372329711914\n",
      "Epoch [25/100] Loss: 74.77510833740234\n",
      "Epoch [26/100] Loss: 73.1826171875\n",
      "Epoch [27/100] Loss: 71.65351867675781\n",
      "Epoch [28/100] Loss: 70.18218994140625\n",
      "Epoch [29/100] Loss: 68.7635269165039\n",
      "Epoch [30/100] Loss: 67.39311981201172\n",
      "Epoch [31/100] Loss: 66.06704711914062\n",
      "Epoch [32/100] Loss: 64.78195190429688\n",
      "Epoch [33/100] Loss: 63.534854888916016\n",
      "Epoch [34/100] Loss: 62.323238372802734\n",
      "Epoch [35/100] Loss: 61.1448974609375\n",
      "Epoch [36/100] Loss: 59.997894287109375\n",
      "Epoch [37/100] Loss: 58.88054656982422\n",
      "Epoch [38/100] Loss: 57.79140090942383\n",
      "Epoch [39/100] Loss: 56.729156494140625\n",
      "Epoch [40/100] Loss: 55.69265365600586\n",
      "Epoch [41/100] Loss: 54.68084716796875\n",
      "Epoch [42/100] Loss: 53.69281005859375\n",
      "Epoch [43/100] Loss: 52.7276611328125\n",
      "Epoch [44/100] Loss: 51.78457260131836\n",
      "Epoch [45/100] Loss: 50.862831115722656\n",
      "Epoch [46/100] Loss: 49.96172332763672\n",
      "Epoch [47/100] Loss: 49.08058547973633\n",
      "Epoch [48/100] Loss: 48.21883010864258\n",
      "Epoch [49/100] Loss: 47.37590026855469\n",
      "Epoch [50/100] Loss: 46.5512809753418\n",
      "Epoch [51/100] Loss: 45.744503021240234\n",
      "Epoch [52/100] Loss: 44.95515823364258\n",
      "Epoch [53/100] Loss: 44.1828498840332\n",
      "Epoch [54/100] Loss: 43.4272346496582\n",
      "Epoch [55/100] Loss: 42.687984466552734\n",
      "Epoch [56/100] Loss: 41.96478271484375\n",
      "Epoch [57/100] Loss: 41.25734329223633\n",
      "Epoch [58/100] Loss: 40.5654182434082\n",
      "Epoch [59/100] Loss: 39.888694763183594\n",
      "Epoch [60/100] Loss: 39.22696304321289\n",
      "Epoch [61/100] Loss: 38.57994079589844\n",
      "Epoch [62/100] Loss: 37.94737243652344\n",
      "Epoch [63/100] Loss: 37.329036712646484\n",
      "Epoch [64/100] Loss: 36.72466278076172\n",
      "Epoch [65/100] Loss: 36.13401794433594\n",
      "Epoch [66/100] Loss: 35.556854248046875\n",
      "Epoch [67/100] Loss: 34.99292755126953\n",
      "Epoch [68/100] Loss: 34.4420051574707\n",
      "Epoch [69/100] Loss: 33.903846740722656\n",
      "Epoch [70/100] Loss: 33.378211975097656\n",
      "Epoch [71/100] Loss: 32.86486053466797\n",
      "Epoch [72/100] Loss: 32.363563537597656\n",
      "Epoch [73/100] Loss: 31.87409210205078\n",
      "Epoch [74/100] Loss: 31.396211624145508\n",
      "Epoch [75/100] Loss: 30.929691314697266\n",
      "Epoch [76/100] Loss: 30.474323272705078\n",
      "Epoch [77/100] Loss: 30.029869079589844\n",
      "Epoch [78/100] Loss: 29.596113204956055\n",
      "Epoch [79/100] Loss: 29.172840118408203\n",
      "Epoch [80/100] Loss: 28.759841918945312\n",
      "Epoch [81/100] Loss: 28.356908798217773\n",
      "Epoch [82/100] Loss: 27.963817596435547\n",
      "Epoch [83/100] Loss: 27.58037757873535\n",
      "Epoch [84/100] Loss: 27.206390380859375\n",
      "Epoch [85/100] Loss: 26.841650009155273\n",
      "Epoch [86/100] Loss: 26.485980987548828\n",
      "Epoch [87/100] Loss: 26.139171600341797\n",
      "Epoch [88/100] Loss: 25.80103302001953\n",
      "Epoch [89/100] Loss: 25.471399307250977\n",
      "Epoch [90/100] Loss: 25.15007781982422\n",
      "Epoch [91/100] Loss: 24.836896896362305\n",
      "Epoch [92/100] Loss: 24.531673431396484\n",
      "Epoch [93/100] Loss: 24.234237670898438\n",
      "Epoch [94/100] Loss: 23.94443130493164\n",
      "Epoch [95/100] Loss: 23.662076950073242\n",
      "Epoch [96/100] Loss: 23.387022018432617\n",
      "Epoch [97/100] Loss: 23.119096755981445\n",
      "Epoch [98/100] Loss: 22.858154296875\n",
      "Epoch [99/100] Loss: 22.604034423828125\n",
      "Epoch [100/100] Loss: 22.356597900390625\n",
      "Predicted days_remaining for parent_id 76: 8.979046821594238\n",
      "Training for parent_id 80...\n",
      "Epoch [1/100] Loss: 149.04632568359375\n",
      "Epoch [2/100] Loss: 143.1544189453125\n",
      "Epoch [3/100] Loss: 137.47911071777344\n",
      "Epoch [4/100] Loss: 132.04898071289062\n",
      "Epoch [5/100] Loss: 126.89727020263672\n",
      "Epoch [6/100] Loss: 122.04342651367188\n",
      "Epoch [7/100] Loss: 117.48866271972656\n",
      "Epoch [8/100] Loss: 113.21956634521484\n",
      "Epoch [9/100] Loss: 109.21479797363281\n",
      "Epoch [10/100] Loss: 105.45101165771484\n",
      "Epoch [11/100] Loss: 101.90682983398438\n",
      "Epoch [12/100] Loss: 98.56412506103516\n",
      "Epoch [13/100] Loss: 95.40818786621094\n",
      "Epoch [14/100] Loss: 92.42730712890625\n",
      "Epoch [15/100] Loss: 89.61225891113281\n",
      "Epoch [16/100] Loss: 86.95515441894531\n",
      "Epoch [17/100] Loss: 84.44828033447266\n",
      "Epoch [18/100] Loss: 82.08320617675781\n",
      "Epoch [19/100] Loss: 79.85057830810547\n",
      "Epoch [20/100] Loss: 77.74030303955078\n",
      "Epoch [21/100] Loss: 75.74188232421875\n",
      "Epoch [22/100] Loss: 73.8450927734375\n",
      "Epoch [23/100] Loss: 72.03995513916016\n",
      "Epoch [24/100] Loss: 70.31739807128906\n",
      "Epoch [25/100] Loss: 68.66917419433594\n",
      "Epoch [26/100] Loss: 67.08815002441406\n",
      "Epoch [27/100] Loss: 65.56851959228516\n",
      "Epoch [28/100] Loss: 64.10568237304688\n",
      "Epoch [29/100] Loss: 62.696407318115234\n",
      "Epoch [30/100] Loss: 61.338619232177734\n",
      "Epoch [31/100] Loss: 60.03110885620117\n",
      "Epoch [32/100] Loss: 58.773048400878906\n",
      "Epoch [33/100] Loss: 57.563541412353516\n",
      "Epoch [34/100] Loss: 56.40116500854492\n",
      "Epoch [35/100] Loss: 55.28384780883789\n",
      "Epoch [36/100] Loss: 54.209041595458984\n",
      "Epoch [37/100] Loss: 53.17378616333008\n",
      "Epoch [38/100] Loss: 52.175106048583984\n",
      "Epoch [39/100] Loss: 51.210060119628906\n",
      "Epoch [40/100] Loss: 50.27597427368164\n",
      "Epoch [41/100] Loss: 49.37046813964844\n",
      "Epoch [42/100] Loss: 48.491416931152344\n",
      "Epoch [43/100] Loss: 47.63703155517578\n",
      "Epoch [44/100] Loss: 46.80574035644531\n",
      "Epoch [45/100] Loss: 45.996185302734375\n",
      "Epoch [46/100] Loss: 45.20720291137695\n",
      "Epoch [47/100] Loss: 44.437774658203125\n",
      "Epoch [48/100] Loss: 43.6870002746582\n",
      "Epoch [49/100] Loss: 42.95407485961914\n",
      "Epoch [50/100] Loss: 42.23829650878906\n",
      "Epoch [51/100] Loss: 41.53901290893555\n",
      "Epoch [52/100] Loss: 40.85566329956055\n",
      "Epoch [53/100] Loss: 40.18771743774414\n",
      "Epoch [54/100] Loss: 39.53470230102539\n",
      "Epoch [55/100] Loss: 38.89619827270508\n",
      "Epoch [56/100] Loss: 38.271793365478516\n",
      "Epoch [57/100] Loss: 37.66114044189453\n",
      "Epoch [58/100] Loss: 37.06389617919922\n",
      "Epoch [59/100] Loss: 36.47975540161133\n",
      "Epoch [60/100] Loss: 35.90843200683594\n",
      "Epoch [61/100] Loss: 35.349639892578125\n",
      "Epoch [62/100] Loss: 34.803138732910156\n",
      "Epoch [63/100] Loss: 34.26869201660156\n",
      "Epoch [64/100] Loss: 33.746055603027344\n",
      "Epoch [65/100] Loss: 33.23501968383789\n",
      "Epoch [66/100] Loss: 32.7353630065918\n",
      "Epoch [67/100] Loss: 32.24690628051758\n",
      "Epoch [68/100] Loss: 31.76941680908203\n",
      "Epoch [69/100] Loss: 31.302719116210938\n",
      "Epoch [70/100] Loss: 30.846620559692383\n",
      "Epoch [71/100] Loss: 30.400930404663086\n",
      "Epoch [72/100] Loss: 29.965463638305664\n",
      "Epoch [73/100] Loss: 29.540037155151367\n",
      "Epoch [74/100] Loss: 29.124473571777344\n",
      "Epoch [75/100] Loss: 28.718595504760742\n",
      "Epoch [76/100] Loss: 28.322216033935547\n",
      "Epoch [77/100] Loss: 27.9351806640625\n",
      "Epoch [78/100] Loss: 27.557300567626953\n",
      "Epoch [79/100] Loss: 27.188396453857422\n",
      "Epoch [80/100] Loss: 26.828325271606445\n",
      "Epoch [81/100] Loss: 26.47689437866211\n",
      "Epoch [82/100] Loss: 26.133953094482422\n",
      "Epoch [83/100] Loss: 25.799341201782227\n",
      "Epoch [84/100] Loss: 25.472881317138672\n",
      "Epoch [85/100] Loss: 25.154430389404297\n",
      "Epoch [86/100] Loss: 24.84381675720215\n",
      "Epoch [87/100] Loss: 24.540889739990234\n",
      "Epoch [88/100] Loss: 24.245500564575195\n",
      "Epoch [89/100] Loss: 23.957496643066406\n",
      "Epoch [90/100] Loss: 23.676734924316406\n",
      "Epoch [91/100] Loss: 23.403057098388672\n",
      "Epoch [92/100] Loss: 23.136314392089844\n",
      "Epoch [93/100] Loss: 22.876388549804688\n",
      "Epoch [94/100] Loss: 22.62312126159668\n",
      "Epoch [95/100] Loss: 22.37637710571289\n",
      "Epoch [96/100] Loss: 22.136009216308594\n",
      "Epoch [97/100] Loss: 21.901899337768555\n",
      "Epoch [98/100] Loss: 21.67391586303711\n",
      "Epoch [99/100] Loss: 21.45191764831543\n",
      "Epoch [100/100] Loss: 21.235780715942383\n",
      "Predicted days_remaining for parent_id 80: 9.183311462402344\n",
      "Training for parent_id 82...\n",
      "Epoch [1/100] Loss: 292.2224426269531\n",
      "Epoch [2/100] Loss: 283.9437255859375\n",
      "Epoch [3/100] Loss: 275.9607849121094\n",
      "Epoch [4/100] Loss: 268.30694580078125\n",
      "Epoch [5/100] Loss: 261.00848388671875\n",
      "Epoch [6/100] Loss: 254.078125\n",
      "Epoch [7/100] Loss: 247.50877380371094\n",
      "Epoch [8/100] Loss: 241.28378295898438\n",
      "Epoch [9/100] Loss: 235.3893280029297\n",
      "Epoch [10/100] Loss: 229.81991577148438\n",
      "Epoch [11/100] Loss: 224.57684326171875\n",
      "Epoch [12/100] Loss: 219.6630401611328\n",
      "Epoch [13/100] Loss: 215.0782928466797\n",
      "Epoch [14/100] Loss: 210.81434631347656\n",
      "Epoch [15/100] Loss: 206.8519744873047\n",
      "Epoch [16/100] Loss: 203.16073608398438\n",
      "Epoch [17/100] Loss: 199.70330810546875\n",
      "Epoch [18/100] Loss: 196.44183349609375\n",
      "Epoch [19/100] Loss: 193.34255981445312\n",
      "Epoch [20/100] Loss: 190.3779296875\n",
      "Epoch [21/100] Loss: 187.526611328125\n",
      "Epoch [22/100] Loss: 184.77316284179688\n",
      "Epoch [23/100] Loss: 182.10658264160156\n",
      "Epoch [24/100] Loss: 179.51986694335938\n",
      "Epoch [25/100] Loss: 177.0084228515625\n",
      "Epoch [26/100] Loss: 174.56961059570312\n",
      "Epoch [27/100] Loss: 172.20172119140625\n",
      "Epoch [28/100] Loss: 169.9036102294922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] Loss: 167.67401123046875\n",
      "Epoch [30/100] Loss: 165.51144409179688\n",
      "Epoch [31/100] Loss: 163.41371154785156\n",
      "Epoch [32/100] Loss: 161.377685546875\n",
      "Epoch [33/100] Loss: 159.39947509765625\n",
      "Epoch [34/100] Loss: 157.4744110107422\n",
      "Epoch [35/100] Loss: 155.597412109375\n",
      "Epoch [36/100] Loss: 153.76353454589844\n",
      "Epoch [37/100] Loss: 151.9683074951172\n",
      "Epoch [38/100] Loss: 150.20770263671875\n",
      "Epoch [39/100] Loss: 148.4785919189453\n",
      "Epoch [40/100] Loss: 146.77833557128906\n",
      "Epoch [41/100] Loss: 145.104736328125\n",
      "Epoch [42/100] Loss: 143.45611572265625\n",
      "Epoch [43/100] Loss: 141.83108520507812\n",
      "Epoch [44/100] Loss: 140.22854614257812\n",
      "Epoch [45/100] Loss: 138.64743041992188\n",
      "Epoch [46/100] Loss: 137.0869903564453\n",
      "Epoch [47/100] Loss: 135.54649353027344\n",
      "Epoch [48/100] Loss: 134.0253448486328\n",
      "Epoch [49/100] Loss: 132.52294921875\n",
      "Epoch [50/100] Loss: 131.03884887695312\n",
      "Epoch [51/100] Loss: 129.5725555419922\n",
      "Epoch [52/100] Loss: 128.12368774414062\n",
      "Epoch [53/100] Loss: 126.69189453125\n",
      "Epoch [54/100] Loss: 125.27682495117188\n",
      "Epoch [55/100] Loss: 123.87816619873047\n",
      "Epoch [56/100] Loss: 122.49564361572266\n",
      "Epoch [57/100] Loss: 121.12895965576172\n",
      "Epoch [58/100] Loss: 119.77788543701172\n",
      "Epoch [59/100] Loss: 118.44214630126953\n",
      "Epoch [60/100] Loss: 117.12157440185547\n",
      "Epoch [61/100] Loss: 115.8158950805664\n",
      "Epoch [62/100] Loss: 114.52494812011719\n",
      "Epoch [63/100] Loss: 113.24852752685547\n",
      "Epoch [64/100] Loss: 111.9864273071289\n",
      "Epoch [65/100] Loss: 110.73848724365234\n",
      "Epoch [66/100] Loss: 109.50452423095703\n",
      "Epoch [67/100] Loss: 108.28434753417969\n",
      "Epoch [68/100] Loss: 107.07785034179688\n",
      "Epoch [69/100] Loss: 105.88480377197266\n",
      "Epoch [70/100] Loss: 104.70510864257812\n",
      "Epoch [71/100] Loss: 103.53857421875\n",
      "Epoch [72/100] Loss: 102.38510131835938\n",
      "Epoch [73/100] Loss: 101.24447631835938\n",
      "Epoch [74/100] Loss: 100.11660766601562\n",
      "Epoch [75/100] Loss: 99.0013198852539\n",
      "Epoch [76/100] Loss: 97.89851379394531\n",
      "Epoch [77/100] Loss: 96.80802154541016\n",
      "Epoch [78/100] Loss: 95.72972869873047\n",
      "Epoch [79/100] Loss: 94.66348266601562\n",
      "Epoch [80/100] Loss: 93.60919952392578\n",
      "Epoch [81/100] Loss: 92.56671142578125\n",
      "Epoch [82/100] Loss: 91.5359115600586\n",
      "Epoch [83/100] Loss: 90.5167007446289\n",
      "Epoch [84/100] Loss: 89.5088882446289\n",
      "Epoch [85/100] Loss: 88.5124282836914\n",
      "Epoch [86/100] Loss: 87.52716827392578\n",
      "Epoch [87/100] Loss: 86.55300903320312\n",
      "Epoch [88/100] Loss: 85.58984375\n",
      "Epoch [89/100] Loss: 84.63751983642578\n",
      "Epoch [90/100] Loss: 83.69595336914062\n",
      "Epoch [91/100] Loss: 82.76507568359375\n",
      "Epoch [92/100] Loss: 81.8447265625\n",
      "Epoch [93/100] Loss: 80.934814453125\n",
      "Epoch [94/100] Loss: 80.03523254394531\n",
      "Epoch [95/100] Loss: 79.14591979980469\n",
      "Epoch [96/100] Loss: 78.26669311523438\n",
      "Epoch [97/100] Loss: 77.39754486083984\n",
      "Epoch [98/100] Loss: 76.53832244873047\n",
      "Epoch [99/100] Loss: 75.68891143798828\n",
      "Epoch [100/100] Loss: 74.8492660522461\n",
      "Predicted days_remaining for parent_id 82: 9.031076431274414\n",
      "Training for parent_id 84...\n",
      "Epoch [1/100] Loss: 868.0431518554688\n",
      "Epoch [2/100] Loss: 852.1009521484375\n",
      "Epoch [3/100] Loss: 836.4551391601562\n",
      "Epoch [4/100] Loss: 821.339599609375\n",
      "Epoch [5/100] Loss: 806.9038696289062\n",
      "Epoch [6/100] Loss: 793.19580078125\n",
      "Epoch [7/100] Loss: 780.1931762695312\n",
      "Epoch [8/100] Loss: 767.848876953125\n",
      "Epoch [9/100] Loss: 756.1202392578125\n",
      "Epoch [10/100] Loss: 744.972900390625\n",
      "Epoch [11/100] Loss: 734.373779296875\n",
      "Epoch [12/100] Loss: 724.2882080078125\n",
      "Epoch [13/100] Loss: 714.6797485351562\n",
      "Epoch [14/100] Loss: 705.51318359375\n",
      "Epoch [15/100] Loss: 696.7583618164062\n",
      "Epoch [16/100] Loss: 688.3898315429688\n",
      "Epoch [17/100] Loss: 680.3866577148438\n",
      "Epoch [18/100] Loss: 672.7300415039062\n",
      "Epoch [19/100] Loss: 665.4015502929688\n",
      "Epoch [20/100] Loss: 658.3814697265625\n",
      "Epoch [21/100] Loss: 651.6509399414062\n",
      "Epoch [22/100] Loss: 645.1915283203125\n",
      "Epoch [23/100] Loss: 638.9868774414062\n",
      "Epoch [24/100] Loss: 633.0225830078125\n",
      "Epoch [25/100] Loss: 627.2860107421875\n",
      "Epoch [26/100] Loss: 621.7654418945312\n",
      "Epoch [27/100] Loss: 616.4495239257812\n",
      "Epoch [28/100] Loss: 611.3267211914062\n",
      "Epoch [29/100] Loss: 606.3844604492188\n",
      "Epoch [30/100] Loss: 601.609375\n",
      "Epoch [31/100] Loss: 596.9878540039062\n",
      "Epoch [32/100] Loss: 592.5057373046875\n",
      "Epoch [33/100] Loss: 588.1493530273438\n",
      "Epoch [34/100] Loss: 583.9057006835938\n",
      "Epoch [35/100] Loss: 579.762939453125\n",
      "Epoch [36/100] Loss: 575.7100219726562\n",
      "Epoch [37/100] Loss: 571.7374267578125\n",
      "Epoch [38/100] Loss: 567.8368530273438\n",
      "Epoch [39/100] Loss: 564.0009765625\n",
      "Epoch [40/100] Loss: 560.2234497070312\n",
      "Epoch [41/100] Loss: 556.4994506835938\n",
      "Epoch [42/100] Loss: 552.8239135742188\n",
      "Epoch [43/100] Loss: 549.1934814453125\n",
      "Epoch [44/100] Loss: 545.6044311523438\n",
      "Epoch [45/100] Loss: 542.0540161132812\n",
      "Epoch [46/100] Loss: 538.53955078125\n",
      "Epoch [47/100] Loss: 535.0587158203125\n",
      "Epoch [48/100] Loss: 531.6091918945312\n",
      "Epoch [49/100] Loss: 528.1891479492188\n",
      "Epoch [50/100] Loss: 524.796875\n",
      "Epoch [51/100] Loss: 521.4310302734375\n",
      "Epoch [52/100] Loss: 518.0905151367188\n",
      "Epoch [53/100] Loss: 514.7742309570312\n",
      "Epoch [54/100] Loss: 511.4813537597656\n",
      "Epoch [55/100] Loss: 508.21136474609375\n",
      "Epoch [56/100] Loss: 504.96368408203125\n",
      "Epoch [57/100] Loss: 501.7375793457031\n",
      "Epoch [58/100] Loss: 498.5330810546875\n",
      "Epoch [59/100] Loss: 495.3497009277344\n",
      "Epoch [60/100] Loss: 492.1870422363281\n",
      "Epoch [61/100] Loss: 489.045166015625\n",
      "Epoch [62/100] Loss: 485.923828125\n",
      "Epoch [63/100] Loss: 482.8228759765625\n",
      "Epoch [64/100] Loss: 479.7420654296875\n",
      "Epoch [65/100] Loss: 476.6814270019531\n",
      "Epoch [66/100] Loss: 473.6410217285156\n",
      "Epoch [67/100] Loss: 470.6205139160156\n",
      "Epoch [68/100] Loss: 467.6198425292969\n",
      "Epoch [69/100] Loss: 464.6390380859375\n",
      "Epoch [70/100] Loss: 461.67791748046875\n",
      "Epoch [71/100] Loss: 458.73626708984375\n",
      "Epoch [72/100] Loss: 455.8140869140625\n",
      "Epoch [73/100] Loss: 452.9112243652344\n",
      "Epoch [74/100] Loss: 450.02740478515625\n",
      "Epoch [75/100] Loss: 447.16259765625\n",
      "Epoch [76/100] Loss: 444.31658935546875\n",
      "Epoch [77/100] Loss: 441.4891357421875\n",
      "Epoch [78/100] Loss: 438.68017578125\n",
      "Epoch [79/100] Loss: 435.8894348144531\n",
      "Epoch [80/100] Loss: 433.1168212890625\n",
      "Epoch [81/100] Loss: 430.3620300292969\n",
      "Epoch [82/100] Loss: 427.6250305175781\n",
      "Epoch [83/100] Loss: 424.9056701660156\n",
      "Epoch [84/100] Loss: 422.2034912109375\n",
      "Epoch [85/100] Loss: 419.5186462402344\n",
      "Epoch [86/100] Loss: 416.8507995605469\n",
      "Epoch [87/100] Loss: 414.1997985839844\n",
      "Epoch [88/100] Loss: 411.5655517578125\n",
      "Epoch [89/100] Loss: 408.9478759765625\n",
      "Epoch [90/100] Loss: 406.3465881347656\n",
      "Epoch [91/100] Loss: 403.76141357421875\n",
      "Epoch [92/100] Loss: 401.1925354003906\n",
      "Epoch [93/100] Loss: 398.639404296875\n",
      "Epoch [94/100] Loss: 396.10223388671875\n",
      "Epoch [95/100] Loss: 393.5806884765625\n",
      "Epoch [96/100] Loss: 391.0746154785156\n",
      "Epoch [97/100] Loss: 388.5838928222656\n",
      "Epoch [98/100] Loss: 386.1084899902344\n",
      "Epoch [99/100] Loss: 383.6481628417969\n",
      "Epoch [100/100] Loss: 381.2027282714844\n",
      "Predicted days_remaining for parent_id 84: 9.662445068359375\n",
      "Training for parent_id 88...\n",
      "Epoch [1/100] Loss: 152.80889892578125\n",
      "Epoch [2/100] Loss: 147.39913940429688\n",
      "Epoch [3/100] Loss: 142.23733520507812\n",
      "Epoch [4/100] Loss: 137.3337860107422\n",
      "Epoch [5/100] Loss: 132.67578125\n",
      "Epoch [6/100] Loss: 128.24192810058594\n",
      "Epoch [7/100] Loss: 124.0065689086914\n",
      "Epoch [8/100] Loss: 119.94576263427734\n",
      "Epoch [9/100] Loss: 116.04498291015625\n",
      "Epoch [10/100] Loss: 112.29962921142578\n",
      "Epoch [11/100] Loss: 108.71118927001953\n",
      "Epoch [12/100] Loss: 105.28304290771484\n",
      "Epoch [13/100] Loss: 102.01832580566406\n",
      "Epoch [14/100] Loss: 98.91852569580078\n",
      "Epoch [15/100] Loss: 95.98289489746094\n",
      "Epoch [16/100] Loss: 93.2078857421875\n",
      "Epoch [17/100] Loss: 90.58712768554688\n",
      "Epoch [18/100] Loss: 88.11197662353516\n",
      "Epoch [19/100] Loss: 85.7724380493164\n",
      "Epoch [20/100] Loss: 83.55828857421875\n",
      "Epoch [21/100] Loss: 81.45963287353516\n",
      "Epoch [22/100] Loss: 79.46721649169922\n",
      "Epoch [23/100] Loss: 77.572509765625\n",
      "Epoch [24/100] Loss: 75.76795959472656\n",
      "Epoch [25/100] Loss: 74.04685974121094\n",
      "Epoch [26/100] Loss: 72.40341186523438\n",
      "Epoch [27/100] Loss: 70.83253479003906\n",
      "Epoch [28/100] Loss: 69.32982635498047\n",
      "Epoch [29/100] Loss: 67.89118194580078\n",
      "Epoch [30/100] Loss: 66.51285552978516\n",
      "Epoch [31/100] Loss: 65.19107818603516\n",
      "Epoch [32/100] Loss: 63.922245025634766\n",
      "Epoch [33/100] Loss: 62.702659606933594\n",
      "Epoch [34/100] Loss: 61.528743743896484\n",
      "Epoch [35/100] Loss: 60.39695739746094\n",
      "Epoch [36/100] Loss: 59.303958892822266\n",
      "Epoch [37/100] Loss: 58.24660110473633\n",
      "Epoch [38/100] Loss: 57.22196578979492\n",
      "Epoch [39/100] Loss: 56.227439880371094\n",
      "Epoch [40/100] Loss: 55.260684967041016\n",
      "Epoch [41/100] Loss: 54.319618225097656\n",
      "Epoch [42/100] Loss: 53.40244674682617\n",
      "Epoch [43/100] Loss: 52.50754928588867\n",
      "Epoch [44/100] Loss: 51.63355255126953\n",
      "Epoch [45/100] Loss: 50.779258728027344\n",
      "Epoch [46/100] Loss: 49.9435920715332\n",
      "Epoch [47/100] Loss: 49.12568283081055\n",
      "Epoch [48/100] Loss: 48.32466506958008\n",
      "Epoch [49/100] Loss: 47.539886474609375\n",
      "Epoch [50/100] Loss: 46.77070617675781\n",
      "Epoch [51/100] Loss: 46.0166015625\n",
      "Epoch [52/100] Loss: 45.277122497558594\n",
      "Epoch [53/100] Loss: 44.55186462402344\n",
      "Epoch [54/100] Loss: 43.84052276611328\n",
      "Epoch [55/100] Loss: 43.142784118652344\n",
      "Epoch [56/100] Loss: 42.458431243896484\n",
      "Epoch [57/100] Loss: 41.78727340698242\n",
      "Epoch [58/100] Loss: 41.1291389465332\n",
      "Epoch [59/100] Loss: 40.48386001586914\n",
      "Epoch [60/100] Loss: 39.85131072998047\n",
      "Epoch [61/100] Loss: 39.231361389160156\n",
      "Epoch [62/100] Loss: 38.62385559082031\n",
      "Epoch [63/100] Loss: 38.028656005859375\n",
      "Epoch [64/100] Loss: 37.445621490478516\n",
      "Epoch [65/100] Loss: 36.87458419799805\n",
      "Epoch [66/100] Loss: 36.31541061401367\n",
      "Epoch [67/100] Loss: 35.767906188964844\n",
      "Epoch [68/100] Loss: 35.23191833496094\n",
      "Epoch [69/100] Loss: 34.70726776123047\n",
      "Epoch [70/100] Loss: 34.19377517700195\n",
      "Epoch [71/100] Loss: 33.69126892089844\n",
      "Epoch [72/100] Loss: 33.19957733154297\n",
      "Epoch [73/100] Loss: 32.71851348876953\n",
      "Epoch [74/100] Loss: 32.24789047241211\n",
      "Epoch [75/100] Loss: 31.78754234313965\n",
      "Epoch [76/100] Loss: 31.3372802734375\n",
      "Epoch [77/100] Loss: 30.896940231323242\n",
      "Epoch [78/100] Loss: 30.466337203979492\n",
      "Epoch [79/100] Loss: 30.045312881469727\n",
      "Epoch [80/100] Loss: 29.633686065673828\n",
      "Epoch [81/100] Loss: 29.231290817260742\n",
      "Epoch [82/100] Loss: 28.83795166015625\n",
      "Epoch [83/100] Loss: 28.45352554321289\n",
      "Epoch [84/100] Loss: 28.077831268310547\n",
      "Epoch [85/100] Loss: 27.71072769165039\n",
      "Epoch [86/100] Loss: 27.35202407836914\n",
      "Epoch [87/100] Loss: 27.001605987548828\n",
      "Epoch [88/100] Loss: 26.659282684326172\n",
      "Epoch [89/100] Loss: 26.324941635131836\n",
      "Epoch [90/100] Loss: 25.99839210510254\n",
      "Epoch [91/100] Loss: 25.67951202392578\n",
      "Epoch [92/100] Loss: 25.368148803710938\n",
      "Epoch [93/100] Loss: 25.064149856567383\n",
      "Epoch [94/100] Loss: 24.767404556274414\n",
      "Epoch [95/100] Loss: 24.477739334106445\n",
      "Epoch [96/100] Loss: 24.19503402709961\n",
      "Epoch [97/100] Loss: 23.919147491455078\n",
      "Epoch [98/100] Loss: 23.649944305419922\n",
      "Epoch [99/100] Loss: 23.387304306030273\n",
      "Epoch [100/100] Loss: 23.131080627441406\n",
      "Predicted days_remaining for parent_id 88: 8.84420108795166\n",
      "Training for parent_id 91...\n",
      "Epoch [1/100] Loss: 1166.7896728515625\n",
      "Epoch [2/100] Loss: 1146.5494384765625\n",
      "Epoch [3/100] Loss: 1126.7977294921875\n",
      "Epoch [4/100] Loss: 1107.6446533203125\n",
      "Epoch [5/100] Loss: 1089.1907958984375\n",
      "Epoch [6/100] Loss: 1071.518798828125\n",
      "Epoch [7/100] Loss: 1054.6993408203125\n",
      "Epoch [8/100] Loss: 1038.7711181640625\n",
      "Epoch [9/100] Loss: 1023.7238159179688\n",
      "Epoch [10/100] Loss: 1009.5060424804688\n",
      "Epoch [11/100] Loss: 996.0428466796875\n",
      "Epoch [12/100] Loss: 983.25634765625\n",
      "Epoch [13/100] Loss: 971.080322265625\n",
      "Epoch [14/100] Loss: 959.4619140625\n",
      "Epoch [15/100] Loss: 948.3616333007812\n",
      "Epoch [16/100] Loss: 937.7490844726562\n",
      "Epoch [17/100] Loss: 927.6001586914062\n",
      "Epoch [18/100] Loss: 917.8932495117188\n",
      "Epoch [19/100] Loss: 908.6085205078125\n",
      "Epoch [20/100] Loss: 899.7265625\n",
      "Epoch [21/100] Loss: 891.2279052734375\n",
      "Epoch [22/100] Loss: 883.0933837890625\n",
      "Epoch [23/100] Loss: 875.3037719726562\n",
      "Epoch [24/100] Loss: 867.8395385742188\n",
      "Epoch [25/100] Loss: 860.6810302734375\n",
      "Epoch [26/100] Loss: 853.8084716796875\n",
      "Epoch [27/100] Loss: 847.2019653320312\n",
      "Epoch [28/100] Loss: 840.8417358398438\n",
      "Epoch [29/100] Loss: 834.7080078125\n",
      "Epoch [30/100] Loss: 828.781982421875\n",
      "Epoch [31/100] Loss: 823.0447387695312\n",
      "Epoch [32/100] Loss: 817.4788818359375\n",
      "Epoch [33/100] Loss: 812.0684814453125\n",
      "Epoch [34/100] Loss: 806.7989501953125\n",
      "Epoch [35/100] Loss: 801.6575317382812\n",
      "Epoch [36/100] Loss: 796.6336669921875\n",
      "Epoch [37/100] Loss: 791.7178955078125\n",
      "Epoch [38/100] Loss: 786.90185546875\n",
      "Epoch [39/100] Loss: 782.1778564453125\n",
      "Epoch [40/100] Loss: 777.5379028320312\n",
      "Epoch [41/100] Loss: 772.9739990234375\n",
      "Epoch [42/100] Loss: 768.4783935546875\n",
      "Epoch [43/100] Loss: 764.04345703125\n",
      "Epoch [44/100] Loss: 759.6621704101562\n",
      "Epoch [45/100] Loss: 755.3289184570312\n",
      "Epoch [46/100] Loss: 751.0383911132812\n",
      "Epoch [47/100] Loss: 746.7872924804688\n",
      "Epoch [48/100] Loss: 742.572265625\n",
      "Epoch [49/100] Loss: 738.3911743164062\n",
      "Epoch [50/100] Loss: 734.2423706054688\n",
      "Epoch [51/100] Loss: 730.1246337890625\n",
      "Epoch [52/100] Loss: 726.0366821289062\n",
      "Epoch [53/100] Loss: 721.9777221679688\n",
      "Epoch [54/100] Loss: 717.9472045898438\n",
      "Epoch [55/100] Loss: 713.9442138671875\n",
      "Epoch [56/100] Loss: 709.96826171875\n",
      "Epoch [57/100] Loss: 706.0185546875\n",
      "Epoch [58/100] Loss: 702.0946044921875\n",
      "Epoch [59/100] Loss: 698.19580078125\n",
      "Epoch [60/100] Loss: 694.3214721679688\n",
      "Epoch [61/100] Loss: 690.4708862304688\n",
      "Epoch [62/100] Loss: 686.6436767578125\n",
      "Epoch [63/100] Loss: 682.8390502929688\n",
      "Epoch [64/100] Loss: 679.0565185546875\n",
      "Epoch [65/100] Loss: 675.2955932617188\n",
      "Epoch [66/100] Loss: 671.555908203125\n",
      "Epoch [67/100] Loss: 667.837158203125\n",
      "Epoch [68/100] Loss: 664.1387329101562\n",
      "Epoch [69/100] Loss: 660.4608764648438\n",
      "Epoch [70/100] Loss: 656.8029174804688\n",
      "Epoch [71/100] Loss: 653.1651000976562\n",
      "Epoch [72/100] Loss: 649.5473022460938\n",
      "Epoch [73/100] Loss: 645.949462890625\n",
      "Epoch [74/100] Loss: 642.3717651367188\n",
      "Epoch [75/100] Loss: 638.8139038085938\n",
      "Epoch [76/100] Loss: 635.2763671875\n",
      "Epoch [77/100] Loss: 631.7587280273438\n",
      "Epoch [78/100] Loss: 628.26123046875\n",
      "Epoch [79/100] Loss: 624.7836303710938\n",
      "Epoch [80/100] Loss: 621.3262939453125\n",
      "Epoch [81/100] Loss: 617.888671875\n",
      "Epoch [82/100] Loss: 614.4708862304688\n",
      "Epoch [83/100] Loss: 611.0730590820312\n",
      "Epoch [84/100] Loss: 607.6947631835938\n",
      "Epoch [85/100] Loss: 604.3359985351562\n",
      "Epoch [86/100] Loss: 600.9966430664062\n",
      "Epoch [87/100] Loss: 597.6766357421875\n",
      "Epoch [88/100] Loss: 594.3756713867188\n",
      "Epoch [89/100] Loss: 591.0935668945312\n",
      "Epoch [90/100] Loss: 587.8305053710938\n",
      "Epoch [91/100] Loss: 584.5859375\n",
      "Epoch [92/100] Loss: 581.3596801757812\n",
      "Epoch [93/100] Loss: 578.151611328125\n",
      "Epoch [94/100] Loss: 574.9616088867188\n",
      "Epoch [95/100] Loss: 571.7894897460938\n",
      "Epoch [96/100] Loss: 568.6348876953125\n",
      "Epoch [97/100] Loss: 565.4976806640625\n",
      "Epoch [98/100] Loss: 562.3773193359375\n",
      "Epoch [99/100] Loss: 559.2740478515625\n",
      "Epoch [100/100] Loss: 556.1873779296875\n",
      "Predicted days_remaining for parent_id 91: 10.540532112121582\n",
      "Training for parent_id 96...\n",
      "Epoch [1/100] Loss: 1084.9559326171875\n",
      "Epoch [2/100] Loss: 1070.941162109375\n",
      "Epoch [3/100] Loss: 1057.2760009765625\n",
      "Epoch [4/100] Loss: 1044.015625\n",
      "Epoch [5/100] Loss: 1031.16162109375\n",
      "Epoch [6/100] Loss: 1018.68212890625\n",
      "Epoch [7/100] Loss: 1006.5474243164062\n",
      "Epoch [8/100] Loss: 994.7289428710938\n",
      "Epoch [9/100] Loss: 983.1948852539062\n",
      "Epoch [10/100] Loss: 971.9132080078125\n",
      "Epoch [11/100] Loss: 960.8552856445312\n",
      "Epoch [12/100] Loss: 950.0000610351562\n",
      "Epoch [13/100] Loss: 939.3358154296875\n",
      "Epoch [14/100] Loss: 928.8619384765625\n",
      "Epoch [15/100] Loss: 918.587646484375\n",
      "Epoch [16/100] Loss: 908.5297241210938\n",
      "Epoch [17/100] Loss: 898.7086181640625\n",
      "Epoch [18/100] Loss: 889.1461181640625\n",
      "Epoch [19/100] Loss: 879.861572265625\n",
      "Epoch [20/100] Loss: 870.8700561523438\n",
      "Epoch [21/100] Loss: 862.1817016601562\n",
      "Epoch [22/100] Loss: 853.801513671875\n",
      "Epoch [23/100] Loss: 845.729248046875\n",
      "Epoch [24/100] Loss: 837.9601440429688\n",
      "Epoch [25/100] Loss: 830.4849243164062\n",
      "Epoch [26/100] Loss: 823.2918701171875\n",
      "Epoch [27/100] Loss: 816.3658447265625\n",
      "Epoch [28/100] Loss: 809.6903686523438\n",
      "Epoch [29/100] Loss: 803.2479858398438\n",
      "Epoch [30/100] Loss: 797.0205688476562\n",
      "Epoch [31/100] Loss: 790.9906005859375\n",
      "Epoch [32/100] Loss: 785.1409912109375\n",
      "Epoch [33/100] Loss: 779.4555053710938\n",
      "Epoch [34/100] Loss: 773.9192504882812\n",
      "Epoch [35/100] Loss: 768.5188598632812\n",
      "Epoch [36/100] Loss: 763.2427368164062\n",
      "Epoch [37/100] Loss: 758.08056640625\n",
      "Epoch [38/100] Loss: 753.0235595703125\n",
      "Epoch [39/100] Loss: 748.0646362304688\n",
      "Epoch [40/100] Loss: 743.197509765625\n",
      "Epoch [41/100] Loss: 738.4164428710938\n",
      "Epoch [42/100] Loss: 733.7163696289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100] Loss: 729.0922241210938\n",
      "Epoch [44/100] Loss: 724.539306640625\n",
      "Epoch [45/100] Loss: 720.0529174804688\n",
      "Epoch [46/100] Loss: 715.6287231445312\n",
      "Epoch [47/100] Loss: 711.2625732421875\n",
      "Epoch [48/100] Loss: 706.950927734375\n",
      "Epoch [49/100] Loss: 702.6903686523438\n",
      "Epoch [50/100] Loss: 698.47802734375\n",
      "Epoch [51/100] Loss: 694.3110961914062\n",
      "Epoch [52/100] Loss: 690.1874389648438\n",
      "Epoch [53/100] Loss: 686.1051025390625\n",
      "Epoch [54/100] Loss: 682.062255859375\n",
      "Epoch [55/100] Loss: 678.056884765625\n",
      "Epoch [56/100] Loss: 674.0877685546875\n",
      "Epoch [57/100] Loss: 670.1535034179688\n",
      "Epoch [58/100] Loss: 666.2529907226562\n",
      "Epoch [59/100] Loss: 662.3849487304688\n",
      "Epoch [60/100] Loss: 658.54833984375\n",
      "Epoch [61/100] Loss: 654.7424926757812\n",
      "Epoch [62/100] Loss: 650.96630859375\n",
      "Epoch [63/100] Loss: 647.21923828125\n",
      "Epoch [64/100] Loss: 643.5003662109375\n",
      "Epoch [65/100] Loss: 639.8091430664062\n",
      "Epoch [66/100] Loss: 636.1448364257812\n",
      "Epoch [67/100] Loss: 632.5071411132812\n",
      "Epoch [68/100] Loss: 628.8951416015625\n",
      "Epoch [69/100] Loss: 625.3086547851562\n",
      "Epoch [70/100] Loss: 621.7470703125\n",
      "Epoch [71/100] Loss: 618.2098999023438\n",
      "Epoch [72/100] Loss: 614.69677734375\n",
      "Epoch [73/100] Loss: 611.2071533203125\n",
      "Epoch [74/100] Loss: 607.7407836914062\n",
      "Epoch [75/100] Loss: 604.2972412109375\n",
      "Epoch [76/100] Loss: 600.8761596679688\n",
      "Epoch [77/100] Loss: 597.4771118164062\n",
      "Epoch [78/100] Loss: 594.099853515625\n",
      "Epoch [79/100] Loss: 590.7440795898438\n",
      "Epoch [80/100] Loss: 587.409423828125\n",
      "Epoch [81/100] Loss: 584.0955200195312\n",
      "Epoch [82/100] Loss: 580.8023071289062\n",
      "Epoch [83/100] Loss: 577.5292358398438\n",
      "Epoch [84/100] Loss: 574.2761840820312\n",
      "Epoch [85/100] Loss: 571.0427856445312\n",
      "Epoch [86/100] Loss: 567.8289794921875\n",
      "Epoch [87/100] Loss: 564.6342163085938\n",
      "Epoch [88/100] Loss: 561.4586181640625\n",
      "Epoch [89/100] Loss: 558.3016967773438\n",
      "Epoch [90/100] Loss: 555.1632690429688\n",
      "Epoch [91/100] Loss: 552.043212890625\n",
      "Epoch [92/100] Loss: 548.941162109375\n",
      "Epoch [93/100] Loss: 545.8570556640625\n",
      "Epoch [94/100] Loss: 542.7907104492188\n",
      "Epoch [95/100] Loss: 539.741943359375\n",
      "Epoch [96/100] Loss: 536.7103881835938\n",
      "Epoch [97/100] Loss: 533.6959228515625\n",
      "Epoch [98/100] Loss: 530.698486328125\n",
      "Epoch [99/100] Loss: 527.7179565429688\n",
      "Epoch [100/100] Loss: 524.7538452148438\n",
      "Predicted days_remaining for parent_id 96: 10.225151062011719\n",
      "Training for parent_id 99...\n",
      "Epoch [1/100] Loss: 127.65857696533203\n",
      "Epoch [2/100] Loss: 122.45471954345703\n",
      "Epoch [3/100] Loss: 117.47917938232422\n",
      "Epoch [4/100] Loss: 112.7371826171875\n",
      "Epoch [5/100] Loss: 108.23255157470703\n",
      "Epoch [6/100] Loss: 103.95878601074219\n",
      "Epoch [7/100] Loss: 99.90401458740234\n",
      "Epoch [8/100] Loss: 96.05559539794922\n",
      "Epoch [9/100] Loss: 92.40151977539062\n",
      "Epoch [10/100] Loss: 88.93094635009766\n",
      "Epoch [11/100] Loss: 85.63404846191406\n",
      "Epoch [12/100] Loss: 82.50167846679688\n",
      "Epoch [13/100] Loss: 79.52557373046875\n",
      "Epoch [14/100] Loss: 76.69807434082031\n",
      "Epoch [15/100] Loss: 74.01226043701172\n",
      "Epoch [16/100] Loss: 71.46163177490234\n",
      "Epoch [17/100] Loss: 69.03995513916016\n",
      "Epoch [18/100] Loss: 66.74098205566406\n",
      "Epoch [19/100] Loss: 64.55850982666016\n",
      "Epoch [20/100] Loss: 62.48634719848633\n",
      "Epoch [21/100] Loss: 60.518436431884766\n",
      "Epoch [22/100] Loss: 58.64911651611328\n",
      "Epoch [23/100] Loss: 56.8730583190918\n",
      "Epoch [24/100] Loss: 55.18544006347656\n",
      "Epoch [25/100] Loss: 53.58174514770508\n",
      "Epoch [26/100] Loss: 52.05765914916992\n",
      "Epoch [27/100] Loss: 50.60889434814453\n",
      "Epoch [28/100] Loss: 49.231143951416016\n",
      "Epoch [29/100] Loss: 47.920005798339844\n",
      "Epoch [30/100] Loss: 46.670982360839844\n",
      "Epoch [31/100] Loss: 45.47954177856445\n",
      "Epoch [32/100] Loss: 44.34132766723633\n",
      "Epoch [33/100] Loss: 43.252166748046875\n",
      "Epoch [34/100] Loss: 42.208213806152344\n",
      "Epoch [35/100] Loss: 41.20596694946289\n",
      "Epoch [36/100] Loss: 40.242366790771484\n",
      "Epoch [37/100] Loss: 39.314720153808594\n",
      "Epoch [38/100] Loss: 38.42072296142578\n",
      "Epoch [39/100] Loss: 37.558387756347656\n",
      "Epoch [40/100] Loss: 36.726051330566406\n",
      "Epoch [41/100] Loss: 35.92225646972656\n",
      "Epoch [42/100] Loss: 35.145782470703125\n",
      "Epoch [43/100] Loss: 34.39558792114258\n",
      "Epoch [44/100] Loss: 33.67073440551758\n",
      "Epoch [45/100] Loss: 32.970428466796875\n",
      "Epoch [46/100] Loss: 32.29393005371094\n",
      "Epoch [47/100] Loss: 31.64053726196289\n",
      "Epoch [48/100] Loss: 31.009626388549805\n",
      "Epoch [49/100] Loss: 30.400535583496094\n",
      "Epoch [50/100] Loss: 29.81263542175293\n",
      "Epoch [51/100] Loss: 29.245250701904297\n",
      "Epoch [52/100] Loss: 28.69774627685547\n",
      "Epoch [53/100] Loss: 28.169462203979492\n",
      "Epoch [54/100] Loss: 27.659744262695312\n",
      "Epoch [55/100] Loss: 27.16795539855957\n",
      "Epoch [56/100] Loss: 26.693462371826172\n",
      "Epoch [57/100] Loss: 26.23567008972168\n",
      "Epoch [58/100] Loss: 25.793991088867188\n",
      "Epoch [59/100] Loss: 25.367862701416016\n",
      "Epoch [60/100] Loss: 24.95674705505371\n",
      "Epoch [61/100] Loss: 24.560108184814453\n",
      "Epoch [62/100] Loss: 24.177444458007812\n",
      "Epoch [63/100] Loss: 23.808269500732422\n",
      "Epoch [64/100] Loss: 23.452091217041016\n",
      "Epoch [65/100] Loss: 23.108455657958984\n",
      "Epoch [66/100] Loss: 22.776914596557617\n",
      "Epoch [67/100] Loss: 22.45703125\n",
      "Epoch [68/100] Loss: 22.14838218688965\n",
      "Epoch [69/100] Loss: 21.85057830810547\n",
      "Epoch [70/100] Loss: 21.56321907043457\n",
      "Epoch [71/100] Loss: 21.285930633544922\n",
      "Epoch [72/100] Loss: 21.01835823059082\n",
      "Epoch [73/100] Loss: 20.760154724121094\n",
      "Epoch [74/100] Loss: 20.511009216308594\n",
      "Epoch [75/100] Loss: 20.270584106445312\n",
      "Epoch [76/100] Loss: 20.038593292236328\n",
      "Epoch [77/100] Loss: 19.81475067138672\n",
      "Epoch [78/100] Loss: 19.598773956298828\n",
      "Epoch [79/100] Loss: 19.390398025512695\n",
      "Epoch [80/100] Loss: 19.189369201660156\n",
      "Epoch [81/100] Loss: 18.99544906616211\n",
      "Epoch [82/100] Loss: 18.808395385742188\n",
      "Epoch [83/100] Loss: 18.62799072265625\n",
      "Epoch [84/100] Loss: 18.45401954650879\n",
      "Epoch [85/100] Loss: 18.286266326904297\n",
      "Epoch [86/100] Loss: 18.124536514282227\n",
      "Epoch [87/100] Loss: 17.968643188476562\n",
      "Epoch [88/100] Loss: 17.818389892578125\n",
      "Epoch [89/100] Loss: 17.673606872558594\n",
      "Epoch [90/100] Loss: 17.53411293029785\n",
      "Epoch [91/100] Loss: 17.399749755859375\n",
      "Epoch [92/100] Loss: 17.270355224609375\n",
      "Epoch [93/100] Loss: 17.14577293395996\n",
      "Epoch [94/100] Loss: 17.025854110717773\n",
      "Epoch [95/100] Loss: 16.91045570373535\n",
      "Epoch [96/100] Loss: 16.799434661865234\n",
      "Epoch [97/100] Loss: 16.69266128540039\n",
      "Epoch [98/100] Loss: 16.59000015258789\n",
      "Epoch [99/100] Loss: 16.491321563720703\n",
      "Epoch [100/100] Loss: 16.396509170532227\n",
      "Predicted days_remaining for parent_id 99: 9.383275985717773\n",
      "Training for parent_id 103...\n",
      "Epoch [1/100] Loss: 178.2300567626953\n",
      "Epoch [2/100] Loss: 171.9058380126953\n",
      "Epoch [3/100] Loss: 165.7631072998047\n",
      "Epoch [4/100] Loss: 159.8553466796875\n",
      "Epoch [5/100] Loss: 154.22323608398438\n",
      "Epoch [6/100] Loss: 148.8879852294922\n",
      "Epoch [7/100] Loss: 143.84963989257812\n",
      "Epoch [8/100] Loss: 139.09405517578125\n",
      "Epoch [9/100] Loss: 134.60079956054688\n",
      "Epoch [10/100] Loss: 130.3489532470703\n",
      "Epoch [11/100] Loss: 126.32020568847656\n",
      "Epoch [12/100] Loss: 122.49946594238281\n",
      "Epoch [13/100] Loss: 118.87446594238281\n",
      "Epoch [14/100] Loss: 115.43505859375\n",
      "Epoch [15/100] Loss: 112.17237091064453\n",
      "Epoch [16/100] Loss: 109.077880859375\n",
      "Epoch [17/100] Loss: 106.14295196533203\n",
      "Epoch [18/100] Loss: 103.3586196899414\n",
      "Epoch [19/100] Loss: 100.71556854248047\n",
      "Epoch [20/100] Loss: 98.20436096191406\n",
      "Epoch [21/100] Loss: 95.81565856933594\n",
      "Epoch [22/100] Loss: 93.54031372070312\n",
      "Epoch [23/100] Loss: 91.36958312988281\n",
      "Epoch [24/100] Loss: 89.29496765136719\n",
      "Epoch [25/100] Loss: 87.30846405029297\n",
      "Epoch [26/100] Loss: 85.40242767333984\n",
      "Epoch [27/100] Loss: 83.56988525390625\n",
      "Epoch [28/100] Loss: 81.80441284179688\n",
      "Epoch [29/100] Loss: 80.10026550292969\n",
      "Epoch [30/100] Loss: 78.45233917236328\n",
      "Epoch [31/100] Loss: 76.85598754882812\n",
      "Epoch [32/100] Loss: 75.30703735351562\n",
      "Epoch [33/100] Loss: 73.80170440673828\n",
      "Epoch [34/100] Loss: 72.33660888671875\n",
      "Epoch [35/100] Loss: 70.9088363647461\n",
      "Epoch [36/100] Loss: 69.51587677001953\n",
      "Epoch [37/100] Loss: 68.15572357177734\n",
      "Epoch [38/100] Loss: 66.82683563232422\n",
      "Epoch [39/100] Loss: 65.52802276611328\n",
      "Epoch [40/100] Loss: 64.25862121582031\n",
      "Epoch [41/100] Loss: 63.01826095581055\n",
      "Epoch [42/100] Loss: 61.806880950927734\n",
      "Epoch [43/100] Loss: 60.62461853027344\n",
      "Epoch [44/100] Loss: 59.471641540527344\n",
      "Epoch [45/100] Loss: 58.34817123413086\n",
      "Epoch [46/100] Loss: 57.254215240478516\n",
      "Epoch [47/100] Loss: 56.189598083496094\n",
      "Epoch [48/100] Loss: 55.15389633178711\n",
      "Epoch [49/100] Loss: 54.146446228027344\n",
      "Epoch [50/100] Loss: 53.16642379760742\n",
      "Epoch [51/100] Loss: 52.21289825439453\n",
      "Epoch [52/100] Loss: 51.284854888916016\n",
      "Epoch [53/100] Loss: 50.38134765625\n",
      "Epoch [54/100] Loss: 49.50138473510742\n",
      "Epoch [55/100] Loss: 48.64405059814453\n",
      "Epoch [56/100] Loss: 47.80850601196289\n",
      "Epoch [57/100] Loss: 46.99392318725586\n",
      "Epoch [58/100] Loss: 46.19956588745117\n",
      "Epoch [59/100] Loss: 45.42472457885742\n",
      "Epoch [60/100] Loss: 44.668739318847656\n",
      "Epoch [61/100] Loss: 43.930992126464844\n",
      "Epoch [62/100] Loss: 43.21089553833008\n",
      "Epoch [63/100] Loss: 42.507904052734375\n",
      "Epoch [64/100] Loss: 41.821502685546875\n",
      "Epoch [65/100] Loss: 41.15119552612305\n",
      "Epoch [66/100] Loss: 40.49651336669922\n",
      "Epoch [67/100] Loss: 39.85700225830078\n",
      "Epoch [68/100] Loss: 39.23225021362305\n",
      "Epoch [69/100] Loss: 38.62187576293945\n",
      "Epoch [70/100] Loss: 38.02547073364258\n",
      "Epoch [71/100] Loss: 37.44267272949219\n",
      "Epoch [72/100] Loss: 36.87314224243164\n",
      "Epoch [73/100] Loss: 36.3165397644043\n",
      "Epoch [74/100] Loss: 35.77254867553711\n",
      "Epoch [75/100] Loss: 35.240875244140625\n",
      "Epoch [76/100] Loss: 34.72118377685547\n",
      "Epoch [77/100] Loss: 34.213226318359375\n",
      "Epoch [78/100] Loss: 33.716732025146484\n",
      "Epoch [79/100] Loss: 33.23141860961914\n",
      "Epoch [80/100] Loss: 32.75703048706055\n",
      "Epoch [81/100] Loss: 32.29334259033203\n",
      "Epoch [82/100] Loss: 31.840091705322266\n",
      "Epoch [83/100] Loss: 31.397071838378906\n",
      "Epoch [84/100] Loss: 30.964046478271484\n",
      "Epoch [85/100] Loss: 30.540809631347656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100] Loss: 30.127140045166016\n",
      "Epoch [87/100] Loss: 29.72283935546875\n",
      "Epoch [88/100] Loss: 29.32771110534668\n",
      "Epoch [89/100] Loss: 28.94156265258789\n",
      "Epoch [90/100] Loss: 28.564208984375\n",
      "Epoch [91/100] Loss: 28.195451736450195\n",
      "Epoch [92/100] Loss: 27.835140228271484\n",
      "Epoch [93/100] Loss: 27.483078002929688\n",
      "Epoch [94/100] Loss: 27.13909339904785\n",
      "Epoch [95/100] Loss: 26.803049087524414\n",
      "Epoch [96/100] Loss: 26.47475242614746\n",
      "Epoch [97/100] Loss: 26.15406036376953\n",
      "Epoch [98/100] Loss: 25.840810775756836\n",
      "Epoch [99/100] Loss: 25.53485870361328\n",
      "Epoch [100/100] Loss: 25.236061096191406\n",
      "Predicted days_remaining for parent_id 103: 9.508586883544922\n",
      "Training for parent_id 105...\n",
      "Epoch [1/100] Loss: 299.25140380859375\n",
      "Epoch [2/100] Loss: 291.9168701171875\n",
      "Epoch [3/100] Loss: 284.85992431640625\n",
      "Epoch [4/100] Loss: 278.04803466796875\n",
      "Epoch [5/100] Loss: 271.44500732421875\n",
      "Epoch [6/100] Loss: 265.00982666015625\n",
      "Epoch [7/100] Loss: 258.7059326171875\n",
      "Epoch [8/100] Loss: 252.50927734375\n",
      "Epoch [9/100] Loss: 246.41094970703125\n",
      "Epoch [10/100] Loss: 240.414794921875\n",
      "Epoch [11/100] Loss: 234.53468322753906\n",
      "Epoch [12/100] Loss: 228.7914581298828\n",
      "Epoch [13/100] Loss: 223.20904541015625\n",
      "Epoch [14/100] Loss: 217.81109619140625\n",
      "Epoch [15/100] Loss: 212.61734008789062\n",
      "Epoch [16/100] Loss: 207.64109802246094\n",
      "Epoch [17/100] Loss: 202.8884735107422\n",
      "Epoch [18/100] Loss: 198.35914611816406\n",
      "Epoch [19/100] Loss: 194.0482177734375\n",
      "Epoch [20/100] Loss: 189.9479522705078\n",
      "Epoch [21/100] Loss: 186.04864501953125\n",
      "Epoch [22/100] Loss: 182.33984375\n",
      "Epoch [23/100] Loss: 178.81048583984375\n",
      "Epoch [24/100] Loss: 175.44956970214844\n",
      "Epoch [25/100] Loss: 172.2461395263672\n",
      "Epoch [26/100] Loss: 169.18931579589844\n",
      "Epoch [27/100] Loss: 166.26832580566406\n",
      "Epoch [28/100] Loss: 163.47259521484375\n",
      "Epoch [29/100] Loss: 160.7919921875\n",
      "Epoch [30/100] Loss: 158.21681213378906\n",
      "Epoch [31/100] Loss: 155.73800659179688\n",
      "Epoch [32/100] Loss: 153.34727478027344\n",
      "Epoch [33/100] Loss: 151.03720092773438\n",
      "Epoch [34/100] Loss: 148.80128479003906\n",
      "Epoch [35/100] Loss: 146.6337432861328\n",
      "Epoch [36/100] Loss: 144.52952575683594\n",
      "Epoch [37/100] Loss: 142.48431396484375\n",
      "Epoch [38/100] Loss: 140.49407958984375\n",
      "Epoch [39/100] Loss: 138.55532836914062\n",
      "Epoch [40/100] Loss: 136.6646270751953\n",
      "Epoch [41/100] Loss: 134.81906127929688\n",
      "Epoch [42/100] Loss: 133.01580810546875\n",
      "Epoch [43/100] Loss: 131.252197265625\n",
      "Epoch [44/100] Loss: 129.52590942382812\n",
      "Epoch [45/100] Loss: 127.83470916748047\n",
      "Epoch [46/100] Loss: 126.17662811279297\n",
      "Epoch [47/100] Loss: 124.54989624023438\n",
      "Epoch [48/100] Loss: 122.95281219482422\n",
      "Epoch [49/100] Loss: 121.38394927978516\n",
      "Epoch [50/100] Loss: 119.84194946289062\n",
      "Epoch [51/100] Loss: 118.3255615234375\n",
      "Epoch [52/100] Loss: 116.83373260498047\n",
      "Epoch [53/100] Loss: 115.36542510986328\n",
      "Epoch [54/100] Loss: 113.91968536376953\n",
      "Epoch [55/100] Loss: 112.49573516845703\n",
      "Epoch [56/100] Loss: 111.09271240234375\n",
      "Epoch [57/100] Loss: 109.70988464355469\n",
      "Epoch [58/100] Loss: 108.34669494628906\n",
      "Epoch [59/100] Loss: 107.00242614746094\n",
      "Epoch [60/100] Loss: 105.67660522460938\n",
      "Epoch [61/100] Loss: 104.36871337890625\n",
      "Epoch [62/100] Loss: 103.07833099365234\n",
      "Epoch [63/100] Loss: 101.8050537109375\n",
      "Epoch [64/100] Loss: 100.54857635498047\n",
      "Epoch [65/100] Loss: 99.30859375\n",
      "Epoch [66/100] Loss: 98.0848617553711\n",
      "Epoch [67/100] Loss: 96.87706756591797\n",
      "Epoch [68/100] Loss: 95.68505859375\n",
      "Epoch [69/100] Loss: 94.50860595703125\n",
      "Epoch [70/100] Loss: 93.34748077392578\n",
      "Epoch [71/100] Loss: 92.2015151977539\n",
      "Epoch [72/100] Loss: 91.07044982910156\n",
      "Epoch [73/100] Loss: 89.95416259765625\n",
      "Epoch [74/100] Loss: 88.85240173339844\n",
      "Epoch [75/100] Loss: 87.7650146484375\n",
      "Epoch [76/100] Loss: 86.6917495727539\n",
      "Epoch [77/100] Loss: 85.6324691772461\n",
      "Epoch [78/100] Loss: 84.58699798583984\n",
      "Epoch [79/100] Loss: 83.55509185791016\n",
      "Epoch [80/100] Loss: 82.53662872314453\n",
      "Epoch [81/100] Loss: 81.53136444091797\n",
      "Epoch [82/100] Loss: 80.53921508789062\n",
      "Epoch [83/100] Loss: 79.55990600585938\n",
      "Epoch [84/100] Loss: 78.59334564208984\n",
      "Epoch [85/100] Loss: 77.63936614990234\n",
      "Epoch [86/100] Loss: 76.69776153564453\n",
      "Epoch [87/100] Loss: 75.76839447021484\n",
      "Epoch [88/100] Loss: 74.85113525390625\n",
      "Epoch [89/100] Loss: 73.9458236694336\n",
      "Epoch [90/100] Loss: 73.05228424072266\n",
      "Epoch [91/100] Loss: 72.1703872680664\n",
      "Epoch [92/100] Loss: 71.29999542236328\n",
      "Epoch [93/100] Loss: 70.44094848632812\n",
      "Epoch [94/100] Loss: 69.59315490722656\n",
      "Epoch [95/100] Loss: 68.75641632080078\n",
      "Epoch [96/100] Loss: 67.9306411743164\n",
      "Epoch [97/100] Loss: 67.11568450927734\n",
      "Epoch [98/100] Loss: 66.31140899658203\n",
      "Epoch [99/100] Loss: 65.51771545410156\n",
      "Epoch [100/100] Loss: 64.73445129394531\n",
      "Predicted days_remaining for parent_id 105: 9.712672233581543\n",
      "Training for parent_id 107...\n",
      "Epoch [1/100] Loss: 130.09567260742188\n",
      "Epoch [2/100] Loss: 125.00800323486328\n",
      "Epoch [3/100] Loss: 120.13043975830078\n",
      "Epoch [4/100] Loss: 115.4936752319336\n",
      "Epoch [5/100] Loss: 111.10460662841797\n",
      "Epoch [6/100] Loss: 106.95537567138672\n",
      "Epoch [7/100] Loss: 103.03269958496094\n",
      "Epoch [8/100] Loss: 99.32159423828125\n",
      "Epoch [9/100] Loss: 95.80725860595703\n",
      "Epoch [10/100] Loss: 92.47710418701172\n",
      "Epoch [11/100] Loss: 89.32112121582031\n",
      "Epoch [12/100] Loss: 86.33100128173828\n",
      "Epoch [13/100] Loss: 83.4989013671875\n",
      "Epoch [14/100] Loss: 80.81690979003906\n",
      "Epoch [15/100] Loss: 78.27692413330078\n",
      "Epoch [16/100] Loss: 75.87060546875\n",
      "Epoch [17/100] Loss: 73.589599609375\n",
      "Epoch [18/100] Loss: 71.425537109375\n",
      "Epoch [19/100] Loss: 69.37019348144531\n",
      "Epoch [20/100] Loss: 67.41584777832031\n",
      "Epoch [21/100] Loss: 65.55518341064453\n",
      "Epoch [22/100] Loss: 63.781585693359375\n",
      "Epoch [23/100] Loss: 62.089054107666016\n",
      "Epoch [24/100] Loss: 60.47222137451172\n",
      "Epoch [25/100] Loss: 58.92627716064453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] Loss: 57.44697189331055\n",
      "Epoch [27/100] Loss: 56.03053665161133\n",
      "Epoch [28/100] Loss: 54.673583984375\n",
      "Epoch [29/100] Loss: 53.37309646606445\n",
      "Epoch [30/100] Loss: 52.126312255859375\n",
      "Epoch [31/100] Loss: 50.930538177490234\n",
      "Epoch [32/100] Loss: 49.783172607421875\n",
      "Epoch [33/100] Loss: 48.68156433105469\n",
      "Epoch [34/100] Loss: 47.623023986816406\n",
      "Epoch [35/100] Loss: 46.604835510253906\n",
      "Epoch [36/100] Loss: 45.6242790222168\n",
      "Epoch [37/100] Loss: 44.67867660522461\n",
      "Epoch [38/100] Loss: 43.76549530029297\n",
      "Epoch [39/100] Loss: 42.882381439208984\n",
      "Epoch [40/100] Loss: 42.027198791503906\n",
      "Epoch [41/100] Loss: 41.198081970214844\n",
      "Epoch [42/100] Loss: 40.3934211730957\n",
      "Epoch [43/100] Loss: 39.6119270324707\n",
      "Epoch [44/100] Loss: 38.85248947143555\n",
      "Epoch [45/100] Loss: 38.11422348022461\n",
      "Epoch [46/100] Loss: 37.39639663696289\n",
      "Epoch [47/100] Loss: 36.69839859008789\n",
      "Epoch [48/100] Loss: 36.0196647644043\n",
      "Epoch [49/100] Loss: 35.359703063964844\n",
      "Epoch [50/100] Loss: 34.7180061340332\n",
      "Epoch [51/100] Loss: 34.09413146972656\n",
      "Epoch [52/100] Loss: 33.48760986328125\n",
      "Epoch [53/100] Loss: 32.89799499511719\n",
      "Epoch [54/100] Loss: 32.32484436035156\n",
      "Epoch [55/100] Loss: 31.76772117614746\n",
      "Epoch [56/100] Loss: 31.226205825805664\n",
      "Epoch [57/100] Loss: 30.699888229370117\n",
      "Epoch [58/100] Loss: 30.1883602142334\n",
      "Epoch [59/100] Loss: 29.691240310668945\n",
      "Epoch [60/100] Loss: 29.208154678344727\n",
      "Epoch [61/100] Loss: 28.738727569580078\n",
      "Epoch [62/100] Loss: 28.282615661621094\n",
      "Epoch [63/100] Loss: 27.83945655822754\n",
      "Epoch [64/100] Loss: 27.408931732177734\n",
      "Epoch [65/100] Loss: 26.990694046020508\n",
      "Epoch [66/100] Loss: 26.584436416625977\n",
      "Epoch [67/100] Loss: 26.189847946166992\n",
      "Epoch [68/100] Loss: 25.806621551513672\n",
      "Epoch [69/100] Loss: 25.434463500976562\n",
      "Epoch [70/100] Loss: 25.073078155517578\n",
      "Epoch [71/100] Loss: 24.722183227539062\n",
      "Epoch [72/100] Loss: 24.381515502929688\n",
      "Epoch [73/100] Loss: 24.050798416137695\n",
      "Epoch [74/100] Loss: 23.729766845703125\n",
      "Epoch [75/100] Loss: 23.418182373046875\n",
      "Epoch [76/100] Loss: 23.115779876708984\n",
      "Epoch [77/100] Loss: 22.822324752807617\n",
      "Epoch [78/100] Loss: 22.537593841552734\n",
      "Epoch [79/100] Loss: 22.26133918762207\n",
      "Epoch [80/100] Loss: 21.99333953857422\n",
      "Epoch [81/100] Loss: 21.733396530151367\n",
      "Epoch [82/100] Loss: 21.48128890991211\n",
      "Epoch [83/100] Loss: 21.236801147460938\n",
      "Epoch [84/100] Loss: 20.999736785888672\n",
      "Epoch [85/100] Loss: 20.769908905029297\n",
      "Epoch [86/100] Loss: 20.547115325927734\n",
      "Epoch [87/100] Loss: 20.331180572509766\n",
      "Epoch [88/100] Loss: 20.121915817260742\n",
      "Epoch [89/100] Loss: 19.919145584106445\n",
      "Epoch [90/100] Loss: 19.72269058227539\n",
      "Epoch [91/100] Loss: 19.532398223876953\n",
      "Epoch [92/100] Loss: 19.34808349609375\n",
      "Epoch [93/100] Loss: 19.169599533081055\n",
      "Epoch [94/100] Loss: 18.996788024902344\n",
      "Epoch [95/100] Loss: 18.829490661621094\n",
      "Epoch [96/100] Loss: 18.667564392089844\n",
      "Epoch [97/100] Loss: 18.51085662841797\n",
      "Epoch [98/100] Loss: 18.359228134155273\n",
      "Epoch [99/100] Loss: 18.21254539489746\n",
      "Epoch [100/100] Loss: 18.070655822753906\n",
      "Predicted days_remaining for parent_id 107: 8.880255699157715\n",
      "Training for parent_id 113...\n",
      "Epoch [1/100] Loss: 647.2086791992188\n",
      "Epoch [2/100] Loss: 635.6268310546875\n",
      "Epoch [3/100] Loss: 624.1961669921875\n",
      "Epoch [4/100] Loss: 612.9520263671875\n",
      "Epoch [5/100] Loss: 601.9050903320312\n",
      "Epoch [6/100] Loss: 591.0777587890625\n",
      "Epoch [7/100] Loss: 580.494140625\n",
      "Epoch [8/100] Loss: 570.1773071289062\n",
      "Epoch [9/100] Loss: 560.1464233398438\n",
      "Epoch [10/100] Loss: 550.41650390625\n",
      "Epoch [11/100] Loss: 541.0007934570312\n",
      "Epoch [12/100] Loss: 531.9120483398438\n",
      "Epoch [13/100] Loss: 523.1615600585938\n",
      "Epoch [14/100] Loss: 514.7589111328125\n",
      "Epoch [15/100] Loss: 506.7093811035156\n",
      "Epoch [16/100] Loss: 499.01373291015625\n",
      "Epoch [17/100] Loss: 491.6690979003906\n",
      "Epoch [18/100] Loss: 484.6693420410156\n",
      "Epoch [19/100] Loss: 478.0049743652344\n",
      "Epoch [20/100] Loss: 471.6631774902344\n",
      "Epoch [21/100] Loss: 465.6280822753906\n",
      "Epoch [22/100] Loss: 459.88104248046875\n",
      "Epoch [23/100] Loss: 454.4022216796875\n",
      "Epoch [24/100] Loss: 449.1706237792969\n",
      "Epoch [25/100] Loss: 444.1656494140625\n",
      "Epoch [26/100] Loss: 439.36767578125\n",
      "Epoch [27/100] Loss: 434.7579040527344\n",
      "Epoch [28/100] Loss: 430.31927490234375\n",
      "Epoch [29/100] Loss: 426.0357360839844\n",
      "Epoch [30/100] Loss: 421.8927917480469\n",
      "Epoch [31/100] Loss: 417.8773193359375\n",
      "Epoch [32/100] Loss: 413.9770202636719\n",
      "Epoch [33/100] Loss: 410.1810607910156\n",
      "Epoch [34/100] Loss: 406.4794921875\n",
      "Epoch [35/100] Loss: 402.863525390625\n",
      "Epoch [36/100] Loss: 399.3255310058594\n",
      "Epoch [37/100] Loss: 395.8583984375\n",
      "Epoch [38/100] Loss: 392.456298828125\n",
      "Epoch [39/100] Loss: 389.1136474609375\n",
      "Epoch [40/100] Loss: 385.8255920410156\n",
      "Epoch [41/100] Loss: 382.5879821777344\n",
      "Epoch [42/100] Loss: 379.3968200683594\n",
      "Epoch [43/100] Loss: 376.2489013671875\n",
      "Epoch [44/100] Loss: 373.1413269042969\n",
      "Epoch [45/100] Loss: 370.071533203125\n",
      "Epoch [46/100] Loss: 367.0375061035156\n",
      "Epoch [47/100] Loss: 364.0374450683594\n",
      "Epoch [48/100] Loss: 361.06976318359375\n",
      "Epoch [49/100] Loss: 358.13336181640625\n",
      "Epoch [50/100] Loss: 355.22723388671875\n",
      "Epoch [51/100] Loss: 352.3504333496094\n",
      "Epoch [52/100] Loss: 349.5022888183594\n",
      "Epoch [53/100] Loss: 346.68243408203125\n",
      "Epoch [54/100] Loss: 343.8902587890625\n",
      "Epoch [55/100] Loss: 341.125244140625\n",
      "Epoch [56/100] Loss: 338.3872375488281\n",
      "Epoch [57/100] Loss: 335.6758117675781\n",
      "Epoch [58/100] Loss: 332.99066162109375\n",
      "Epoch [59/100] Loss: 330.331298828125\n",
      "Epoch [60/100] Loss: 327.6976013183594\n",
      "Epoch [61/100] Loss: 325.08917236328125\n",
      "Epoch [62/100] Loss: 322.5054626464844\n",
      "Epoch [63/100] Loss: 319.9462890625\n",
      "Epoch [64/100] Loss: 317.4111633300781\n",
      "Epoch [65/100] Loss: 314.8998107910156\n",
      "Epoch [66/100] Loss: 312.4117126464844\n",
      "Epoch [67/100] Loss: 309.9465637207031\n",
      "Epoch [68/100] Loss: 307.5038757324219\n",
      "Epoch [69/100] Loss: 305.083251953125\n",
      "Epoch [70/100] Loss: 302.6844177246094\n",
      "Epoch [71/100] Loss: 300.3069152832031\n",
      "Epoch [72/100] Loss: 297.95025634765625\n",
      "Epoch [73/100] Loss: 295.6144104003906\n",
      "Epoch [74/100] Loss: 293.2986145019531\n",
      "Epoch [75/100] Loss: 291.0028076171875\n",
      "Epoch [76/100] Loss: 288.72662353515625\n",
      "Epoch [77/100] Loss: 286.4696350097656\n",
      "Epoch [78/100] Loss: 284.23162841796875\n",
      "Epoch [79/100] Loss: 282.012451171875\n",
      "Epoch [80/100] Loss: 279.8115234375\n",
      "Epoch [81/100] Loss: 277.62872314453125\n",
      "Epoch [82/100] Loss: 275.4638366699219\n",
      "Epoch [83/100] Loss: 273.3165588378906\n",
      "Epoch [84/100] Loss: 271.18658447265625\n",
      "Epoch [85/100] Loss: 269.0737609863281\n",
      "Epoch [86/100] Loss: 266.9779052734375\n",
      "Epoch [87/100] Loss: 264.8986511230469\n",
      "Epoch [88/100] Loss: 262.83587646484375\n",
      "Epoch [89/100] Loss: 260.7893371582031\n",
      "Epoch [90/100] Loss: 258.7588806152344\n",
      "Epoch [91/100] Loss: 256.7442932128906\n",
      "Epoch [92/100] Loss: 254.7454071044922\n",
      "Epoch [93/100] Loss: 252.76199340820312\n",
      "Epoch [94/100] Loss: 250.7938690185547\n",
      "Epoch [95/100] Loss: 248.84095764160156\n",
      "Epoch [96/100] Loss: 246.90296936035156\n",
      "Epoch [97/100] Loss: 244.97982788085938\n",
      "Epoch [98/100] Loss: 243.07125854492188\n",
      "Epoch [99/100] Loss: 241.17727661132812\n",
      "Epoch [100/100] Loss: 239.29762268066406\n",
      "Predicted days_remaining for parent_id 113: 9.816993713378906\n",
      "Training for parent_id 123...\n",
      "Epoch [1/100] Loss: 786.1299438476562\n",
      "Epoch [2/100] Loss: 772.7835693359375\n",
      "Epoch [3/100] Loss: 759.7655029296875\n",
      "Epoch [4/100] Loss: 747.1729125976562\n",
      "Epoch [5/100] Loss: 735.0750122070312\n",
      "Epoch [6/100] Loss: 723.515625\n",
      "Epoch [7/100] Loss: 712.5079956054688\n",
      "Epoch [8/100] Loss: 702.0377807617188\n",
      "Epoch [9/100] Loss: 692.0703735351562\n",
      "Epoch [10/100] Loss: 682.5623168945312\n",
      "Epoch [11/100] Loss: 673.4694213867188\n",
      "Epoch [12/100] Loss: 664.7520751953125\n",
      "Epoch [13/100] Loss: 656.377685546875\n",
      "Epoch [14/100] Loss: 648.3212890625\n",
      "Epoch [15/100] Loss: 640.5665893554688\n",
      "Epoch [16/100] Loss: 633.1075439453125\n",
      "Epoch [17/100] Loss: 625.9462280273438\n",
      "Epoch [18/100] Loss: 619.0892333984375\n",
      "Epoch [19/100] Loss: 612.5408325195312\n",
      "Epoch [20/100] Loss: 606.29736328125\n",
      "Epoch [21/100] Loss: 600.3446044921875\n",
      "Epoch [22/100] Loss: 594.6602172851562\n",
      "Epoch [23/100] Loss: 589.2175903320312\n",
      "Epoch [24/100] Loss: 583.9896240234375\n",
      "Epoch [25/100] Loss: 578.9510498046875\n",
      "Epoch [26/100] Loss: 574.079345703125\n",
      "Epoch [27/100] Loss: 569.3548583984375\n",
      "Epoch [28/100] Loss: 564.7603759765625\n",
      "Epoch [29/100] Loss: 560.2808227539062\n",
      "Epoch [30/100] Loss: 555.9026489257812\n",
      "Epoch [31/100] Loss: 551.6134033203125\n",
      "Epoch [32/100] Loss: 547.4017333984375\n",
      "Epoch [33/100] Loss: 543.2564086914062\n",
      "Epoch [34/100] Loss: 539.1671142578125\n",
      "Epoch [35/100] Loss: 535.1239013671875\n",
      "Epoch [36/100] Loss: 531.1177978515625\n",
      "Epoch [37/100] Loss: 527.14208984375\n",
      "Epoch [38/100] Loss: 523.1926879882812\n",
      "Epoch [39/100] Loss: 519.2700805664062\n",
      "Epoch [40/100] Loss: 515.3782348632812\n",
      "Epoch [41/100] Loss: 511.5250244140625\n",
      "Epoch [42/100] Loss: 507.719482421875\n",
      "Epoch [43/100] Loss: 503.96954345703125\n",
      "Epoch [44/100] Loss: 500.2807922363281\n",
      "Epoch [45/100] Loss: 496.6549377441406\n",
      "Epoch [46/100] Loss: 493.09033203125\n",
      "Epoch [47/100] Loss: 489.5829162597656\n",
      "Epoch [48/100] Loss: 486.12744140625\n",
      "Epoch [49/100] Loss: 482.7186584472656\n",
      "Epoch [50/100] Loss: 479.3518371582031\n",
      "Epoch [51/100] Loss: 476.0230712890625\n",
      "Epoch [52/100] Loss: 472.7292175292969\n",
      "Epoch [53/100] Loss: 469.46783447265625\n",
      "Epoch [54/100] Loss: 466.2371826171875\n",
      "Epoch [55/100] Loss: 463.0359191894531\n",
      "Epoch [56/100] Loss: 459.8630065917969\n",
      "Epoch [57/100] Loss: 456.7176818847656\n",
      "Epoch [58/100] Loss: 453.59912109375\n",
      "Epoch [59/100] Loss: 450.5070495605469\n",
      "Epoch [60/100] Loss: 447.4406433105469\n",
      "Epoch [61/100] Loss: 444.39971923828125\n",
      "Epoch [62/100] Loss: 441.38360595703125\n",
      "Epoch [63/100] Loss: 438.39202880859375\n",
      "Epoch [64/100] Loss: 435.424560546875\n",
      "Epoch [65/100] Loss: 432.4807434082031\n",
      "Epoch [66/100] Loss: 429.5602111816406\n",
      "Epoch [67/100] Loss: 426.6625061035156\n",
      "Epoch [68/100] Loss: 423.7874755859375\n",
      "Epoch [69/100] Loss: 420.9344482421875\n",
      "Epoch [70/100] Loss: 418.1032409667969\n",
      "Epoch [71/100] Loss: 415.2935485839844\n",
      "Epoch [72/100] Loss: 412.5048522949219\n",
      "Epoch [73/100] Loss: 409.7369689941406\n",
      "Epoch [74/100] Loss: 406.989501953125\n",
      "Epoch [75/100] Loss: 404.26226806640625\n",
      "Epoch [76/100] Loss: 401.5549011230469\n",
      "Epoch [77/100] Loss: 398.8670959472656\n",
      "Epoch [78/100] Loss: 396.1984558105469\n",
      "Epoch [79/100] Loss: 393.5490417480469\n",
      "Epoch [80/100] Loss: 390.91827392578125\n",
      "Epoch [81/100] Loss: 388.3060607910156\n",
      "Epoch [82/100] Loss: 385.71221923828125\n",
      "Epoch [83/100] Loss: 383.1363220214844\n",
      "Epoch [84/100] Loss: 380.5782775878906\n",
      "Epoch [85/100] Loss: 378.0379638671875\n",
      "Epoch [86/100] Loss: 375.5148620605469\n",
      "Epoch [87/100] Loss: 373.0090026855469\n",
      "Epoch [88/100] Loss: 370.5201416015625\n",
      "Epoch [89/100] Loss: 368.04803466796875\n",
      "Epoch [90/100] Loss: 365.5923767089844\n",
      "Epoch [91/100] Loss: 363.15338134765625\n",
      "Epoch [92/100] Loss: 360.7304992675781\n",
      "Epoch [93/100] Loss: 358.3236999511719\n",
      "Epoch [94/100] Loss: 355.9326477050781\n",
      "Epoch [95/100] Loss: 353.5574645996094\n",
      "Epoch [96/100] Loss: 351.1977233886719\n",
      "Epoch [97/100] Loss: 348.8534240722656\n",
      "Epoch [98/100] Loss: 346.5243835449219\n",
      "Epoch [99/100] Loss: 344.2103576660156\n",
      "Epoch [100/100] Loss: 341.9112548828125\n",
      "Predicted days_remaining for parent_id 123: 9.716987609863281\n",
      "Training for parent_id 124...\n",
      "Epoch [1/100] Loss: 232.7918243408203\n",
      "Epoch [2/100] Loss: 226.18386840820312\n",
      "Epoch [3/100] Loss: 219.69696044921875\n",
      "Epoch [4/100] Loss: 213.3429718017578\n",
      "Epoch [5/100] Loss: 207.13026428222656\n",
      "Epoch [6/100] Loss: 201.0798797607422\n",
      "Epoch [7/100] Loss: 195.22003173828125\n",
      "Epoch [8/100] Loss: 189.5756072998047\n",
      "Epoch [9/100] Loss: 184.1629180908203\n",
      "Epoch [10/100] Loss: 178.98968505859375\n",
      "Epoch [11/100] Loss: 174.05770874023438\n",
      "Epoch [12/100] Loss: 169.36537170410156\n",
      "Epoch [13/100] Loss: 164.90902709960938\n",
      "Epoch [14/100] Loss: 160.6839599609375\n",
      "Epoch [15/100] Loss: 156.6847686767578\n",
      "Epoch [16/100] Loss: 152.9057159423828\n",
      "Epoch [17/100] Loss: 149.33990478515625\n",
      "Epoch [18/100] Loss: 145.97903442382812\n",
      "Epoch [19/100] Loss: 142.81272888183594\n",
      "Epoch [20/100] Loss: 139.82899475097656\n",
      "Epoch [21/100] Loss: 137.0145263671875\n",
      "Epoch [22/100] Loss: 134.35549926757812\n",
      "Epoch [23/100] Loss: 131.8380126953125\n",
      "Epoch [24/100] Loss: 129.44863891601562\n",
      "Epoch [25/100] Loss: 127.1745834350586\n",
      "Epoch [26/100] Loss: 125.00392150878906\n",
      "Epoch [27/100] Loss: 122.9256362915039\n",
      "Epoch [28/100] Loss: 120.9298095703125\n",
      "Epoch [29/100] Loss: 119.00762939453125\n",
      "Epoch [30/100] Loss: 117.15128326416016\n",
      "Epoch [31/100] Loss: 115.35406494140625\n",
      "Epoch [32/100] Loss: 113.61023712158203\n",
      "Epoch [33/100] Loss: 111.9148178100586\n",
      "Epoch [34/100] Loss: 110.26362609863281\n",
      "Epoch [35/100] Loss: 108.65296173095703\n",
      "Epoch [36/100] Loss: 107.07977294921875\n",
      "Epoch [37/100] Loss: 105.54143524169922\n",
      "Epoch [38/100] Loss: 104.03559112548828\n",
      "Epoch [39/100] Loss: 102.560302734375\n",
      "Epoch [40/100] Loss: 101.1138687133789\n",
      "Epoch [41/100] Loss: 99.69479370117188\n",
      "Epoch [42/100] Loss: 98.30180358886719\n",
      "Epoch [43/100] Loss: 96.9336929321289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100] Loss: 95.58948516845703\n",
      "Epoch [45/100] Loss: 94.2682113647461\n",
      "Epoch [46/100] Loss: 92.96900939941406\n",
      "Epoch [47/100] Loss: 91.6910171508789\n",
      "Epoch [48/100] Loss: 90.43354797363281\n",
      "Epoch [49/100] Loss: 89.19583129882812\n",
      "Epoch [50/100] Loss: 87.97713470458984\n",
      "Epoch [51/100] Loss: 86.77678680419922\n",
      "Epoch [52/100] Loss: 85.59417724609375\n",
      "Epoch [53/100] Loss: 84.42861938476562\n",
      "Epoch [54/100] Loss: 83.27952575683594\n",
      "Epoch [55/100] Loss: 82.14627838134766\n",
      "Epoch [56/100] Loss: 81.02835845947266\n",
      "Epoch [57/100] Loss: 79.9251708984375\n",
      "Epoch [58/100] Loss: 78.83628845214844\n",
      "Epoch [59/100] Loss: 77.76121520996094\n",
      "Epoch [60/100] Loss: 76.69959259033203\n",
      "Epoch [61/100] Loss: 75.65110778808594\n",
      "Epoch [62/100] Loss: 74.61548614501953\n",
      "Epoch [63/100] Loss: 73.59259796142578\n",
      "Epoch [64/100] Loss: 72.58233642578125\n",
      "Epoch [65/100] Loss: 71.58466339111328\n",
      "Epoch [66/100] Loss: 70.59961700439453\n",
      "Epoch [67/100] Loss: 69.6272964477539\n",
      "Epoch [68/100] Loss: 68.66781616210938\n",
      "Epoch [69/100] Loss: 67.72128295898438\n",
      "Epoch [70/100] Loss: 66.78778839111328\n",
      "Epoch [71/100] Loss: 65.8674545288086\n",
      "Epoch [72/100] Loss: 64.96038055419922\n",
      "Epoch [73/100] Loss: 64.06654357910156\n",
      "Epoch [74/100] Loss: 63.18600845336914\n",
      "Epoch [75/100] Loss: 62.31871795654297\n",
      "Epoch [76/100] Loss: 61.46460723876953\n",
      "Epoch [77/100] Loss: 60.62361526489258\n",
      "Epoch [78/100] Loss: 59.795623779296875\n",
      "Epoch [79/100] Loss: 58.98049545288086\n",
      "Epoch [80/100] Loss: 58.17810821533203\n",
      "Epoch [81/100] Loss: 57.38832473754883\n",
      "Epoch [82/100] Loss: 56.6109504699707\n",
      "Epoch [83/100] Loss: 55.845855712890625\n",
      "Epoch [84/100] Loss: 55.092864990234375\n",
      "Epoch [85/100] Loss: 54.35182571411133\n",
      "Epoch [86/100] Loss: 53.62257385253906\n",
      "Epoch [87/100] Loss: 52.904937744140625\n",
      "Epoch [88/100] Loss: 52.19876480102539\n",
      "Epoch [89/100] Loss: 51.50386047363281\n",
      "Epoch [90/100] Loss: 50.82011413574219\n",
      "Epoch [91/100] Loss: 50.14733123779297\n",
      "Epoch [92/100] Loss: 49.485374450683594\n",
      "Epoch [93/100] Loss: 48.834075927734375\n",
      "Epoch [94/100] Loss: 48.193294525146484\n",
      "Epoch [95/100] Loss: 47.56288146972656\n",
      "Epoch [96/100] Loss: 46.94267272949219\n",
      "Epoch [97/100] Loss: 46.33255386352539\n",
      "Epoch [98/100] Loss: 45.732364654541016\n",
      "Epoch [99/100] Loss: 45.141937255859375\n",
      "Epoch [100/100] Loss: 44.561161041259766\n",
      "Predicted days_remaining for parent_id 124: 9.313787460327148\n",
      "Training for parent_id 138...\n",
      "Epoch [1/100] Loss: 200.97607421875\n",
      "Epoch [2/100] Loss: 194.95643615722656\n",
      "Epoch [3/100] Loss: 189.02056884765625\n",
      "Epoch [4/100] Loss: 183.22100830078125\n",
      "Epoch [5/100] Loss: 177.60748291015625\n",
      "Epoch [6/100] Loss: 172.21209716796875\n",
      "Epoch [7/100] Loss: 167.05587768554688\n",
      "Epoch [8/100] Loss: 162.15545654296875\n",
      "Epoch [9/100] Loss: 157.5213165283203\n",
      "Epoch [10/100] Loss: 153.1538848876953\n",
      "Epoch [11/100] Loss: 149.04202270507812\n",
      "Epoch [12/100] Loss: 145.1656951904297\n",
      "Epoch [13/100] Loss: 141.50137329101562\n",
      "Epoch [14/100] Loss: 138.02723693847656\n",
      "Epoch [15/100] Loss: 134.72549438476562\n",
      "Epoch [16/100] Loss: 131.58233642578125\n",
      "Epoch [17/100] Loss: 128.58718872070312\n",
      "Epoch [18/100] Loss: 125.7317123413086\n",
      "Epoch [19/100] Loss: 123.00892639160156\n",
      "Epoch [20/100] Loss: 120.4124984741211\n",
      "Epoch [21/100] Loss: 117.93606567382812\n",
      "Epoch [22/100] Loss: 115.5730972290039\n",
      "Epoch [23/100] Loss: 113.31661987304688\n",
      "Epoch [24/100] Loss: 111.15918731689453\n",
      "Epoch [25/100] Loss: 109.09302520751953\n",
      "Epoch [26/100] Loss: 107.1102294921875\n",
      "Epoch [27/100] Loss: 105.203125\n",
      "Epoch [28/100] Loss: 103.36444854736328\n",
      "Epoch [29/100] Loss: 101.58753967285156\n",
      "Epoch [30/100] Loss: 99.86656951904297\n",
      "Epoch [31/100] Loss: 98.19644165039062\n",
      "Epoch [32/100] Loss: 96.57283782958984\n",
      "Epoch [33/100] Loss: 94.99208068847656\n",
      "Epoch [34/100] Loss: 93.4510269165039\n",
      "Epoch [35/100] Loss: 91.9470443725586\n",
      "Epoch [36/100] Loss: 90.47791290283203\n",
      "Epoch [37/100] Loss: 89.04167938232422\n",
      "Epoch [38/100] Loss: 87.6366958618164\n",
      "Epoch [39/100] Loss: 86.26155090332031\n",
      "Epoch [40/100] Loss: 84.91490173339844\n",
      "Epoch [41/100] Loss: 83.595703125\n",
      "Epoch [42/100] Loss: 82.30294799804688\n",
      "Epoch [43/100] Loss: 81.03573608398438\n",
      "Epoch [44/100] Loss: 79.79322052001953\n",
      "Epoch [45/100] Loss: 78.57464599609375\n",
      "Epoch [46/100] Loss: 77.37931823730469\n",
      "Epoch [47/100] Loss: 76.20658111572266\n",
      "Epoch [48/100] Loss: 75.05582427978516\n",
      "Epoch [49/100] Loss: 73.92640686035156\n",
      "Epoch [50/100] Loss: 72.81781768798828\n",
      "Epoch [51/100] Loss: 71.72947692871094\n",
      "Epoch [52/100] Loss: 70.66089630126953\n",
      "Epoch [53/100] Loss: 69.61154174804688\n",
      "Epoch [54/100] Loss: 68.58099365234375\n",
      "Epoch [55/100] Loss: 67.56876373291016\n",
      "Epoch [56/100] Loss: 66.57445526123047\n",
      "Epoch [57/100] Loss: 65.59762573242188\n",
      "Epoch [58/100] Loss: 64.6379165649414\n",
      "Epoch [59/100] Loss: 63.694915771484375\n",
      "Epoch [60/100] Loss: 62.76829147338867\n",
      "Epoch [61/100] Loss: 61.857696533203125\n",
      "Epoch [62/100] Loss: 60.96277618408203\n",
      "Epoch [63/100] Loss: 60.083248138427734\n",
      "Epoch [64/100] Loss: 59.21878433227539\n",
      "Epoch [65/100] Loss: 58.36910629272461\n",
      "Epoch [66/100] Loss: 57.53394317626953\n",
      "Epoch [67/100] Loss: 56.71299743652344\n",
      "Epoch [68/100] Loss: 55.90602111816406\n",
      "Epoch [69/100] Loss: 55.11276626586914\n",
      "Epoch [70/100] Loss: 54.332977294921875\n",
      "Epoch [71/100] Loss: 53.56642532348633\n",
      "Epoch [72/100] Loss: 52.8128776550293\n",
      "Epoch [73/100] Loss: 52.072120666503906\n",
      "Epoch [74/100] Loss: 51.343936920166016\n",
      "Epoch [75/100] Loss: 50.628089904785156\n",
      "Epoch [76/100] Loss: 49.92441940307617\n",
      "Epoch [77/100] Loss: 49.23269271850586\n",
      "Epoch [78/100] Loss: 48.5527229309082\n",
      "Epoch [79/100] Loss: 47.884315490722656\n",
      "Epoch [80/100] Loss: 47.22730255126953\n",
      "Epoch [81/100] Loss: 46.581478118896484\n",
      "Epoch [82/100] Loss: 45.94668197631836\n",
      "Epoch [83/100] Loss: 45.32275390625\n",
      "Epoch [84/100] Loss: 44.70948791503906\n",
      "Epoch [85/100] Loss: 44.10675811767578\n",
      "Epoch [86/100] Loss: 43.51436233520508\n",
      "Epoch [87/100] Loss: 42.93217468261719\n",
      "Epoch [88/100] Loss: 42.36001205444336\n",
      "Epoch [89/100] Loss: 41.79774856567383\n",
      "Epoch [90/100] Loss: 41.24521255493164\n",
      "Epoch [91/100] Loss: 40.702239990234375\n",
      "Epoch [92/100] Loss: 40.16873550415039\n",
      "Epoch [93/100] Loss: 39.6445198059082\n",
      "Epoch [94/100] Loss: 39.12945556640625\n",
      "Epoch [95/100] Loss: 38.62342071533203\n",
      "Epoch [96/100] Loss: 38.12626266479492\n",
      "Epoch [97/100] Loss: 37.63785934448242\n",
      "Epoch [98/100] Loss: 37.15806579589844\n",
      "Epoch [99/100] Loss: 36.686763763427734\n",
      "Epoch [100/100] Loss: 36.22381591796875\n",
      "Predicted days_remaining for parent_id 138: 9.13138198852539\n",
      "Training for parent_id 144...\n",
      "Epoch [1/100] Loss: 127.1044921875\n",
      "Epoch [2/100] Loss: 122.09732818603516\n",
      "Epoch [3/100] Loss: 117.30012512207031\n",
      "Epoch [4/100] Loss: 112.78305053710938\n",
      "Epoch [5/100] Loss: 108.57852172851562\n",
      "Epoch [6/100] Loss: 104.67253875732422\n",
      "Epoch [7/100] Loss: 101.02577209472656\n",
      "Epoch [8/100] Loss: 97.59583282470703\n",
      "Epoch [9/100] Loss: 94.34867858886719\n",
      "Epoch [10/100] Loss: 91.26219940185547\n",
      "Epoch [11/100] Loss: 88.32382202148438\n",
      "Epoch [12/100] Loss: 85.5263442993164\n",
      "Epoch [13/100] Loss: 82.8642349243164\n",
      "Epoch [14/100] Loss: 80.33135986328125\n",
      "Epoch [15/100] Loss: 77.92059326171875\n",
      "Epoch [16/100] Loss: 75.62399291992188\n",
      "Epoch [17/100] Loss: 73.43362426757812\n",
      "Epoch [18/100] Loss: 71.34182739257812\n",
      "Epoch [19/100] Loss: 69.34149932861328\n",
      "Epoch [20/100] Loss: 67.42640686035156\n",
      "Epoch [21/100] Loss: 65.5912094116211\n",
      "Epoch [22/100] Loss: 63.83159637451172\n",
      "Epoch [23/100] Loss: 62.1442756652832\n",
      "Epoch [24/100] Loss: 60.52680206298828\n",
      "Epoch [25/100] Loss: 58.97754669189453\n",
      "Epoch [26/100] Loss: 57.49546813964844\n",
      "Epoch [27/100] Loss: 56.079856872558594\n",
      "Epoch [28/100] Loss: 54.730098724365234\n",
      "Epoch [29/100] Loss: 53.44528579711914\n",
      "Epoch [30/100] Loss: 52.22401428222656\n",
      "Epoch [31/100] Loss: 51.06423568725586\n",
      "Epoch [32/100] Loss: 49.96309280395508\n",
      "Epoch [33/100] Loss: 48.91718673706055\n",
      "Epoch [34/100] Loss: 47.9224853515625\n",
      "Epoch [35/100] Loss: 46.974708557128906\n",
      "Epoch [36/100] Loss: 46.06951141357422\n",
      "Epoch [37/100] Loss: 45.20268630981445\n",
      "Epoch [38/100] Loss: 44.37036895751953\n",
      "Epoch [39/100] Loss: 43.5690803527832\n",
      "Epoch [40/100] Loss: 42.795814514160156\n",
      "Epoch [41/100] Loss: 42.047996520996094\n",
      "Epoch [42/100] Loss: 41.323429107666016\n",
      "Epoch [43/100] Loss: 40.62029266357422\n",
      "Epoch [44/100] Loss: 39.9370231628418\n",
      "Epoch [45/100] Loss: 39.272308349609375\n",
      "Epoch [46/100] Loss: 38.62501525878906\n",
      "Epoch [47/100] Loss: 37.99417495727539\n",
      "Epoch [48/100] Loss: 37.378944396972656\n",
      "Epoch [49/100] Loss: 36.7785758972168\n",
      "Epoch [50/100] Loss: 36.19241714477539\n",
      "Epoch [51/100] Loss: 35.619895935058594\n",
      "Epoch [52/100] Loss: 35.060489654541016\n",
      "Epoch [53/100] Loss: 34.51374816894531\n",
      "Epoch [54/100] Loss: 33.979270935058594\n",
      "Epoch [55/100] Loss: 33.456687927246094\n",
      "Epoch [56/100] Loss: 32.9456672668457\n",
      "Epoch [57/100] Loss: 32.44590759277344\n",
      "Epoch [58/100] Loss: 31.9571533203125\n",
      "Epoch [59/100] Loss: 31.479145050048828\n",
      "Epoch [60/100] Loss: 31.011674880981445\n",
      "Epoch [61/100] Loss: 30.554521560668945\n",
      "Epoch [62/100] Loss: 30.107511520385742\n",
      "Epoch [63/100] Loss: 29.670446395874023\n",
      "Epoch [64/100] Loss: 29.2431640625\n",
      "Epoch [65/100] Loss: 28.82550621032715\n",
      "Epoch [66/100] Loss: 28.417335510253906\n",
      "Epoch [67/100] Loss: 28.018482208251953\n",
      "Epoch [68/100] Loss: 27.628812789916992\n",
      "Epoch [69/100] Loss: 27.248205184936523\n",
      "Epoch [70/100] Loss: 26.876501083374023\n",
      "Epoch [71/100] Loss: 26.513591766357422\n",
      "Epoch [72/100] Loss: 26.159320831298828\n",
      "Epoch [73/100] Loss: 25.813573837280273\n",
      "Epoch [74/100] Loss: 25.476207733154297\n",
      "Epoch [75/100] Loss: 25.1470890045166\n",
      "Epoch [76/100] Loss: 24.82608985900879\n",
      "Epoch [77/100] Loss: 24.513063430786133\n",
      "Epoch [78/100] Loss: 24.2078857421875\n",
      "Epoch [79/100] Loss: 23.910402297973633\n",
      "Epoch [80/100] Loss: 23.6204833984375\n",
      "Epoch [81/100] Loss: 23.337993621826172\n",
      "Epoch [82/100] Loss: 23.062786102294922\n",
      "Epoch [83/100] Loss: 22.794727325439453\n",
      "Epoch [84/100] Loss: 22.533676147460938\n",
      "Epoch [85/100] Loss: 22.279499053955078\n",
      "Epoch [86/100] Loss: 22.03204917907715\n",
      "Epoch [87/100] Loss: 21.79120635986328\n",
      "Epoch [88/100] Loss: 21.55681610107422\n",
      "Epoch [89/100] Loss: 21.328765869140625\n",
      "Epoch [90/100] Loss: 21.106910705566406\n",
      "Epoch [91/100] Loss: 20.891124725341797\n",
      "Epoch [92/100] Loss: 20.6812686920166\n",
      "Epoch [93/100] Loss: 20.47722625732422\n",
      "Epoch [94/100] Loss: 20.278865814208984\n",
      "Epoch [95/100] Loss: 20.086071014404297\n",
      "Epoch [96/100] Loss: 19.89871597290039\n",
      "Epoch [97/100] Loss: 19.71666717529297\n",
      "Epoch [98/100] Loss: 19.539817810058594\n",
      "Epoch [99/100] Loss: 19.3680477142334\n",
      "Epoch [100/100] Loss: 19.201242446899414\n",
      "Predicted days_remaining for parent_id 144: 8.60482406616211\n",
      "Training for parent_id 150...\n",
      "Epoch [1/100] Loss: 202.14559936523438\n",
      "Epoch [2/100] Loss: 194.2599334716797\n",
      "Epoch [3/100] Loss: 186.76873779296875\n",
      "Epoch [4/100] Loss: 179.65858459472656\n",
      "Epoch [5/100] Loss: 172.91189575195312\n",
      "Epoch [6/100] Loss: 166.51560974121094\n",
      "Epoch [7/100] Loss: 160.45672607421875\n",
      "Epoch [8/100] Loss: 154.7180633544922\n",
      "Epoch [9/100] Loss: 149.27879333496094\n",
      "Epoch [10/100] Loss: 144.1171112060547\n",
      "Epoch [11/100] Loss: 139.21322631835938\n",
      "Epoch [12/100] Loss: 134.55101013183594\n",
      "Epoch [13/100] Loss: 130.1188201904297\n",
      "Epoch [14/100] Loss: 125.90922546386719\n",
      "Epoch [15/100] Loss: 121.9176254272461\n",
      "Epoch [16/100] Loss: 118.14083099365234\n",
      "Epoch [17/100] Loss: 114.57562255859375\n",
      "Epoch [18/100] Loss: 111.21809387207031\n",
      "Epoch [19/100] Loss: 108.06324005126953\n",
      "Epoch [20/100] Loss: 105.10462951660156\n",
      "Epoch [21/100] Loss: 102.33444213867188\n",
      "Epoch [22/100] Loss: 99.74340057373047\n",
      "Epoch [23/100] Loss: 97.32090759277344\n",
      "Epoch [24/100] Loss: 95.05531311035156\n",
      "Epoch [25/100] Loss: 92.93417358398438\n",
      "Epoch [26/100] Loss: 90.94465637207031\n",
      "Epoch [27/100] Loss: 89.073974609375\n",
      "Epoch [28/100] Loss: 87.30970764160156\n",
      "Epoch [29/100] Loss: 85.64012145996094\n",
      "Epoch [30/100] Loss: 84.05445861816406\n",
      "Epoch [31/100] Loss: 82.54290771484375\n",
      "Epoch [32/100] Loss: 81.09683227539062\n",
      "Epoch [33/100] Loss: 79.70870208740234\n",
      "Epoch [34/100] Loss: 78.37191009521484\n",
      "Epoch [35/100] Loss: 77.08087158203125\n",
      "Epoch [36/100] Loss: 75.83078002929688\n",
      "Epoch [37/100] Loss: 74.61763000488281\n",
      "Epoch [38/100] Loss: 73.4380111694336\n",
      "Epoch [39/100] Loss: 72.28898620605469\n",
      "Epoch [40/100] Loss: 71.16819763183594\n",
      "Epoch [41/100] Loss: 70.07355499267578\n",
      "Epoch [42/100] Loss: 69.00334167480469\n",
      "Epoch [43/100] Loss: 67.95604705810547\n",
      "Epoch [44/100] Loss: 66.93045043945312\n",
      "Epoch [45/100] Loss: 65.92544555664062\n",
      "Epoch [46/100] Loss: 64.94007873535156\n",
      "Epoch [47/100] Loss: 63.9735221862793\n",
      "Epoch [48/100] Loss: 63.025089263916016\n",
      "Epoch [49/100] Loss: 62.09410095214844\n",
      "Epoch [50/100] Loss: 61.18000793457031\n",
      "Epoch [51/100] Loss: 60.28223419189453\n",
      "Epoch [52/100] Loss: 59.40038299560547\n",
      "Epoch [53/100] Loss: 58.533958435058594\n",
      "Epoch [54/100] Loss: 57.68258285522461\n",
      "Epoch [55/100] Loss: 56.84587860107422\n",
      "Epoch [56/100] Loss: 56.02350616455078\n",
      "Epoch [57/100] Loss: 55.21512985229492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100] Loss: 54.42045211791992\n",
      "Epoch [59/100] Loss: 53.63918685913086\n",
      "Epoch [60/100] Loss: 52.871070861816406\n",
      "Epoch [61/100] Loss: 52.11582565307617\n",
      "Epoch [62/100] Loss: 51.37324523925781\n",
      "Epoch [63/100] Loss: 50.64308166503906\n",
      "Epoch [64/100] Loss: 49.92512893676758\n",
      "Epoch [65/100] Loss: 49.21918487548828\n",
      "Epoch [66/100] Loss: 48.525054931640625\n",
      "Epoch [67/100] Loss: 47.842559814453125\n",
      "Epoch [68/100] Loss: 47.17151641845703\n",
      "Epoch [69/100] Loss: 46.511741638183594\n",
      "Epoch [70/100] Loss: 45.863136291503906\n",
      "Epoch [71/100] Loss: 45.22547912597656\n",
      "Epoch [72/100] Loss: 44.59865951538086\n",
      "Epoch [73/100] Loss: 43.98249816894531\n",
      "Epoch [74/100] Loss: 43.37688064575195\n",
      "Epoch [75/100] Loss: 42.781646728515625\n",
      "Epoch [76/100] Loss: 42.1966552734375\n",
      "Epoch [77/100] Loss: 41.62178421020508\n",
      "Epoch [78/100] Loss: 41.05686950683594\n",
      "Epoch [79/100] Loss: 40.50181198120117\n",
      "Epoch [80/100] Loss: 39.95644760131836\n",
      "Epoch [81/100] Loss: 39.42066192626953\n",
      "Epoch [82/100] Loss: 38.894309997558594\n",
      "Epoch [83/100] Loss: 38.37727737426758\n",
      "Epoch [84/100] Loss: 37.869422912597656\n",
      "Epoch [85/100] Loss: 37.3706169128418\n",
      "Epoch [86/100] Loss: 36.8807487487793\n",
      "Epoch [87/100] Loss: 36.39965057373047\n",
      "Epoch [88/100] Loss: 35.92725372314453\n",
      "Epoch [89/100] Loss: 35.46339797973633\n",
      "Epoch [90/100] Loss: 35.00796890258789\n",
      "Epoch [91/100] Loss: 34.56083679199219\n",
      "Epoch [92/100] Loss: 34.12190628051758\n",
      "Epoch [93/100] Loss: 33.691036224365234\n",
      "Epoch [94/100] Loss: 33.26811599731445\n",
      "Epoch [95/100] Loss: 32.853023529052734\n",
      "Epoch [96/100] Loss: 32.445648193359375\n",
      "Epoch [97/100] Loss: 32.04587936401367\n",
      "Epoch [98/100] Loss: 31.65358543395996\n",
      "Epoch [99/100] Loss: 31.268686294555664\n",
      "Epoch [100/100] Loss: 30.891056060791016\n",
      "Predicted days_remaining for parent_id 150: 9.739627838134766\n",
      "Training for parent_id 166...\n",
      "Epoch [1/100] Loss: 2692.883544921875\n",
      "Epoch [2/100] Loss: 2666.676025390625\n",
      "Epoch [3/100] Loss: 2641.658935546875\n",
      "Epoch [4/100] Loss: 2617.840576171875\n",
      "Epoch [5/100] Loss: 2595.221923828125\n",
      "Epoch [6/100] Loss: 2573.788818359375\n",
      "Epoch [7/100] Loss: 2553.49951171875\n",
      "Epoch [8/100] Loss: 2534.291259765625\n",
      "Epoch [9/100] Loss: 2516.0830078125\n",
      "Epoch [10/100] Loss: 2498.7890625\n",
      "Epoch [11/100] Loss: 2482.323486328125\n",
      "Epoch [12/100] Loss: 2466.6083984375\n",
      "Epoch [13/100] Loss: 2451.57421875\n",
      "Epoch [14/100] Loss: 2437.1630859375\n",
      "Epoch [15/100] Loss: 2423.322998046875\n",
      "Epoch [16/100] Loss: 2410.0126953125\n",
      "Epoch [17/100] Loss: 2397.197021484375\n",
      "Epoch [18/100] Loss: 2384.845947265625\n",
      "Epoch [19/100] Loss: 2372.93310546875\n",
      "Epoch [20/100] Loss: 2361.435546875\n",
      "Epoch [21/100] Loss: 2350.328857421875\n",
      "Epoch [22/100] Loss: 2339.587890625\n",
      "Epoch [23/100] Loss: 2329.18505859375\n",
      "Epoch [24/100] Loss: 2319.09228515625\n",
      "Epoch [25/100] Loss: 2309.280029296875\n",
      "Epoch [26/100] Loss: 2299.72021484375\n",
      "Epoch [27/100] Loss: 2290.385986328125\n",
      "Epoch [28/100] Loss: 2281.252197265625\n",
      "Epoch [29/100] Loss: 2272.29541015625\n",
      "Epoch [30/100] Loss: 2263.494873046875\n",
      "Epoch [31/100] Loss: 2254.832763671875\n",
      "Epoch [32/100] Loss: 2246.294677734375\n",
      "Epoch [33/100] Loss: 2237.86767578125\n",
      "Epoch [34/100] Loss: 2229.542724609375\n",
      "Epoch [35/100] Loss: 2221.312744140625\n",
      "Epoch [36/100] Loss: 2213.171875\n",
      "Epoch [37/100] Loss: 2205.115478515625\n",
      "Epoch [38/100] Loss: 2197.14111328125\n",
      "Epoch [39/100] Loss: 2189.244873046875\n",
      "Epoch [40/100] Loss: 2181.4248046875\n",
      "Epoch [41/100] Loss: 2173.6787109375\n",
      "Epoch [42/100] Loss: 2166.00439453125\n",
      "Epoch [43/100] Loss: 2158.399169921875\n",
      "Epoch [44/100] Loss: 2150.861083984375\n",
      "Epoch [45/100] Loss: 2143.387451171875\n",
      "Epoch [46/100] Loss: 2135.97607421875\n",
      "Epoch [47/100] Loss: 2128.623779296875\n",
      "Epoch [48/100] Loss: 2121.32861328125\n",
      "Epoch [49/100] Loss: 2114.087158203125\n",
      "Epoch [50/100] Loss: 2106.896728515625\n",
      "Epoch [51/100] Loss: 2099.754150390625\n",
      "Epoch [52/100] Loss: 2092.656494140625\n",
      "Epoch [53/100] Loss: 2085.600830078125\n",
      "Epoch [54/100] Loss: 2078.58447265625\n",
      "Epoch [55/100] Loss: 2071.604248046875\n",
      "Epoch [56/100] Loss: 2064.65771484375\n",
      "Epoch [57/100] Loss: 2057.74267578125\n",
      "Epoch [58/100] Loss: 2050.85693359375\n",
      "Epoch [59/100] Loss: 2043.997802734375\n",
      "Epoch [60/100] Loss: 2037.164306640625\n",
      "Epoch [61/100] Loss: 2030.3536376953125\n",
      "Epoch [62/100] Loss: 2023.565185546875\n",
      "Epoch [63/100] Loss: 2016.7972412109375\n",
      "Epoch [64/100] Loss: 2010.0491943359375\n",
      "Epoch [65/100] Loss: 2003.321533203125\n",
      "Epoch [66/100] Loss: 1996.614013671875\n",
      "Epoch [67/100] Loss: 1989.9281005859375\n",
      "Epoch [68/100] Loss: 1983.2652587890625\n",
      "Epoch [69/100] Loss: 1976.627685546875\n",
      "Epoch [70/100] Loss: 1970.0167236328125\n",
      "Epoch [71/100] Loss: 1963.4349365234375\n",
      "Epoch [72/100] Loss: 1956.8829345703125\n",
      "Epoch [73/100] Loss: 1950.3623046875\n",
      "Epoch [74/100] Loss: 1943.8734130859375\n",
      "Epoch [75/100] Loss: 1937.416748046875\n",
      "Epoch [76/100] Loss: 1930.991943359375\n",
      "Epoch [77/100] Loss: 1924.5986328125\n",
      "Epoch [78/100] Loss: 1918.2373046875\n",
      "Epoch [79/100] Loss: 1911.90625\n",
      "Epoch [80/100] Loss: 1905.60595703125\n",
      "Epoch [81/100] Loss: 1899.3350830078125\n",
      "Epoch [82/100] Loss: 1893.0936279296875\n",
      "Epoch [83/100] Loss: 1886.880615234375\n",
      "Epoch [84/100] Loss: 1880.6959228515625\n",
      "Epoch [85/100] Loss: 1874.538818359375\n",
      "Epoch [86/100] Loss: 1868.4090576171875\n",
      "Epoch [87/100] Loss: 1862.3056640625\n",
      "Epoch [88/100] Loss: 1856.2291259765625\n",
      "Epoch [89/100] Loss: 1850.177978515625\n",
      "Epoch [90/100] Loss: 1844.15234375\n",
      "Epoch [91/100] Loss: 1838.152099609375\n",
      "Epoch [92/100] Loss: 1832.17626953125\n",
      "Epoch [93/100] Loss: 1826.225341796875\n",
      "Epoch [94/100] Loss: 1820.2977294921875\n",
      "Epoch [95/100] Loss: 1814.394775390625\n",
      "Epoch [96/100] Loss: 1808.5147705078125\n",
      "Epoch [97/100] Loss: 1802.65771484375\n",
      "Epoch [98/100] Loss: 1796.8236083984375\n",
      "Epoch [99/100] Loss: 1791.0123291015625\n",
      "Epoch [100/100] Loss: 1785.2227783203125\n",
      "Predicted days_remaining for parent_id 166: 9.737882614135742\n",
      "Training for parent_id 168...\n",
      "Epoch [1/100] Loss: 418.39862060546875\n",
      "Epoch [2/100] Loss: 407.8901672363281\n",
      "Epoch [3/100] Loss: 397.880615234375\n",
      "Epoch [4/100] Loss: 388.4692077636719\n",
      "Epoch [5/100] Loss: 379.7179870605469\n",
      "Epoch [6/100] Loss: 371.60650634765625\n",
      "Epoch [7/100] Loss: 364.0413513183594\n",
      "Epoch [8/100] Loss: 356.9130859375\n",
      "Epoch [9/100] Loss: 350.13232421875\n",
      "Epoch [10/100] Loss: 343.6355895996094\n",
      "Epoch [11/100] Loss: 337.3815612792969\n",
      "Epoch [12/100] Loss: 331.3448791503906\n",
      "Epoch [13/100] Loss: 325.5103454589844\n",
      "Epoch [14/100] Loss: 319.8698425292969\n",
      "Epoch [15/100] Loss: 314.4189453125\n",
      "Epoch [16/100] Loss: 309.1555480957031\n",
      "Epoch [17/100] Loss: 304.0783386230469\n",
      "Epoch [18/100] Loss: 299.1852111816406\n",
      "Epoch [19/100] Loss: 294.4728088378906\n",
      "Epoch [20/100] Loss: 289.9358825683594\n",
      "Epoch [21/100] Loss: 285.5676574707031\n",
      "Epoch [22/100] Loss: 281.36029052734375\n",
      "Epoch [23/100] Loss: 277.3060302734375\n",
      "Epoch [24/100] Loss: 273.3976745605469\n",
      "Epoch [25/100] Loss: 269.6289367675781\n",
      "Epoch [26/100] Loss: 265.9939880371094\n",
      "Epoch [27/100] Loss: 262.4871520996094\n",
      "Epoch [28/100] Loss: 259.1016845703125\n",
      "Epoch [29/100] Loss: 255.82986450195312\n",
      "Epoch [30/100] Loss: 252.66273498535156\n",
      "Epoch [31/100] Loss: 249.59092712402344\n",
      "Epoch [32/100] Loss: 246.6053466796875\n",
      "Epoch [33/100] Loss: 243.6979217529297\n",
      "Epoch [34/100] Loss: 240.86212158203125\n",
      "Epoch [35/100] Loss: 238.0924835205078\n",
      "Epoch [36/100] Loss: 235.38490295410156\n",
      "Epoch [37/100] Loss: 232.73574829101562\n",
      "Epoch [38/100] Loss: 230.14195251464844\n",
      "Epoch [39/100] Loss: 227.6006317138672\n",
      "Epoch [40/100] Loss: 225.1090850830078\n",
      "Epoch [41/100] Loss: 222.66482543945312\n",
      "Epoch [42/100] Loss: 220.26527404785156\n",
      "Epoch [43/100] Loss: 217.90823364257812\n",
      "Epoch [44/100] Loss: 215.59149169921875\n",
      "Epoch [45/100] Loss: 213.31297302246094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100] Loss: 211.0708770751953\n",
      "Epoch [47/100] Loss: 208.8634490966797\n",
      "Epoch [48/100] Loss: 206.68898010253906\n",
      "Epoch [49/100] Loss: 204.54611206054688\n",
      "Epoch [50/100] Loss: 202.43331909179688\n",
      "Epoch [51/100] Loss: 200.3494415283203\n",
      "Epoch [52/100] Loss: 198.29324340820312\n",
      "Epoch [53/100] Loss: 196.26361083984375\n",
      "Epoch [54/100] Loss: 194.25955200195312\n",
      "Epoch [55/100] Loss: 192.28012084960938\n",
      "Epoch [56/100] Loss: 190.32444763183594\n",
      "Epoch [57/100] Loss: 188.39178466796875\n",
      "Epoch [58/100] Loss: 186.48138427734375\n",
      "Epoch [59/100] Loss: 184.59271240234375\n",
      "Epoch [60/100] Loss: 182.72509765625\n",
      "Epoch [61/100] Loss: 180.8780517578125\n",
      "Epoch [62/100] Loss: 179.05123901367188\n",
      "Epoch [63/100] Loss: 177.244140625\n",
      "Epoch [64/100] Loss: 175.45645141601562\n",
      "Epoch [65/100] Loss: 173.6878204345703\n",
      "Epoch [66/100] Loss: 171.93800354003906\n",
      "Epoch [67/100] Loss: 170.2066192626953\n",
      "Epoch [68/100] Loss: 168.49342346191406\n",
      "Epoch [69/100] Loss: 166.79823303222656\n",
      "Epoch [70/100] Loss: 165.12063598632812\n",
      "Epoch [71/100] Loss: 163.4605712890625\n",
      "Epoch [72/100] Loss: 161.8176727294922\n",
      "Epoch [73/100] Loss: 160.1917266845703\n",
      "Epoch [74/100] Loss: 158.58255004882812\n",
      "Epoch [75/100] Loss: 156.98989868164062\n",
      "Epoch [76/100] Loss: 155.41360473632812\n",
      "Epoch [77/100] Loss: 153.85336303710938\n",
      "Epoch [78/100] Loss: 152.30906677246094\n",
      "Epoch [79/100] Loss: 150.78054809570312\n",
      "Epoch [80/100] Loss: 149.26748657226562\n",
      "Epoch [81/100] Loss: 147.7698211669922\n",
      "Epoch [82/100] Loss: 146.2873077392578\n",
      "Epoch [83/100] Loss: 144.81979370117188\n",
      "Epoch [84/100] Loss: 143.3671112060547\n",
      "Epoch [85/100] Loss: 141.9290771484375\n",
      "Epoch [86/100] Loss: 140.50552368164062\n",
      "Epoch [87/100] Loss: 139.09629821777344\n",
      "Epoch [88/100] Loss: 137.70123291015625\n",
      "Epoch [89/100] Loss: 136.32020568847656\n",
      "Epoch [90/100] Loss: 134.95298767089844\n",
      "Epoch [91/100] Loss: 133.5995635986328\n",
      "Epoch [92/100] Loss: 132.25965881347656\n",
      "Epoch [93/100] Loss: 130.9331817626953\n",
      "Epoch [94/100] Loss: 129.61993408203125\n",
      "Epoch [95/100] Loss: 128.31985473632812\n",
      "Epoch [96/100] Loss: 127.03277587890625\n",
      "Epoch [97/100] Loss: 125.758544921875\n",
      "Epoch [98/100] Loss: 124.4970474243164\n",
      "Epoch [99/100] Loss: 123.24813079833984\n",
      "Epoch [100/100] Loss: 122.01171112060547\n",
      "Predicted days_remaining for parent_id 168: 9.43738079071045\n",
      "Training for parent_id 177...\n",
      "Epoch [1/100] Loss: 2907.362548828125\n",
      "Epoch [2/100] Loss: 2877.025146484375\n",
      "Epoch [3/100] Loss: 2847.426513671875\n",
      "Epoch [4/100] Loss: 2818.7216796875\n",
      "Epoch [5/100] Loss: 2791.00927734375\n",
      "Epoch [6/100] Loss: 2764.339599609375\n",
      "Epoch [7/100] Loss: 2738.73876953125\n",
      "Epoch [8/100] Loss: 2714.20703125\n",
      "Epoch [9/100] Loss: 2690.724853515625\n",
      "Epoch [10/100] Loss: 2668.26220703125\n",
      "Epoch [11/100] Loss: 2646.784423828125\n",
      "Epoch [12/100] Loss: 2626.258056640625\n",
      "Epoch [13/100] Loss: 2606.648681640625\n",
      "Epoch [14/100] Loss: 2587.921142578125\n",
      "Epoch [15/100] Loss: 2570.03076171875\n",
      "Epoch [16/100] Loss: 2552.92578125\n",
      "Epoch [17/100] Loss: 2536.546875\n",
      "Epoch [18/100] Loss: 2520.835693359375\n",
      "Epoch [19/100] Loss: 2505.739990234375\n",
      "Epoch [20/100] Loss: 2491.21630859375\n",
      "Epoch [21/100] Loss: 2477.234375\n",
      "Epoch [22/100] Loss: 2463.774169921875\n",
      "Epoch [23/100] Loss: 2450.825927734375\n",
      "Epoch [24/100] Loss: 2438.37890625\n",
      "Epoch [25/100] Loss: 2426.420654296875\n",
      "Epoch [26/100] Loss: 2414.932861328125\n",
      "Epoch [27/100] Loss: 2403.88916015625\n",
      "Epoch [28/100] Loss: 2393.2587890625\n",
      "Epoch [29/100] Loss: 2383.00732421875\n",
      "Epoch [30/100] Loss: 2373.099609375\n",
      "Epoch [31/100] Loss: 2363.500244140625\n",
      "Epoch [32/100] Loss: 2354.17529296875\n",
      "Epoch [33/100] Loss: 2345.09375\n",
      "Epoch [34/100] Loss: 2336.22412109375\n",
      "Epoch [35/100] Loss: 2327.540771484375\n",
      "Epoch [36/100] Loss: 2319.01953125\n",
      "Epoch [37/100] Loss: 2310.638671875\n",
      "Epoch [38/100] Loss: 2302.3818359375\n",
      "Epoch [39/100] Loss: 2294.232421875\n",
      "Epoch [40/100] Loss: 2286.17919921875\n",
      "Epoch [41/100] Loss: 2278.21240234375\n",
      "Epoch [42/100] Loss: 2270.322021484375\n",
      "Epoch [43/100] Loss: 2262.50244140625\n",
      "Epoch [44/100] Loss: 2254.748291015625\n",
      "Epoch [45/100] Loss: 2247.0537109375\n",
      "Epoch [46/100] Loss: 2239.416015625\n",
      "Epoch [47/100] Loss: 2231.83154296875\n",
      "Epoch [48/100] Loss: 2224.2978515625\n",
      "Epoch [49/100] Loss: 2216.8125\n",
      "Epoch [50/100] Loss: 2209.373779296875\n",
      "Epoch [51/100] Loss: 2201.980224609375\n",
      "Epoch [52/100] Loss: 2194.630859375\n",
      "Epoch [53/100] Loss: 2187.32275390625\n",
      "Epoch [54/100] Loss: 2180.057373046875\n",
      "Epoch [55/100] Loss: 2172.831787109375\n",
      "Epoch [56/100] Loss: 2165.64599609375\n",
      "Epoch [57/100] Loss: 2158.499267578125\n",
      "Epoch [58/100] Loss: 2151.390380859375\n",
      "Epoch [59/100] Loss: 2144.3193359375\n",
      "Epoch [60/100] Loss: 2137.28564453125\n",
      "Epoch [61/100] Loss: 2130.28857421875\n",
      "Epoch [62/100] Loss: 2123.32666015625\n",
      "Epoch [63/100] Loss: 2116.40087890625\n",
      "Epoch [64/100] Loss: 2109.51025390625\n",
      "Epoch [65/100] Loss: 2102.65380859375\n",
      "Epoch [66/100] Loss: 2095.83154296875\n",
      "Epoch [67/100] Loss: 2089.042724609375\n",
      "Epoch [68/100] Loss: 2082.28759765625\n",
      "Epoch [69/100] Loss: 2075.564697265625\n",
      "Epoch [70/100] Loss: 2068.8740234375\n",
      "Epoch [71/100] Loss: 2062.215087890625\n",
      "Epoch [72/100] Loss: 2055.587646484375\n",
      "Epoch [73/100] Loss: 2048.990966796875\n",
      "Epoch [74/100] Loss: 2042.4248046875\n",
      "Epoch [75/100] Loss: 2035.8887939453125\n",
      "Epoch [76/100] Loss: 2029.38232421875\n",
      "Epoch [77/100] Loss: 2022.9049072265625\n",
      "Epoch [78/100] Loss: 2016.4564208984375\n",
      "Epoch [79/100] Loss: 2010.036376953125\n",
      "Epoch [80/100] Loss: 2003.64404296875\n",
      "Epoch [81/100] Loss: 1997.279541015625\n",
      "Epoch [82/100] Loss: 1990.942138671875\n",
      "Epoch [83/100] Loss: 1984.6314697265625\n",
      "Epoch [84/100] Loss: 1978.34716796875\n",
      "Epoch [85/100] Loss: 1972.089111328125\n",
      "Epoch [86/100] Loss: 1965.856689453125\n",
      "Epoch [87/100] Loss: 1959.6495361328125\n",
      "Epoch [88/100] Loss: 1953.46728515625\n",
      "Epoch [89/100] Loss: 1947.3096923828125\n",
      "Epoch [90/100] Loss: 1941.1768798828125\n",
      "Epoch [91/100] Loss: 1935.0675048828125\n",
      "Epoch [92/100] Loss: 1928.9820556640625\n",
      "Epoch [93/100] Loss: 1922.920166015625\n",
      "Epoch [94/100] Loss: 1916.88134765625\n",
      "Epoch [95/100] Loss: 1910.865234375\n",
      "Epoch [96/100] Loss: 1904.8719482421875\n",
      "Epoch [97/100] Loss: 1898.900390625\n",
      "Epoch [98/100] Loss: 1892.951171875\n",
      "Epoch [99/100] Loss: 1887.0235595703125\n",
      "Epoch [100/100] Loss: 1881.117431640625\n",
      "Predicted days_remaining for parent_id 177: 10.613065719604492\n",
      "Training for parent_id 179...\n",
      "Epoch [1/100] Loss: 209.372314453125\n",
      "Epoch [2/100] Loss: 202.76272583007812\n",
      "Epoch [3/100] Loss: 196.24647521972656\n",
      "Epoch [4/100] Loss: 189.87850952148438\n",
      "Epoch [5/100] Loss: 183.71255493164062\n",
      "Epoch [6/100] Loss: 177.8035430908203\n",
      "Epoch [7/100] Loss: 172.19161987304688\n",
      "Epoch [8/100] Loss: 166.8968048095703\n",
      "Epoch [9/100] Loss: 161.92147827148438\n",
      "Epoch [10/100] Loss: 157.25506591796875\n",
      "Epoch [11/100] Loss: 152.87899780273438\n",
      "Epoch [12/100] Loss: 148.77073669433594\n",
      "Epoch [13/100] Loss: 144.90696716308594\n",
      "Epoch [14/100] Loss: 141.26536560058594\n",
      "Epoch [15/100] Loss: 137.82542419433594\n",
      "Epoch [16/100] Loss: 134.56898498535156\n",
      "Epoch [17/100] Loss: 131.4799346923828\n",
      "Epoch [18/100] Loss: 128.544189453125\n",
      "Epoch [19/100] Loss: 125.74937438964844\n",
      "Epoch [20/100] Loss: 123.08480072021484\n",
      "Epoch [21/100] Loss: 120.54096984863281\n",
      "Epoch [22/100] Loss: 118.10953521728516\n",
      "Epoch [23/100] Loss: 115.78300476074219\n",
      "Epoch [24/100] Loss: 113.55445861816406\n",
      "Epoch [25/100] Loss: 111.4174575805664\n",
      "Epoch [26/100] Loss: 109.36589050292969\n",
      "Epoch [27/100] Loss: 107.39389038085938\n",
      "Epoch [28/100] Loss: 105.49581146240234\n",
      "Epoch [29/100] Loss: 103.6662368774414\n",
      "Epoch [30/100] Loss: 101.90008544921875\n",
      "Epoch [31/100] Loss: 100.1925048828125\n",
      "Epoch [32/100] Loss: 98.53904724121094\n",
      "Epoch [33/100] Loss: 96.93561553955078\n",
      "Epoch [34/100] Loss: 95.37852478027344\n",
      "Epoch [35/100] Loss: 93.86434936523438\n",
      "Epoch [36/100] Loss: 92.39010620117188\n",
      "Epoch [37/100] Loss: 90.95305633544922\n",
      "Epoch [38/100] Loss: 89.55071258544922\n",
      "Epoch [39/100] Loss: 88.1808853149414\n",
      "Epoch [40/100] Loss: 86.84159851074219\n",
      "Epoch [41/100] Loss: 85.53107452392578\n",
      "Epoch [42/100] Loss: 84.24769592285156\n",
      "Epoch [43/100] Loss: 82.99005126953125\n",
      "Epoch [44/100] Loss: 81.75689697265625\n",
      "Epoch [45/100] Loss: 80.54710388183594\n",
      "Epoch [46/100] Loss: 79.35962677001953\n",
      "Epoch [47/100] Loss: 78.19359588623047\n",
      "Epoch [48/100] Loss: 77.04815673828125\n",
      "Epoch [49/100] Loss: 75.92259979248047\n",
      "Epoch [50/100] Loss: 74.81621551513672\n",
      "Epoch [51/100] Loss: 73.72840118408203\n",
      "Epoch [52/100] Loss: 72.6585693359375\n",
      "Epoch [53/100] Loss: 71.60618591308594\n",
      "Epoch [54/100] Loss: 70.57071685791016\n",
      "Epoch [55/100] Loss: 69.55171966552734\n",
      "Epoch [56/100] Loss: 68.54878234863281\n",
      "Epoch [57/100] Loss: 67.56146240234375\n",
      "Epoch [58/100] Loss: 66.58943939208984\n",
      "Epoch [59/100] Loss: 65.63239288330078\n",
      "Epoch [60/100] Loss: 64.69002532958984\n",
      "Epoch [61/100] Loss: 63.76204299926758\n",
      "Epoch [62/100] Loss: 62.8482666015625\n",
      "Epoch [63/100] Loss: 61.94844436645508\n",
      "Epoch [64/100] Loss: 61.062435150146484\n",
      "Epoch [65/100] Loss: 60.19001770019531\n",
      "Epoch [66/100] Loss: 59.3310661315918\n",
      "Epoch [67/100] Loss: 58.48543930053711\n",
      "Epoch [68/100] Loss: 57.652976989746094\n",
      "Epoch [69/100] Loss: 56.83356475830078\n",
      "Epoch [70/100] Loss: 56.02705383300781\n",
      "Epoch [71/100] Loss: 55.23333740234375\n",
      "Epoch [72/100] Loss: 54.452274322509766\n",
      "Epoch [73/100] Loss: 53.683746337890625\n",
      "Epoch [74/100] Loss: 52.927608489990234\n",
      "Epoch [75/100] Loss: 52.183746337890625\n",
      "Epoch [76/100] Loss: 51.45200729370117\n",
      "Epoch [77/100] Loss: 50.73228073120117\n",
      "Epoch [78/100] Loss: 50.02439498901367\n",
      "Epoch [79/100] Loss: 49.3282470703125\n",
      "Epoch [80/100] Loss: 48.643653869628906\n",
      "Epoch [81/100] Loss: 47.970489501953125\n",
      "Epoch [82/100] Loss: 47.30861282348633\n",
      "Epoch [83/100] Loss: 46.657859802246094\n",
      "Epoch [84/100] Loss: 46.018089294433594\n",
      "Epoch [85/100] Loss: 45.389137268066406\n",
      "Epoch [86/100] Loss: 44.7708625793457\n",
      "Epoch [87/100] Loss: 44.1630973815918\n",
      "Epoch [88/100] Loss: 43.565696716308594\n",
      "Epoch [89/100] Loss: 42.97853469848633\n",
      "Epoch [90/100] Loss: 42.40141296386719\n",
      "Epoch [91/100] Loss: 41.8342170715332\n",
      "Epoch [92/100] Loss: 41.27676773071289\n",
      "Epoch [93/100] Loss: 40.72894287109375\n",
      "Epoch [94/100] Loss: 40.19058609008789\n",
      "Epoch [95/100] Loss: 39.66156005859375\n",
      "Epoch [96/100] Loss: 39.141719818115234\n",
      "Epoch [97/100] Loss: 38.63092041015625\n",
      "Epoch [98/100] Loss: 38.12903594970703\n",
      "Epoch [99/100] Loss: 37.63594436645508\n",
      "Epoch [100/100] Loss: 37.15148162841797\n",
      "Predicted days_remaining for parent_id 179: 9.034276008605957\n",
      "Training for parent_id 199...\n",
      "Epoch [1/100] Loss: 332.6224670410156\n",
      "Epoch [2/100] Loss: 323.092041015625\n",
      "Epoch [3/100] Loss: 313.9052734375\n",
      "Epoch [4/100] Loss: 305.116455078125\n",
      "Epoch [5/100] Loss: 296.7499084472656\n",
      "Epoch [6/100] Loss: 288.812255859375\n",
      "Epoch [7/100] Loss: 281.29376220703125\n",
      "Epoch [8/100] Loss: 274.1740417480469\n",
      "Epoch [9/100] Loss: 267.4306335449219\n",
      "Epoch [10/100] Loss: 261.0426940917969\n",
      "Epoch [11/100] Loss: 254.9912872314453\n",
      "Epoch [12/100] Loss: 249.25758361816406\n",
      "Epoch [13/100] Loss: 243.82342529296875\n",
      "Epoch [14/100] Loss: 238.6705322265625\n",
      "Epoch [15/100] Loss: 233.78036499023438\n",
      "Epoch [16/100] Loss: 229.1344451904297\n",
      "Epoch [17/100] Loss: 224.71456909179688\n",
      "Epoch [18/100] Loss: 220.5036163330078\n",
      "Epoch [19/100] Loss: 216.48570251464844\n",
      "Epoch [20/100] Loss: 212.64634704589844\n",
      "Epoch [21/100] Loss: 208.97247314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] Loss: 205.45230102539062\n",
      "Epoch [23/100] Loss: 202.07510375976562\n",
      "Epoch [24/100] Loss: 198.83148193359375\n",
      "Epoch [25/100] Loss: 195.71270751953125\n",
      "Epoch [26/100] Loss: 192.71121215820312\n",
      "Epoch [27/100] Loss: 189.81991577148438\n",
      "Epoch [28/100] Loss: 187.03233337402344\n",
      "Epoch [29/100] Loss: 184.34231567382812\n",
      "Epoch [30/100] Loss: 181.7439727783203\n",
      "Epoch [31/100] Loss: 179.2316436767578\n",
      "Epoch [32/100] Loss: 176.7996826171875\n",
      "Epoch [33/100] Loss: 174.44265747070312\n",
      "Epoch [34/100] Loss: 172.15521240234375\n",
      "Epoch [35/100] Loss: 169.93218994140625\n",
      "Epoch [36/100] Loss: 167.768798828125\n",
      "Epoch [37/100] Loss: 165.66033935546875\n",
      "Epoch [38/100] Loss: 163.60256958007812\n",
      "Epoch [39/100] Loss: 161.5916290283203\n",
      "Epoch [40/100] Loss: 159.62388610839844\n",
      "Epoch [41/100] Loss: 157.6962432861328\n",
      "Epoch [42/100] Loss: 155.8057861328125\n",
      "Epoch [43/100] Loss: 153.9500732421875\n",
      "Epoch [44/100] Loss: 152.12692260742188\n",
      "Epoch [45/100] Loss: 150.3344268798828\n",
      "Epoch [46/100] Loss: 148.5709686279297\n",
      "Epoch [47/100] Loss: 146.8350372314453\n",
      "Epoch [48/100] Loss: 145.12538146972656\n",
      "Epoch [49/100] Loss: 143.4409637451172\n",
      "Epoch [50/100] Loss: 141.78077697753906\n",
      "Epoch [51/100] Loss: 140.1439971923828\n",
      "Epoch [52/100] Loss: 138.5299530029297\n",
      "Epoch [53/100] Loss: 136.93788146972656\n",
      "Epoch [54/100] Loss: 135.3672637939453\n",
      "Epoch [55/100] Loss: 133.81748962402344\n",
      "Epoch [56/100] Loss: 132.28814697265625\n",
      "Epoch [57/100] Loss: 130.7786865234375\n",
      "Epoch [58/100] Loss: 129.2887420654297\n",
      "Epoch [59/100] Loss: 127.81786346435547\n",
      "Epoch [60/100] Loss: 126.36566925048828\n",
      "Epoch [61/100] Loss: 124.93180847167969\n",
      "Epoch [62/100] Loss: 123.51587677001953\n",
      "Epoch [63/100] Loss: 122.11754608154297\n",
      "Epoch [64/100] Loss: 120.73648834228516\n",
      "Epoch [65/100] Loss: 119.37232971191406\n",
      "Epoch [66/100] Loss: 118.02481079101562\n",
      "Epoch [67/100] Loss: 116.6935806274414\n",
      "Epoch [68/100] Loss: 115.37834167480469\n",
      "Epoch [69/100] Loss: 114.07881927490234\n",
      "Epoch [70/100] Loss: 112.79473114013672\n",
      "Epoch [71/100] Loss: 111.52581024169922\n",
      "Epoch [72/100] Loss: 110.27178192138672\n",
      "Epoch [73/100] Loss: 109.03242492675781\n",
      "Epoch [74/100] Loss: 107.8074722290039\n",
      "Epoch [75/100] Loss: 106.5966796875\n",
      "Epoch [76/100] Loss: 105.39987182617188\n",
      "Epoch [77/100] Loss: 104.2168197631836\n",
      "Epoch [78/100] Loss: 103.04730224609375\n",
      "Epoch [79/100] Loss: 101.89115142822266\n",
      "Epoch [80/100] Loss: 100.74821472167969\n",
      "Epoch [81/100] Loss: 99.61824798583984\n",
      "Epoch [82/100] Loss: 98.50114440917969\n",
      "Epoch [83/100] Loss: 97.3967056274414\n",
      "Epoch [84/100] Loss: 96.3047866821289\n",
      "Epoch [85/100] Loss: 95.22528076171875\n",
      "Epoch [86/100] Loss: 94.15801239013672\n",
      "Epoch [87/100] Loss: 93.10283660888672\n",
      "Epoch [88/100] Loss: 92.0596923828125\n",
      "Epoch [89/100] Loss: 91.02835845947266\n",
      "Epoch [90/100] Loss: 90.00875091552734\n",
      "Epoch [91/100] Loss: 89.00074005126953\n",
      "Epoch [92/100] Loss: 88.00422668457031\n",
      "Epoch [93/100] Loss: 87.01909637451172\n",
      "Epoch [94/100] Loss: 86.04521942138672\n",
      "Epoch [95/100] Loss: 85.08246612548828\n",
      "Epoch [96/100] Loss: 84.13072967529297\n",
      "Epoch [97/100] Loss: 83.18994903564453\n",
      "Epoch [98/100] Loss: 82.25995635986328\n",
      "Epoch [99/100] Loss: 81.34066772460938\n",
      "Epoch [100/100] Loss: 80.43199920654297\n",
      "Predicted days_remaining for parent_id 199: 9.681772232055664\n",
      "Training for parent_id 211...\n",
      "Epoch [1/100] Loss: 268.5501708984375\n",
      "Epoch [2/100] Loss: 261.0086975097656\n",
      "Epoch [3/100] Loss: 253.59812927246094\n",
      "Epoch [4/100] Loss: 246.33834838867188\n",
      "Epoch [5/100] Loss: 239.26182556152344\n",
      "Epoch [6/100] Loss: 232.4063262939453\n",
      "Epoch [7/100] Loss: 225.80836486816406\n",
      "Epoch [8/100] Loss: 219.49476623535156\n",
      "Epoch [9/100] Loss: 213.4774169921875\n",
      "Epoch [10/100] Loss: 207.75466918945312\n",
      "Epoch [11/100] Loss: 202.3153533935547\n",
      "Epoch [12/100] Loss: 197.1435546875\n",
      "Epoch [13/100] Loss: 192.22190856933594\n",
      "Epoch [14/100] Loss: 187.5336456298828\n",
      "Epoch [15/100] Loss: 183.0636749267578\n",
      "Epoch [16/100] Loss: 178.79879760742188\n",
      "Epoch [17/100] Loss: 174.7278594970703\n",
      "Epoch [18/100] Loss: 170.84140014648438\n",
      "Epoch [19/100] Loss: 167.13111877441406\n",
      "Epoch [20/100] Loss: 163.58949279785156\n",
      "Epoch [21/100] Loss: 160.2090606689453\n",
      "Epoch [22/100] Loss: 156.98228454589844\n",
      "Epoch [23/100] Loss: 153.90133666992188\n",
      "Epoch [24/100] Loss: 150.9578857421875\n",
      "Epoch [25/100] Loss: 148.1434783935547\n",
      "Epoch [26/100] Loss: 145.4496612548828\n",
      "Epoch [27/100] Loss: 142.86807250976562\n",
      "Epoch [28/100] Loss: 140.39060974121094\n",
      "Epoch [29/100] Loss: 138.009521484375\n",
      "Epoch [30/100] Loss: 135.71719360351562\n",
      "Epoch [31/100] Loss: 133.50656127929688\n",
      "Epoch [32/100] Loss: 131.37083435058594\n",
      "Epoch [33/100] Loss: 129.30374145507812\n",
      "Epoch [34/100] Loss: 127.29947662353516\n",
      "Epoch [35/100] Loss: 125.3528060913086\n",
      "Epoch [36/100] Loss: 123.45906829833984\n",
      "Epoch [37/100] Loss: 121.61407470703125\n",
      "Epoch [38/100] Loss: 119.81415557861328\n",
      "Epoch [39/100] Loss: 118.05618286132812\n",
      "Epoch [40/100] Loss: 116.33736419677734\n",
      "Epoch [41/100] Loss: 114.65544128417969\n",
      "Epoch [42/100] Loss: 113.0083236694336\n",
      "Epoch [43/100] Loss: 111.39434051513672\n",
      "Epoch [44/100] Loss: 109.8119888305664\n",
      "Epoch [45/100] Loss: 108.25997161865234\n",
      "Epoch [46/100] Loss: 106.73715209960938\n",
      "Epoch [47/100] Loss: 105.24250793457031\n",
      "Epoch [48/100] Loss: 103.77506256103516\n",
      "Epoch [49/100] Loss: 102.33395385742188\n",
      "Epoch [50/100] Loss: 100.91832733154297\n",
      "Epoch [51/100] Loss: 99.52738189697266\n",
      "Epoch [52/100] Loss: 98.16030883789062\n",
      "Epoch [53/100] Loss: 96.81640625\n",
      "Epoch [54/100] Loss: 95.494873046875\n",
      "Epoch [55/100] Loss: 94.19510650634766\n",
      "Epoch [56/100] Loss: 92.91639709472656\n",
      "Epoch [57/100] Loss: 91.65814971923828\n",
      "Epoch [58/100] Loss: 90.41974639892578\n",
      "Epoch [59/100] Loss: 89.2007064819336\n",
      "Epoch [60/100] Loss: 88.00048065185547\n",
      "Epoch [61/100] Loss: 86.8186264038086\n",
      "Epoch [62/100] Loss: 85.65467834472656\n",
      "Epoch [63/100] Loss: 84.50823974609375\n",
      "Epoch [64/100] Loss: 83.37897491455078\n",
      "Epoch [65/100] Loss: 82.26648712158203\n",
      "Epoch [66/100] Loss: 81.17049407958984\n",
      "Epoch [67/100] Loss: 80.09068298339844\n",
      "Epoch [68/100] Loss: 79.0267333984375\n",
      "Epoch [69/100] Loss: 77.97843933105469\n",
      "Epoch [70/100] Loss: 76.94549560546875\n",
      "Epoch [71/100] Loss: 75.92768859863281\n",
      "Epoch [72/100] Loss: 74.92477416992188\n",
      "Epoch [73/100] Loss: 73.9365005493164\n",
      "Epoch [74/100] Loss: 72.96267700195312\n",
      "Epoch [75/100] Loss: 72.00308990478516\n",
      "Epoch [76/100] Loss: 71.0575180053711\n",
      "Epoch [77/100] Loss: 70.12574005126953\n",
      "Epoch [78/100] Loss: 69.20758056640625\n",
      "Epoch [79/100] Loss: 68.3028564453125\n",
      "Epoch [80/100] Loss: 67.41130828857422\n",
      "Epoch [81/100] Loss: 66.5328140258789\n",
      "Epoch [82/100] Loss: 65.66716003417969\n",
      "Epoch [83/100] Loss: 64.81415557861328\n",
      "Epoch [84/100] Loss: 63.97361755371094\n",
      "Epoch [85/100] Loss: 63.1453971862793\n",
      "Epoch [86/100] Loss: 62.32928466796875\n",
      "Epoch [87/100] Loss: 61.52512741088867\n",
      "Epoch [88/100] Loss: 60.73276138305664\n",
      "Epoch [89/100] Loss: 59.95201110839844\n",
      "Epoch [90/100] Loss: 59.182735443115234\n",
      "Epoch [91/100] Loss: 58.424774169921875\n",
      "Epoch [92/100] Loss: 57.67794418334961\n",
      "Epoch [93/100] Loss: 56.9421272277832\n",
      "Epoch [94/100] Loss: 56.21714401245117\n",
      "Epoch [95/100] Loss: 55.502864837646484\n",
      "Epoch [96/100] Loss: 54.79914474487305\n",
      "Epoch [97/100] Loss: 54.10585021972656\n",
      "Epoch [98/100] Loss: 53.42285919189453\n",
      "Epoch [99/100] Loss: 52.749977111816406\n",
      "Epoch [100/100] Loss: 52.08711242675781\n",
      "Predicted days_remaining for parent_id 211: 9.667513847351074\n",
      "Training for parent_id 215...\n",
      "Epoch [1/100] Loss: 370.005859375\n",
      "Epoch [2/100] Loss: 360.87786865234375\n",
      "Epoch [3/100] Loss: 352.0160827636719\n",
      "Epoch [4/100] Loss: 343.43035888671875\n",
      "Epoch [5/100] Loss: 335.1278991699219\n",
      "Epoch [6/100] Loss: 327.1023254394531\n",
      "Epoch [7/100] Loss: 319.3388366699219\n",
      "Epoch [8/100] Loss: 311.82342529296875\n",
      "Epoch [9/100] Loss: 304.5488586425781\n",
      "Epoch [10/100] Loss: 297.5140380859375\n",
      "Epoch [11/100] Loss: 290.72113037109375\n",
      "Epoch [12/100] Loss: 284.171875\n",
      "Epoch [13/100] Loss: 277.86676025390625\n",
      "Epoch [14/100] Loss: 271.80426025390625\n",
      "Epoch [15/100] Loss: 265.9820861816406\n",
      "Epoch [16/100] Loss: 260.39691162109375\n",
      "Epoch [17/100] Loss: 255.04466247558594\n",
      "Epoch [18/100] Loss: 249.92054748535156\n",
      "Epoch [19/100] Loss: 245.0189971923828\n",
      "Epoch [20/100] Loss: 240.3335723876953\n",
      "Epoch [21/100] Loss: 235.85772705078125\n",
      "Epoch [22/100] Loss: 231.58482360839844\n",
      "Epoch [23/100] Loss: 227.50790405273438\n",
      "Epoch [24/100] Loss: 223.6193389892578\n",
      "Epoch [25/100] Loss: 219.91055297851562\n",
      "Epoch [26/100] Loss: 216.37159729003906\n",
      "Epoch [27/100] Loss: 212.9911346435547\n",
      "Epoch [28/100] Loss: 209.7569580078125\n",
      "Epoch [29/100] Loss: 206.65652465820312\n",
      "Epoch [30/100] Loss: 203.67764282226562\n",
      "Epoch [31/100] Loss: 200.809326171875\n",
      "Epoch [32/100] Loss: 198.04141235351562\n",
      "Epoch [33/100] Loss: 195.364990234375\n",
      "Epoch [34/100] Loss: 192.77212524414062\n",
      "Epoch [35/100] Loss: 190.255859375\n",
      "Epoch [36/100] Loss: 187.8097381591797\n",
      "Epoch [37/100] Loss: 185.4281005859375\n",
      "Epoch [38/100] Loss: 183.10565185546875\n",
      "Epoch [39/100] Loss: 180.8376922607422\n",
      "Epoch [40/100] Loss: 178.6199493408203\n",
      "Epoch [41/100] Loss: 176.4487762451172\n",
      "Epoch [42/100] Loss: 174.3208770751953\n",
      "Epoch [43/100] Loss: 172.23348999023438\n",
      "Epoch [44/100] Loss: 170.1841583251953\n",
      "Epoch [45/100] Loss: 168.17091369628906\n",
      "Epoch [46/100] Loss: 166.1918487548828\n",
      "Epoch [47/100] Loss: 164.245361328125\n",
      "Epoch [48/100] Loss: 162.3300018310547\n",
      "Epoch [49/100] Loss: 160.44464111328125\n",
      "Epoch [50/100] Loss: 158.58804321289062\n",
      "Epoch [51/100] Loss: 156.7592010498047\n",
      "Epoch [52/100] Loss: 154.957275390625\n",
      "Epoch [53/100] Loss: 153.18125915527344\n",
      "Epoch [54/100] Loss: 151.43045043945312\n",
      "Epoch [55/100] Loss: 149.7040557861328\n",
      "Epoch [56/100] Loss: 148.00144958496094\n",
      "Epoch [57/100] Loss: 146.32199096679688\n",
      "Epoch [58/100] Loss: 144.66497802734375\n",
      "Epoch [59/100] Loss: 143.0298614501953\n",
      "Epoch [60/100] Loss: 141.41619873046875\n",
      "Epoch [61/100] Loss: 139.82333374023438\n",
      "Epoch [62/100] Loss: 138.25091552734375\n",
      "Epoch [63/100] Loss: 136.69837951660156\n",
      "Epoch [64/100] Loss: 135.16534423828125\n",
      "Epoch [65/100] Loss: 133.65139770507812\n",
      "Epoch [66/100] Loss: 132.15614318847656\n",
      "Epoch [67/100] Loss: 130.67921447753906\n",
      "Epoch [68/100] Loss: 129.2202606201172\n",
      "Epoch [69/100] Loss: 127.7789306640625\n",
      "Epoch [70/100] Loss: 126.3549575805664\n",
      "Epoch [71/100] Loss: 124.94798278808594\n",
      "Epoch [72/100] Loss: 123.55775451660156\n",
      "Epoch [73/100] Loss: 122.18397521972656\n",
      "Epoch [74/100] Loss: 120.82640075683594\n",
      "Epoch [75/100] Loss: 119.48479461669922\n",
      "Epoch [76/100] Loss: 118.15889739990234\n",
      "Epoch [77/100] Loss: 116.84843444824219\n",
      "Epoch [78/100] Loss: 115.55323028564453\n",
      "Epoch [79/100] Loss: 114.27305603027344\n",
      "Epoch [80/100] Loss: 113.00767517089844\n",
      "Epoch [81/100] Loss: 111.75689697265625\n",
      "Epoch [82/100] Loss: 110.5205307006836\n",
      "Epoch [83/100] Loss: 109.29839324951172\n",
      "Epoch [84/100] Loss: 108.09025573730469\n",
      "Epoch [85/100] Loss: 106.89595031738281\n",
      "Epoch [86/100] Loss: 105.71532440185547\n",
      "Epoch [87/100] Loss: 104.54817962646484\n",
      "Epoch [88/100] Loss: 103.39434814453125\n",
      "Epoch [89/100] Loss: 102.2536849975586\n",
      "Epoch [90/100] Loss: 101.1259765625\n",
      "Epoch [91/100] Loss: 100.01109313964844\n",
      "Epoch [92/100] Loss: 98.90890502929688\n",
      "Epoch [93/100] Loss: 97.81919860839844\n",
      "Epoch [94/100] Loss: 96.74189758300781\n",
      "Epoch [95/100] Loss: 95.67679595947266\n",
      "Epoch [96/100] Loss: 94.62378692626953\n",
      "Epoch [97/100] Loss: 93.58272552490234\n",
      "Epoch [98/100] Loss: 92.5534439086914\n",
      "Epoch [99/100] Loss: 91.53585815429688\n",
      "Epoch [100/100] Loss: 90.52982330322266\n",
      "Predicted days_remaining for parent_id 215: 10.084108352661133\n",
      "Training for parent_id 218...\n",
      "Epoch [1/100] Loss: 890.8713989257812\n",
      "Epoch [2/100] Loss: 877.8358764648438\n",
      "Epoch [3/100] Loss: 865.0093383789062\n",
      "Epoch [4/100] Loss: 852.37548828125\n",
      "Epoch [5/100] Loss: 839.9446411132812\n",
      "Epoch [6/100] Loss: 827.727783203125\n",
      "Epoch [7/100] Loss: 815.7296752929688\n",
      "Epoch [8/100] Loss: 803.9509887695312\n",
      "Epoch [9/100] Loss: 792.3947143554688\n",
      "Epoch [10/100] Loss: 781.0715942382812\n",
      "Epoch [11/100] Loss: 770.0016479492188\n",
      "Epoch [12/100] Loss: 759.2127685546875\n",
      "Epoch [13/100] Loss: 748.73779296875\n",
      "Epoch [14/100] Loss: 738.609619140625\n",
      "Epoch [15/100] Loss: 728.8585205078125\n",
      "Epoch [16/100] Loss: 719.51025390625\n",
      "Epoch [17/100] Loss: 710.58349609375\n",
      "Epoch [18/100] Loss: 702.0911254882812\n",
      "Epoch [19/100] Loss: 694.037353515625\n",
      "Epoch [20/100] Loss: 686.41748046875\n",
      "Epoch [21/100] Loss: 679.2166137695312\n",
      "Epoch [22/100] Loss: 672.4111328125\n",
      "Epoch [23/100] Loss: 665.9712524414062\n",
      "Epoch [24/100] Loss: 659.8642578125\n",
      "Epoch [25/100] Loss: 654.0567626953125\n",
      "Epoch [26/100] Loss: 648.5166015625\n",
      "Epoch [27/100] Loss: 643.214111328125\n",
      "Epoch [28/100] Loss: 638.1219482421875\n",
      "Epoch [29/100] Loss: 633.2152709960938\n",
      "Epoch [30/100] Loss: 628.4716796875\n",
      "Epoch [31/100] Loss: 623.8711547851562\n",
      "Epoch [32/100] Loss: 619.3956909179688\n",
      "Epoch [33/100] Loss: 615.0294189453125\n",
      "Epoch [34/100] Loss: 610.7586669921875\n",
      "Epoch [35/100] Loss: 606.5715942382812\n",
      "Epoch [36/100] Loss: 602.458251953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100] Loss: 598.410400390625\n",
      "Epoch [38/100] Loss: 594.42138671875\n",
      "Epoch [39/100] Loss: 590.4854125976562\n",
      "Epoch [40/100] Loss: 586.5985107421875\n",
      "Epoch [41/100] Loss: 582.7569580078125\n",
      "Epoch [42/100] Loss: 578.957763671875\n",
      "Epoch [43/100] Loss: 575.1990966796875\n",
      "Epoch [44/100] Loss: 571.478759765625\n",
      "Epoch [45/100] Loss: 567.7952880859375\n",
      "Epoch [46/100] Loss: 564.1475219726562\n",
      "Epoch [47/100] Loss: 560.5339965820312\n",
      "Epoch [48/100] Loss: 556.953857421875\n",
      "Epoch [49/100] Loss: 553.4058837890625\n",
      "Epoch [50/100] Loss: 549.8892211914062\n",
      "Epoch [51/100] Loss: 546.4030151367188\n",
      "Epoch [52/100] Loss: 542.9464721679688\n",
      "Epoch [53/100] Loss: 539.5185546875\n",
      "Epoch [54/100] Loss: 536.118896484375\n",
      "Epoch [55/100] Loss: 532.7464599609375\n",
      "Epoch [56/100] Loss: 529.4007568359375\n",
      "Epoch [57/100] Loss: 526.0811767578125\n",
      "Epoch [58/100] Loss: 522.7869873046875\n",
      "Epoch [59/100] Loss: 519.5178833007812\n",
      "Epoch [60/100] Loss: 516.2733154296875\n",
      "Epoch [61/100] Loss: 513.052490234375\n",
      "Epoch [62/100] Loss: 509.8553466796875\n",
      "Epoch [63/100] Loss: 506.68133544921875\n",
      "Epoch [64/100] Loss: 503.5299987792969\n",
      "Epoch [65/100] Loss: 500.40093994140625\n",
      "Epoch [66/100] Loss: 497.2937927246094\n",
      "Epoch [67/100] Loss: 494.20831298828125\n",
      "Epoch [68/100] Loss: 491.1440124511719\n",
      "Epoch [69/100] Loss: 488.1007385253906\n",
      "Epoch [70/100] Loss: 485.07806396484375\n",
      "Epoch [71/100] Loss: 482.0756530761719\n",
      "Epoch [72/100] Loss: 479.0932922363281\n",
      "Epoch [73/100] Loss: 476.130615234375\n",
      "Epoch [74/100] Loss: 473.1875305175781\n",
      "Epoch [75/100] Loss: 470.2636413574219\n",
      "Epoch [76/100] Loss: 467.3586730957031\n",
      "Epoch [77/100] Loss: 464.4725646972656\n",
      "Epoch [78/100] Loss: 461.604736328125\n",
      "Epoch [79/100] Loss: 458.75543212890625\n",
      "Epoch [80/100] Loss: 455.92401123046875\n",
      "Epoch [81/100] Loss: 453.11053466796875\n",
      "Epoch [82/100] Loss: 450.3146057128906\n",
      "Epoch [83/100] Loss: 447.53619384765625\n",
      "Epoch [84/100] Loss: 444.7749328613281\n",
      "Epoch [85/100] Loss: 442.03070068359375\n",
      "Epoch [86/100] Loss: 439.3034973144531\n",
      "Epoch [87/100] Loss: 436.5928955078125\n",
      "Epoch [88/100] Loss: 433.8988342285156\n",
      "Epoch [89/100] Loss: 431.22100830078125\n",
      "Epoch [90/100] Loss: 428.5594482421875\n",
      "Epoch [91/100] Loss: 425.91400146484375\n",
      "Epoch [92/100] Loss: 423.2843322753906\n",
      "Epoch [93/100] Loss: 420.67041015625\n",
      "Epoch [94/100] Loss: 418.0721130371094\n",
      "Epoch [95/100] Loss: 415.4891357421875\n",
      "Epoch [96/100] Loss: 412.9214782714844\n",
      "Epoch [97/100] Loss: 410.36907958984375\n",
      "Epoch [98/100] Loss: 407.8315734863281\n",
      "Epoch [99/100] Loss: 405.3089599609375\n",
      "Epoch [100/100] Loss: 402.8011474609375\n",
      "Predicted days_remaining for parent_id 218: 10.106412887573242\n",
      "Training for parent_id 234...\n",
      "Epoch [1/100] Loss: 1300.3321533203125\n",
      "Epoch [2/100] Loss: 1284.500732421875\n",
      "Epoch [3/100] Loss: 1268.7437744140625\n",
      "Epoch [4/100] Loss: 1253.109130859375\n",
      "Epoch [5/100] Loss: 1237.632568359375\n",
      "Epoch [6/100] Loss: 1222.34033203125\n",
      "Epoch [7/100] Loss: 1207.2635498046875\n",
      "Epoch [8/100] Loss: 1192.4454345703125\n",
      "Epoch [9/100] Loss: 1177.9356689453125\n",
      "Epoch [10/100] Loss: 1163.78369140625\n",
      "Epoch [11/100] Loss: 1150.0364990234375\n",
      "Epoch [12/100] Loss: 1136.7354736328125\n",
      "Epoch [13/100] Loss: 1123.9134521484375\n",
      "Epoch [14/100] Loss: 1111.590576171875\n",
      "Epoch [15/100] Loss: 1099.7740478515625\n",
      "Epoch [16/100] Loss: 1088.4581298828125\n",
      "Epoch [17/100] Loss: 1077.628662109375\n",
      "Epoch [18/100] Loss: 1067.266845703125\n",
      "Epoch [19/100] Loss: 1057.3519287109375\n",
      "Epoch [20/100] Loss: 1047.8624267578125\n",
      "Epoch [21/100] Loss: 1038.774169921875\n",
      "Epoch [22/100] Loss: 1030.0635986328125\n",
      "Epoch [23/100] Loss: 1021.7079467773438\n",
      "Epoch [24/100] Loss: 1013.6863403320312\n",
      "Epoch [25/100] Loss: 1005.9793090820312\n",
      "Epoch [26/100] Loss: 998.569091796875\n",
      "Epoch [27/100] Loss: 991.4371337890625\n",
      "Epoch [28/100] Loss: 984.5647583007812\n",
      "Epoch [29/100] Loss: 977.932373046875\n",
      "Epoch [30/100] Loss: 971.5200805664062\n",
      "Epoch [31/100] Loss: 965.3075561523438\n",
      "Epoch [32/100] Loss: 959.2752685546875\n",
      "Epoch [33/100] Loss: 953.4053344726562\n",
      "Epoch [34/100] Loss: 947.6808471679688\n",
      "Epoch [35/100] Loss: 942.0870971679688\n",
      "Epoch [36/100] Loss: 936.6112060546875\n",
      "Epoch [37/100] Loss: 931.2420654296875\n",
      "Epoch [38/100] Loss: 925.9700317382812\n",
      "Epoch [39/100] Loss: 920.7869873046875\n",
      "Epoch [40/100] Loss: 915.6857299804688\n",
      "Epoch [41/100] Loss: 910.6600952148438\n",
      "Epoch [42/100] Loss: 905.7049560546875\n",
      "Epoch [43/100] Loss: 900.8150634765625\n",
      "Epoch [44/100] Loss: 895.9866333007812\n",
      "Epoch [45/100] Loss: 891.215576171875\n",
      "Epoch [46/100] Loss: 886.4984130859375\n",
      "Epoch [47/100] Loss: 881.8321533203125\n",
      "Epoch [48/100] Loss: 877.2138671875\n",
      "Epoch [49/100] Loss: 872.6406860351562\n",
      "Epoch [50/100] Loss: 868.1107788085938\n",
      "Epoch [51/100] Loss: 863.6217041015625\n",
      "Epoch [52/100] Loss: 859.1719360351562\n",
      "Epoch [53/100] Loss: 854.7593383789062\n",
      "Epoch [54/100] Loss: 850.3828125\n",
      "Epoch [55/100] Loss: 846.040771484375\n",
      "Epoch [56/100] Loss: 841.73193359375\n",
      "Epoch [57/100] Loss: 837.4552612304688\n",
      "Epoch [58/100] Loss: 833.2097778320312\n",
      "Epoch [59/100] Loss: 828.994384765625\n",
      "Epoch [60/100] Loss: 824.8084106445312\n",
      "Epoch [61/100] Loss: 820.6509399414062\n",
      "Epoch [62/100] Loss: 816.521484375\n",
      "Epoch [63/100] Loss: 812.419189453125\n",
      "Epoch [64/100] Loss: 808.34326171875\n",
      "Epoch [65/100] Loss: 804.2935180664062\n",
      "Epoch [66/100] Loss: 800.269287109375\n",
      "Epoch [67/100] Loss: 796.2698364257812\n",
      "Epoch [68/100] Loss: 792.2948608398438\n",
      "Epoch [69/100] Loss: 788.3439331054688\n",
      "Epoch [70/100] Loss: 784.4166259765625\n",
      "Epoch [71/100] Loss: 780.5123901367188\n",
      "Epoch [72/100] Loss: 776.6311645507812\n",
      "Epoch [73/100] Loss: 772.772216796875\n",
      "Epoch [74/100] Loss: 768.9354248046875\n",
      "Epoch [75/100] Loss: 765.1202392578125\n",
      "Epoch [76/100] Loss: 761.3267211914062\n",
      "Epoch [77/100] Loss: 757.5538940429688\n",
      "Epoch [78/100] Loss: 753.8021240234375\n",
      "Epoch [79/100] Loss: 750.0706787109375\n",
      "Epoch [80/100] Loss: 746.359619140625\n",
      "Epoch [81/100] Loss: 742.6685791015625\n",
      "Epoch [82/100] Loss: 738.9970703125\n",
      "Epoch [83/100] Loss: 735.3451538085938\n",
      "Epoch [84/100] Loss: 731.7122802734375\n",
      "Epoch [85/100] Loss: 728.0984497070312\n",
      "Epoch [86/100] Loss: 724.50341796875\n",
      "Epoch [87/100] Loss: 720.9269409179688\n",
      "Epoch [88/100] Loss: 717.3685302734375\n",
      "Epoch [89/100] Loss: 713.8285522460938\n",
      "Epoch [90/100] Loss: 710.3062744140625\n",
      "Epoch [91/100] Loss: 706.8019409179688\n",
      "Epoch [92/100] Loss: 703.3148803710938\n",
      "Epoch [93/100] Loss: 699.8452758789062\n",
      "Epoch [94/100] Loss: 696.3930053710938\n",
      "Epoch [95/100] Loss: 692.95751953125\n",
      "Epoch [96/100] Loss: 689.5389404296875\n",
      "Epoch [97/100] Loss: 686.1370239257812\n",
      "Epoch [98/100] Loss: 682.751708984375\n",
      "Epoch [99/100] Loss: 679.3826904296875\n",
      "Epoch [100/100] Loss: 676.0299682617188\n",
      "Predicted days_remaining for parent_id 234: 10.093504905700684\n",
      "Training for parent_id 238...\n",
      "Epoch [1/100] Loss: 863.4985961914062\n",
      "Epoch [2/100] Loss: 850.3252563476562\n",
      "Epoch [3/100] Loss: 837.4344482421875\n",
      "Epoch [4/100] Loss: 824.9057006835938\n",
      "Epoch [5/100] Loss: 812.7681274414062\n",
      "Epoch [6/100] Loss: 801.0123291015625\n",
      "Epoch [7/100] Loss: 789.60791015625\n",
      "Epoch [8/100] Loss: 778.5263061523438\n",
      "Epoch [9/100] Loss: 767.7564697265625\n",
      "Epoch [10/100] Loss: 757.3076171875\n",
      "Epoch [11/100] Loss: 747.203125\n",
      "Epoch [12/100] Loss: 737.4724731445312\n",
      "Epoch [13/100] Loss: 728.1436767578125\n",
      "Epoch [14/100] Loss: 719.2359008789062\n",
      "Epoch [15/100] Loss: 710.7556762695312\n",
      "Epoch [16/100] Loss: 702.6939086914062\n",
      "Epoch [17/100] Loss: 695.0283203125\n",
      "Epoch [18/100] Loss: 687.7269287109375\n",
      "Epoch [19/100] Loss: 680.7534790039062\n",
      "Epoch [20/100] Loss: 674.0724487304688\n",
      "Epoch [21/100] Loss: 667.6513671875\n",
      "Epoch [22/100] Loss: 661.4620361328125\n",
      "Epoch [23/100] Loss: 655.4808349609375\n",
      "Epoch [24/100] Loss: 649.6879272460938\n",
      "Epoch [25/100] Loss: 644.0675048828125\n",
      "Epoch [26/100] Loss: 638.6064453125\n",
      "Epoch [27/100] Loss: 633.2945556640625\n",
      "Epoch [28/100] Loss: 628.1234741210938\n",
      "Epoch [29/100] Loss: 623.086181640625\n",
      "Epoch [30/100] Loss: 618.17578125\n",
      "Epoch [31/100] Loss: 613.3854370117188\n",
      "Epoch [32/100] Loss: 608.7077026367188\n",
      "Epoch [33/100] Loss: 604.1345825195312\n",
      "Epoch [34/100] Loss: 599.6578979492188\n",
      "Epoch [35/100] Loss: 595.2696533203125\n",
      "Epoch [36/100] Loss: 590.9622192382812\n",
      "Epoch [37/100] Loss: 586.72900390625\n",
      "Epoch [38/100] Loss: 582.5641479492188\n",
      "Epoch [39/100] Loss: 578.4624633789062\n",
      "Epoch [40/100] Loss: 574.4195556640625\n",
      "Epoch [41/100] Loss: 570.431884765625\n",
      "Epoch [42/100] Loss: 566.4961547851562\n",
      "Epoch [43/100] Loss: 562.6092529296875\n",
      "Epoch [44/100] Loss: 558.7687377929688\n",
      "Epoch [45/100] Loss: 554.9722290039062\n",
      "Epoch [46/100] Loss: 551.2175903320312\n",
      "Epoch [47/100] Loss: 547.502685546875\n",
      "Epoch [48/100] Loss: 543.8260498046875\n",
      "Epoch [49/100] Loss: 540.1859741210938\n",
      "Epoch [50/100] Loss: 536.5810546875\n",
      "Epoch [51/100] Loss: 533.0101318359375\n",
      "Epoch [52/100] Loss: 529.4721069335938\n",
      "Epoch [53/100] Loss: 525.9660034179688\n",
      "Epoch [54/100] Loss: 522.490966796875\n",
      "Epoch [55/100] Loss: 519.04638671875\n",
      "Epoch [56/100] Loss: 515.6312255859375\n",
      "Epoch [57/100] Loss: 512.2449951171875\n",
      "Epoch [58/100] Loss: 508.88714599609375\n",
      "Epoch [59/100] Loss: 505.5570068359375\n",
      "Epoch [60/100] Loss: 502.25408935546875\n",
      "Epoch [61/100] Loss: 498.97784423828125\n",
      "Epoch [62/100] Loss: 495.7278137207031\n",
      "Epoch [63/100] Loss: 492.5035705566406\n",
      "Epoch [64/100] Loss: 489.3045654296875\n",
      "Epoch [65/100] Loss: 486.1304626464844\n",
      "Epoch [66/100] Loss: 482.98077392578125\n",
      "Epoch [67/100] Loss: 479.8551025390625\n",
      "Epoch [68/100] Loss: 476.7530212402344\n",
      "Epoch [69/100] Loss: 473.67425537109375\n",
      "Epoch [70/100] Loss: 470.6184387207031\n",
      "Epoch [71/100] Loss: 467.58502197265625\n",
      "Epoch [72/100] Loss: 464.5739440917969\n",
      "Epoch [73/100] Loss: 461.5845947265625\n",
      "Epoch [74/100] Loss: 458.6167907714844\n",
      "Epoch [75/100] Loss: 455.6702575683594\n",
      "Epoch [76/100] Loss: 452.7445983886719\n",
      "Epoch [77/100] Loss: 449.8395690917969\n",
      "Epoch [78/100] Loss: 446.9548034667969\n",
      "Epoch [79/100] Loss: 444.0902099609375\n",
      "Epoch [80/100] Loss: 441.2452392578125\n",
      "Epoch [81/100] Loss: 438.4198913574219\n",
      "Epoch [82/100] Loss: 435.61370849609375\n",
      "Epoch [83/100] Loss: 432.8266296386719\n",
      "Epoch [84/100] Loss: 430.0583801269531\n",
      "Epoch [85/100] Loss: 427.30859375\n",
      "Epoch [86/100] Loss: 424.5771179199219\n",
      "Epoch [87/100] Loss: 421.8637390136719\n",
      "Epoch [88/100] Loss: 419.1681823730469\n",
      "Epoch [89/100] Loss: 416.4904479980469\n",
      "Epoch [90/100] Loss: 413.8301086425781\n",
      "Epoch [91/100] Loss: 411.1871643066406\n",
      "Epoch [92/100] Loss: 408.56109619140625\n",
      "Epoch [93/100] Loss: 405.95208740234375\n",
      "Epoch [94/100] Loss: 403.35986328125\n",
      "Epoch [95/100] Loss: 400.7840576171875\n",
      "Epoch [96/100] Loss: 398.2246398925781\n",
      "Epoch [97/100] Loss: 395.6815185546875\n",
      "Epoch [98/100] Loss: 393.154296875\n",
      "Epoch [99/100] Loss: 390.64312744140625\n",
      "Epoch [100/100] Loss: 388.1476135253906\n",
      "Predicted days_remaining for parent_id 238: 9.482666969299316\n",
      "Training for parent_id 245...\n",
      "Epoch [1/100] Loss: 1355.6361083984375\n",
      "Epoch [2/100] Loss: 1338.237060546875\n",
      "Epoch [3/100] Loss: 1320.993408203125\n",
      "Epoch [4/100] Loss: 1303.9471435546875\n",
      "Epoch [5/100] Loss: 1287.18603515625\n",
      "Epoch [6/100] Loss: 1270.81689453125\n",
      "Epoch [7/100] Loss: 1254.9383544921875\n",
      "Epoch [8/100] Loss: 1239.62890625\n",
      "Epoch [9/100] Loss: 1224.9388427734375\n",
      "Epoch [10/100] Loss: 1210.8905029296875\n",
      "Epoch [11/100] Loss: 1197.4852294921875\n",
      "Epoch [12/100] Loss: 1184.7120361328125\n",
      "Epoch [13/100] Loss: 1172.5545654296875\n",
      "Epoch [14/100] Loss: 1160.9945068359375\n",
      "Epoch [15/100] Loss: 1150.00927734375\n",
      "Epoch [16/100] Loss: 1139.5733642578125\n",
      "Epoch [17/100] Loss: 1129.656982421875\n",
      "Epoch [18/100] Loss: 1120.2296142578125\n",
      "Epoch [19/100] Loss: 1111.25927734375\n",
      "Epoch [20/100] Loss: 1102.7147216796875\n",
      "Epoch [21/100] Loss: 1094.56591796875\n",
      "Epoch [22/100] Loss: 1086.78271484375\n",
      "Epoch [23/100] Loss: 1079.3367919921875\n",
      "Epoch [24/100] Loss: 1072.1998291015625\n",
      "Epoch [25/100] Loss: 1065.3438720703125\n",
      "Epoch [26/100] Loss: 1058.741943359375\n",
      "Epoch [27/100] Loss: 1052.3670654296875\n",
      "Epoch [28/100] Loss: 1046.19287109375\n",
      "Epoch [29/100] Loss: 1040.1943359375\n",
      "Epoch [30/100] Loss: 1034.3489990234375\n",
      "Epoch [31/100] Loss: 1028.6361083984375\n",
      "Epoch [32/100] Loss: 1023.0372924804688\n",
      "Epoch [33/100] Loss: 1017.5374145507812\n",
      "Epoch [34/100] Loss: 1012.1233520507812\n",
      "Epoch [35/100] Loss: 1006.7846069335938\n",
      "Epoch [36/100] Loss: 1001.5130004882812\n",
      "Epoch [37/100] Loss: 996.3019409179688\n",
      "Epoch [38/100] Loss: 991.1470947265625\n",
      "Epoch [39/100] Loss: 986.0443725585938\n",
      "Epoch [40/100] Loss: 980.9908447265625\n",
      "Epoch [41/100] Loss: 975.98486328125\n",
      "Epoch [42/100] Loss: 971.0245361328125\n",
      "Epoch [43/100] Loss: 966.1088256835938\n",
      "Epoch [44/100] Loss: 961.2362060546875\n",
      "Epoch [45/100] Loss: 956.4055786132812\n",
      "Epoch [46/100] Loss: 951.6160888671875\n",
      "Epoch [47/100] Loss: 946.8663940429688\n",
      "Epoch [48/100] Loss: 942.1556396484375\n",
      "Epoch [49/100] Loss: 937.4827880859375\n",
      "Epoch [50/100] Loss: 932.8468017578125\n",
      "Epoch [51/100] Loss: 928.24658203125\n",
      "Epoch [52/100] Loss: 923.68115234375\n",
      "Epoch [53/100] Loss: 919.1497802734375\n",
      "Epoch [54/100] Loss: 914.6512451171875\n",
      "Epoch [55/100] Loss: 910.1848754882812\n",
      "Epoch [56/100] Loss: 905.7496337890625\n",
      "Epoch [57/100] Loss: 901.3453369140625\n",
      "Epoch [58/100] Loss: 896.9706420898438\n",
      "Epoch [59/100] Loss: 892.6250610351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100] Loss: 888.3079223632812\n",
      "Epoch [61/100] Loss: 884.018798828125\n",
      "Epoch [62/100] Loss: 879.7567749023438\n",
      "Epoch [63/100] Loss: 875.5214233398438\n",
      "Epoch [64/100] Loss: 871.3124389648438\n",
      "Epoch [65/100] Loss: 867.12890625\n",
      "Epoch [66/100] Loss: 862.9706420898438\n",
      "Epoch [67/100] Loss: 858.837158203125\n",
      "Epoch [68/100] Loss: 854.7278442382812\n",
      "Epoch [69/100] Loss: 850.6425170898438\n",
      "Epoch [70/100] Loss: 846.5804443359375\n",
      "Epoch [71/100] Loss: 842.5416259765625\n",
      "Epoch [72/100] Loss: 838.5254516601562\n",
      "Epoch [73/100] Loss: 834.531494140625\n",
      "Epoch [74/100] Loss: 830.5598754882812\n",
      "Epoch [75/100] Loss: 826.6095581054688\n",
      "Epoch [76/100] Loss: 822.6807250976562\n",
      "Epoch [77/100] Loss: 818.7730712890625\n",
      "Epoch [78/100] Loss: 814.885986328125\n",
      "Epoch [79/100] Loss: 811.0194702148438\n",
      "Epoch [80/100] Loss: 807.1729736328125\n",
      "Epoch [81/100] Loss: 803.3465576171875\n",
      "Epoch [82/100] Loss: 799.5397338867188\n",
      "Epoch [83/100] Loss: 795.7523803710938\n",
      "Epoch [84/100] Loss: 791.9842529296875\n",
      "Epoch [85/100] Loss: 788.2349853515625\n",
      "Epoch [86/100] Loss: 784.50439453125\n",
      "Epoch [87/100] Loss: 780.7922973632812\n",
      "Epoch [88/100] Loss: 777.0985107421875\n",
      "Epoch [89/100] Loss: 773.4227905273438\n",
      "Epoch [90/100] Loss: 769.7650146484375\n",
      "Epoch [91/100] Loss: 766.1248779296875\n",
      "Epoch [92/100] Loss: 762.5022583007812\n",
      "Epoch [93/100] Loss: 758.89697265625\n",
      "Epoch [94/100] Loss: 755.308837890625\n",
      "Epoch [95/100] Loss: 751.7376098632812\n",
      "Epoch [96/100] Loss: 748.1832885742188\n",
      "Epoch [97/100] Loss: 744.6455078125\n",
      "Epoch [98/100] Loss: 741.124267578125\n",
      "Epoch [99/100] Loss: 737.6193237304688\n",
      "Epoch [100/100] Loss: 734.1305541992188\n",
      "Predicted days_remaining for parent_id 245: 9.987703323364258\n",
      "Training for parent_id 252...\n",
      "Epoch [1/100] Loss: 125.03658294677734\n",
      "Epoch [2/100] Loss: 119.78514099121094\n",
      "Epoch [3/100] Loss: 114.72355651855469\n",
      "Epoch [4/100] Loss: 109.87086486816406\n",
      "Epoch [5/100] Loss: 105.22805786132812\n",
      "Epoch [6/100] Loss: 100.79264831542969\n",
      "Epoch [7/100] Loss: 96.56403350830078\n",
      "Epoch [8/100] Loss: 92.54325866699219\n",
      "Epoch [9/100] Loss: 88.73220825195312\n",
      "Epoch [10/100] Loss: 85.13237762451172\n",
      "Epoch [11/100] Loss: 81.74384307861328\n",
      "Epoch [12/100] Loss: 78.56465911865234\n",
      "Epoch [13/100] Loss: 75.59049224853516\n",
      "Epoch [14/100] Loss: 72.81460571289062\n",
      "Epoch [15/100] Loss: 70.22801971435547\n",
      "Epoch [16/100] Loss: 67.82003784179688\n",
      "Epoch [17/100] Loss: 65.57890319824219\n",
      "Epoch [18/100] Loss: 63.49225616455078\n",
      "Epoch [19/100] Loss: 61.54774475097656\n",
      "Epoch [20/100] Loss: 59.73320388793945\n",
      "Epoch [21/100] Loss: 58.036991119384766\n",
      "Epoch [22/100] Loss: 56.44809341430664\n",
      "Epoch [23/100] Loss: 54.95615768432617\n",
      "Epoch [24/100] Loss: 53.55166244506836\n",
      "Epoch [25/100] Loss: 52.225894927978516\n",
      "Epoch [26/100] Loss: 50.970977783203125\n",
      "Epoch [27/100] Loss: 49.77985382080078\n",
      "Epoch [28/100] Loss: 48.6462287902832\n",
      "Epoch [29/100] Loss: 47.564544677734375\n",
      "Epoch [30/100] Loss: 46.529945373535156\n",
      "Epoch [31/100] Loss: 45.53813171386719\n",
      "Epoch [32/100] Loss: 44.585384368896484\n",
      "Epoch [33/100] Loss: 43.668460845947266\n",
      "Epoch [34/100] Loss: 42.7845573425293\n",
      "Epoch [35/100] Loss: 41.93123245239258\n",
      "Epoch [36/100] Loss: 41.10637664794922\n",
      "Epoch [37/100] Loss: 40.308162689208984\n",
      "Epoch [38/100] Loss: 39.53499221801758\n",
      "Epoch [39/100] Loss: 38.78545379638672\n",
      "Epoch [40/100] Loss: 38.058326721191406\n",
      "Epoch [41/100] Loss: 37.35251235961914\n",
      "Epoch [42/100] Loss: 36.66702651977539\n",
      "Epoch [43/100] Loss: 36.00096893310547\n",
      "Epoch [44/100] Loss: 35.35353469848633\n",
      "Epoch [45/100] Loss: 34.723976135253906\n",
      "Epoch [46/100] Loss: 34.111595153808594\n",
      "Epoch [47/100] Loss: 33.515750885009766\n",
      "Epoch [48/100] Loss: 32.93584060668945\n",
      "Epoch [49/100] Loss: 32.371299743652344\n",
      "Epoch [50/100] Loss: 31.821590423583984\n",
      "Epoch [51/100] Loss: 31.28621482849121\n",
      "Epoch [52/100] Loss: 30.76471519470215\n",
      "Epoch [53/100] Loss: 30.256656646728516\n",
      "Epoch [54/100] Loss: 29.761642456054688\n",
      "Epoch [55/100] Loss: 29.279312133789062\n",
      "Epoch [56/100] Loss: 28.809329986572266\n",
      "Epoch [57/100] Loss: 28.35139274597168\n",
      "Epoch [58/100] Loss: 27.905235290527344\n",
      "Epoch [59/100] Loss: 27.470605850219727\n",
      "Epoch [60/100] Loss: 27.047286987304688\n",
      "Epoch [61/100] Loss: 26.63508415222168\n",
      "Epoch [62/100] Loss: 26.23377799987793\n",
      "Epoch [63/100] Loss: 25.843177795410156\n",
      "Epoch [64/100] Loss: 25.46312713623047\n",
      "Epoch [65/100] Loss: 25.093406677246094\n",
      "Epoch [66/100] Loss: 24.733835220336914\n",
      "Epoch [67/100] Loss: 24.384233474731445\n",
      "Epoch [68/100] Loss: 24.04441261291504\n",
      "Epoch [69/100] Loss: 23.71417236328125\n",
      "Epoch [70/100] Loss: 23.393325805664062\n",
      "Epoch [71/100] Loss: 23.08169174194336\n",
      "Epoch [72/100] Loss: 22.779081344604492\n",
      "Epoch [73/100] Loss: 22.48530387878418\n",
      "Epoch [74/100] Loss: 22.200176239013672\n",
      "Epoch [75/100] Loss: 21.92352294921875\n",
      "Epoch [76/100] Loss: 21.655155181884766\n",
      "Epoch [77/100] Loss: 21.39488410949707\n",
      "Epoch [78/100] Loss: 21.142547607421875\n",
      "Epoch [79/100] Loss: 20.897951126098633\n",
      "Epoch [80/100] Loss: 20.660919189453125\n",
      "Epoch [81/100] Loss: 20.4312744140625\n",
      "Epoch [82/100] Loss: 20.208852767944336\n",
      "Epoch [83/100] Loss: 19.99346351623535\n",
      "Epoch [84/100] Loss: 19.784954071044922\n",
      "Epoch [85/100] Loss: 19.5831356048584\n",
      "Epoch [86/100] Loss: 19.387849807739258\n",
      "Epoch [87/100] Loss: 19.198932647705078\n",
      "Epoch [88/100] Loss: 19.016223907470703\n",
      "Epoch [89/100] Loss: 18.83954620361328\n",
      "Epoch [90/100] Loss: 18.66874885559082\n",
      "Epoch [91/100] Loss: 18.503684997558594\n",
      "Epoch [92/100] Loss: 18.344188690185547\n",
      "Epoch [93/100] Loss: 18.190109252929688\n",
      "Epoch [94/100] Loss: 18.04129981994629\n",
      "Epoch [95/100] Loss: 17.89761734008789\n",
      "Epoch [96/100] Loss: 17.758914947509766\n",
      "Epoch [97/100] Loss: 17.62505340576172\n",
      "Epoch [98/100] Loss: 17.495887756347656\n",
      "Epoch [99/100] Loss: 17.371288299560547\n",
      "Epoch [100/100] Loss: 17.25112533569336\n",
      "Predicted days_remaining for parent_id 252: 9.107513427734375\n",
      "Training for parent_id 256...\n",
      "Epoch [1/100] Loss: 107.72021484375\n",
      "Epoch [2/100] Loss: 103.04705810546875\n",
      "Epoch [3/100] Loss: 98.54434967041016\n",
      "Epoch [4/100] Loss: 94.2020263671875\n",
      "Epoch [5/100] Loss: 90.01644897460938\n",
      "Epoch [6/100] Loss: 85.99134063720703\n",
      "Epoch [7/100] Loss: 82.1358642578125\n",
      "Epoch [8/100] Loss: 78.46025085449219\n",
      "Epoch [9/100] Loss: 74.97222137451172\n",
      "Epoch [10/100] Loss: 71.67508697509766\n",
      "Epoch [11/100] Loss: 68.5669937133789\n",
      "Epoch [12/100] Loss: 65.64144134521484\n",
      "Epoch [13/100] Loss: 62.889068603515625\n",
      "Epoch [14/100] Loss: 60.29979705810547\n",
      "Epoch [15/100] Loss: 57.864383697509766\n",
      "Epoch [16/100] Loss: 55.57521438598633\n",
      "Epoch [17/100] Loss: 53.426055908203125\n",
      "Epoch [18/100] Loss: 51.411521911621094\n",
      "Epoch [19/100] Loss: 49.526302337646484\n",
      "Epoch [20/100] Loss: 47.764705657958984\n",
      "Epoch [21/100] Loss: 46.12031173706055\n",
      "Epoch [22/100] Loss: 44.58607864379883\n",
      "Epoch [23/100] Loss: 43.15446853637695\n",
      "Epoch [24/100] Loss: 41.81779861450195\n",
      "Epoch [25/100] Loss: 40.56850051879883\n",
      "Epoch [26/100] Loss: 39.399288177490234\n",
      "Epoch [27/100] Loss: 38.3032341003418\n",
      "Epoch [28/100] Loss: 37.27392578125\n",
      "Epoch [29/100] Loss: 36.30540466308594\n",
      "Epoch [30/100] Loss: 35.39220428466797\n",
      "Epoch [31/100] Loss: 34.52935028076172\n",
      "Epoch [32/100] Loss: 33.71232986450195\n",
      "Epoch [33/100] Loss: 32.93709182739258\n",
      "Epoch [34/100] Loss: 32.20000457763672\n",
      "Epoch [35/100] Loss: 31.497840881347656\n",
      "Epoch [36/100] Loss: 30.8277587890625\n",
      "Epoch [37/100] Loss: 30.187231063842773\n",
      "Epoch [38/100] Loss: 29.574058532714844\n",
      "Epoch [39/100] Loss: 28.98627471923828\n",
      "Epoch [40/100] Loss: 28.42217445373535\n",
      "Epoch [41/100] Loss: 27.880260467529297\n",
      "Epoch [42/100] Loss: 27.35919952392578\n",
      "Epoch [43/100] Loss: 26.85780143737793\n",
      "Epoch [44/100] Loss: 26.375028610229492\n",
      "Epoch [45/100] Loss: 25.90993881225586\n",
      "Epoch [46/100] Loss: 25.461694717407227\n",
      "Epoch [47/100] Loss: 25.029529571533203\n",
      "Epoch [48/100] Loss: 24.612762451171875\n",
      "Epoch [49/100] Loss: 24.21074676513672\n",
      "Epoch [50/100] Loss: 23.822914123535156\n",
      "Epoch [51/100] Loss: 23.448726654052734\n",
      "Epoch [52/100] Loss: 23.087678909301758\n",
      "Epoch [53/100] Loss: 22.73931884765625\n",
      "Epoch [54/100] Loss: 22.4031925201416\n",
      "Epoch [55/100] Loss: 22.078899383544922\n",
      "Epoch [56/100] Loss: 21.76604461669922\n",
      "Epoch [57/100] Loss: 21.464256286621094\n",
      "Epoch [58/100] Loss: 21.173187255859375\n",
      "Epoch [59/100] Loss: 20.89249038696289\n",
      "Epoch [60/100] Loss: 20.621845245361328\n",
      "Epoch [61/100] Loss: 20.360944747924805\n",
      "Epoch [62/100] Loss: 20.10947608947754\n",
      "Epoch [63/100] Loss: 19.867155075073242\n",
      "Epoch [64/100] Loss: 19.63370132446289\n",
      "Epoch [65/100] Loss: 19.40884017944336\n",
      "Epoch [66/100] Loss: 19.192310333251953\n",
      "Epoch [67/100] Loss: 18.983856201171875\n",
      "Epoch [68/100] Loss: 18.783226013183594\n",
      "Epoch [69/100] Loss: 18.590171813964844\n",
      "Epoch [70/100] Loss: 18.404470443725586\n",
      "Epoch [71/100] Loss: 18.225887298583984\n",
      "Epoch [72/100] Loss: 18.054189682006836\n",
      "Epoch [73/100] Loss: 17.88916778564453\n",
      "Epoch [74/100] Loss: 17.730615615844727\n",
      "Epoch [75/100] Loss: 17.578317642211914\n",
      "Epoch [76/100] Loss: 17.43207550048828\n",
      "Epoch [77/100] Loss: 17.291688919067383\n",
      "Epoch [78/100] Loss: 17.15696907043457\n",
      "Epoch [79/100] Loss: 17.027729034423828\n",
      "Epoch [80/100] Loss: 16.903785705566406\n",
      "Epoch [81/100] Loss: 16.78496742248535\n",
      "Epoch [82/100] Loss: 16.67108726501465\n",
      "Epoch [83/100] Loss: 16.56199073791504\n",
      "Epoch [84/100] Loss: 16.45750617980957\n",
      "Epoch [85/100] Loss: 16.357479095458984\n",
      "Epoch [86/100] Loss: 16.261741638183594\n",
      "Epoch [87/100] Loss: 16.170154571533203\n",
      "Epoch [88/100] Loss: 16.082563400268555\n",
      "Epoch [89/100] Loss: 15.998825073242188\n",
      "Epoch [90/100] Loss: 15.918797492980957\n",
      "Epoch [91/100] Loss: 15.842351913452148\n",
      "Epoch [92/100] Loss: 15.769346237182617\n",
      "Epoch [93/100] Loss: 15.699657440185547\n",
      "Epoch [94/100] Loss: 15.633159637451172\n",
      "Epoch [95/100] Loss: 15.56972599029541\n",
      "Epoch [96/100] Loss: 15.509243965148926\n",
      "Epoch [97/100] Loss: 15.451602935791016\n",
      "Epoch [98/100] Loss: 15.396679878234863\n",
      "Epoch [99/100] Loss: 15.344375610351562\n",
      "Epoch [100/100] Loss: 15.29458236694336\n",
      "Predicted days_remaining for parent_id 256: 8.850168228149414\n",
      "Training for parent_id 258...\n",
      "Epoch [1/100] Loss: 2246.620849609375\n",
      "Epoch [2/100] Loss: 2220.309814453125\n",
      "Epoch [3/100] Loss: 2194.0244140625\n",
      "Epoch [4/100] Loss: 2168.181396484375\n",
      "Epoch [5/100] Loss: 2143.233642578125\n",
      "Epoch [6/100] Loss: 2119.52294921875\n",
      "Epoch [7/100] Loss: 2097.236328125\n",
      "Epoch [8/100] Loss: 2076.421875\n",
      "Epoch [9/100] Loss: 2057.027587890625\n",
      "Epoch [10/100] Loss: 2038.9453125\n",
      "Epoch [11/100] Loss: 2022.048828125\n",
      "Epoch [12/100] Loss: 2006.221435546875\n",
      "Epoch [13/100] Loss: 1991.3621826171875\n",
      "Epoch [14/100] Loss: 1977.38623046875\n",
      "Epoch [15/100] Loss: 1964.2171630859375\n",
      "Epoch [16/100] Loss: 1951.78271484375\n",
      "Epoch [17/100] Loss: 1940.00830078125\n",
      "Epoch [18/100] Loss: 1928.8245849609375\n",
      "Epoch [19/100] Loss: 1918.166748046875\n",
      "Epoch [20/100] Loss: 1907.9759521484375\n",
      "Epoch [21/100] Loss: 1898.20068359375\n",
      "Epoch [22/100] Loss: 1888.794677734375\n",
      "Epoch [23/100] Loss: 1879.7176513671875\n",
      "Epoch [24/100] Loss: 1870.933837890625\n",
      "Epoch [25/100] Loss: 1862.4122314453125\n",
      "Epoch [26/100] Loss: 1854.1246337890625\n",
      "Epoch [27/100] Loss: 1846.0474853515625\n",
      "Epoch [28/100] Loss: 1838.1591796875\n",
      "Epoch [29/100] Loss: 1830.4417724609375\n",
      "Epoch [30/100] Loss: 1822.878662109375\n",
      "Epoch [31/100] Loss: 1815.456298828125\n",
      "Epoch [32/100] Loss: 1808.1614990234375\n",
      "Epoch [33/100] Loss: 1800.983642578125\n",
      "Epoch [34/100] Loss: 1793.913330078125\n",
      "Epoch [35/100] Loss: 1786.9417724609375\n",
      "Epoch [36/100] Loss: 1780.0615234375\n",
      "Epoch [37/100] Loss: 1773.2667236328125\n",
      "Epoch [38/100] Loss: 1766.55078125\n",
      "Epoch [39/100] Loss: 1759.9088134765625\n",
      "Epoch [40/100] Loss: 1753.3360595703125\n",
      "Epoch [41/100] Loss: 1746.8282470703125\n",
      "Epoch [42/100] Loss: 1740.3809814453125\n",
      "Epoch [43/100] Loss: 1733.990966796875\n",
      "Epoch [44/100] Loss: 1727.655029296875\n",
      "Epoch [45/100] Loss: 1721.3697509765625\n",
      "Epoch [46/100] Loss: 1715.1322021484375\n",
      "Epoch [47/100] Loss: 1708.9405517578125\n",
      "Epoch [48/100] Loss: 1702.791748046875\n",
      "Epoch [49/100] Loss: 1696.6842041015625\n",
      "Epoch [50/100] Loss: 1690.615966796875\n",
      "Epoch [51/100] Loss: 1684.5853271484375\n",
      "Epoch [52/100] Loss: 1678.5909423828125\n",
      "Epoch [53/100] Loss: 1672.6312255859375\n",
      "Epoch [54/100] Loss: 1666.7049560546875\n",
      "Epoch [55/100] Loss: 1660.8115234375\n",
      "Epoch [56/100] Loss: 1654.949462890625\n",
      "Epoch [57/100] Loss: 1649.1181640625\n",
      "Epoch [58/100] Loss: 1643.3162841796875\n",
      "Epoch [59/100] Loss: 1637.5433349609375\n",
      "Epoch [60/100] Loss: 1631.798828125\n",
      "Epoch [61/100] Loss: 1626.08203125\n",
      "Epoch [62/100] Loss: 1620.39208984375\n",
      "Epoch [63/100] Loss: 1614.7288818359375\n",
      "Epoch [64/100] Loss: 1609.0911865234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100] Loss: 1603.4791259765625\n",
      "Epoch [66/100] Loss: 1597.89208984375\n",
      "Epoch [67/100] Loss: 1592.32958984375\n",
      "Epoch [68/100] Loss: 1586.791015625\n",
      "Epoch [69/100] Loss: 1581.2762451171875\n",
      "Epoch [70/100] Loss: 1575.7847900390625\n",
      "Epoch [71/100] Loss: 1570.3160400390625\n",
      "Epoch [72/100] Loss: 1564.8701171875\n",
      "Epoch [73/100] Loss: 1559.4464111328125\n",
      "Epoch [74/100] Loss: 1554.0450439453125\n",
      "Epoch [75/100] Loss: 1548.6646728515625\n",
      "Epoch [76/100] Loss: 1543.30615234375\n",
      "Epoch [77/100] Loss: 1537.96826171875\n",
      "Epoch [78/100] Loss: 1532.6517333984375\n",
      "Epoch [79/100] Loss: 1527.3553466796875\n",
      "Epoch [80/100] Loss: 1522.0792236328125\n",
      "Epoch [81/100] Loss: 1516.8232421875\n",
      "Epoch [82/100] Loss: 1511.5870361328125\n",
      "Epoch [83/100] Loss: 1506.37060546875\n",
      "Epoch [84/100] Loss: 1501.1732177734375\n",
      "Epoch [85/100] Loss: 1495.9951171875\n",
      "Epoch [86/100] Loss: 1490.836181640625\n",
      "Epoch [87/100] Loss: 1485.69580078125\n",
      "Epoch [88/100] Loss: 1480.573974609375\n",
      "Epoch [89/100] Loss: 1475.470703125\n",
      "Epoch [90/100] Loss: 1470.3853759765625\n",
      "Epoch [91/100] Loss: 1465.318115234375\n",
      "Epoch [92/100] Loss: 1460.2689208984375\n",
      "Epoch [93/100] Loss: 1455.237060546875\n",
      "Epoch [94/100] Loss: 1450.2230224609375\n",
      "Epoch [95/100] Loss: 1445.2261962890625\n",
      "Epoch [96/100] Loss: 1440.24658203125\n",
      "Epoch [97/100] Loss: 1435.284423828125\n",
      "Epoch [98/100] Loss: 1430.3385009765625\n",
      "Epoch [99/100] Loss: 1425.4097900390625\n",
      "Epoch [100/100] Loss: 1420.497802734375\n",
      "Predicted days_remaining for parent_id 258: 10.317865371704102\n",
      "Training for parent_id 265...\n",
      "Epoch [1/100] Loss: 111.9654312133789\n",
      "Epoch [2/100] Loss: 106.49786376953125\n",
      "Epoch [3/100] Loss: 101.36735534667969\n",
      "Epoch [4/100] Loss: 96.59403228759766\n",
      "Epoch [5/100] Loss: 92.1717529296875\n",
      "Epoch [6/100] Loss: 88.08352661132812\n",
      "Epoch [7/100] Loss: 84.30708312988281\n",
      "Epoch [8/100] Loss: 80.81693267822266\n",
      "Epoch [9/100] Loss: 77.58556365966797\n",
      "Epoch [10/100] Loss: 74.58495330810547\n",
      "Epoch [11/100] Loss: 71.78839874267578\n",
      "Epoch [12/100] Loss: 69.1717529296875\n",
      "Epoch [13/100] Loss: 66.71441650390625\n",
      "Epoch [14/100] Loss: 64.39933776855469\n",
      "Epoch [15/100] Loss: 62.21283721923828\n",
      "Epoch [16/100] Loss: 60.144126892089844\n",
      "Epoch [17/100] Loss: 58.184783935546875\n",
      "Epoch [18/100] Loss: 56.32826614379883\n",
      "Epoch [19/100] Loss: 54.5693244934082\n",
      "Epoch [20/100] Loss: 52.90363693237305\n",
      "Epoch [21/100] Loss: 51.3272705078125\n",
      "Epoch [22/100] Loss: 49.836368560791016\n",
      "Epoch [23/100] Loss: 48.426822662353516\n",
      "Epoch [24/100] Loss: 47.094093322753906\n",
      "Epoch [25/100] Loss: 45.833309173583984\n",
      "Epoch [26/100] Loss: 44.639312744140625\n",
      "Epoch [27/100] Loss: 43.506919860839844\n",
      "Epoch [28/100] Loss: 42.4310302734375\n",
      "Epoch [29/100] Loss: 41.40687561035156\n",
      "Epoch [30/100] Loss: 40.430023193359375\n",
      "Epoch [31/100] Loss: 39.49650955200195\n",
      "Epoch [32/100] Loss: 38.60279846191406\n",
      "Epoch [33/100] Loss: 37.74578857421875\n",
      "Epoch [34/100] Loss: 36.92279815673828\n",
      "Epoch [35/100] Loss: 36.13147735595703\n",
      "Epoch [36/100] Loss: 35.36980438232422\n",
      "Epoch [37/100] Loss: 34.635982513427734\n",
      "Epoch [38/100] Loss: 33.9284782409668\n",
      "Epoch [39/100] Loss: 33.24588394165039\n",
      "Epoch [40/100] Loss: 32.58696365356445\n",
      "Epoch [41/100] Loss: 31.950592041015625\n",
      "Epoch [42/100] Loss: 31.33574867248535\n",
      "Epoch [43/100] Loss: 30.74146842956543\n",
      "Epoch [44/100] Loss: 30.166898727416992\n",
      "Epoch [45/100] Loss: 29.611217498779297\n",
      "Epoch [46/100] Loss: 29.073678970336914\n",
      "Epoch [47/100] Loss: 28.553569793701172\n",
      "Epoch [48/100] Loss: 28.05025291442871\n",
      "Epoch [49/100] Loss: 27.563112258911133\n",
      "Epoch [50/100] Loss: 27.09159278869629\n",
      "Epoch [51/100] Loss: 26.63514518737793\n",
      "Epoch [52/100] Loss: 26.193275451660156\n",
      "Epoch [53/100] Loss: 25.765512466430664\n",
      "Epoch [54/100] Loss: 25.351409912109375\n",
      "Epoch [55/100] Loss: 24.95054817199707\n",
      "Epoch [56/100] Loss: 24.562509536743164\n",
      "Epoch [57/100] Loss: 24.186922073364258\n",
      "Epoch [58/100] Loss: 23.82340431213379\n",
      "Epoch [59/100] Loss: 23.47160530090332\n",
      "Epoch [60/100] Loss: 23.131179809570312\n",
      "Epoch [61/100] Loss: 22.80179214477539\n",
      "Epoch [62/100] Loss: 22.483137130737305\n",
      "Epoch [63/100] Loss: 22.174901962280273\n",
      "Epoch [64/100] Loss: 21.876781463623047\n",
      "Epoch [65/100] Loss: 21.588497161865234\n",
      "Epoch [66/100] Loss: 21.309764862060547\n",
      "Epoch [67/100] Loss: 21.04031753540039\n",
      "Epoch [68/100] Loss: 20.779891967773438\n",
      "Epoch [69/100] Loss: 20.528234481811523\n",
      "Epoch [70/100] Loss: 20.285097122192383\n",
      "Epoch [71/100] Loss: 20.050235748291016\n",
      "Epoch [72/100] Loss: 19.823423385620117\n",
      "Epoch [73/100] Loss: 19.604429244995117\n",
      "Epoch [74/100] Loss: 19.393028259277344\n",
      "Epoch [75/100] Loss: 19.189006805419922\n",
      "Epoch [76/100] Loss: 18.992155075073242\n",
      "Epoch [77/100] Loss: 18.802263259887695\n",
      "Epoch [78/100] Loss: 18.6191349029541\n",
      "Epoch [79/100] Loss: 18.44257354736328\n",
      "Epoch [80/100] Loss: 18.272382736206055\n",
      "Epoch [81/100] Loss: 18.108383178710938\n",
      "Epoch [82/100] Loss: 17.95038414001465\n",
      "Epoch [83/100] Loss: 17.7982120513916\n",
      "Epoch [84/100] Loss: 17.65168571472168\n",
      "Epoch [85/100] Loss: 17.510644912719727\n",
      "Epoch [86/100] Loss: 17.37491226196289\n",
      "Epoch [87/100] Loss: 17.24432945251465\n",
      "Epoch [88/100] Loss: 17.118743896484375\n",
      "Epoch [89/100] Loss: 16.997982025146484\n",
      "Epoch [90/100] Loss: 16.881908416748047\n",
      "Epoch [91/100] Loss: 16.770368576049805\n",
      "Epoch [92/100] Loss: 16.663219451904297\n",
      "Epoch [93/100] Loss: 16.560314178466797\n",
      "Epoch [94/100] Loss: 16.46152114868164\n",
      "Epoch [95/100] Loss: 16.36669921875\n",
      "Epoch [96/100] Loss: 16.275724411010742\n",
      "Epoch [97/100] Loss: 16.188461303710938\n",
      "Epoch [98/100] Loss: 16.104787826538086\n",
      "Epoch [99/100] Loss: 16.024581909179688\n",
      "Epoch [100/100] Loss: 15.947722434997559\n",
      "Predicted days_remaining for parent_id 265: 8.551417350769043\n",
      "Training for parent_id 266...\n",
      "Epoch [1/100] Loss: 5744.25244140625\n",
      "Epoch [2/100] Loss: 5712.0\n",
      "Epoch [3/100] Loss: 5679.70654296875\n",
      "Epoch [4/100] Loss: 5647.4052734375\n",
      "Epoch [5/100] Loss: 5615.2705078125\n",
      "Epoch [6/100] Loss: 5583.505859375\n",
      "Epoch [7/100] Loss: 5552.298828125\n",
      "Epoch [8/100] Loss: 5521.81982421875\n",
      "Epoch [9/100] Loss: 5492.2119140625\n",
      "Epoch [10/100] Loss: 5463.5830078125\n",
      "Epoch [11/100] Loss: 5435.998046875\n",
      "Epoch [12/100] Loss: 5409.48193359375\n",
      "Epoch [13/100] Loss: 5384.02490234375\n",
      "Epoch [14/100] Loss: 5359.5908203125\n",
      "Epoch [15/100] Loss: 5336.13720703125\n",
      "Epoch [16/100] Loss: 5313.61181640625\n",
      "Epoch [17/100] Loss: 5291.9619140625\n",
      "Epoch [18/100] Loss: 5271.14013671875\n",
      "Epoch [19/100] Loss: 5251.10693359375\n",
      "Epoch [20/100] Loss: 5231.82763671875\n",
      "Epoch [21/100] Loss: 5213.275390625\n",
      "Epoch [22/100] Loss: 5195.42138671875\n",
      "Epoch [23/100] Loss: 5178.2392578125\n",
      "Epoch [24/100] Loss: 5161.69970703125\n",
      "Epoch [25/100] Loss: 5145.771484375\n",
      "Epoch [26/100] Loss: 5130.42236328125\n",
      "Epoch [27/100] Loss: 5115.61279296875\n",
      "Epoch [28/100] Loss: 5101.30517578125\n",
      "Epoch [29/100] Loss: 5087.455078125\n",
      "Epoch [30/100] Loss: 5074.02099609375\n",
      "Epoch [31/100] Loss: 5060.9619140625\n",
      "Epoch [32/100] Loss: 5048.23779296875\n",
      "Epoch [33/100] Loss: 5035.81201171875\n",
      "Epoch [34/100] Loss: 5023.6494140625\n",
      "Epoch [35/100] Loss: 5011.72216796875\n",
      "Epoch [36/100] Loss: 5000.0029296875\n",
      "Epoch [37/100] Loss: 4988.46826171875\n",
      "Epoch [38/100] Loss: 4977.10107421875\n",
      "Epoch [39/100] Loss: 4965.8818359375\n",
      "Epoch [40/100] Loss: 4954.7978515625\n",
      "Epoch [41/100] Loss: 4943.83544921875\n",
      "Epoch [42/100] Loss: 4932.9833984375\n",
      "Epoch [43/100] Loss: 4922.23046875\n",
      "Epoch [44/100] Loss: 4911.56689453125\n",
      "Epoch [45/100] Loss: 4900.9853515625\n",
      "Epoch [46/100] Loss: 4890.47705078125\n",
      "Epoch [47/100] Loss: 4880.03564453125\n",
      "Epoch [48/100] Loss: 4869.65576171875\n",
      "Epoch [49/100] Loss: 4859.33349609375\n",
      "Epoch [50/100] Loss: 4849.0634765625\n",
      "Epoch [51/100] Loss: 4838.84619140625\n",
      "Epoch [52/100] Loss: 4828.67724609375\n",
      "Epoch [53/100] Loss: 4818.5546875\n",
      "Epoch [54/100] Loss: 4808.4775390625\n",
      "Epoch [55/100] Loss: 4798.4453125\n",
      "Epoch [56/100] Loss: 4788.45556640625\n",
      "Epoch [57/100] Loss: 4778.5078125\n",
      "Epoch [58/100] Loss: 4768.60302734375\n",
      "Epoch [59/100] Loss: 4758.73681640625\n",
      "Epoch [60/100] Loss: 4748.9111328125\n",
      "Epoch [61/100] Loss: 4739.12353515625\n",
      "Epoch [62/100] Loss: 4729.37109375\n",
      "Epoch [63/100] Loss: 4719.65625\n",
      "Epoch [64/100] Loss: 4709.9755859375\n",
      "Epoch [65/100] Loss: 4700.3271484375\n",
      "Epoch [66/100] Loss: 4690.7119140625\n",
      "Epoch [67/100] Loss: 4681.1279296875\n",
      "Epoch [68/100] Loss: 4671.57568359375\n",
      "Epoch [69/100] Loss: 4662.0517578125\n",
      "Epoch [70/100] Loss: 4652.55859375\n",
      "Epoch [71/100] Loss: 4643.09423828125\n",
      "Epoch [72/100] Loss: 4633.65771484375\n",
      "Epoch [73/100] Loss: 4624.24755859375\n",
      "Epoch [74/100] Loss: 4614.865234375\n",
      "Epoch [75/100] Loss: 4605.5087890625\n",
      "Epoch [76/100] Loss: 4596.1787109375\n",
      "Epoch [77/100] Loss: 4586.87353515625\n",
      "Epoch [78/100] Loss: 4577.591796875\n",
      "Epoch [79/100] Loss: 4568.33447265625\n",
      "Epoch [80/100] Loss: 4559.09814453125\n",
      "Epoch [81/100] Loss: 4549.88427734375\n",
      "Epoch [82/100] Loss: 4540.69091796875\n",
      "Epoch [83/100] Loss: 4531.51611328125\n",
      "Epoch [84/100] Loss: 4522.359375\n",
      "Epoch [85/100] Loss: 4513.21923828125\n",
      "Epoch [86/100] Loss: 4504.095703125\n",
      "Epoch [87/100] Loss: 4494.98681640625\n",
      "Epoch [88/100] Loss: 4485.8916015625\n",
      "Epoch [89/100] Loss: 4476.8095703125\n",
      "Epoch [90/100] Loss: 4467.7392578125\n",
      "Epoch [91/100] Loss: 4458.68017578125\n",
      "Epoch [92/100] Loss: 4449.63232421875\n",
      "Epoch [93/100] Loss: 4440.5947265625\n",
      "Epoch [94/100] Loss: 4431.56982421875\n",
      "Epoch [95/100] Loss: 4422.5556640625\n",
      "Epoch [96/100] Loss: 4413.55322265625\n",
      "Epoch [97/100] Loss: 4404.56298828125\n",
      "Epoch [98/100] Loss: 4395.583984375\n",
      "Epoch [99/100] Loss: 4386.6201171875\n",
      "Epoch [100/100] Loss: 4377.66943359375\n",
      "Predicted days_remaining for parent_id 266: 9.762906074523926\n",
      "Training for parent_id 277...\n",
      "Epoch [1/100] Loss: 576.5425415039062\n",
      "Epoch [2/100] Loss: 563.8302612304688\n",
      "Epoch [3/100] Loss: 551.5098876953125\n",
      "Epoch [4/100] Loss: 539.6107177734375\n",
      "Epoch [5/100] Loss: 528.156982421875\n",
      "Epoch [6/100] Loss: 517.1486206054688\n",
      "Epoch [7/100] Loss: 506.5949401855469\n",
      "Epoch [8/100] Loss: 496.50738525390625\n",
      "Epoch [9/100] Loss: 486.88946533203125\n",
      "Epoch [10/100] Loss: 477.7311096191406\n",
      "Epoch [11/100] Loss: 469.00714111328125\n",
      "Epoch [12/100] Loss: 460.6856384277344\n",
      "Epoch [13/100] Loss: 452.7379150390625\n",
      "Epoch [14/100] Loss: 445.1427917480469\n",
      "Epoch [15/100] Loss: 437.88629150390625\n",
      "Epoch [16/100] Loss: 430.9577941894531\n",
      "Epoch [17/100] Loss: 424.347900390625\n",
      "Epoch [18/100] Loss: 418.0464782714844\n",
      "Epoch [19/100] Loss: 412.0420837402344\n",
      "Epoch [20/100] Loss: 406.322265625\n",
      "Epoch [21/100] Loss: 400.8738098144531\n",
      "Epoch [22/100] Loss: 395.68304443359375\n",
      "Epoch [23/100] Loss: 390.7362976074219\n",
      "Epoch [24/100] Loss: 386.0198974609375\n",
      "Epoch [25/100] Loss: 381.52008056640625\n",
      "Epoch [26/100] Loss: 377.22314453125\n",
      "Epoch [27/100] Loss: 373.1149597167969\n",
      "Epoch [28/100] Loss: 369.1811218261719\n",
      "Epoch [29/100] Loss: 365.4072570800781\n",
      "Epoch [30/100] Loss: 361.7790222167969\n",
      "Epoch [31/100] Loss: 358.282470703125\n",
      "Epoch [32/100] Loss: 354.9041748046875\n",
      "Epoch [33/100] Loss: 351.631591796875\n",
      "Epoch [34/100] Loss: 348.4534606933594\n",
      "Epoch [35/100] Loss: 345.359130859375\n",
      "Epoch [36/100] Loss: 342.33935546875\n",
      "Epoch [37/100] Loss: 339.385986328125\n",
      "Epoch [38/100] Loss: 336.4916076660156\n",
      "Epoch [39/100] Loss: 333.65008544921875\n",
      "Epoch [40/100] Loss: 330.85589599609375\n",
      "Epoch [41/100] Loss: 328.1043701171875\n",
      "Epoch [42/100] Loss: 325.39129638671875\n",
      "Epoch [43/100] Loss: 322.71343994140625\n",
      "Epoch [44/100] Loss: 320.067626953125\n",
      "Epoch [45/100] Loss: 317.4515686035156\n",
      "Epoch [46/100] Loss: 314.86309814453125\n",
      "Epoch [47/100] Loss: 312.300537109375\n",
      "Epoch [48/100] Loss: 309.7624816894531\n",
      "Epoch [49/100] Loss: 307.2479553222656\n",
      "Epoch [50/100] Loss: 304.756103515625\n",
      "Epoch [51/100] Loss: 302.2863464355469\n",
      "Epoch [52/100] Loss: 299.8382263183594\n",
      "Epoch [53/100] Loss: 297.41156005859375\n",
      "Epoch [54/100] Loss: 295.0060729980469\n",
      "Epoch [55/100] Loss: 292.6215515136719\n",
      "Epoch [56/100] Loss: 290.2580261230469\n",
      "Epoch [57/100] Loss: 287.9151306152344\n",
      "Epoch [58/100] Loss: 285.5928955078125\n",
      "Epoch [59/100] Loss: 283.291015625\n",
      "Epoch [60/100] Loss: 281.00927734375\n",
      "Epoch [61/100] Loss: 278.74749755859375\n",
      "Epoch [62/100] Loss: 276.5054016113281\n",
      "Epoch [63/100] Loss: 274.28265380859375\n",
      "Epoch [64/100] Loss: 272.0789794921875\n",
      "Epoch [65/100] Loss: 269.8941955566406\n",
      "Epoch [66/100] Loss: 267.7279052734375\n",
      "Epoch [67/100] Loss: 265.57989501953125\n",
      "Epoch [68/100] Loss: 263.44989013671875\n",
      "Epoch [69/100] Loss: 261.33758544921875\n",
      "Epoch [70/100] Loss: 259.2428283691406\n",
      "Epoch [71/100] Loss: 257.16522216796875\n",
      "Epoch [72/100] Loss: 255.10458374023438\n",
      "Epoch [73/100] Loss: 253.06069946289062\n",
      "Epoch [74/100] Loss: 251.0333709716797\n",
      "Epoch [75/100] Loss: 249.02236938476562\n",
      "Epoch [76/100] Loss: 247.0274658203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100] Loss: 245.0484161376953\n",
      "Epoch [78/100] Loss: 243.08509826660156\n",
      "Epoch [79/100] Loss: 241.13734436035156\n",
      "Epoch [80/100] Loss: 239.2049102783203\n",
      "Epoch [81/100] Loss: 237.28762817382812\n",
      "Epoch [82/100] Loss: 235.38536071777344\n",
      "Epoch [83/100] Loss: 233.4979248046875\n",
      "Epoch [84/100] Loss: 231.6251983642578\n",
      "Epoch [85/100] Loss: 229.7669219970703\n",
      "Epoch [86/100] Loss: 227.92306518554688\n",
      "Epoch [87/100] Loss: 226.0934295654297\n",
      "Epoch [88/100] Loss: 224.27780151367188\n",
      "Epoch [89/100] Loss: 222.47616577148438\n",
      "Epoch [90/100] Loss: 220.68838500976562\n",
      "Epoch [91/100] Loss: 218.91415405273438\n",
      "Epoch [92/100] Loss: 217.1534881591797\n",
      "Epoch [93/100] Loss: 215.4062957763672\n",
      "Epoch [94/100] Loss: 213.67233276367188\n",
      "Epoch [95/100] Loss: 211.9515838623047\n",
      "Epoch [96/100] Loss: 210.24383544921875\n",
      "Epoch [97/100] Loss: 208.54905700683594\n",
      "Epoch [98/100] Loss: 206.8670654296875\n",
      "Epoch [99/100] Loss: 205.19784545898438\n",
      "Epoch [100/100] Loss: 203.5411376953125\n",
      "Predicted days_remaining for parent_id 277: 10.058414459228516\n",
      "Training for parent_id 281...\n",
      "Epoch [1/100] Loss: 268.8190002441406\n",
      "Epoch [2/100] Loss: 261.5626525878906\n",
      "Epoch [3/100] Loss: 254.5681915283203\n",
      "Epoch [4/100] Loss: 247.83935546875\n",
      "Epoch [5/100] Loss: 241.36013793945312\n",
      "Epoch [6/100] Loss: 235.11093139648438\n",
      "Epoch [7/100] Loss: 229.0724639892578\n",
      "Epoch [8/100] Loss: 223.22909545898438\n",
      "Epoch [9/100] Loss: 217.5675048828125\n",
      "Epoch [10/100] Loss: 212.0745849609375\n",
      "Epoch [11/100] Loss: 206.73956298828125\n",
      "Epoch [12/100] Loss: 201.55612182617188\n",
      "Epoch [13/100] Loss: 196.5239715576172\n",
      "Epoch [14/100] Loss: 191.64813232421875\n",
      "Epoch [15/100] Loss: 186.93719482421875\n",
      "Epoch [16/100] Loss: 182.40069580078125\n",
      "Epoch [17/100] Loss: 178.0471954345703\n",
      "Epoch [18/100] Loss: 173.88250732421875\n",
      "Epoch [19/100] Loss: 169.90892028808594\n",
      "Epoch [20/100] Loss: 166.12503051757812\n",
      "Epoch [21/100] Loss: 162.5260772705078\n",
      "Epoch [22/100] Loss: 159.104736328125\n",
      "Epoch [23/100] Loss: 155.8516082763672\n",
      "Epoch [24/100] Loss: 152.75621032714844\n",
      "Epoch [25/100] Loss: 149.80784606933594\n",
      "Epoch [26/100] Loss: 146.9958953857422\n",
      "Epoch [27/100] Loss: 144.31027221679688\n",
      "Epoch [28/100] Loss: 141.74151611328125\n",
      "Epoch [29/100] Loss: 139.28082275390625\n",
      "Epoch [30/100] Loss: 136.92010498046875\n",
      "Epoch [31/100] Loss: 134.65174865722656\n",
      "Epoch [32/100] Loss: 132.46873474121094\n",
      "Epoch [33/100] Loss: 130.36439514160156\n",
      "Epoch [34/100] Loss: 128.33261108398438\n",
      "Epoch [35/100] Loss: 126.36748504638672\n",
      "Epoch [36/100] Loss: 124.46366882324219\n",
      "Epoch [37/100] Loss: 122.61616516113281\n",
      "Epoch [38/100] Loss: 120.82030487060547\n",
      "Epoch [39/100] Loss: 119.07185363769531\n",
      "Epoch [40/100] Loss: 117.36709594726562\n",
      "Epoch [41/100] Loss: 115.70257568359375\n",
      "Epoch [42/100] Loss: 114.0753402709961\n",
      "Epoch [43/100] Loss: 112.48282623291016\n",
      "Epoch [44/100] Loss: 110.92282104492188\n",
      "Epoch [45/100] Loss: 109.39339447021484\n",
      "Epoch [46/100] Loss: 107.89293670654297\n",
      "Epoch [47/100] Loss: 106.42001342773438\n",
      "Epoch [48/100] Loss: 104.97340393066406\n",
      "Epoch [49/100] Loss: 103.55201721191406\n",
      "Epoch [50/100] Loss: 102.15489959716797\n",
      "Epoch [51/100] Loss: 100.78116607666016\n",
      "Epoch [52/100] Loss: 99.43003845214844\n",
      "Epoch [53/100] Loss: 98.10084533691406\n",
      "Epoch [54/100] Loss: 96.79285430908203\n",
      "Epoch [55/100] Loss: 95.50554656982422\n",
      "Epoch [56/100] Loss: 94.23828125\n",
      "Epoch [57/100] Loss: 92.9906005859375\n",
      "Epoch [58/100] Loss: 91.76200866699219\n",
      "Epoch [59/100] Loss: 90.55205535888672\n",
      "Epoch [60/100] Loss: 89.36026763916016\n",
      "Epoch [61/100] Loss: 88.18630981445312\n",
      "Epoch [62/100] Loss: 87.02974700927734\n",
      "Epoch [63/100] Loss: 85.89027404785156\n",
      "Epoch [64/100] Loss: 84.767578125\n",
      "Epoch [65/100] Loss: 83.66124725341797\n",
      "Epoch [66/100] Loss: 82.571044921875\n",
      "Epoch [67/100] Loss: 81.49664306640625\n",
      "Epoch [68/100] Loss: 80.43778228759766\n",
      "Epoch [69/100] Loss: 79.39421844482422\n",
      "Epoch [70/100] Loss: 78.36561584472656\n",
      "Epoch [71/100] Loss: 77.35176849365234\n",
      "Epoch [72/100] Loss: 76.3524398803711\n",
      "Epoch [73/100] Loss: 75.36740112304688\n",
      "Epoch [74/100] Loss: 74.39639282226562\n",
      "Epoch [75/100] Loss: 73.43922424316406\n",
      "Epoch [76/100] Loss: 72.49569702148438\n",
      "Epoch [77/100] Loss: 71.56553649902344\n",
      "Epoch [78/100] Loss: 70.64863586425781\n",
      "Epoch [79/100] Loss: 69.74472045898438\n",
      "Epoch [80/100] Loss: 68.85363006591797\n",
      "Epoch [81/100] Loss: 67.97517395019531\n",
      "Epoch [82/100] Loss: 67.10917663574219\n",
      "Epoch [83/100] Loss: 66.25548553466797\n",
      "Epoch [84/100] Loss: 65.41387939453125\n",
      "Epoch [85/100] Loss: 64.58419799804688\n",
      "Epoch [86/100] Loss: 63.76628494262695\n",
      "Epoch [87/100] Loss: 62.960018157958984\n",
      "Epoch [88/100] Loss: 62.1651725769043\n",
      "Epoch [89/100] Loss: 61.38164520263672\n",
      "Epoch [90/100] Loss: 60.609230041503906\n",
      "Epoch [91/100] Loss: 59.847816467285156\n",
      "Epoch [92/100] Loss: 59.0972785949707\n",
      "Epoch [93/100] Loss: 58.35743713378906\n",
      "Epoch [94/100] Loss: 57.62814712524414\n",
      "Epoch [95/100] Loss: 56.909305572509766\n",
      "Epoch [96/100] Loss: 56.20073318481445\n",
      "Epoch [97/100] Loss: 55.5023193359375\n",
      "Epoch [98/100] Loss: 54.81393051147461\n",
      "Epoch [99/100] Loss: 54.13545608520508\n",
      "Epoch [100/100] Loss: 53.466758728027344\n",
      "Predicted days_remaining for parent_id 281: 9.555631637573242\n",
      "Training for parent_id 284...\n",
      "Epoch [1/100] Loss: 299.2600402832031\n",
      "Epoch [2/100] Loss: 291.5789489746094\n",
      "Epoch [3/100] Loss: 284.1114807128906\n",
      "Epoch [4/100] Loss: 276.90234375\n",
      "Epoch [5/100] Loss: 269.96759033203125\n",
      "Epoch [6/100] Loss: 263.3074645996094\n",
      "Epoch [7/100] Loss: 256.914306640625\n",
      "Epoch [8/100] Loss: 250.77662658691406\n",
      "Epoch [9/100] Loss: 244.88055419921875\n",
      "Epoch [10/100] Loss: 239.2111358642578\n",
      "Epoch [11/100] Loss: 233.75448608398438\n",
      "Epoch [12/100] Loss: 228.49960327148438\n",
      "Epoch [13/100] Loss: 223.43914794921875\n",
      "Epoch [14/100] Loss: 218.56954956054688\n",
      "Epoch [15/100] Loss: 213.89007568359375\n",
      "Epoch [16/100] Loss: 209.40151977539062\n",
      "Epoch [17/100] Loss: 205.10479736328125\n",
      "Epoch [18/100] Loss: 200.99954223632812\n",
      "Epoch [19/100] Loss: 197.08334350585938\n",
      "Epoch [20/100] Loss: 193.35137939453125\n",
      "Epoch [21/100] Loss: 189.79641723632812\n",
      "Epoch [22/100] Loss: 186.4096221923828\n",
      "Epoch [23/100] Loss: 183.18128967285156\n",
      "Epoch [24/100] Loss: 180.1012420654297\n",
      "Epoch [25/100] Loss: 177.15939331054688\n",
      "Epoch [26/100] Loss: 174.345703125\n",
      "Epoch [27/100] Loss: 171.65020751953125\n",
      "Epoch [28/100] Loss: 169.06329345703125\n",
      "Epoch [29/100] Loss: 166.57546997070312\n",
      "Epoch [30/100] Loss: 164.1776580810547\n",
      "Epoch [31/100] Loss: 161.86143493652344\n",
      "Epoch [32/100] Loss: 159.61900329589844\n",
      "Epoch [33/100] Loss: 157.44349670410156\n",
      "Epoch [34/100] Loss: 155.32867431640625\n",
      "Epoch [35/100] Loss: 153.2692108154297\n",
      "Epoch [36/100] Loss: 151.260498046875\n",
      "Epoch [37/100] Loss: 149.29840087890625\n",
      "Epoch [38/100] Loss: 147.37945556640625\n",
      "Epoch [39/100] Loss: 145.50059509277344\n",
      "Epoch [40/100] Loss: 143.6590576171875\n",
      "Epoch [41/100] Loss: 141.85255432128906\n",
      "Epoch [42/100] Loss: 140.07901000976562\n",
      "Epoch [43/100] Loss: 138.3365936279297\n",
      "Epoch [44/100] Loss: 136.62368774414062\n",
      "Epoch [45/100] Loss: 134.93893432617188\n",
      "Epoch [46/100] Loss: 133.281005859375\n",
      "Epoch [47/100] Loss: 131.64881896972656\n",
      "Epoch [48/100] Loss: 130.04139709472656\n",
      "Epoch [49/100] Loss: 128.45787048339844\n",
      "Epoch [50/100] Loss: 126.89739990234375\n",
      "Epoch [51/100] Loss: 125.35930633544922\n",
      "Epoch [52/100] Loss: 123.84297180175781\n",
      "Epoch [53/100] Loss: 122.34774017333984\n",
      "Epoch [54/100] Loss: 120.87310028076172\n",
      "Epoch [55/100] Loss: 119.41853332519531\n",
      "Epoch [56/100] Loss: 117.98355865478516\n",
      "Epoch [57/100] Loss: 116.56781005859375\n",
      "Epoch [58/100] Loss: 115.17080688476562\n",
      "Epoch [59/100] Loss: 113.79222106933594\n",
      "Epoch [60/100] Loss: 112.43163299560547\n",
      "Epoch [61/100] Loss: 111.0887451171875\n",
      "Epoch [62/100] Loss: 109.76321411132812\n",
      "Epoch [63/100] Loss: 108.4547348022461\n",
      "Epoch [64/100] Loss: 107.16303253173828\n",
      "Epoch [65/100] Loss: 105.88777160644531\n",
      "Epoch [66/100] Loss: 104.6287612915039\n",
      "Epoch [67/100] Loss: 103.38565826416016\n",
      "Epoch [68/100] Loss: 102.15823364257812\n",
      "Epoch [69/100] Loss: 100.9462890625\n",
      "Epoch [70/100] Loss: 99.74951934814453\n",
      "Epoch [71/100] Loss: 98.56778717041016\n",
      "Epoch [72/100] Loss: 97.40077209472656\n",
      "Epoch [73/100] Loss: 96.24835205078125\n",
      "Epoch [74/100] Loss: 95.1102294921875\n",
      "Epoch [75/100] Loss: 93.98631286621094\n",
      "Epoch [76/100] Loss: 92.87632751464844\n",
      "Epoch [77/100] Loss: 91.78007507324219\n",
      "Epoch [78/100] Loss: 90.69742584228516\n",
      "Epoch [79/100] Loss: 89.62818145751953\n",
      "Epoch [80/100] Loss: 88.57215118408203\n",
      "Epoch [81/100] Loss: 87.52919006347656\n",
      "Epoch [82/100] Loss: 86.49909973144531\n",
      "Epoch [83/100] Loss: 85.48174285888672\n",
      "Epoch [84/100] Loss: 84.47696685791016\n",
      "Epoch [85/100] Loss: 83.48458099365234\n",
      "Epoch [86/100] Loss: 82.50444030761719\n",
      "Epoch [87/100] Loss: 81.53640747070312\n",
      "Epoch [88/100] Loss: 80.5803451538086\n",
      "Epoch [89/100] Loss: 79.63607788085938\n",
      "Epoch [90/100] Loss: 78.7034912109375\n",
      "Epoch [91/100] Loss: 77.78244018554688\n",
      "Epoch [92/100] Loss: 76.87279510498047\n",
      "Epoch [93/100] Loss: 75.97439575195312\n",
      "Epoch [94/100] Loss: 75.0871353149414\n",
      "Epoch [95/100] Loss: 74.21089172363281\n",
      "Epoch [96/100] Loss: 73.34551239013672\n",
      "Epoch [97/100] Loss: 72.49089050292969\n",
      "Epoch [98/100] Loss: 71.64691162109375\n",
      "Epoch [99/100] Loss: 70.81340026855469\n",
      "Epoch [100/100] Loss: 69.99032592773438\n",
      "Predicted days_remaining for parent_id 284: 9.351350784301758\n",
      "Training for parent_id 289...\n",
      "Epoch [1/100] Loss: 167.32095336914062\n",
      "Epoch [2/100] Loss: 160.59625244140625\n",
      "Epoch [3/100] Loss: 154.18727111816406\n",
      "Epoch [4/100] Loss: 148.08660888671875\n",
      "Epoch [5/100] Loss: 142.3065643310547\n",
      "Epoch [6/100] Loss: 136.86709594726562\n",
      "Epoch [7/100] Loss: 131.7888946533203\n",
      "Epoch [8/100] Loss: 127.08551025390625\n",
      "Epoch [9/100] Loss: 122.75708770751953\n",
      "Epoch [10/100] Loss: 118.7896957397461\n",
      "Epoch [11/100] Loss: 115.15918731689453\n",
      "Epoch [12/100] Loss: 111.83615112304688\n",
      "Epoch [13/100] Loss: 108.79015350341797\n",
      "Epoch [14/100] Loss: 105.99161529541016\n",
      "Epoch [15/100] Loss: 103.41255950927734\n",
      "Epoch [16/100] Loss: 101.02622985839844\n",
      "Epoch [17/100] Loss: 98.80728912353516\n",
      "Epoch [18/100] Loss: 96.73230743408203\n",
      "Epoch [19/100] Loss: 94.78053283691406\n",
      "Epoch [20/100] Loss: 92.93452453613281\n",
      "Epoch [21/100] Loss: 91.18009185791016\n",
      "Epoch [22/100] Loss: 89.50598907470703\n",
      "Epoch [23/100] Loss: 87.90322875976562\n",
      "Epoch [24/100] Loss: 86.36444854736328\n",
      "Epoch [25/100] Loss: 84.88343811035156\n",
      "Epoch [26/100] Loss: 83.45476531982422\n",
      "Epoch [27/100] Loss: 82.07357788085938\n",
      "Epoch [28/100] Loss: 80.73551940917969\n",
      "Epoch [29/100] Loss: 79.43670654296875\n",
      "Epoch [30/100] Loss: 78.17365264892578\n",
      "Epoch [31/100] Loss: 76.94324493408203\n",
      "Epoch [32/100] Loss: 75.74276733398438\n",
      "Epoch [33/100] Loss: 74.56985473632812\n",
      "Epoch [34/100] Loss: 73.42237091064453\n",
      "Epoch [35/100] Loss: 72.29849243164062\n",
      "Epoch [36/100] Loss: 71.19667053222656\n",
      "Epoch [37/100] Loss: 70.1155014038086\n",
      "Epoch [38/100] Loss: 69.05379486083984\n",
      "Epoch [39/100] Loss: 68.01057434082031\n",
      "Epoch [40/100] Loss: 66.9849853515625\n",
      "Epoch [41/100] Loss: 65.9762954711914\n",
      "Epoch [42/100] Loss: 64.98388671875\n",
      "Epoch [43/100] Loss: 64.00724792480469\n",
      "Epoch [44/100] Loss: 63.04591369628906\n",
      "Epoch [45/100] Loss: 62.099464416503906\n",
      "Epoch [46/100] Loss: 61.16754150390625\n",
      "Epoch [47/100] Loss: 60.24982452392578\n",
      "Epoch [48/100] Loss: 59.345970153808594\n",
      "Epoch [49/100] Loss: 58.45570373535156\n",
      "Epoch [50/100] Loss: 57.578765869140625\n",
      "Epoch [51/100] Loss: 56.714908599853516\n",
      "Epoch [52/100] Loss: 55.86392593383789\n",
      "Epoch [53/100] Loss: 55.0256462097168\n",
      "Epoch [54/100] Loss: 54.199913024902344\n",
      "Epoch [55/100] Loss: 53.386592864990234\n",
      "Epoch [56/100] Loss: 52.58559799194336\n",
      "Epoch [57/100] Loss: 51.79684829711914\n",
      "Epoch [58/100] Loss: 51.02028274536133\n",
      "Epoch [59/100] Loss: 50.255882263183594\n",
      "Epoch [60/100] Loss: 49.503578186035156\n",
      "Epoch [61/100] Loss: 48.763336181640625\n",
      "Epoch [62/100] Loss: 48.03512191772461\n",
      "Epoch [63/100] Loss: 47.31888961791992\n",
      "Epoch [64/100] Loss: 46.61458969116211\n",
      "Epoch [65/100] Loss: 45.92216110229492\n",
      "Epoch [66/100] Loss: 45.24153137207031\n",
      "Epoch [67/100] Loss: 44.57265853881836\n",
      "Epoch [68/100] Loss: 43.91542053222656\n",
      "Epoch [69/100] Loss: 43.26974105834961\n",
      "Epoch [70/100] Loss: 42.635501861572266\n",
      "Epoch [71/100] Loss: 42.01261520385742\n",
      "Epoch [72/100] Loss: 41.40095901489258\n",
      "Epoch [73/100] Loss: 40.8004150390625\n",
      "Epoch [74/100] Loss: 40.210853576660156\n",
      "Epoch [75/100] Loss: 39.63216018676758\n",
      "Epoch [76/100] Loss: 39.0641975402832\n",
      "Epoch [77/100] Loss: 38.50682830810547\n",
      "Epoch [78/100] Loss: 37.95992660522461\n",
      "Epoch [79/100] Loss: 37.4233512878418\n",
      "Epoch [80/100] Loss: 36.89699935913086\n",
      "Epoch [81/100] Loss: 36.380680084228516\n",
      "Epoch [82/100] Loss: 35.874305725097656\n",
      "Epoch [83/100] Loss: 35.37771224975586\n",
      "Epoch [84/100] Loss: 34.89078140258789\n",
      "Epoch [85/100] Loss: 34.41336441040039\n",
      "Epoch [86/100] Loss: 33.94533920288086\n",
      "Epoch [87/100] Loss: 33.48654556274414\n",
      "Epoch [88/100] Loss: 33.036869049072266\n",
      "Epoch [89/100] Loss: 32.59613800048828\n",
      "Epoch [90/100] Loss: 32.16426086425781\n",
      "Epoch [91/100] Loss: 31.7410888671875\n",
      "Epoch [92/100] Loss: 31.326473236083984\n",
      "Epoch [93/100] Loss: 30.920289993286133\n",
      "Epoch [94/100] Loss: 30.522397994995117\n",
      "Epoch [95/100] Loss: 30.13268280029297\n",
      "Epoch [96/100] Loss: 29.750995635986328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100] Loss: 29.377212524414062\n",
      "Epoch [98/100] Loss: 29.011207580566406\n",
      "Epoch [99/100] Loss: 28.652847290039062\n",
      "Epoch [100/100] Loss: 28.30200958251953\n",
      "Predicted days_remaining for parent_id 289: 9.072900772094727\n",
      "Training for parent_id 299...\n",
      "Epoch [1/100] Loss: 573.9853515625\n",
      "Epoch [2/100] Loss: 561.3779907226562\n",
      "Epoch [3/100] Loss: 548.9786376953125\n",
      "Epoch [4/100] Loss: 536.94140625\n",
      "Epoch [5/100] Loss: 525.3782958984375\n",
      "Epoch [6/100] Loss: 514.3455810546875\n",
      "Epoch [7/100] Loss: 503.873779296875\n",
      "Epoch [8/100] Loss: 493.9761657714844\n",
      "Epoch [9/100] Loss: 484.64764404296875\n",
      "Epoch [10/100] Loss: 475.8684387207031\n",
      "Epoch [11/100] Loss: 467.6097106933594\n",
      "Epoch [12/100] Loss: 459.8359375\n",
      "Epoch [13/100] Loss: 452.5072937011719\n",
      "Epoch [14/100] Loss: 445.5828857421875\n",
      "Epoch [15/100] Loss: 439.024658203125\n",
      "Epoch [16/100] Loss: 432.7984924316406\n",
      "Epoch [17/100] Loss: 426.8760986328125\n",
      "Epoch [18/100] Loss: 421.2349548339844\n",
      "Epoch [19/100] Loss: 415.8573303222656\n",
      "Epoch [20/100] Loss: 410.72857666015625\n",
      "Epoch [21/100] Loss: 405.8352966308594\n",
      "Epoch [22/100] Loss: 401.1641540527344\n",
      "Epoch [23/100] Loss: 396.70037841796875\n",
      "Epoch [24/100] Loss: 392.428466796875\n",
      "Epoch [25/100] Loss: 388.33203125\n",
      "Epoch [26/100] Loss: 384.394775390625\n",
      "Epoch [27/100] Loss: 380.6014099121094\n",
      "Epoch [28/100] Loss: 376.9375915527344\n",
      "Epoch [29/100] Loss: 373.3902587890625\n",
      "Epoch [30/100] Loss: 369.9477844238281\n",
      "Epoch [31/100] Loss: 366.5997009277344\n",
      "Epoch [32/100] Loss: 363.33642578125\n",
      "Epoch [33/100] Loss: 360.14947509765625\n",
      "Epoch [34/100] Loss: 357.0311279296875\n",
      "Epoch [35/100] Loss: 353.9744873046875\n",
      "Epoch [36/100] Loss: 350.9735412597656\n",
      "Epoch [37/100] Loss: 348.0230407714844\n",
      "Epoch [38/100] Loss: 345.1183166503906\n",
      "Epoch [39/100] Loss: 342.25555419921875\n",
      "Epoch [40/100] Loss: 339.4312438964844\n",
      "Epoch [41/100] Loss: 336.64263916015625\n",
      "Epoch [42/100] Loss: 333.88726806640625\n",
      "Epoch [43/100] Loss: 331.16314697265625\n",
      "Epoch [44/100] Loss: 328.4685974121094\n",
      "Epoch [45/100] Loss: 325.80206298828125\n",
      "Epoch [46/100] Loss: 323.162353515625\n",
      "Epoch [47/100] Loss: 320.54852294921875\n",
      "Epoch [48/100] Loss: 317.95965576171875\n",
      "Epoch [49/100] Loss: 315.39495849609375\n",
      "Epoch [50/100] Loss: 312.8538818359375\n",
      "Epoch [51/100] Loss: 310.3359069824219\n",
      "Epoch [52/100] Loss: 307.84051513671875\n",
      "Epoch [53/100] Loss: 305.3673400878906\n",
      "Epoch [54/100] Loss: 302.916015625\n",
      "Epoch [55/100] Loss: 300.48614501953125\n",
      "Epoch [56/100] Loss: 298.07757568359375\n",
      "Epoch [57/100] Loss: 295.68994140625\n",
      "Epoch [58/100] Loss: 293.3229675292969\n",
      "Epoch [59/100] Loss: 290.976318359375\n",
      "Epoch [60/100] Loss: 288.6498107910156\n",
      "Epoch [61/100] Loss: 286.3431396484375\n",
      "Epoch [62/100] Loss: 284.05609130859375\n",
      "Epoch [63/100] Loss: 281.788330078125\n",
      "Epoch [64/100] Loss: 279.5397033691406\n",
      "Epoch [65/100] Loss: 277.3098449707031\n",
      "Epoch [66/100] Loss: 275.0986633300781\n",
      "Epoch [67/100] Loss: 272.9058837890625\n",
      "Epoch [68/100] Loss: 270.73114013671875\n",
      "Epoch [69/100] Loss: 268.5743408203125\n",
      "Epoch [70/100] Loss: 266.4351806640625\n",
      "Epoch [71/100] Loss: 264.3134460449219\n",
      "Epoch [72/100] Loss: 262.20892333984375\n",
      "Epoch [73/100] Loss: 260.1214599609375\n",
      "Epoch [74/100] Loss: 258.05078125\n",
      "Epoch [75/100] Loss: 255.99661254882812\n",
      "Epoch [76/100] Loss: 253.9589080810547\n",
      "Epoch [77/100] Loss: 251.93740844726562\n",
      "Epoch [78/100] Loss: 249.93191528320312\n",
      "Epoch [79/100] Loss: 247.9422149658203\n",
      "Epoch [80/100] Loss: 245.9682159423828\n",
      "Epoch [81/100] Loss: 244.00962829589844\n",
      "Epoch [82/100] Loss: 242.06642150878906\n",
      "Epoch [83/100] Loss: 240.13832092285156\n",
      "Epoch [84/100] Loss: 238.2251739501953\n",
      "Epoch [85/100] Loss: 236.32688903808594\n",
      "Epoch [86/100] Loss: 234.4431915283203\n",
      "Epoch [87/100] Loss: 232.57406616210938\n",
      "Epoch [88/100] Loss: 230.71923828125\n",
      "Epoch [89/100] Loss: 228.8787384033203\n",
      "Epoch [90/100] Loss: 227.05221557617188\n",
      "Epoch [91/100] Loss: 225.23965454101562\n",
      "Epoch [92/100] Loss: 223.4409637451172\n",
      "Epoch [93/100] Loss: 221.65585327148438\n",
      "Epoch [94/100] Loss: 219.8843231201172\n",
      "Epoch [95/100] Loss: 218.12625122070312\n",
      "Epoch [96/100] Loss: 216.38143920898438\n",
      "Epoch [97/100] Loss: 214.64979553222656\n",
      "Epoch [98/100] Loss: 212.93121337890625\n",
      "Epoch [99/100] Loss: 211.22561645507812\n",
      "Epoch [100/100] Loss: 209.53277587890625\n",
      "Predicted days_remaining for parent_id 299: 9.842619895935059\n",
      "Training for parent_id 302...\n",
      "Epoch [1/100] Loss: 128.85757446289062\n",
      "Epoch [2/100] Loss: 123.86585998535156\n",
      "Epoch [3/100] Loss: 119.04573822021484\n",
      "Epoch [4/100] Loss: 114.41618347167969\n",
      "Epoch [5/100] Loss: 109.97860717773438\n",
      "Epoch [6/100] Loss: 105.73270416259766\n",
      "Epoch [7/100] Loss: 101.67866516113281\n",
      "Epoch [8/100] Loss: 97.8152847290039\n",
      "Epoch [9/100] Loss: 94.1403579711914\n",
      "Epoch [10/100] Loss: 90.65086364746094\n",
      "Epoch [11/100] Loss: 87.3426742553711\n",
      "Epoch [12/100] Loss: 84.21063232421875\n",
      "Epoch [13/100] Loss: 81.24855041503906\n",
      "Epoch [14/100] Loss: 78.44953155517578\n",
      "Epoch [15/100] Loss: 75.80599212646484\n",
      "Epoch [16/100] Loss: 73.3100814819336\n",
      "Epoch [17/100] Loss: 70.95396423339844\n",
      "Epoch [18/100] Loss: 68.7300796508789\n",
      "Epoch [19/100] Loss: 66.6309814453125\n",
      "Epoch [20/100] Loss: 64.6493911743164\n",
      "Epoch [21/100] Loss: 62.778076171875\n",
      "Epoch [22/100] Loss: 61.00985336303711\n",
      "Epoch [23/100] Loss: 59.33749771118164\n",
      "Epoch [24/100] Loss: 57.75385665893555\n",
      "Epoch [25/100] Loss: 56.251914978027344\n",
      "Epoch [26/100] Loss: 54.824974060058594\n",
      "Epoch [27/100] Loss: 53.466705322265625\n",
      "Epoch [28/100] Loss: 52.17134094238281\n",
      "Epoch [29/100] Loss: 50.93367385864258\n",
      "Epoch [30/100] Loss: 49.749046325683594\n",
      "Epoch [31/100] Loss: 48.61338424682617\n",
      "Epoch [32/100] Loss: 47.52304458618164\n",
      "Epoch [33/100] Loss: 46.47484588623047\n",
      "Epoch [34/100] Loss: 45.4659423828125\n",
      "Epoch [35/100] Loss: 44.49382019042969\n",
      "Epoch [36/100] Loss: 43.55620574951172\n",
      "Epoch [37/100] Loss: 42.65108871459961\n",
      "Epoch [38/100] Loss: 41.776615142822266\n",
      "Epoch [39/100] Loss: 40.931121826171875\n",
      "Epoch [40/100] Loss: 40.11312484741211\n",
      "Epoch [41/100] Loss: 39.32122802734375\n",
      "Epoch [42/100] Loss: 38.55423355102539\n",
      "Epoch [43/100] Loss: 37.810935974121094\n",
      "Epoch [44/100] Loss: 37.0903434753418\n",
      "Epoch [45/100] Loss: 36.39146423339844\n",
      "Epoch [46/100] Loss: 35.71345138549805\n",
      "Epoch [47/100] Loss: 35.05546569824219\n",
      "Epoch [48/100] Loss: 34.41675567626953\n",
      "Epoch [49/100] Loss: 33.796653747558594\n",
      "Epoch [50/100] Loss: 33.19447708129883\n",
      "Epoch [51/100] Loss: 32.60963439941406\n",
      "Epoch [52/100] Loss: 32.04155731201172\n",
      "Epoch [53/100] Loss: 31.48971176147461\n",
      "Epoch [54/100] Loss: 30.953575134277344\n",
      "Epoch [55/100] Loss: 30.432689666748047\n",
      "Epoch [56/100] Loss: 29.926584243774414\n",
      "Epoch [57/100] Loss: 29.434843063354492\n",
      "Epoch [58/100] Loss: 28.957033157348633\n",
      "Epoch [59/100] Loss: 28.492769241333008\n",
      "Epoch [60/100] Loss: 28.041669845581055\n",
      "Epoch [61/100] Loss: 27.603376388549805\n",
      "Epoch [62/100] Loss: 27.177534103393555\n",
      "Epoch [63/100] Loss: 26.76380729675293\n",
      "Epoch [64/100] Loss: 26.36187744140625\n",
      "Epoch [65/100] Loss: 25.971416473388672\n",
      "Epoch [66/100] Loss: 25.59214210510254\n",
      "Epoch [67/100] Loss: 25.22374153137207\n",
      "Epoch [68/100] Loss: 24.865955352783203\n",
      "Epoch [69/100] Loss: 24.51849365234375\n",
      "Epoch [70/100] Loss: 24.181093215942383\n",
      "Epoch [71/100] Loss: 23.853504180908203\n",
      "Epoch [72/100] Loss: 23.53546142578125\n",
      "Epoch [73/100] Loss: 23.226743698120117\n",
      "Epoch [74/100] Loss: 22.927095413208008\n",
      "Epoch [75/100] Loss: 22.63629150390625\n",
      "Epoch [76/100] Loss: 22.35411262512207\n",
      "Epoch [77/100] Loss: 22.080333709716797\n",
      "Epoch [78/100] Loss: 21.814748764038086\n",
      "Epoch [79/100] Loss: 21.557146072387695\n",
      "Epoch [80/100] Loss: 21.30731201171875\n",
      "Epoch [81/100] Loss: 21.065073013305664\n",
      "Epoch [82/100] Loss: 20.83021354675293\n",
      "Epoch [83/100] Loss: 20.602550506591797\n",
      "Epoch [84/100] Loss: 20.38190460205078\n",
      "Epoch [85/100] Loss: 20.168088912963867\n",
      "Epoch [86/100] Loss: 19.960926055908203\n",
      "Epoch [87/100] Loss: 19.760251998901367\n",
      "Epoch [88/100] Loss: 19.56587791442871\n",
      "Epoch [89/100] Loss: 19.377666473388672\n",
      "Epoch [90/100] Loss: 19.195436477661133\n",
      "Epoch [91/100] Loss: 19.019031524658203\n",
      "Epoch [92/100] Loss: 18.848302841186523\n",
      "Epoch [93/100] Loss: 18.683094024658203\n",
      "Epoch [94/100] Loss: 18.52325439453125\n",
      "Epoch [95/100] Loss: 18.368648529052734\n",
      "Epoch [96/100] Loss: 18.21912956237793\n",
      "Epoch [97/100] Loss: 18.074554443359375\n",
      "Epoch [98/100] Loss: 17.93478775024414\n",
      "Epoch [99/100] Loss: 17.799705505371094\n",
      "Epoch [100/100] Loss: 17.66916275024414\n",
      "Predicted days_remaining for parent_id 302: 8.987743377685547\n",
      "Training for parent_id 310...\n",
      "Epoch [1/100] Loss: 3015.268798828125\n",
      "Epoch [2/100] Loss: 2987.426025390625\n",
      "Epoch [3/100] Loss: 2960.275146484375\n",
      "Epoch [4/100] Loss: 2934.030029296875\n",
      "Epoch [5/100] Loss: 2908.854248046875\n",
      "Epoch [6/100] Loss: 2884.854736328125\n",
      "Epoch [7/100] Loss: 2862.100830078125\n",
      "Epoch [8/100] Loss: 2840.623779296875\n",
      "Epoch [9/100] Loss: 2820.416748046875\n",
      "Epoch [10/100] Loss: 2801.43994140625\n",
      "Epoch [11/100] Loss: 2783.618408203125\n",
      "Epoch [12/100] Loss: 2766.84423828125\n",
      "Epoch [13/100] Loss: 2750.989013671875\n",
      "Epoch [14/100] Loss: 2735.92626953125\n",
      "Epoch [15/100] Loss: 2721.552001953125\n",
      "Epoch [16/100] Loss: 2707.797607421875\n",
      "Epoch [17/100] Loss: 2694.62646484375\n",
      "Epoch [18/100] Loss: 2682.02197265625\n",
      "Epoch [19/100] Loss: 2669.97119140625\n",
      "Epoch [20/100] Loss: 2658.4482421875\n",
      "Epoch [21/100] Loss: 2647.40966796875\n",
      "Epoch [22/100] Loss: 2636.7978515625\n",
      "Epoch [23/100] Loss: 2626.556884765625\n",
      "Epoch [24/100] Loss: 2616.63232421875\n",
      "Epoch [25/100] Loss: 2606.978759765625\n",
      "Epoch [26/100] Loss: 2597.56005859375\n",
      "Epoch [27/100] Loss: 2588.3486328125\n",
      "Epoch [28/100] Loss: 2579.321044921875\n",
      "Epoch [29/100] Loss: 2570.46044921875\n",
      "Epoch [30/100] Loss: 2561.750244140625\n",
      "Epoch [31/100] Loss: 2553.17529296875\n",
      "Epoch [32/100] Loss: 2544.722900390625\n",
      "Epoch [33/100] Loss: 2536.3779296875\n",
      "Epoch [34/100] Loss: 2528.130615234375\n",
      "Epoch [35/100] Loss: 2519.96923828125\n",
      "Epoch [36/100] Loss: 2511.885986328125\n",
      "Epoch [37/100] Loss: 2503.87353515625\n",
      "Epoch [38/100] Loss: 2495.9267578125\n",
      "Epoch [39/100] Loss: 2488.04052734375\n",
      "Epoch [40/100] Loss: 2480.212158203125\n",
      "Epoch [41/100] Loss: 2472.437744140625\n",
      "Epoch [42/100] Loss: 2464.715576171875\n",
      "Epoch [43/100] Loss: 2457.04296875\n",
      "Epoch [44/100] Loss: 2449.41796875\n",
      "Epoch [45/100] Loss: 2441.8388671875\n",
      "Epoch [46/100] Loss: 2434.303955078125\n",
      "Epoch [47/100] Loss: 2426.811767578125\n",
      "Epoch [48/100] Loss: 2419.362060546875\n",
      "Epoch [49/100] Loss: 2411.95166015625\n",
      "Epoch [50/100] Loss: 2404.580078125\n",
      "Epoch [51/100] Loss: 2397.24658203125\n",
      "Epoch [52/100] Loss: 2389.950439453125\n",
      "Epoch [53/100] Loss: 2382.68994140625\n",
      "Epoch [54/100] Loss: 2375.464599609375\n",
      "Epoch [55/100] Loss: 2368.2744140625\n",
      "Epoch [56/100] Loss: 2361.117431640625\n",
      "Epoch [57/100] Loss: 2353.994384765625\n",
      "Epoch [58/100] Loss: 2346.9033203125\n",
      "Epoch [59/100] Loss: 2339.8447265625\n",
      "Epoch [60/100] Loss: 2332.817138671875\n",
      "Epoch [61/100] Loss: 2325.8203125\n",
      "Epoch [62/100] Loss: 2318.8544921875\n",
      "Epoch [63/100] Loss: 2311.917236328125\n",
      "Epoch [64/100] Loss: 2305.009765625\n",
      "Epoch [65/100] Loss: 2298.131591796875\n",
      "Epoch [66/100] Loss: 2291.28076171875\n",
      "Epoch [67/100] Loss: 2284.458251953125\n",
      "Epoch [68/100] Loss: 2277.662841796875\n",
      "Epoch [69/100] Loss: 2270.89501953125\n",
      "Epoch [70/100] Loss: 2264.153076171875\n",
      "Epoch [71/100] Loss: 2257.438232421875\n",
      "Epoch [72/100] Loss: 2250.748779296875\n",
      "Epoch [73/100] Loss: 2244.08544921875\n",
      "Epoch [74/100] Loss: 2237.447021484375\n",
      "Epoch [75/100] Loss: 2230.833984375\n",
      "Epoch [76/100] Loss: 2224.24560546875\n",
      "Epoch [77/100] Loss: 2217.680908203125\n",
      "Epoch [78/100] Loss: 2211.14111328125\n",
      "Epoch [79/100] Loss: 2204.624267578125\n",
      "Epoch [80/100] Loss: 2198.130615234375\n",
      "Epoch [81/100] Loss: 2191.66064453125\n",
      "Epoch [82/100] Loss: 2185.213623046875\n",
      "Epoch [83/100] Loss: 2178.7890625\n",
      "Epoch [84/100] Loss: 2172.38623046875\n",
      "Epoch [85/100] Loss: 2166.005859375\n",
      "Epoch [86/100] Loss: 2159.647705078125\n",
      "Epoch [87/100] Loss: 2153.310546875\n",
      "Epoch [88/100] Loss: 2146.99462890625\n",
      "Epoch [89/100] Loss: 2140.7001953125\n",
      "Epoch [90/100] Loss: 2134.426513671875\n",
      "Epoch [91/100] Loss: 2128.17333984375\n",
      "Epoch [92/100] Loss: 2121.94091796875\n",
      "Epoch [93/100] Loss: 2115.728271484375\n",
      "Epoch [94/100] Loss: 2109.53564453125\n",
      "Epoch [95/100] Loss: 2103.36328125\n",
      "Epoch [96/100] Loss: 2097.210205078125\n",
      "Epoch [97/100] Loss: 2091.076904296875\n",
      "Epoch [98/100] Loss: 2084.962890625\n",
      "Epoch [99/100] Loss: 2078.86767578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100] Loss: 2072.79150390625\n",
      "Predicted days_remaining for parent_id 310: 9.447718620300293\n",
      "Training for parent_id 312...\n",
      "Epoch [1/100] Loss: 1344.815185546875\n",
      "Epoch [2/100] Loss: 1325.662109375\n",
      "Epoch [3/100] Loss: 1306.7763671875\n",
      "Epoch [4/100] Loss: 1288.2628173828125\n",
      "Epoch [5/100] Loss: 1270.22265625\n",
      "Epoch [6/100] Loss: 1252.7420654296875\n",
      "Epoch [7/100] Loss: 1235.87744140625\n",
      "Epoch [8/100] Loss: 1219.65283203125\n",
      "Epoch [9/100] Loss: 1204.07666015625\n",
      "Epoch [10/100] Loss: 1189.1473388671875\n",
      "Epoch [11/100] Loss: 1174.847412109375\n",
      "Epoch [12/100] Loss: 1161.1458740234375\n",
      "Epoch [13/100] Loss: 1148.0054931640625\n",
      "Epoch [14/100] Loss: 1135.38720703125\n",
      "Epoch [15/100] Loss: 1123.255615234375\n",
      "Epoch [16/100] Loss: 1111.581298828125\n",
      "Epoch [17/100] Loss: 1100.3414306640625\n",
      "Epoch [18/100] Loss: 1089.5186767578125\n",
      "Epoch [19/100] Loss: 1079.0994873046875\n",
      "Epoch [20/100] Loss: 1069.07177734375\n",
      "Epoch [21/100] Loss: 1059.42431640625\n",
      "Epoch [22/100] Loss: 1050.14453125\n",
      "Epoch [23/100] Loss: 1041.21923828125\n",
      "Epoch [24/100] Loss: 1032.634033203125\n",
      "Epoch [25/100] Loss: 1024.373291015625\n",
      "Epoch [26/100] Loss: 1016.4208374023438\n",
      "Epoch [27/100] Loss: 1008.7603759765625\n",
      "Epoch [28/100] Loss: 1001.375\n",
      "Epoch [29/100] Loss: 994.248779296875\n",
      "Epoch [30/100] Loss: 987.3653564453125\n",
      "Epoch [31/100] Loss: 980.7094116210938\n",
      "Epoch [32/100] Loss: 974.2654418945312\n",
      "Epoch [33/100] Loss: 968.0183715820312\n",
      "Epoch [34/100] Loss: 961.9530029296875\n",
      "Epoch [35/100] Loss: 956.0548095703125\n",
      "Epoch [36/100] Loss: 950.30859375\n",
      "Epoch [37/100] Loss: 944.7005615234375\n",
      "Epoch [38/100] Loss: 939.2179565429688\n",
      "Epoch [39/100] Loss: 933.848388671875\n",
      "Epoch [40/100] Loss: 928.5821533203125\n",
      "Epoch [41/100] Loss: 923.4097290039062\n",
      "Epoch [42/100] Loss: 918.3235473632812\n",
      "Epoch [43/100] Loss: 913.3170166015625\n",
      "Epoch [44/100] Loss: 908.3841552734375\n",
      "Epoch [45/100] Loss: 903.51953125\n",
      "Epoch [46/100] Loss: 898.718505859375\n",
      "Epoch [47/100] Loss: 893.97705078125\n",
      "Epoch [48/100] Loss: 889.290771484375\n",
      "Epoch [49/100] Loss: 884.6565551757812\n",
      "Epoch [50/100] Loss: 880.0709228515625\n",
      "Epoch [51/100] Loss: 875.5309448242188\n",
      "Epoch [52/100] Loss: 871.0341796875\n",
      "Epoch [53/100] Loss: 866.5782470703125\n",
      "Epoch [54/100] Loss: 862.1609497070312\n",
      "Epoch [55/100] Loss: 857.7807006835938\n",
      "Epoch [56/100] Loss: 853.4354248046875\n",
      "Epoch [57/100] Loss: 849.1240844726562\n",
      "Epoch [58/100] Loss: 844.844970703125\n",
      "Epoch [59/100] Loss: 840.5972900390625\n",
      "Epoch [60/100] Loss: 836.3799438476562\n",
      "Epoch [61/100] Loss: 832.19189453125\n",
      "Epoch [62/100] Loss: 828.0323486328125\n",
      "Epoch [63/100] Loss: 823.9004516601562\n",
      "Epoch [64/100] Loss: 819.795654296875\n",
      "Epoch [65/100] Loss: 815.7171630859375\n",
      "Epoch [66/100] Loss: 811.6644287109375\n",
      "Epoch [67/100] Loss: 807.6369018554688\n",
      "Epoch [68/100] Loss: 803.6339721679688\n",
      "Epoch [69/100] Loss: 799.6553344726562\n",
      "Epoch [70/100] Loss: 795.700439453125\n",
      "Epoch [71/100] Loss: 791.76904296875\n",
      "Epoch [72/100] Loss: 787.8602294921875\n",
      "Epoch [73/100] Loss: 783.97412109375\n",
      "Epoch [74/100] Loss: 780.1102905273438\n",
      "Epoch [75/100] Loss: 776.2680053710938\n",
      "Epoch [76/100] Loss: 772.4473266601562\n",
      "Epoch [77/100] Loss: 768.6478881835938\n",
      "Epoch [78/100] Loss: 764.8690795898438\n",
      "Epoch [79/100] Loss: 761.11083984375\n",
      "Epoch [80/100] Loss: 757.3731079101562\n",
      "Epoch [81/100] Loss: 753.6552124023438\n",
      "Epoch [82/100] Loss: 749.9571533203125\n",
      "Epoch [83/100] Loss: 746.2785034179688\n",
      "Epoch [84/100] Loss: 742.6189575195312\n",
      "Epoch [85/100] Loss: 738.9785766601562\n",
      "Epoch [86/100] Loss: 735.3568725585938\n",
      "Epoch [87/100] Loss: 731.7537841796875\n",
      "Epoch [88/100] Loss: 728.1690673828125\n",
      "Epoch [89/100] Loss: 724.6024169921875\n",
      "Epoch [90/100] Loss: 721.0537109375\n",
      "Epoch [91/100] Loss: 717.5227661132812\n",
      "Epoch [92/100] Loss: 714.0093994140625\n",
      "Epoch [93/100] Loss: 710.513427734375\n",
      "Epoch [94/100] Loss: 707.0345458984375\n",
      "Epoch [95/100] Loss: 703.5726928710938\n",
      "Epoch [96/100] Loss: 700.1278076171875\n",
      "Epoch [97/100] Loss: 696.6995239257812\n",
      "Epoch [98/100] Loss: 693.287841796875\n",
      "Epoch [99/100] Loss: 689.8925170898438\n",
      "Epoch [100/100] Loss: 686.513427734375\n",
      "Predicted days_remaining for parent_id 312: 9.890515327453613\n",
      "Training for parent_id 315...\n",
      "Epoch [1/100] Loss: 1510.5169677734375\n",
      "Epoch [2/100] Loss: 1492.5859375\n",
      "Epoch [3/100] Loss: 1474.849609375\n",
      "Epoch [4/100] Loss: 1457.4412841796875\n",
      "Epoch [5/100] Loss: 1440.47265625\n",
      "Epoch [6/100] Loss: 1424.0211181640625\n",
      "Epoch [7/100] Loss: 1408.1180419921875\n",
      "Epoch [8/100] Loss: 1392.7659912109375\n",
      "Epoch [9/100] Loss: 1377.9578857421875\n",
      "Epoch [10/100] Loss: 1363.687744140625\n",
      "Epoch [11/100] Loss: 1349.9534912109375\n",
      "Epoch [12/100] Loss: 1336.7523193359375\n",
      "Epoch [13/100] Loss: 1324.08154296875\n",
      "Epoch [14/100] Loss: 1311.933837890625\n",
      "Epoch [15/100] Loss: 1300.2967529296875\n",
      "Epoch [16/100] Loss: 1289.1527099609375\n",
      "Epoch [17/100] Loss: 1278.479736328125\n",
      "Epoch [18/100] Loss: 1268.25244140625\n",
      "Epoch [19/100] Loss: 1258.4447021484375\n",
      "Epoch [20/100] Loss: 1249.02978515625\n",
      "Epoch [21/100] Loss: 1239.981689453125\n",
      "Epoch [22/100] Loss: 1231.27783203125\n",
      "Epoch [23/100] Loss: 1222.89697265625\n",
      "Epoch [24/100] Loss: 1214.8218994140625\n",
      "Epoch [25/100] Loss: 1207.03662109375\n",
      "Epoch [26/100] Loss: 1199.5260009765625\n",
      "Epoch [27/100] Loss: 1192.275390625\n",
      "Epoch [28/100] Loss: 1185.2685546875\n",
      "Epoch [29/100] Loss: 1178.4859619140625\n",
      "Epoch [30/100] Loss: 1171.9088134765625\n",
      "Epoch [31/100] Loss: 1165.514892578125\n",
      "Epoch [32/100] Loss: 1159.284912109375\n",
      "Epoch [33/100] Loss: 1153.19873046875\n",
      "Epoch [34/100] Loss: 1147.240234375\n",
      "Epoch [35/100] Loss: 1141.3941650390625\n",
      "Epoch [36/100] Loss: 1135.6483154296875\n",
      "Epoch [37/100] Loss: 1129.992431640625\n",
      "Epoch [38/100] Loss: 1124.4171142578125\n",
      "Epoch [39/100] Loss: 1118.916015625\n",
      "Epoch [40/100] Loss: 1113.4825439453125\n",
      "Epoch [41/100] Loss: 1108.112060546875\n",
      "Epoch [42/100] Loss: 1102.8004150390625\n",
      "Epoch [43/100] Loss: 1097.543701171875\n",
      "Epoch [44/100] Loss: 1092.3397216796875\n",
      "Epoch [45/100] Loss: 1087.18505859375\n",
      "Epoch [46/100] Loss: 1082.0777587890625\n",
      "Epoch [47/100] Loss: 1077.01513671875\n",
      "Epoch [48/100] Loss: 1071.9951171875\n",
      "Epoch [49/100] Loss: 1067.0150146484375\n",
      "Epoch [50/100] Loss: 1062.07275390625\n",
      "Epoch [51/100] Loss: 1057.1658935546875\n",
      "Epoch [52/100] Loss: 1052.2923583984375\n",
      "Epoch [53/100] Loss: 1047.449951171875\n",
      "Epoch [54/100] Loss: 1042.6365966796875\n",
      "Epoch [55/100] Loss: 1037.8507080078125\n",
      "Epoch [56/100] Loss: 1033.0909423828125\n",
      "Epoch [57/100] Loss: 1028.3563232421875\n",
      "Epoch [58/100] Loss: 1023.6456909179688\n",
      "Epoch [59/100] Loss: 1018.958251953125\n",
      "Epoch [60/100] Loss: 1014.2937622070312\n",
      "Epoch [61/100] Loss: 1009.651611328125\n",
      "Epoch [62/100] Loss: 1005.0316162109375\n",
      "Epoch [63/100] Loss: 1000.4332885742188\n",
      "Epoch [64/100] Loss: 995.856689453125\n",
      "Epoch [65/100] Loss: 991.3011474609375\n",
      "Epoch [66/100] Loss: 986.7669677734375\n",
      "Epoch [67/100] Loss: 982.2535400390625\n",
      "Epoch [68/100] Loss: 977.760986328125\n",
      "Epoch [69/100] Loss: 973.289306640625\n",
      "Epoch [70/100] Loss: 968.837890625\n",
      "Epoch [71/100] Loss: 964.406982421875\n",
      "Epoch [72/100] Loss: 959.9964599609375\n",
      "Epoch [73/100] Loss: 955.606201171875\n",
      "Epoch [74/100] Loss: 951.2365112304688\n",
      "Epoch [75/100] Loss: 946.8870239257812\n",
      "Epoch [76/100] Loss: 942.5580444335938\n",
      "Epoch [77/100] Loss: 938.2492065429688\n",
      "Epoch [78/100] Loss: 933.9608154296875\n",
      "Epoch [79/100] Loss: 929.6923828125\n",
      "Epoch [80/100] Loss: 925.4442138671875\n",
      "Epoch [81/100] Loss: 921.2161254882812\n",
      "Epoch [82/100] Loss: 917.00830078125\n",
      "Epoch [83/100] Loss: 912.8202514648438\n",
      "Epoch [84/100] Loss: 908.652099609375\n",
      "Epoch [85/100] Loss: 904.5037841796875\n",
      "Epoch [86/100] Loss: 900.3751831054688\n",
      "Epoch [87/100] Loss: 896.2662353515625\n",
      "Epoch [88/100] Loss: 892.1766967773438\n",
      "Epoch [89/100] Loss: 888.106689453125\n",
      "Epoch [90/100] Loss: 884.0558471679688\n",
      "Epoch [91/100] Loss: 880.0242919921875\n",
      "Epoch [92/100] Loss: 876.0115356445312\n",
      "Epoch [93/100] Loss: 872.0178833007812\n",
      "Epoch [94/100] Loss: 868.04296875\n",
      "Epoch [95/100] Loss: 864.086669921875\n",
      "Epoch [96/100] Loss: 860.14892578125\n",
      "Epoch [97/100] Loss: 856.2293701171875\n",
      "Epoch [98/100] Loss: 852.3282470703125\n",
      "Epoch [99/100] Loss: 848.4449462890625\n",
      "Epoch [100/100] Loss: 844.580078125\n",
      "Predicted days_remaining for parent_id 315: 10.004650115966797\n",
      "Training for parent_id 316...\n",
      "Epoch [1/100] Loss: 303.5954895019531\n",
      "Epoch [2/100] Loss: 296.3509521484375\n",
      "Epoch [3/100] Loss: 289.26708984375\n",
      "Epoch [4/100] Loss: 282.3658142089844\n",
      "Epoch [5/100] Loss: 275.64776611328125\n",
      "Epoch [6/100] Loss: 269.1062927246094\n",
      "Epoch [7/100] Loss: 262.73626708984375\n",
      "Epoch [8/100] Loss: 256.5338439941406\n",
      "Epoch [9/100] Loss: 250.4952850341797\n",
      "Epoch [10/100] Loss: 244.61827087402344\n",
      "Epoch [11/100] Loss: 238.90142822265625\n",
      "Epoch [12/100] Loss: 233.3438720703125\n",
      "Epoch [13/100] Loss: 227.94554138183594\n",
      "Epoch [14/100] Loss: 222.707275390625\n",
      "Epoch [15/100] Loss: 217.63131713867188\n",
      "Epoch [16/100] Loss: 212.72047424316406\n",
      "Epoch [17/100] Loss: 207.97755432128906\n",
      "Epoch [18/100] Loss: 203.4046630859375\n",
      "Epoch [19/100] Loss: 199.0032958984375\n",
      "Epoch [20/100] Loss: 194.77420043945312\n",
      "Epoch [21/100] Loss: 190.7171630859375\n",
      "Epoch [22/100] Loss: 186.830322265625\n",
      "Epoch [23/100] Loss: 183.11021423339844\n",
      "Epoch [24/100] Loss: 179.5514373779297\n",
      "Epoch [25/100] Loss: 176.14697265625\n",
      "Epoch [26/100] Loss: 172.88832092285156\n",
      "Epoch [27/100] Loss: 169.7660369873047\n",
      "Epoch [28/100] Loss: 166.77001953125\n",
      "Epoch [29/100] Loss: 163.88995361328125\n",
      "Epoch [30/100] Loss: 161.11572265625\n",
      "Epoch [31/100] Loss: 158.43809509277344\n",
      "Epoch [32/100] Loss: 155.8489532470703\n",
      "Epoch [33/100] Loss: 153.34156799316406\n",
      "Epoch [34/100] Loss: 150.91087341308594\n",
      "Epoch [35/100] Loss: 148.5531463623047\n",
      "Epoch [36/100] Loss: 146.26576232910156\n",
      "Epoch [37/100] Loss: 144.0469512939453\n",
      "Epoch [38/100] Loss: 141.89503479003906\n",
      "Epoch [39/100] Loss: 139.80836486816406\n",
      "Epoch [40/100] Loss: 137.7847900390625\n",
      "Epoch [41/100] Loss: 135.82171630859375\n",
      "Epoch [42/100] Loss: 133.91603088378906\n",
      "Epoch [43/100] Loss: 132.06431579589844\n",
      "Epoch [44/100] Loss: 130.2628936767578\n",
      "Epoch [45/100] Loss: 128.5082550048828\n",
      "Epoch [46/100] Loss: 126.79700469970703\n",
      "Epoch [47/100] Loss: 125.12602996826172\n",
      "Epoch [48/100] Loss: 123.4925308227539\n",
      "Epoch [49/100] Loss: 121.89398193359375\n",
      "Epoch [50/100] Loss: 120.32821655273438\n",
      "Epoch [51/100] Loss: 118.79319763183594\n",
      "Epoch [52/100] Loss: 117.28714752197266\n",
      "Epoch [53/100] Loss: 115.8084487915039\n",
      "Epoch [54/100] Loss: 114.3556137084961\n",
      "Epoch [55/100] Loss: 112.92723846435547\n",
      "Epoch [56/100] Loss: 111.52213287353516\n",
      "Epoch [57/100] Loss: 110.13908386230469\n",
      "Epoch [58/100] Loss: 108.77716064453125\n",
      "Epoch [59/100] Loss: 107.43535614013672\n",
      "Epoch [60/100] Loss: 106.1129379272461\n",
      "Epoch [61/100] Loss: 104.80915832519531\n",
      "Epoch [62/100] Loss: 103.52338409423828\n",
      "Epoch [63/100] Loss: 102.25503540039062\n",
      "Epoch [64/100] Loss: 101.00366973876953\n",
      "Epoch [65/100] Loss: 99.76880645751953\n",
      "Epoch [66/100] Loss: 98.5500259399414\n",
      "Epoch [67/100] Loss: 97.34695434570312\n",
      "Epoch [68/100] Loss: 96.1592788696289\n",
      "Epoch [69/100] Loss: 94.98661804199219\n",
      "Epoch [70/100] Loss: 93.82867431640625\n",
      "Epoch [71/100] Loss: 92.68515014648438\n",
      "Epoch [72/100] Loss: 91.55575561523438\n",
      "Epoch [73/100] Loss: 90.44022369384766\n",
      "Epoch [74/100] Loss: 89.33834838867188\n",
      "Epoch [75/100] Loss: 88.24983215332031\n",
      "Epoch [76/100] Loss: 87.17455291748047\n",
      "Epoch [77/100] Loss: 86.11222076416016\n",
      "Epoch [78/100] Loss: 85.0627212524414\n",
      "Epoch [79/100] Loss: 84.02584075927734\n",
      "Epoch [80/100] Loss: 83.00146484375\n",
      "Epoch [81/100] Loss: 81.98946380615234\n",
      "Epoch [82/100] Loss: 80.98966979980469\n",
      "Epoch [83/100] Loss: 80.0019760131836\n",
      "Epoch [84/100] Loss: 79.0262680053711\n",
      "Epoch [85/100] Loss: 78.06241607666016\n",
      "Epoch [86/100] Loss: 77.11034393310547\n",
      "Epoch [87/100] Loss: 76.16995239257812\n",
      "Epoch [88/100] Loss: 75.24112701416016\n",
      "Epoch [89/100] Loss: 74.32381439208984\n",
      "Epoch [90/100] Loss: 73.41788482666016\n",
      "Epoch [91/100] Loss: 72.52326202392578\n",
      "Epoch [92/100] Loss: 71.63988494873047\n",
      "Epoch [93/100] Loss: 70.7676010131836\n",
      "Epoch [94/100] Loss: 69.90640258789062\n",
      "Epoch [95/100] Loss: 69.05616760253906\n",
      "Epoch [96/100] Loss: 68.21678924560547\n",
      "Epoch [97/100] Loss: 67.38816833496094\n",
      "Epoch [98/100] Loss: 66.57025146484375\n",
      "Epoch [99/100] Loss: 65.76293182373047\n",
      "Epoch [100/100] Loss: 64.96611785888672\n",
      "Predicted days_remaining for parent_id 316: 9.697183609008789\n",
      "Training for parent_id 317...\n",
      "Epoch [1/100] Loss: 1419.33544921875\n",
      "Epoch [2/100] Loss: 1405.1754150390625\n",
      "Epoch [3/100] Loss: 1391.388916015625\n",
      "Epoch [4/100] Loss: 1377.9212646484375\n",
      "Epoch [5/100] Loss: 1364.7138671875\n",
      "Epoch [6/100] Loss: 1351.7174072265625\n",
      "Epoch [7/100] Loss: 1338.8839111328125\n",
      "Epoch [8/100] Loss: 1326.176513671875\n",
      "Epoch [9/100] Loss: 1313.572998046875\n",
      "Epoch [10/100] Loss: 1301.0703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Loss: 1288.681884765625\n",
      "Epoch [12/100] Loss: 1276.43359375\n",
      "Epoch [13/100] Loss: 1264.357666015625\n",
      "Epoch [14/100] Loss: 1252.4844970703125\n",
      "Epoch [15/100] Loss: 1240.8409423828125\n",
      "Epoch [16/100] Loss: 1229.44970703125\n",
      "Epoch [17/100] Loss: 1218.32958984375\n",
      "Epoch [18/100] Loss: 1207.49560546875\n",
      "Epoch [19/100] Loss: 1196.960205078125\n",
      "Epoch [20/100] Loss: 1186.7353515625\n",
      "Epoch [21/100] Loss: 1176.8304443359375\n",
      "Epoch [22/100] Loss: 1167.25390625\n",
      "Epoch [23/100] Loss: 1158.011962890625\n",
      "Epoch [24/100] Loss: 1149.106689453125\n",
      "Epoch [25/100] Loss: 1140.5355224609375\n",
      "Epoch [26/100] Loss: 1132.291748046875\n",
      "Epoch [27/100] Loss: 1124.363037109375\n",
      "Epoch [28/100] Loss: 1116.7335205078125\n",
      "Epoch [29/100] Loss: 1109.3836669921875\n",
      "Epoch [30/100] Loss: 1102.2919921875\n",
      "Epoch [31/100] Loss: 1095.4356689453125\n",
      "Epoch [32/100] Loss: 1088.7911376953125\n",
      "Epoch [33/100] Loss: 1082.33642578125\n",
      "Epoch [34/100] Loss: 1076.0494384765625\n",
      "Epoch [35/100] Loss: 1069.9110107421875\n",
      "Epoch [36/100] Loss: 1063.9039306640625\n",
      "Epoch [37/100] Loss: 1058.01318359375\n",
      "Epoch [38/100] Loss: 1052.22607421875\n",
      "Epoch [39/100] Loss: 1046.5321044921875\n",
      "Epoch [40/100] Loss: 1040.9228515625\n",
      "Epoch [41/100] Loss: 1035.3912353515625\n",
      "Epoch [42/100] Loss: 1029.93212890625\n",
      "Epoch [43/100] Loss: 1024.5411376953125\n",
      "Epoch [44/100] Loss: 1019.2145385742188\n",
      "Epoch [45/100] Loss: 1013.9496459960938\n",
      "Epoch [46/100] Loss: 1008.7437744140625\n",
      "Epoch [47/100] Loss: 1003.5940551757812\n",
      "Epoch [48/100] Loss: 998.498291015625\n",
      "Epoch [49/100] Loss: 993.4541625976562\n",
      "Epoch [50/100] Loss: 988.4591064453125\n",
      "Epoch [51/100] Loss: 983.5109252929688\n",
      "Epoch [52/100] Loss: 978.6074829101562\n",
      "Epoch [53/100] Loss: 973.7467041015625\n",
      "Epoch [54/100] Loss: 968.9266967773438\n",
      "Epoch [55/100] Loss: 964.1461181640625\n",
      "Epoch [56/100] Loss: 959.4031982421875\n",
      "Epoch [57/100] Loss: 954.6964111328125\n",
      "Epoch [58/100] Loss: 950.0247192382812\n",
      "Epoch [59/100] Loss: 945.3870239257812\n",
      "Epoch [60/100] Loss: 940.7822875976562\n",
      "Epoch [61/100] Loss: 936.209716796875\n",
      "Epoch [62/100] Loss: 931.6683349609375\n",
      "Epoch [63/100] Loss: 927.1575317382812\n",
      "Epoch [64/100] Loss: 922.6768188476562\n",
      "Epoch [65/100] Loss: 918.2250366210938\n",
      "Epoch [66/100] Loss: 913.802001953125\n",
      "Epoch [67/100] Loss: 909.4070434570312\n",
      "Epoch [68/100] Loss: 905.03955078125\n",
      "Epoch [69/100] Loss: 900.69921875\n",
      "Epoch [70/100] Loss: 896.3854370117188\n",
      "Epoch [71/100] Loss: 892.0975341796875\n",
      "Epoch [72/100] Loss: 887.83544921875\n",
      "Epoch [73/100] Loss: 883.5985107421875\n",
      "Epoch [74/100] Loss: 879.38623046875\n",
      "Epoch [75/100] Loss: 875.1986083984375\n",
      "Epoch [76/100] Loss: 871.034912109375\n",
      "Epoch [77/100] Loss: 866.8946533203125\n",
      "Epoch [78/100] Loss: 862.77783203125\n",
      "Epoch [79/100] Loss: 858.683837890625\n",
      "Epoch [80/100] Loss: 854.6123046875\n",
      "Epoch [81/100] Loss: 850.5631103515625\n",
      "Epoch [82/100] Loss: 846.5358276367188\n",
      "Epoch [83/100] Loss: 842.5302734375\n",
      "Epoch [84/100] Loss: 838.5459594726562\n",
      "Epoch [85/100] Loss: 834.582763671875\n",
      "Epoch [86/100] Loss: 830.6403198242188\n",
      "Epoch [87/100] Loss: 826.7183837890625\n",
      "Epoch [88/100] Loss: 822.8165893554688\n",
      "Epoch [89/100] Loss: 818.9349365234375\n",
      "Epoch [90/100] Loss: 815.0728759765625\n",
      "Epoch [91/100] Loss: 811.2305297851562\n",
      "Epoch [92/100] Loss: 807.407470703125\n",
      "Epoch [93/100] Loss: 803.603271484375\n",
      "Epoch [94/100] Loss: 799.818115234375\n",
      "Epoch [95/100] Loss: 796.0514526367188\n",
      "Epoch [96/100] Loss: 792.3034057617188\n",
      "Epoch [97/100] Loss: 788.5733642578125\n",
      "Epoch [98/100] Loss: 784.8616333007812\n",
      "Epoch [99/100] Loss: 781.1676025390625\n",
      "Epoch [100/100] Loss: 777.491455078125\n",
      "Predicted days_remaining for parent_id 317: 10.192873001098633\n",
      "Training for parent_id 320...\n",
      "Epoch [1/100] Loss: 1885.4276123046875\n",
      "Epoch [2/100] Loss: 1863.646484375\n",
      "Epoch [3/100] Loss: 1842.533203125\n",
      "Epoch [4/100] Loss: 1822.2236328125\n",
      "Epoch [5/100] Loss: 1802.766357421875\n",
      "Epoch [6/100] Loss: 1784.1279296875\n",
      "Epoch [7/100] Loss: 1766.2398681640625\n",
      "Epoch [8/100] Loss: 1749.0340576171875\n",
      "Epoch [9/100] Loss: 1732.4556884765625\n",
      "Epoch [10/100] Loss: 1716.4674072265625\n",
      "Epoch [11/100] Loss: 1701.0489501953125\n",
      "Epoch [12/100] Loss: 1686.1912841796875\n",
      "Epoch [13/100] Loss: 1671.890625\n",
      "Epoch [14/100] Loss: 1658.1434326171875\n",
      "Epoch [15/100] Loss: 1644.9432373046875\n",
      "Epoch [16/100] Loss: 1632.27734375\n",
      "Epoch [17/100] Loss: 1620.1328125\n",
      "Epoch [18/100] Loss: 1608.4932861328125\n",
      "Epoch [19/100] Loss: 1597.3427734375\n",
      "Epoch [20/100] Loss: 1586.663818359375\n",
      "Epoch [21/100] Loss: 1576.4371337890625\n",
      "Epoch [22/100] Loss: 1566.6407470703125\n",
      "Epoch [23/100] Loss: 1557.2489013671875\n",
      "Epoch [24/100] Loss: 1548.2342529296875\n",
      "Epoch [25/100] Loss: 1539.56689453125\n",
      "Epoch [26/100] Loss: 1531.2164306640625\n",
      "Epoch [27/100] Loss: 1523.1529541015625\n",
      "Epoch [28/100] Loss: 1515.346923828125\n",
      "Epoch [29/100] Loss: 1507.7724609375\n",
      "Epoch [30/100] Loss: 1500.405029296875\n",
      "Epoch [31/100] Loss: 1493.2232666015625\n",
      "Epoch [32/100] Loss: 1486.208740234375\n",
      "Epoch [33/100] Loss: 1479.3455810546875\n",
      "Epoch [34/100] Loss: 1472.61962890625\n",
      "Epoch [35/100] Loss: 1466.0189208984375\n",
      "Epoch [36/100] Loss: 1459.5330810546875\n",
      "Epoch [37/100] Loss: 1453.152587890625\n",
      "Epoch [38/100] Loss: 1446.8685302734375\n",
      "Epoch [39/100] Loss: 1440.673583984375\n",
      "Epoch [40/100] Loss: 1434.5606689453125\n",
      "Epoch [41/100] Loss: 1428.5224609375\n",
      "Epoch [42/100] Loss: 1422.5531005859375\n",
      "Epoch [43/100] Loss: 1416.6473388671875\n",
      "Epoch [44/100] Loss: 1410.7994384765625\n",
      "Epoch [45/100] Loss: 1405.005126953125\n",
      "Epoch [46/100] Loss: 1399.26025390625\n",
      "Epoch [47/100] Loss: 1393.5611572265625\n",
      "Epoch [48/100] Loss: 1387.9052734375\n",
      "Epoch [49/100] Loss: 1382.2896728515625\n",
      "Epoch [50/100] Loss: 1376.7120361328125\n",
      "Epoch [51/100] Loss: 1371.1705322265625\n",
      "Epoch [52/100] Loss: 1365.664306640625\n",
      "Epoch [53/100] Loss: 1360.1910400390625\n",
      "Epoch [54/100] Loss: 1354.749755859375\n",
      "Epoch [55/100] Loss: 1349.3397216796875\n",
      "Epoch [56/100] Loss: 1343.9598388671875\n",
      "Epoch [57/100] Loss: 1338.6092529296875\n",
      "Epoch [58/100] Loss: 1333.2872314453125\n",
      "Epoch [59/100] Loss: 1327.9930419921875\n",
      "Epoch [60/100] Loss: 1322.726318359375\n",
      "Epoch [61/100] Loss: 1317.486328125\n",
      "Epoch [62/100] Loss: 1312.2724609375\n",
      "Epoch [63/100] Loss: 1307.08447265625\n",
      "Epoch [64/100] Loss: 1301.9215087890625\n",
      "Epoch [65/100] Loss: 1296.78369140625\n",
      "Epoch [66/100] Loss: 1291.670166015625\n",
      "Epoch [67/100] Loss: 1286.580810546875\n",
      "Epoch [68/100] Loss: 1281.5152587890625\n",
      "Epoch [69/100] Loss: 1276.4730224609375\n",
      "Epoch [70/100] Loss: 1271.4544677734375\n",
      "Epoch [71/100] Loss: 1266.45849609375\n",
      "Epoch [72/100] Loss: 1261.485107421875\n",
      "Epoch [73/100] Loss: 1256.5343017578125\n",
      "Epoch [74/100] Loss: 1251.60595703125\n",
      "Epoch [75/100] Loss: 1246.69921875\n",
      "Epoch [76/100] Loss: 1241.814453125\n",
      "Epoch [77/100] Loss: 1236.951171875\n",
      "Epoch [78/100] Loss: 1232.1092529296875\n",
      "Epoch [79/100] Loss: 1227.288330078125\n",
      "Epoch [80/100] Loss: 1222.48828125\n",
      "Epoch [81/100] Loss: 1217.70947265625\n",
      "Epoch [82/100] Loss: 1212.9508056640625\n",
      "Epoch [83/100] Loss: 1208.2127685546875\n",
      "Epoch [84/100] Loss: 1203.49462890625\n",
      "Epoch [85/100] Loss: 1198.7967529296875\n",
      "Epoch [86/100] Loss: 1194.1182861328125\n",
      "Epoch [87/100] Loss: 1189.4595947265625\n",
      "Epoch [88/100] Loss: 1184.820556640625\n",
      "Epoch [89/100] Loss: 1180.2005615234375\n",
      "Epoch [90/100] Loss: 1175.5997314453125\n",
      "Epoch [91/100] Loss: 1171.0177001953125\n",
      "Epoch [92/100] Loss: 1166.45458984375\n",
      "Epoch [93/100] Loss: 1161.909912109375\n",
      "Epoch [94/100] Loss: 1157.3836669921875\n",
      "Epoch [95/100] Loss: 1152.8756103515625\n",
      "Epoch [96/100] Loss: 1148.3856201171875\n",
      "Epoch [97/100] Loss: 1143.91357421875\n",
      "Epoch [98/100] Loss: 1139.459228515625\n",
      "Epoch [99/100] Loss: 1135.0224609375\n",
      "Epoch [100/100] Loss: 1130.6031494140625\n",
      "Predicted days_remaining for parent_id 320: 10.406880378723145\n",
      "Training for parent_id 321...\n",
      "Epoch [1/100] Loss: 795.8399658203125\n",
      "Epoch [2/100] Loss: 785.270751953125\n",
      "Epoch [3/100] Loss: 775.0147705078125\n",
      "Epoch [4/100] Loss: 765.0999755859375\n",
      "Epoch [5/100] Loss: 755.5219116210938\n",
      "Epoch [6/100] Loss: 746.2666015625\n",
      "Epoch [7/100] Loss: 737.3059692382812\n",
      "Epoch [8/100] Loss: 728.6004638671875\n",
      "Epoch [9/100] Loss: 720.1053466796875\n",
      "Epoch [10/100] Loss: 711.776123046875\n",
      "Epoch [11/100] Loss: 703.5721435546875\n",
      "Epoch [12/100] Loss: 695.4601440429688\n",
      "Epoch [13/100] Loss: 687.4172973632812\n",
      "Epoch [14/100] Loss: 679.4315795898438\n",
      "Epoch [15/100] Loss: 671.5012817382812\n",
      "Epoch [16/100] Loss: 663.6331176757812\n",
      "Epoch [17/100] Loss: 655.8399047851562\n",
      "Epoch [18/100] Loss: 648.13916015625\n",
      "Epoch [19/100] Loss: 640.55029296875\n",
      "Epoch [20/100] Loss: 633.0932006835938\n",
      "Epoch [21/100] Loss: 625.7865600585938\n",
      "Epoch [22/100] Loss: 618.6463012695312\n",
      "Epoch [23/100] Loss: 611.68505859375\n",
      "Epoch [24/100] Loss: 604.9110717773438\n",
      "Epoch [25/100] Loss: 598.3287963867188\n",
      "Epoch [26/100] Loss: 591.939453125\n",
      "Epoch [27/100] Loss: 585.740966796875\n",
      "Epoch [28/100] Loss: 579.729248046875\n",
      "Epoch [29/100] Loss: 573.898193359375\n",
      "Epoch [30/100] Loss: 568.2406005859375\n",
      "Epoch [31/100] Loss: 562.748291015625\n",
      "Epoch [32/100] Loss: 557.4119262695312\n",
      "Epoch [33/100] Loss: 552.2221069335938\n",
      "Epoch [34/100] Loss: 547.1688232421875\n",
      "Epoch [35/100] Loss: 542.2420654296875\n",
      "Epoch [36/100] Loss: 537.4319458007812\n",
      "Epoch [37/100] Loss: 532.7291870117188\n",
      "Epoch [38/100] Loss: 528.1248779296875\n",
      "Epoch [39/100] Loss: 523.6113891601562\n",
      "Epoch [40/100] Loss: 519.1817626953125\n",
      "Epoch [41/100] Loss: 514.8299560546875\n",
      "Epoch [42/100] Loss: 510.5509338378906\n",
      "Epoch [43/100] Loss: 506.34033203125\n",
      "Epoch [44/100] Loss: 502.1945495605469\n",
      "Epoch [45/100] Loss: 498.110107421875\n",
      "Epoch [46/100] Loss: 494.0841064453125\n",
      "Epoch [47/100] Loss: 490.1136474609375\n",
      "Epoch [48/100] Loss: 486.19622802734375\n",
      "Epoch [49/100] Loss: 482.32928466796875\n",
      "Epoch [50/100] Loss: 478.5103454589844\n",
      "Epoch [51/100] Loss: 474.7373046875\n",
      "Epoch [52/100] Loss: 471.0082702636719\n",
      "Epoch [53/100] Loss: 467.3216552734375\n",
      "Epoch [54/100] Loss: 463.67572021484375\n",
      "Epoch [55/100] Loss: 460.06951904296875\n",
      "Epoch [56/100] Loss: 456.50201416015625\n",
      "Epoch [57/100] Loss: 452.9722595214844\n",
      "Epoch [58/100] Loss: 449.4795837402344\n",
      "Epoch [59/100] Loss: 446.0232849121094\n",
      "Epoch [60/100] Loss: 442.6029052734375\n",
      "Epoch [61/100] Loss: 439.2176208496094\n",
      "Epoch [62/100] Loss: 435.8671569824219\n",
      "Epoch [63/100] Loss: 432.55072021484375\n",
      "Epoch [64/100] Loss: 429.2679138183594\n",
      "Epoch [65/100] Loss: 426.01812744140625\n",
      "Epoch [66/100] Loss: 422.80096435546875\n",
      "Epoch [67/100] Loss: 419.61566162109375\n",
      "Epoch [68/100] Loss: 416.4617919921875\n",
      "Epoch [69/100] Loss: 413.33880615234375\n",
      "Epoch [70/100] Loss: 410.2460632324219\n",
      "Epoch [71/100] Loss: 407.1830749511719\n",
      "Epoch [72/100] Loss: 404.1494445800781\n",
      "Epoch [73/100] Loss: 401.1445007324219\n",
      "Epoch [74/100] Loss: 398.16790771484375\n",
      "Epoch [75/100] Loss: 395.21917724609375\n",
      "Epoch [76/100] Loss: 392.2978210449219\n",
      "Epoch [77/100] Loss: 389.4034729003906\n",
      "Epoch [78/100] Loss: 386.5357971191406\n",
      "Epoch [79/100] Loss: 383.6943664550781\n",
      "Epoch [80/100] Loss: 380.8787841796875\n",
      "Epoch [81/100] Loss: 378.088623046875\n",
      "Epoch [82/100] Loss: 375.32354736328125\n",
      "Epoch [83/100] Loss: 372.5831298828125\n",
      "Epoch [84/100] Loss: 369.8670654296875\n",
      "Epoch [85/100] Loss: 367.17486572265625\n",
      "Epoch [86/100] Loss: 364.5061950683594\n",
      "Epoch [87/100] Loss: 361.8606262207031\n",
      "Epoch [88/100] Loss: 359.2377624511719\n",
      "Epoch [89/100] Loss: 356.6370849609375\n",
      "Epoch [90/100] Loss: 354.0583801269531\n",
      "Epoch [91/100] Loss: 351.5013427734375\n",
      "Epoch [92/100] Loss: 348.9652404785156\n",
      "Epoch [93/100] Loss: 346.45001220703125\n",
      "Epoch [94/100] Loss: 343.9553527832031\n",
      "Epoch [95/100] Loss: 341.480712890625\n",
      "Epoch [96/100] Loss: 339.0257263183594\n",
      "Epoch [97/100] Loss: 336.5903625488281\n",
      "Epoch [98/100] Loss: 334.17401123046875\n",
      "Epoch [99/100] Loss: 331.7765808105469\n",
      "Epoch [100/100] Loss: 329.39776611328125\n",
      "Predicted days_remaining for parent_id 321: 10.069512367248535\n",
      "Training for parent_id 326...\n",
      "Epoch [1/100] Loss: 234.35267639160156\n",
      "Epoch [2/100] Loss: 227.86973571777344\n",
      "Epoch [3/100] Loss: 221.5600128173828\n",
      "Epoch [4/100] Loss: 215.4488067626953\n",
      "Epoch [5/100] Loss: 209.53945922851562\n",
      "Epoch [6/100] Loss: 203.82449340820312\n",
      "Epoch [7/100] Loss: 198.28976440429688\n",
      "Epoch [8/100] Loss: 192.91912841796875\n",
      "Epoch [9/100] Loss: 187.69869995117188\n",
      "Epoch [10/100] Loss: 182.6194305419922\n",
      "Epoch [11/100] Loss: 177.67745971679688\n",
      "Epoch [12/100] Loss: 172.87303161621094\n",
      "Epoch [13/100] Loss: 168.20912170410156\n",
      "Epoch [14/100] Loss: 163.69041442871094\n",
      "Epoch [15/100] Loss: 159.3225860595703\n",
      "Epoch [16/100] Loss: 155.11160278320312\n",
      "Epoch [17/100] Loss: 151.0629425048828\n",
      "Epoch [18/100] Loss: 147.18069458007812\n",
      "Epoch [19/100] Loss: 143.46685791015625\n",
      "Epoch [20/100] Loss: 139.92076110839844\n",
      "Epoch [21/100] Loss: 136.53919982910156\n",
      "Epoch [22/100] Loss: 133.3164825439453\n",
      "Epoch [23/100] Loss: 130.24510192871094\n",
      "Epoch [24/100] Loss: 127.31634521484375\n",
      "Epoch [25/100] Loss: 124.52082061767578\n",
      "Epoch [26/100] Loss: 121.84917449951172\n",
      "Epoch [27/100] Loss: 119.29248046875\n",
      "Epoch [28/100] Loss: 116.84236907958984\n",
      "Epoch [29/100] Loss: 114.49118041992188\n",
      "Epoch [30/100] Loss: 112.23184967041016\n",
      "Epoch [31/100] Loss: 110.05795288085938\n",
      "Epoch [32/100] Loss: 107.96359252929688\n",
      "Epoch [33/100] Loss: 105.94326782226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] Loss: 103.9920654296875\n",
      "Epoch [35/100] Loss: 102.10552978515625\n",
      "Epoch [36/100] Loss: 100.27958679199219\n",
      "Epoch [37/100] Loss: 98.5105209350586\n",
      "Epoch [38/100] Loss: 96.79503631591797\n",
      "Epoch [39/100] Loss: 95.13006591796875\n",
      "Epoch [40/100] Loss: 93.51276397705078\n",
      "Epoch [41/100] Loss: 91.94053649902344\n",
      "Epoch [42/100] Loss: 90.41101837158203\n",
      "Epoch [43/100] Loss: 88.92191314697266\n",
      "Epoch [44/100] Loss: 87.47119903564453\n",
      "Epoch [45/100] Loss: 86.05696105957031\n",
      "Epoch [46/100] Loss: 84.67738342285156\n",
      "Epoch [47/100] Loss: 83.33079528808594\n",
      "Epoch [48/100] Loss: 82.01564025878906\n",
      "Epoch [49/100] Loss: 80.73053741455078\n",
      "Epoch [50/100] Loss: 79.4740982055664\n",
      "Epoch [51/100] Loss: 78.2451171875\n",
      "Epoch [52/100] Loss: 77.04244995117188\n",
      "Epoch [53/100] Loss: 75.86504364013672\n",
      "Epoch [54/100] Loss: 74.71192169189453\n",
      "Epoch [55/100] Loss: 73.5821762084961\n",
      "Epoch [56/100] Loss: 72.4749755859375\n",
      "Epoch [57/100] Loss: 71.38951110839844\n",
      "Epoch [58/100] Loss: 70.32508087158203\n",
      "Epoch [59/100] Loss: 69.28102111816406\n",
      "Epoch [60/100] Loss: 68.25666809082031\n",
      "Epoch [61/100] Loss: 67.25145721435547\n",
      "Epoch [62/100] Loss: 66.26480865478516\n",
      "Epoch [63/100] Loss: 65.29621124267578\n",
      "Epoch [64/100] Loss: 64.34518432617188\n",
      "Epoch [65/100] Loss: 63.4112434387207\n",
      "Epoch [66/100] Loss: 62.49396896362305\n",
      "Epoch [67/100] Loss: 61.59294509887695\n",
      "Epoch [68/100] Loss: 60.707767486572266\n",
      "Epoch [69/100] Loss: 59.838104248046875\n",
      "Epoch [70/100] Loss: 58.98356628417969\n",
      "Epoch [71/100] Loss: 58.143856048583984\n",
      "Epoch [72/100] Loss: 57.31865310668945\n",
      "Epoch [73/100] Loss: 56.50761032104492\n",
      "Epoch [74/100] Loss: 55.71049880981445\n",
      "Epoch [75/100] Loss: 54.927024841308594\n",
      "Epoch [76/100] Loss: 54.15693664550781\n",
      "Epoch [77/100] Loss: 53.39997863769531\n",
      "Epoch [78/100] Loss: 52.6558837890625\n",
      "Epoch [79/100] Loss: 51.92447280883789\n",
      "Epoch [80/100] Loss: 51.20547103881836\n",
      "Epoch [81/100] Loss: 50.49870300292969\n",
      "Epoch [82/100] Loss: 49.803916931152344\n",
      "Epoch [83/100] Loss: 49.1209716796875\n",
      "Epoch [84/100] Loss: 48.44960021972656\n",
      "Epoch [85/100] Loss: 47.78964614868164\n",
      "Epoch [86/100] Loss: 47.140926361083984\n",
      "Epoch [87/100] Loss: 46.503257751464844\n",
      "Epoch [88/100] Loss: 45.87644577026367\n",
      "Epoch [89/100] Loss: 45.260337829589844\n",
      "Epoch [90/100] Loss: 44.654754638671875\n",
      "Epoch [91/100] Loss: 44.05952072143555\n",
      "Epoch [92/100] Loss: 43.47447967529297\n",
      "Epoch [93/100] Loss: 42.89949417114258\n",
      "Epoch [94/100] Loss: 42.33436965942383\n",
      "Epoch [95/100] Loss: 41.77898406982422\n",
      "Epoch [96/100] Loss: 41.23316955566406\n",
      "Epoch [97/100] Loss: 40.696781158447266\n",
      "Epoch [98/100] Loss: 40.16969680786133\n",
      "Epoch [99/100] Loss: 39.651737213134766\n",
      "Epoch [100/100] Loss: 39.142784118652344\n",
      "Predicted days_remaining for parent_id 326: 9.830122947692871\n",
      "Training for parent_id 332...\n",
      "Epoch [1/100] Loss: 143.61700439453125\n",
      "Epoch [2/100] Loss: 138.80453491210938\n",
      "Epoch [3/100] Loss: 134.19064331054688\n",
      "Epoch [4/100] Loss: 129.7893524169922\n",
      "Epoch [5/100] Loss: 125.60890197753906\n",
      "Epoch [6/100] Loss: 121.65097045898438\n",
      "Epoch [7/100] Loss: 117.91008758544922\n",
      "Epoch [8/100] Loss: 114.37549591064453\n",
      "Epoch [9/100] Loss: 111.03345489501953\n",
      "Epoch [10/100] Loss: 107.86869812011719\n",
      "Epoch [11/100] Loss: 104.86505126953125\n",
      "Epoch [12/100] Loss: 102.0063247680664\n",
      "Epoch [13/100] Loss: 99.2775650024414\n",
      "Epoch [14/100] Loss: 96.66620635986328\n",
      "Epoch [15/100] Loss: 94.16254425048828\n",
      "Epoch [16/100] Loss: 91.75956726074219\n",
      "Epoch [17/100] Loss: 89.4524154663086\n",
      "Epoch [18/100] Loss: 87.23765563964844\n",
      "Epoch [19/100] Loss: 85.11245727539062\n",
      "Epoch [20/100] Loss: 83.07414245605469\n",
      "Epoch [21/100] Loss: 81.11981201171875\n",
      "Epoch [22/100] Loss: 79.24616241455078\n",
      "Epoch [23/100] Loss: 77.44938659667969\n",
      "Epoch [24/100] Loss: 75.72535705566406\n",
      "Epoch [25/100] Loss: 74.06962585449219\n",
      "Epoch [26/100] Loss: 72.47761535644531\n",
      "Epoch [27/100] Loss: 70.94477844238281\n",
      "Epoch [28/100] Loss: 69.46672821044922\n",
      "Epoch [29/100] Loss: 68.03941345214844\n",
      "Epoch [30/100] Loss: 66.65912628173828\n",
      "Epoch [31/100] Loss: 65.32254028320312\n",
      "Epoch [32/100] Loss: 64.02677917480469\n",
      "Epoch [33/100] Loss: 62.769287109375\n",
      "Epoch [34/100] Loss: 61.547847747802734\n",
      "Epoch [35/100] Loss: 60.36063766479492\n",
      "Epoch [36/100] Loss: 59.20603561401367\n",
      "Epoch [37/100] Loss: 58.08268356323242\n",
      "Epoch [38/100] Loss: 56.98942947387695\n",
      "Epoch [39/100] Loss: 55.9251708984375\n",
      "Epoch [40/100] Loss: 54.88900375366211\n",
      "Epoch [41/100] Loss: 53.880001068115234\n",
      "Epoch [42/100] Loss: 52.897335052490234\n",
      "Epoch [43/100] Loss: 51.94013977050781\n",
      "Epoch [44/100] Loss: 51.00759506225586\n",
      "Epoch [45/100] Loss: 50.09886932373047\n",
      "Epoch [46/100] Loss: 49.21316909790039\n",
      "Epoch [47/100] Loss: 48.34971618652344\n",
      "Epoch [48/100] Loss: 47.507720947265625\n",
      "Epoch [49/100] Loss: 46.68646240234375\n",
      "Epoch [50/100] Loss: 45.88523483276367\n",
      "Epoch [51/100] Loss: 45.103336334228516\n",
      "Epoch [52/100] Loss: 44.34016799926758\n",
      "Epoch [53/100] Loss: 43.5950927734375\n",
      "Epoch [54/100] Loss: 42.86756896972656\n",
      "Epoch [55/100] Loss: 42.15703582763672\n",
      "Epoch [56/100] Loss: 41.46299362182617\n",
      "Epoch [57/100] Loss: 40.784976959228516\n",
      "Epoch [58/100] Loss: 40.12253952026367\n",
      "Epoch [59/100] Loss: 39.47526168823242\n",
      "Epoch [60/100] Loss: 38.842742919921875\n",
      "Epoch [61/100] Loss: 38.2245979309082\n",
      "Epoch [62/100] Loss: 37.6204833984375\n",
      "Epoch [63/100] Loss: 37.030052185058594\n",
      "Epoch [64/100] Loss: 36.45298767089844\n",
      "Epoch [65/100] Loss: 35.88896560668945\n",
      "Epoch [66/100] Loss: 35.33770751953125\n",
      "Epoch [67/100] Loss: 34.79890823364258\n",
      "Epoch [68/100] Loss: 34.272315979003906\n",
      "Epoch [69/100] Loss: 33.757633209228516\n",
      "Epoch [70/100] Loss: 33.254634857177734\n",
      "Epoch [71/100] Loss: 32.7630615234375\n",
      "Epoch [72/100] Loss: 32.28267288208008\n",
      "Epoch [73/100] Loss: 31.81324005126953\n",
      "Epoch [74/100] Loss: 31.35451889038086\n",
      "Epoch [75/100] Loss: 30.906309127807617\n",
      "Epoch [76/100] Loss: 30.4683837890625\n",
      "Epoch [77/100] Loss: 30.040538787841797\n",
      "Epoch [78/100] Loss: 29.62257194519043\n",
      "Epoch [79/100] Loss: 29.214265823364258\n",
      "Epoch [80/100] Loss: 28.815448760986328\n",
      "Epoch [81/100] Loss: 28.425918579101562\n",
      "Epoch [82/100] Loss: 28.04548454284668\n",
      "Epoch [83/100] Loss: 27.67397689819336\n",
      "Epoch [84/100] Loss: 27.31120491027832\n",
      "Epoch [85/100] Loss: 26.956989288330078\n",
      "Epoch [86/100] Loss: 26.611175537109375\n",
      "Epoch [87/100] Loss: 26.273578643798828\n",
      "Epoch [88/100] Loss: 25.944046020507812\n",
      "Epoch [89/100] Loss: 25.622413635253906\n",
      "Epoch [90/100] Loss: 25.308507919311523\n",
      "Epoch [91/100] Loss: 25.002187728881836\n",
      "Epoch [92/100] Loss: 24.70330238342285\n",
      "Epoch [93/100] Loss: 24.411693572998047\n",
      "Epoch [94/100] Loss: 24.12721061706543\n",
      "Epoch [95/100] Loss: 23.84970474243164\n",
      "Epoch [96/100] Loss: 23.579057693481445\n",
      "Epoch [97/100] Loss: 23.31510353088379\n",
      "Epoch [98/100] Loss: 23.05771827697754\n",
      "Epoch [99/100] Loss: 22.806766510009766\n",
      "Epoch [100/100] Loss: 22.56210708618164\n",
      "Predicted days_remaining for parent_id 332: 8.941777229309082\n",
      "Training for parent_id 340...\n",
      "Epoch [1/100] Loss: 251.88931274414062\n",
      "Epoch [2/100] Loss: 244.8124542236328\n",
      "Epoch [3/100] Loss: 237.96099853515625\n",
      "Epoch [4/100] Loss: 231.33993530273438\n",
      "Epoch [5/100] Loss: 224.9387664794922\n",
      "Epoch [6/100] Loss: 218.75411987304688\n",
      "Epoch [7/100] Loss: 212.79200744628906\n",
      "Epoch [8/100] Loss: 207.0601043701172\n",
      "Epoch [9/100] Loss: 201.56362915039062\n",
      "Epoch [10/100] Loss: 196.3059844970703\n",
      "Epoch [11/100] Loss: 191.2894287109375\n",
      "Epoch [12/100] Loss: 186.51478576660156\n",
      "Epoch [13/100] Loss: 181.98072814941406\n",
      "Epoch [14/100] Loss: 177.68356323242188\n",
      "Epoch [15/100] Loss: 173.61752319335938\n",
      "Epoch [16/100] Loss: 169.77525329589844\n",
      "Epoch [17/100] Loss: 166.1481475830078\n",
      "Epoch [18/100] Loss: 162.72671508789062\n",
      "Epoch [19/100] Loss: 159.50050354003906\n",
      "Epoch [20/100] Loss: 156.45816040039062\n",
      "Epoch [21/100] Loss: 153.587646484375\n",
      "Epoch [22/100] Loss: 150.8761749267578\n",
      "Epoch [23/100] Loss: 148.310302734375\n",
      "Epoch [24/100] Loss: 145.8765106201172\n",
      "Epoch [25/100] Loss: 143.5614776611328\n",
      "Epoch [26/100] Loss: 141.35232543945312\n",
      "Epoch [27/100] Loss: 139.23715209960938\n",
      "Epoch [28/100] Loss: 137.2049560546875\n",
      "Epoch [29/100] Loss: 135.2460174560547\n",
      "Epoch [30/100] Loss: 133.35171508789062\n",
      "Epoch [31/100] Loss: 131.51463317871094\n",
      "Epoch [32/100] Loss: 129.7284698486328\n",
      "Epoch [33/100] Loss: 127.98783111572266\n",
      "Epoch [34/100] Loss: 126.2882308959961\n",
      "Epoch [35/100] Loss: 124.62591552734375\n",
      "Epoch [36/100] Loss: 122.99783325195312\n",
      "Epoch [37/100] Loss: 121.40138244628906\n",
      "Epoch [38/100] Loss: 119.83455657958984\n",
      "Epoch [39/100] Loss: 118.29556274414062\n",
      "Epoch [40/100] Loss: 116.7829360961914\n",
      "Epoch [41/100] Loss: 115.29550170898438\n",
      "Epoch [42/100] Loss: 113.83216094970703\n",
      "Epoch [43/100] Loss: 112.3919448852539\n",
      "Epoch [44/100] Loss: 110.9740219116211\n",
      "Epoch [45/100] Loss: 109.57758331298828\n",
      "Epoch [46/100] Loss: 108.20186614990234\n",
      "Epoch [47/100] Loss: 106.84629821777344\n",
      "Epoch [48/100] Loss: 105.5101547241211\n",
      "Epoch [49/100] Loss: 104.19288635253906\n",
      "Epoch [50/100] Loss: 102.89398956298828\n",
      "Epoch [51/100] Loss: 101.61287689208984\n",
      "Epoch [52/100] Loss: 100.34908294677734\n",
      "Epoch [53/100] Loss: 99.10215759277344\n",
      "Epoch [54/100] Loss: 97.87165069580078\n",
      "Epoch [55/100] Loss: 96.65711212158203\n",
      "Epoch [56/100] Loss: 95.45816802978516\n",
      "Epoch [57/100] Loss: 94.27442169189453\n",
      "Epoch [58/100] Loss: 93.10551452636719\n",
      "Epoch [59/100] Loss: 91.95118713378906\n",
      "Epoch [60/100] Loss: 90.81108093261719\n",
      "Epoch [61/100] Loss: 89.6849594116211\n",
      "Epoch [62/100] Loss: 88.57257843017578\n",
      "Epoch [63/100] Loss: 87.47370147705078\n",
      "Epoch [64/100] Loss: 86.38807678222656\n",
      "Epoch [65/100] Loss: 85.31560516357422\n",
      "Epoch [66/100] Loss: 84.25601959228516\n",
      "Epoch [67/100] Loss: 83.2092056274414\n",
      "Epoch [68/100] Loss: 82.17498016357422\n",
      "Epoch [69/100] Loss: 81.15325164794922\n",
      "Epoch [70/100] Loss: 80.14384460449219\n",
      "Epoch [71/100] Loss: 79.14665222167969\n",
      "Epoch [72/100] Loss: 78.16158294677734\n",
      "Epoch [73/100] Loss: 77.18852996826172\n",
      "Epoch [74/100] Loss: 76.22742462158203\n",
      "Epoch [75/100] Loss: 75.27811431884766\n",
      "Epoch [76/100] Loss: 74.340576171875\n",
      "Epoch [77/100] Loss: 73.41473388671875\n",
      "Epoch [78/100] Loss: 72.50048828125\n",
      "Epoch [79/100] Loss: 71.59778594970703\n",
      "Epoch [80/100] Loss: 70.70651245117188\n",
      "Epoch [81/100] Loss: 69.82664489746094\n",
      "Epoch [82/100] Loss: 68.95806884765625\n",
      "Epoch [83/100] Loss: 68.1007308959961\n",
      "Epoch [84/100] Loss: 67.25450897216797\n",
      "Epoch [85/100] Loss: 66.41935729980469\n",
      "Epoch [86/100] Loss: 65.59519958496094\n",
      "Epoch [87/100] Loss: 64.78191375732422\n",
      "Epoch [88/100] Loss: 63.97941589355469\n",
      "Epoch [89/100] Loss: 63.18761444091797\n",
      "Epoch [90/100] Loss: 62.406402587890625\n",
      "Epoch [91/100] Loss: 61.63569259643555\n",
      "Epoch [92/100] Loss: 60.8753776550293\n",
      "Epoch [93/100] Loss: 60.12535858154297\n",
      "Epoch [94/100] Loss: 59.38554000854492\n",
      "Epoch [95/100] Loss: 58.65580749511719\n",
      "Epoch [96/100] Loss: 57.93606948852539\n",
      "Epoch [97/100] Loss: 57.226192474365234\n",
      "Epoch [98/100] Loss: 56.5261116027832\n",
      "Epoch [99/100] Loss: 55.835689544677734\n",
      "Epoch [100/100] Loss: 55.15480422973633\n",
      "Predicted days_remaining for parent_id 340: 9.42181396484375\n",
      "Training for parent_id 346...\n",
      "Epoch [1/100] Loss: 356.6326599121094\n",
      "Epoch [2/100] Loss: 348.626220703125\n",
      "Epoch [3/100] Loss: 340.8638000488281\n",
      "Epoch [4/100] Loss: 333.3429260253906\n",
      "Epoch [5/100] Loss: 326.0625305175781\n",
      "Epoch [6/100] Loss: 319.0245056152344\n",
      "Epoch [7/100] Loss: 312.22125244140625\n",
      "Epoch [8/100] Loss: 305.635009765625\n",
      "Epoch [9/100] Loss: 299.2450866699219\n",
      "Epoch [10/100] Loss: 293.0345764160156\n",
      "Epoch [11/100] Loss: 286.9942932128906\n",
      "Epoch [12/100] Loss: 281.1236267089844\n",
      "Epoch [13/100] Loss: 275.42987060546875\n",
      "Epoch [14/100] Loss: 269.925048828125\n",
      "Epoch [15/100] Loss: 264.62225341796875\n",
      "Epoch [16/100] Loss: 259.5320129394531\n",
      "Epoch [17/100] Loss: 254.65988159179688\n",
      "Epoch [18/100] Loss: 250.00514221191406\n",
      "Epoch [19/100] Loss: 245.5613250732422\n",
      "Epoch [20/100] Loss: 241.31776428222656\n",
      "Epoch [21/100] Loss: 237.26171875\n",
      "Epoch [22/100] Loss: 233.37974548339844\n",
      "Epoch [23/100] Loss: 229.65879821777344\n",
      "Epoch [24/100] Loss: 226.0868682861328\n",
      "Epoch [25/100] Loss: 222.65310668945312\n",
      "Epoch [26/100] Loss: 219.34793090820312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] Loss: 216.1625213623047\n",
      "Epoch [28/100] Loss: 213.08897399902344\n",
      "Epoch [29/100] Loss: 210.1199493408203\n",
      "Epoch [30/100] Loss: 207.24868774414062\n",
      "Epoch [31/100] Loss: 204.46876525878906\n",
      "Epoch [32/100] Loss: 201.77432250976562\n",
      "Epoch [33/100] Loss: 199.1597900390625\n",
      "Epoch [34/100] Loss: 196.6200408935547\n",
      "Epoch [35/100] Loss: 194.15023803710938\n",
      "Epoch [36/100] Loss: 191.7457733154297\n",
      "Epoch [37/100] Loss: 189.4025115966797\n",
      "Epoch [38/100] Loss: 187.1163330078125\n",
      "Epoch [39/100] Loss: 184.88357543945312\n",
      "Epoch [40/100] Loss: 182.70054626464844\n",
      "Epoch [41/100] Loss: 180.56393432617188\n",
      "Epoch [42/100] Loss: 178.470458984375\n",
      "Epoch [43/100] Loss: 176.41722106933594\n",
      "Epoch [44/100] Loss: 174.40122985839844\n",
      "Epoch [45/100] Loss: 172.41995239257812\n",
      "Epoch [46/100] Loss: 170.47085571289062\n",
      "Epoch [47/100] Loss: 168.55174255371094\n",
      "Epoch [48/100] Loss: 166.6605682373047\n",
      "Epoch [49/100] Loss: 164.79559326171875\n",
      "Epoch [50/100] Loss: 162.95535278320312\n",
      "Epoch [51/100] Loss: 161.1387176513672\n",
      "Epoch [52/100] Loss: 159.3448486328125\n",
      "Epoch [53/100] Loss: 157.57308959960938\n",
      "Epoch [54/100] Loss: 155.8230438232422\n",
      "Epoch [55/100] Loss: 154.094482421875\n",
      "Epoch [56/100] Loss: 152.38720703125\n",
      "Epoch [57/100] Loss: 150.7010955810547\n",
      "Epoch [58/100] Loss: 149.0359649658203\n",
      "Epoch [59/100] Loss: 147.3916473388672\n",
      "Epoch [60/100] Loss: 145.76780700683594\n",
      "Epoch [61/100] Loss: 144.16421508789062\n",
      "Epoch [62/100] Loss: 142.5804901123047\n",
      "Epoch [63/100] Loss: 141.01629638671875\n",
      "Epoch [64/100] Loss: 139.4712371826172\n",
      "Epoch [65/100] Loss: 137.9449462890625\n",
      "Epoch [66/100] Loss: 136.4370574951172\n",
      "Epoch [67/100] Loss: 134.947265625\n",
      "Epoch [68/100] Loss: 133.47512817382812\n",
      "Epoch [69/100] Loss: 132.02037048339844\n",
      "Epoch [70/100] Loss: 130.5826416015625\n",
      "Epoch [71/100] Loss: 129.1616668701172\n",
      "Epoch [72/100] Loss: 127.7571792602539\n",
      "Epoch [73/100] Loss: 126.3688735961914\n",
      "Epoch [74/100] Loss: 124.99644470214844\n",
      "Epoch [75/100] Loss: 123.63973999023438\n",
      "Epoch [76/100] Loss: 122.29846954345703\n",
      "Epoch [77/100] Loss: 120.97239685058594\n",
      "Epoch [78/100] Loss: 119.66131591796875\n",
      "Epoch [79/100] Loss: 118.36498260498047\n",
      "Epoch [80/100] Loss: 117.08321380615234\n",
      "Epoch [81/100] Loss: 115.8158187866211\n",
      "Epoch [82/100] Loss: 114.56258392333984\n",
      "Epoch [83/100] Loss: 113.32331848144531\n",
      "Epoch [84/100] Loss: 112.09786987304688\n",
      "Epoch [85/100] Loss: 110.88601684570312\n",
      "Epoch [86/100] Loss: 109.68762969970703\n",
      "Epoch [87/100] Loss: 108.5024642944336\n",
      "Epoch [88/100] Loss: 107.33042907714844\n",
      "Epoch [89/100] Loss: 106.1713638305664\n",
      "Epoch [90/100] Loss: 105.02507781982422\n",
      "Epoch [91/100] Loss: 103.89142608642578\n",
      "Epoch [92/100] Loss: 102.77027893066406\n",
      "Epoch [93/100] Loss: 101.66146087646484\n",
      "Epoch [94/100] Loss: 100.56484985351562\n",
      "Epoch [95/100] Loss: 99.48028564453125\n",
      "Epoch [96/100] Loss: 98.40765380859375\n",
      "Epoch [97/100] Loss: 97.3468017578125\n",
      "Epoch [98/100] Loss: 96.29762268066406\n",
      "Epoch [99/100] Loss: 95.25996398925781\n",
      "Epoch [100/100] Loss: 94.23369598388672\n",
      "Predicted days_remaining for parent_id 346: 9.874122619628906\n",
      "Training for parent_id 347...\n",
      "Epoch [1/100] Loss: 325.40228271484375\n",
      "Epoch [2/100] Loss: 316.5086975097656\n",
      "Epoch [3/100] Loss: 307.9352722167969\n",
      "Epoch [4/100] Loss: 299.70037841796875\n",
      "Epoch [5/100] Loss: 291.78741455078125\n",
      "Epoch [6/100] Loss: 284.1590270996094\n",
      "Epoch [7/100] Loss: 276.78485107421875\n",
      "Epoch [8/100] Loss: 269.65582275390625\n",
      "Epoch [9/100] Loss: 262.7805480957031\n",
      "Epoch [10/100] Loss: 256.1745910644531\n",
      "Epoch [11/100] Loss: 249.85336303710938\n",
      "Epoch [12/100] Loss: 243.8272705078125\n",
      "Epoch [13/100] Loss: 238.1014404296875\n",
      "Epoch [14/100] Loss: 232.6758270263672\n",
      "Epoch [15/100] Loss: 227.54638671875\n",
      "Epoch [16/100] Loss: 222.70584106445312\n",
      "Epoch [17/100] Loss: 218.1439971923828\n",
      "Epoch [18/100] Loss: 213.84835815429688\n",
      "Epoch [19/100] Loss: 209.80404663085938\n",
      "Epoch [20/100] Loss: 205.994384765625\n",
      "Epoch [21/100] Loss: 202.40171813964844\n",
      "Epoch [22/100] Loss: 199.00782775878906\n",
      "Epoch [23/100] Loss: 195.794677734375\n",
      "Epoch [24/100] Loss: 192.74496459960938\n",
      "Epoch [25/100] Loss: 189.84228515625\n",
      "Epoch [26/100] Loss: 187.0716094970703\n",
      "Epoch [27/100] Loss: 184.4192352294922\n",
      "Epoch [28/100] Loss: 181.8727569580078\n",
      "Epoch [29/100] Loss: 179.42127990722656\n",
      "Epoch [30/100] Loss: 177.05508422851562\n",
      "Epoch [31/100] Loss: 174.76577758789062\n",
      "Epoch [32/100] Loss: 172.5460205078125\n",
      "Epoch [33/100] Loss: 170.3894805908203\n",
      "Epoch [34/100] Loss: 168.29078674316406\n",
      "Epoch [35/100] Loss: 166.24505615234375\n",
      "Epoch [36/100] Loss: 164.2482452392578\n",
      "Epoch [37/100] Loss: 162.2967071533203\n",
      "Epoch [38/100] Loss: 160.38710021972656\n",
      "Epoch [39/100] Loss: 158.5164794921875\n",
      "Epoch [40/100] Loss: 156.68206787109375\n",
      "Epoch [41/100] Loss: 154.88134765625\n",
      "Epoch [42/100] Loss: 153.11192321777344\n",
      "Epoch [43/100] Loss: 151.3717041015625\n",
      "Epoch [44/100] Loss: 149.65867614746094\n",
      "Epoch [45/100] Loss: 147.97113037109375\n",
      "Epoch [46/100] Loss: 146.3076171875\n",
      "Epoch [47/100] Loss: 144.66676330566406\n",
      "Epoch [48/100] Loss: 143.04745483398438\n",
      "Epoch [49/100] Loss: 141.44873046875\n",
      "Epoch [50/100] Loss: 139.8698272705078\n",
      "Epoch [51/100] Loss: 138.30999755859375\n",
      "Epoch [52/100] Loss: 136.7686767578125\n",
      "Epoch [53/100] Loss: 135.24534606933594\n",
      "Epoch [54/100] Loss: 133.73948669433594\n",
      "Epoch [55/100] Loss: 132.2506866455078\n",
      "Epoch [56/100] Loss: 130.7784881591797\n",
      "Epoch [57/100] Loss: 129.32260131835938\n",
      "Epoch [58/100] Loss: 127.88261413574219\n",
      "Epoch [59/100] Loss: 126.45818328857422\n",
      "Epoch [60/100] Loss: 125.0490493774414\n",
      "Epoch [61/100] Loss: 123.65483856201172\n",
      "Epoch [62/100] Loss: 122.27531433105469\n",
      "Epoch [63/100] Loss: 120.9101791381836\n",
      "Epoch [64/100] Loss: 119.55917358398438\n",
      "Epoch [65/100] Loss: 118.22199249267578\n",
      "Epoch [66/100] Loss: 116.89836120605469\n",
      "Epoch [67/100] Loss: 115.58802795410156\n",
      "Epoch [68/100] Loss: 114.29074096679688\n",
      "Epoch [69/100] Loss: 113.00623321533203\n",
      "Epoch [70/100] Loss: 111.73426818847656\n",
      "Epoch [71/100] Loss: 110.47462463378906\n",
      "Epoch [72/100] Loss: 109.22709655761719\n",
      "Epoch [73/100] Loss: 107.99159240722656\n",
      "Epoch [74/100] Loss: 106.76800537109375\n",
      "Epoch [75/100] Loss: 105.55623626708984\n",
      "Epoch [76/100] Loss: 104.35631561279297\n",
      "Epoch [77/100] Loss: 103.1682357788086\n",
      "Epoch [78/100] Loss: 101.99209594726562\n",
      "Epoch [79/100] Loss: 100.82794952392578\n",
      "Epoch [80/100] Loss: 99.67588806152344\n",
      "Epoch [81/100] Loss: 98.53596496582031\n",
      "Epoch [82/100] Loss: 97.40831756591797\n",
      "Epoch [83/100] Loss: 96.29296112060547\n",
      "Epoch [84/100] Loss: 95.18992614746094\n",
      "Epoch [85/100] Loss: 94.09928894042969\n",
      "Epoch [86/100] Loss: 93.02100372314453\n",
      "Epoch [87/100] Loss: 91.95504760742188\n",
      "Epoch [88/100] Loss: 90.90142059326172\n",
      "Epoch [89/100] Loss: 89.8599853515625\n",
      "Epoch [90/100] Loss: 88.83072662353516\n",
      "Epoch [91/100] Loss: 87.81356811523438\n",
      "Epoch [92/100] Loss: 86.8083724975586\n",
      "Epoch [93/100] Loss: 85.81505584716797\n",
      "Epoch [94/100] Loss: 84.83352661132812\n",
      "Epoch [95/100] Loss: 83.86363983154297\n",
      "Epoch [96/100] Loss: 82.90532684326172\n",
      "Epoch [97/100] Loss: 81.9584732055664\n",
      "Epoch [98/100] Loss: 81.02290344238281\n",
      "Epoch [99/100] Loss: 80.09852600097656\n",
      "Epoch [100/100] Loss: 79.18524169921875\n",
      "Predicted days_remaining for parent_id 347: 9.759666442871094\n",
      "Training for parent_id 348...\n",
      "Epoch [1/100] Loss: 1020.4152221679688\n",
      "Epoch [2/100] Loss: 1004.5551147460938\n",
      "Epoch [3/100] Loss: 988.965087890625\n",
      "Epoch [4/100] Loss: 973.8710327148438\n",
      "Epoch [5/100] Loss: 959.3905029296875\n",
      "Epoch [6/100] Loss: 945.5728759765625\n",
      "Epoch [7/100] Loss: 932.4104614257812\n",
      "Epoch [8/100] Loss: 919.8628540039062\n",
      "Epoch [9/100] Loss: 907.8776245117188\n",
      "Epoch [10/100] Loss: 896.3994140625\n",
      "Epoch [11/100] Loss: 885.380859375\n",
      "Epoch [12/100] Loss: 874.7871704101562\n",
      "Epoch [13/100] Loss: 864.5961303710938\n",
      "Epoch [14/100] Loss: 854.7948608398438\n",
      "Epoch [15/100] Loss: 845.3748779296875\n",
      "Epoch [16/100] Loss: 836.3281860351562\n",
      "Epoch [17/100] Loss: 827.6453857421875\n",
      "Epoch [18/100] Loss: 819.3131103515625\n",
      "Epoch [19/100] Loss: 811.3143310546875\n",
      "Epoch [20/100] Loss: 803.629150390625\n",
      "Epoch [21/100] Loss: 796.2349853515625\n",
      "Epoch [22/100] Loss: 789.1082763671875\n",
      "Epoch [23/100] Loss: 782.2262573242188\n",
      "Epoch [24/100] Loss: 775.5682983398438\n",
      "Epoch [25/100] Loss: 769.1162109375\n",
      "Epoch [26/100] Loss: 762.8554077148438\n",
      "Epoch [27/100] Loss: 756.774169921875\n",
      "Epoch [28/100] Loss: 750.8639526367188\n",
      "Epoch [29/100] Loss: 745.1173095703125\n",
      "Epoch [30/100] Loss: 739.5283203125\n",
      "Epoch [31/100] Loss: 734.091064453125\n",
      "Epoch [32/100] Loss: 728.7997436523438\n",
      "Epoch [33/100] Loss: 723.6476440429688\n",
      "Epoch [34/100] Loss: 718.627685546875\n",
      "Epoch [35/100] Loss: 713.7322387695312\n",
      "Epoch [36/100] Loss: 708.952880859375\n",
      "Epoch [37/100] Loss: 704.281494140625\n",
      "Epoch [38/100] Loss: 699.7095947265625\n",
      "Epoch [39/100] Loss: 695.229248046875\n",
      "Epoch [40/100] Loss: 690.8327026367188\n",
      "Epoch [41/100] Loss: 686.512939453125\n",
      "Epoch [42/100] Loss: 682.263671875\n",
      "Epoch [43/100] Loss: 678.078857421875\n",
      "Epoch [44/100] Loss: 673.9532470703125\n",
      "Epoch [45/100] Loss: 669.882080078125\n",
      "Epoch [46/100] Loss: 665.8616943359375\n",
      "Epoch [47/100] Loss: 661.8881225585938\n",
      "Epoch [48/100] Loss: 657.95849609375\n",
      "Epoch [49/100] Loss: 654.0698852539062\n",
      "Epoch [50/100] Loss: 650.2200317382812\n",
      "Epoch [51/100] Loss: 646.4069213867188\n",
      "Epoch [52/100] Loss: 642.6287231445312\n",
      "Epoch [53/100] Loss: 638.883544921875\n",
      "Epoch [54/100] Loss: 635.1703491210938\n",
      "Epoch [55/100] Loss: 631.4878540039062\n",
      "Epoch [56/100] Loss: 627.8347778320312\n",
      "Epoch [57/100] Loss: 624.2102661132812\n",
      "Epoch [58/100] Loss: 620.6134643554688\n",
      "Epoch [59/100] Loss: 617.0433959960938\n",
      "Epoch [60/100] Loss: 613.4995727539062\n",
      "Epoch [61/100] Loss: 609.981201171875\n",
      "Epoch [62/100] Loss: 606.4876708984375\n",
      "Epoch [63/100] Loss: 603.0184326171875\n",
      "Epoch [64/100] Loss: 599.5730590820312\n",
      "Epoch [65/100] Loss: 596.1510009765625\n",
      "Epoch [66/100] Loss: 592.7516479492188\n",
      "Epoch [67/100] Loss: 589.374755859375\n",
      "Epoch [68/100] Loss: 586.0200805664062\n",
      "Epoch [69/100] Loss: 582.6868896484375\n",
      "Epoch [70/100] Loss: 579.375\n",
      "Epoch [71/100] Loss: 576.0841064453125\n",
      "Epoch [72/100] Loss: 572.8139038085938\n",
      "Epoch [73/100] Loss: 569.5638427734375\n",
      "Epoch [74/100] Loss: 566.3339233398438\n",
      "Epoch [75/100] Loss: 563.123779296875\n",
      "Epoch [76/100] Loss: 559.932861328125\n",
      "Epoch [77/100] Loss: 556.7612915039062\n",
      "Epoch [78/100] Loss: 553.608642578125\n",
      "Epoch [79/100] Loss: 550.4747314453125\n",
      "Epoch [80/100] Loss: 547.3593139648438\n",
      "Epoch [81/100] Loss: 544.2618408203125\n",
      "Epoch [82/100] Loss: 541.1825561523438\n",
      "Epoch [83/100] Loss: 538.1212158203125\n",
      "Epoch [84/100] Loss: 535.0774536132812\n",
      "Epoch [85/100] Loss: 532.0508422851562\n",
      "Epoch [86/100] Loss: 529.0415649414062\n",
      "Epoch [87/100] Loss: 526.0493774414062\n",
      "Epoch [88/100] Loss: 523.0740356445312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100] Loss: 520.1151733398438\n",
      "Epoch [90/100] Loss: 517.1729125976562\n",
      "Epoch [91/100] Loss: 514.2469482421875\n",
      "Epoch [92/100] Loss: 511.3373107910156\n",
      "Epoch [93/100] Loss: 508.4434814453125\n",
      "Epoch [94/100] Loss: 505.5656433105469\n",
      "Epoch [95/100] Loss: 502.7034606933594\n",
      "Epoch [96/100] Loss: 499.8569030761719\n",
      "Epoch [97/100] Loss: 497.0257873535156\n",
      "Epoch [98/100] Loss: 494.2098693847656\n",
      "Epoch [99/100] Loss: 491.4092102050781\n",
      "Epoch [100/100] Loss: 488.62359619140625\n",
      "Predicted days_remaining for parent_id 348: 10.0378999710083\n",
      "Training for parent_id 352...\n",
      "Epoch [1/100] Loss: 198.87741088867188\n",
      "Epoch [2/100] Loss: 194.0473175048828\n",
      "Epoch [3/100] Loss: 189.28546142578125\n",
      "Epoch [4/100] Loss: 184.5845184326172\n",
      "Epoch [5/100] Loss: 179.94140625\n",
      "Epoch [6/100] Loss: 175.35736083984375\n",
      "Epoch [7/100] Loss: 170.8363037109375\n",
      "Epoch [8/100] Loss: 166.38363647460938\n",
      "Epoch [9/100] Loss: 162.00689697265625\n",
      "Epoch [10/100] Loss: 157.7161407470703\n",
      "Epoch [11/100] Loss: 153.52330017089844\n",
      "Epoch [12/100] Loss: 149.44073486328125\n",
      "Epoch [13/100] Loss: 145.48025512695312\n",
      "Epoch [14/100] Loss: 141.652099609375\n",
      "Epoch [15/100] Loss: 137.9643096923828\n",
      "Epoch [16/100] Loss: 134.42213439941406\n",
      "Epoch [17/100] Loss: 131.02789306640625\n",
      "Epoch [18/100] Loss: 127.78099060058594\n",
      "Epoch [19/100] Loss: 124.67835235595703\n",
      "Epoch [20/100] Loss: 121.71499633789062\n",
      "Epoch [21/100] Loss: 118.88450622558594\n",
      "Epoch [22/100] Loss: 116.17960357666016\n",
      "Epoch [23/100] Loss: 113.5925064086914\n",
      "Epoch [24/100] Loss: 111.11542510986328\n",
      "Epoch [25/100] Loss: 108.7406234741211\n",
      "Epoch [26/100] Loss: 106.46088409423828\n",
      "Epoch [27/100] Loss: 104.2693099975586\n",
      "Epoch [28/100] Loss: 102.15962219238281\n",
      "Epoch [29/100] Loss: 100.12596130371094\n",
      "Epoch [30/100] Loss: 98.16303253173828\n",
      "Epoch [31/100] Loss: 96.26605987548828\n",
      "Epoch [32/100] Loss: 94.43074798583984\n",
      "Epoch [33/100] Loss: 92.6532974243164\n",
      "Epoch [34/100] Loss: 90.9303207397461\n",
      "Epoch [35/100] Loss: 89.2588119506836\n",
      "Epoch [36/100] Loss: 87.63603973388672\n",
      "Epoch [37/100] Loss: 86.05953216552734\n",
      "Epoch [38/100] Loss: 84.52690887451172\n",
      "Epoch [39/100] Loss: 83.03596496582031\n",
      "Epoch [40/100] Loss: 81.58455657958984\n",
      "Epoch [41/100] Loss: 80.1706314086914\n",
      "Epoch [42/100] Loss: 78.79217529296875\n",
      "Epoch [43/100] Loss: 77.44725799560547\n",
      "Epoch [44/100] Loss: 76.13394927978516\n",
      "Epoch [45/100] Loss: 74.85052490234375\n",
      "Epoch [46/100] Loss: 73.5953140258789\n",
      "Epoch [47/100] Loss: 72.36677551269531\n",
      "Epoch [48/100] Loss: 71.16357421875\n",
      "Epoch [49/100] Loss: 69.98452758789062\n",
      "Epoch [50/100] Loss: 68.82862854003906\n",
      "Epoch [51/100] Loss: 67.6950454711914\n",
      "Epoch [52/100] Loss: 66.5831069946289\n",
      "Epoch [53/100] Loss: 65.49222564697266\n",
      "Epoch [54/100] Loss: 64.42189025878906\n",
      "Epoch [55/100] Loss: 63.371673583984375\n",
      "Epoch [56/100] Loss: 62.341121673583984\n",
      "Epoch [57/100] Loss: 61.329833984375\n",
      "Epoch [58/100] Loss: 60.33736801147461\n",
      "Epoch [59/100] Loss: 59.363277435302734\n",
      "Epoch [60/100] Loss: 58.407161712646484\n",
      "Epoch [61/100] Loss: 57.468589782714844\n",
      "Epoch [62/100] Loss: 56.547183990478516\n",
      "Epoch [63/100] Loss: 55.64261245727539\n",
      "Epoch [64/100] Loss: 54.75453186035156\n",
      "Epoch [65/100] Loss: 53.882667541503906\n",
      "Epoch [66/100] Loss: 53.02680206298828\n",
      "Epoch [67/100] Loss: 52.18671798706055\n",
      "Epoch [68/100] Loss: 51.362213134765625\n",
      "Epoch [69/100] Loss: 50.55315399169922\n",
      "Epoch [70/100] Loss: 49.759342193603516\n",
      "Epoch [71/100] Loss: 48.98064041137695\n",
      "Epoch [72/100] Loss: 48.2169075012207\n",
      "Epoch [73/100] Loss: 47.467960357666016\n",
      "Epoch [74/100] Loss: 46.73365020751953\n",
      "Epoch [75/100] Loss: 46.0137939453125\n",
      "Epoch [76/100] Loss: 45.308204650878906\n",
      "Epoch [77/100] Loss: 44.6166877746582\n",
      "Epoch [78/100] Loss: 43.93904495239258\n",
      "Epoch [79/100] Loss: 43.27503967285156\n",
      "Epoch [80/100] Loss: 42.62450408935547\n",
      "Epoch [81/100] Loss: 41.987144470214844\n",
      "Epoch [82/100] Loss: 41.36277389526367\n",
      "Epoch [83/100] Loss: 40.751155853271484\n",
      "Epoch [84/100] Loss: 40.15203094482422\n",
      "Epoch [85/100] Loss: 39.565185546875\n",
      "Epoch [86/100] Loss: 38.99040985107422\n",
      "Epoch [87/100] Loss: 38.42743682861328\n",
      "Epoch [88/100] Loss: 37.87605667114258\n",
      "Epoch [89/100] Loss: 37.336029052734375\n",
      "Epoch [90/100] Loss: 36.807151794433594\n",
      "Epoch [91/100] Loss: 36.28919219970703\n",
      "Epoch [92/100] Loss: 35.781951904296875\n",
      "Epoch [93/100] Loss: 35.285221099853516\n",
      "Epoch [94/100] Loss: 34.79879379272461\n",
      "Epoch [95/100] Loss: 34.32245635986328\n",
      "Epoch [96/100] Loss: 33.85601806640625\n",
      "Epoch [97/100] Loss: 33.39931106567383\n",
      "Epoch [98/100] Loss: 32.95212936401367\n",
      "Epoch [99/100] Loss: 32.514278411865234\n",
      "Epoch [100/100] Loss: 32.08561325073242\n",
      "Predicted days_remaining for parent_id 352: 9.599287986755371\n",
      "Training for parent_id 360...\n",
      "Epoch [1/100] Loss: 484.04071044921875\n",
      "Epoch [2/100] Loss: 471.49200439453125\n",
      "Epoch [3/100] Loss: 459.553955078125\n",
      "Epoch [4/100] Loss: 448.27410888671875\n",
      "Epoch [5/100] Loss: 437.6287536621094\n",
      "Epoch [6/100] Loss: 427.566650390625\n",
      "Epoch [7/100] Loss: 418.04193115234375\n",
      "Epoch [8/100] Loss: 409.0205383300781\n",
      "Epoch [9/100] Loss: 400.476806640625\n",
      "Epoch [10/100] Loss: 392.38787841796875\n",
      "Epoch [11/100] Loss: 384.73297119140625\n",
      "Epoch [12/100] Loss: 377.4917907714844\n",
      "Epoch [13/100] Loss: 370.6433410644531\n",
      "Epoch [14/100] Loss: 364.1647033691406\n",
      "Epoch [15/100] Loss: 358.0309753417969\n",
      "Epoch [16/100] Loss: 352.2156677246094\n",
      "Epoch [17/100] Loss: 346.69317626953125\n",
      "Epoch [18/100] Loss: 341.43914794921875\n",
      "Epoch [19/100] Loss: 336.4324951171875\n",
      "Epoch [20/100] Loss: 331.65582275390625\n",
      "Epoch [21/100] Loss: 327.0951843261719\n",
      "Epoch [22/100] Loss: 322.73895263671875\n",
      "Epoch [23/100] Loss: 318.5767517089844\n",
      "Epoch [24/100] Loss: 314.59747314453125\n",
      "Epoch [25/100] Loss: 310.7891540527344\n",
      "Epoch [26/100] Loss: 307.1379089355469\n",
      "Epoch [27/100] Loss: 303.62957763671875\n",
      "Epoch [28/100] Loss: 300.2499694824219\n",
      "Epoch [29/100] Loss: 296.9859619140625\n",
      "Epoch [30/100] Loss: 293.8254089355469\n",
      "Epoch [31/100] Loss: 290.7579650878906\n",
      "Epoch [32/100] Loss: 287.7741394042969\n",
      "Epoch [33/100] Loss: 284.86572265625\n",
      "Epoch [34/100] Loss: 282.025390625\n",
      "Epoch [35/100] Loss: 279.24676513671875\n",
      "Epoch [36/100] Loss: 276.52398681640625\n",
      "Epoch [37/100] Loss: 273.85174560546875\n",
      "Epoch [38/100] Loss: 271.2253723144531\n",
      "Epoch [39/100] Loss: 268.6399841308594\n",
      "Epoch [40/100] Loss: 266.091552734375\n",
      "Epoch [41/100] Loss: 263.5758972167969\n",
      "Epoch [42/100] Loss: 261.08929443359375\n",
      "Epoch [43/100] Loss: 258.62847900390625\n",
      "Epoch [44/100] Loss: 256.19110107421875\n",
      "Epoch [45/100] Loss: 253.77584838867188\n",
      "Epoch [46/100] Loss: 251.3829803466797\n",
      "Epoch [47/100] Loss: 249.01437377929688\n",
      "Epoch [48/100] Loss: 246.67323303222656\n",
      "Epoch [49/100] Loss: 244.3634490966797\n",
      "Epoch [50/100] Loss: 242.08872985839844\n",
      "Epoch [51/100] Loss: 239.85134887695312\n",
      "Epoch [52/100] Loss: 237.6520538330078\n",
      "Epoch [53/100] Loss: 235.4899139404297\n",
      "Epoch [54/100] Loss: 233.36279296875\n",
      "Epoch [55/100] Loss: 231.26795959472656\n",
      "Epoch [56/100] Loss: 229.2025909423828\n",
      "Epoch [57/100] Loss: 227.1639862060547\n",
      "Epoch [58/100] Loss: 225.15000915527344\n",
      "Epoch [59/100] Loss: 223.15878295898438\n",
      "Epoch [60/100] Loss: 221.1888885498047\n",
      "Epoch [61/100] Loss: 219.23928833007812\n",
      "Epoch [62/100] Loss: 217.30909729003906\n",
      "Epoch [63/100] Loss: 215.39752197265625\n",
      "Epoch [64/100] Loss: 213.50408935546875\n",
      "Epoch [65/100] Loss: 211.62835693359375\n",
      "Epoch [66/100] Loss: 209.76986694335938\n",
      "Epoch [67/100] Loss: 207.92828369140625\n",
      "Epoch [68/100] Loss: 206.10333251953125\n",
      "Epoch [69/100] Loss: 204.2947540283203\n",
      "Epoch [70/100] Loss: 202.50228881835938\n",
      "Epoch [71/100] Loss: 200.72567749023438\n",
      "Epoch [72/100] Loss: 198.96463012695312\n",
      "Epoch [73/100] Loss: 197.2190399169922\n",
      "Epoch [74/100] Loss: 195.4886932373047\n",
      "Epoch [75/100] Loss: 193.77333068847656\n",
      "Epoch [76/100] Loss: 192.07281494140625\n",
      "Epoch [77/100] Loss: 190.3870086669922\n",
      "Epoch [78/100] Loss: 188.71566772460938\n",
      "Epoch [79/100] Loss: 187.05865478515625\n",
      "Epoch [80/100] Loss: 185.41580200195312\n",
      "Epoch [81/100] Loss: 183.7869110107422\n",
      "Epoch [82/100] Loss: 182.171875\n",
      "Epoch [83/100] Loss: 180.5706024169922\n",
      "Epoch [84/100] Loss: 178.9828338623047\n",
      "Epoch [85/100] Loss: 177.4084930419922\n",
      "Epoch [86/100] Loss: 175.8474578857422\n",
      "Epoch [87/100] Loss: 174.29953002929688\n",
      "Epoch [88/100] Loss: 172.7646026611328\n",
      "Epoch [89/100] Loss: 171.2425994873047\n",
      "Epoch [90/100] Loss: 169.7333984375\n",
      "Epoch [91/100] Loss: 168.23675537109375\n",
      "Epoch [92/100] Loss: 166.75271606445312\n",
      "Epoch [93/100] Loss: 165.28103637695312\n",
      "Epoch [94/100] Loss: 163.82164001464844\n",
      "Epoch [95/100] Loss: 162.37442016601562\n",
      "Epoch [96/100] Loss: 160.93934631347656\n",
      "Epoch [97/100] Loss: 159.5161590576172\n",
      "Epoch [98/100] Loss: 158.1049041748047\n",
      "Epoch [99/100] Loss: 156.70533752441406\n",
      "Epoch [100/100] Loss: 155.31747436523438\n",
      "Predicted days_remaining for parent_id 360: 9.938833236694336\n",
      "Training for parent_id 362...\n",
      "Epoch [1/100] Loss: 1155.27197265625\n",
      "Epoch [2/100] Loss: 1141.261962890625\n",
      "Epoch [3/100] Loss: 1127.7882080078125\n",
      "Epoch [4/100] Loss: 1114.819580078125\n",
      "Epoch [5/100] Loss: 1102.3182373046875\n",
      "Epoch [6/100] Loss: 1090.23583984375\n",
      "Epoch [7/100] Loss: 1078.5341796875\n",
      "Epoch [8/100] Loss: 1067.181884765625\n",
      "Epoch [9/100] Loss: 1056.152587890625\n",
      "Epoch [10/100] Loss: 1045.427978515625\n",
      "Epoch [11/100] Loss: 1034.99658203125\n",
      "Epoch [12/100] Loss: 1024.8564453125\n",
      "Epoch [13/100] Loss: 1015.0093383789062\n",
      "Epoch [14/100] Loss: 1005.4589233398438\n",
      "Epoch [15/100] Loss: 996.2081909179688\n",
      "Epoch [16/100] Loss: 987.25732421875\n",
      "Epoch [17/100] Loss: 978.603271484375\n",
      "Epoch [18/100] Loss: 970.2379150390625\n",
      "Epoch [19/100] Loss: 962.1495971679688\n",
      "Epoch [20/100] Loss: 954.3233642578125\n",
      "Epoch [21/100] Loss: 946.742431640625\n",
      "Epoch [22/100] Loss: 939.388916015625\n",
      "Epoch [23/100] Loss: 932.2449951171875\n",
      "Epoch [24/100] Loss: 925.293701171875\n",
      "Epoch [25/100] Loss: 918.5191650390625\n",
      "Epoch [26/100] Loss: 911.9067993164062\n",
      "Epoch [27/100] Loss: 905.444580078125\n",
      "Epoch [28/100] Loss: 899.1221923828125\n",
      "Epoch [29/100] Loss: 892.9317626953125\n",
      "Epoch [30/100] Loss: 886.86669921875\n",
      "Epoch [31/100] Loss: 880.9224243164062\n",
      "Epoch [32/100] Loss: 875.0948486328125\n",
      "Epoch [33/100] Loss: 869.3804321289062\n",
      "Epoch [34/100] Loss: 863.7758178710938\n",
      "Epoch [35/100] Loss: 858.2769775390625\n",
      "Epoch [36/100] Loss: 852.8799438476562\n",
      "Epoch [37/100] Loss: 847.5797119140625\n",
      "Epoch [38/100] Loss: 842.3712158203125\n",
      "Epoch [39/100] Loss: 837.2490844726562\n",
      "Epoch [40/100] Loss: 832.2074584960938\n",
      "Epoch [41/100] Loss: 827.2407836914062\n",
      "Epoch [42/100] Loss: 822.3436279296875\n",
      "Epoch [43/100] Loss: 817.51025390625\n",
      "Epoch [44/100] Loss: 812.7357788085938\n",
      "Epoch [45/100] Loss: 808.0153198242188\n",
      "Epoch [46/100] Loss: 803.3446044921875\n",
      "Epoch [47/100] Loss: 798.7202758789062\n",
      "Epoch [48/100] Loss: 794.1392211914062\n",
      "Epoch [49/100] Loss: 789.59912109375\n",
      "Epoch [50/100] Loss: 785.0984497070312\n",
      "Epoch [51/100] Loss: 780.6358032226562\n",
      "Epoch [52/100] Loss: 776.2102661132812\n",
      "Epoch [53/100] Loss: 771.821533203125\n",
      "Epoch [54/100] Loss: 767.468994140625\n",
      "Epoch [55/100] Loss: 763.1524658203125\n",
      "Epoch [56/100] Loss: 758.8712158203125\n",
      "Epoch [57/100] Loss: 754.6249389648438\n",
      "Epoch [58/100] Loss: 750.4134521484375\n",
      "Epoch [59/100] Loss: 746.2357788085938\n",
      "Epoch [60/100] Loss: 742.0913696289062\n",
      "Epoch [61/100] Loss: 737.9797973632812\n",
      "Epoch [62/100] Loss: 733.9000244140625\n",
      "Epoch [63/100] Loss: 729.8517456054688\n",
      "Epoch [64/100] Loss: 725.8341064453125\n",
      "Epoch [65/100] Loss: 721.8463745117188\n",
      "Epoch [66/100] Loss: 717.887939453125\n",
      "Epoch [67/100] Loss: 713.958251953125\n",
      "Epoch [68/100] Loss: 710.056640625\n",
      "Epoch [69/100] Loss: 706.1824340820312\n",
      "Epoch [70/100] Loss: 702.3352661132812\n",
      "Epoch [71/100] Loss: 698.5144653320312\n",
      "Epoch [72/100] Loss: 694.7197265625\n",
      "Epoch [73/100] Loss: 690.9503784179688\n",
      "Epoch [74/100] Loss: 687.2060546875\n",
      "Epoch [75/100] Loss: 683.4863891601562\n",
      "Epoch [76/100] Loss: 679.790771484375\n",
      "Epoch [77/100] Loss: 676.1190185546875\n",
      "Epoch [78/100] Loss: 672.470703125\n",
      "Epoch [79/100] Loss: 668.8453369140625\n",
      "Epoch [80/100] Loss: 665.24267578125\n",
      "Epoch [81/100] Loss: 661.6622314453125\n",
      "Epoch [82/100] Loss: 658.1038208007812\n",
      "Epoch [83/100] Loss: 654.5670776367188\n",
      "Epoch [84/100] Loss: 651.0518188476562\n",
      "Epoch [85/100] Loss: 647.5574951171875\n",
      "Epoch [86/100] Loss: 644.0840454101562\n",
      "Epoch [87/100] Loss: 640.6310424804688\n",
      "Epoch [88/100] Loss: 637.1984252929688\n",
      "Epoch [89/100] Loss: 633.7855224609375\n",
      "Epoch [90/100] Loss: 630.3925170898438\n",
      "Epoch [91/100] Loss: 627.0189819335938\n",
      "Epoch [92/100] Loss: 623.6646118164062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100] Loss: 620.3292236328125\n",
      "Epoch [94/100] Loss: 617.0126953125\n",
      "Epoch [95/100] Loss: 613.7147827148438\n",
      "Epoch [96/100] Loss: 610.4351196289062\n",
      "Epoch [97/100] Loss: 607.1736450195312\n",
      "Epoch [98/100] Loss: 603.9301147460938\n",
      "Epoch [99/100] Loss: 600.704345703125\n",
      "Epoch [100/100] Loss: 597.49609375\n",
      "Predicted days_remaining for parent_id 362: 9.6695556640625\n",
      "Training for parent_id 366...\n",
      "Epoch [1/100] Loss: 956.5789794921875\n",
      "Epoch [2/100] Loss: 942.6150512695312\n",
      "Epoch [3/100] Loss: 928.9755859375\n",
      "Epoch [4/100] Loss: 915.7225341796875\n",
      "Epoch [5/100] Loss: 902.8890380859375\n",
      "Epoch [6/100] Loss: 890.491455078125\n",
      "Epoch [7/100] Loss: 878.5302124023438\n",
      "Epoch [8/100] Loss: 866.9974365234375\n",
      "Epoch [9/100] Loss: 855.8812255859375\n",
      "Epoch [10/100] Loss: 845.174072265625\n",
      "Epoch [11/100] Loss: 834.8778686523438\n",
      "Epoch [12/100] Loss: 824.9991455078125\n",
      "Epoch [13/100] Loss: 815.5430297851562\n",
      "Epoch [14/100] Loss: 806.5081176757812\n",
      "Epoch [15/100] Loss: 797.88525390625\n",
      "Epoch [16/100] Loss: 789.6573486328125\n",
      "Epoch [17/100] Loss: 781.8013916015625\n",
      "Epoch [18/100] Loss: 774.2899780273438\n",
      "Epoch [19/100] Loss: 767.0952758789062\n",
      "Epoch [20/100] Loss: 760.1898803710938\n",
      "Epoch [21/100] Loss: 753.5498657226562\n",
      "Epoch [22/100] Loss: 747.1536865234375\n",
      "Epoch [23/100] Loss: 740.9840087890625\n",
      "Epoch [24/100] Loss: 735.0260009765625\n",
      "Epoch [25/100] Loss: 729.2676391601562\n",
      "Epoch [26/100] Loss: 723.6986694335938\n",
      "Epoch [27/100] Loss: 718.3092651367188\n",
      "Epoch [28/100] Loss: 713.0903930664062\n",
      "Epoch [29/100] Loss: 708.0326538085938\n",
      "Epoch [30/100] Loss: 703.1261596679688\n",
      "Epoch [31/100] Loss: 698.3604736328125\n",
      "Epoch [32/100] Loss: 693.7247924804688\n",
      "Epoch [33/100] Loss: 689.2075805664062\n",
      "Epoch [34/100] Loss: 684.7968139648438\n",
      "Epoch [35/100] Loss: 680.4801025390625\n",
      "Epoch [36/100] Loss: 676.2453002929688\n",
      "Epoch [37/100] Loss: 672.0810546875\n",
      "Epoch [38/100] Loss: 667.9765625\n",
      "Epoch [39/100] Loss: 663.9225463867188\n",
      "Epoch [40/100] Loss: 659.911376953125\n",
      "Epoch [41/100] Loss: 655.9363403320312\n",
      "Epoch [42/100] Loss: 651.9931030273438\n",
      "Epoch [43/100] Loss: 648.0775756835938\n",
      "Epoch [44/100] Loss: 644.187744140625\n",
      "Epoch [45/100] Loss: 640.322265625\n",
      "Epoch [46/100] Loss: 636.4802856445312\n",
      "Epoch [47/100] Loss: 632.6616821289062\n",
      "Epoch [48/100] Loss: 628.8670043945312\n",
      "Epoch [49/100] Loss: 625.0963745117188\n",
      "Epoch [50/100] Loss: 621.350341796875\n",
      "Epoch [51/100] Loss: 617.6290893554688\n",
      "Epoch [52/100] Loss: 613.9329833984375\n",
      "Epoch [53/100] Loss: 610.2618408203125\n",
      "Epoch [54/100] Loss: 606.615966796875\n",
      "Epoch [55/100] Loss: 602.9951782226562\n",
      "Epoch [56/100] Loss: 599.399169921875\n",
      "Epoch [57/100] Loss: 595.8280029296875\n",
      "Epoch [58/100] Loss: 592.2814331054688\n",
      "Epoch [59/100] Loss: 588.759033203125\n",
      "Epoch [60/100] Loss: 585.2607421875\n",
      "Epoch [61/100] Loss: 581.7862548828125\n",
      "Epoch [62/100] Loss: 578.3355102539062\n",
      "Epoch [63/100] Loss: 574.9080200195312\n",
      "Epoch [64/100] Loss: 571.5037841796875\n",
      "Epoch [65/100] Loss: 568.1223754882812\n",
      "Epoch [66/100] Loss: 564.763671875\n",
      "Epoch [67/100] Loss: 561.4273071289062\n",
      "Epoch [68/100] Loss: 558.1131591796875\n",
      "Epoch [69/100] Loss: 554.8208618164062\n",
      "Epoch [70/100] Loss: 551.5501708984375\n",
      "Epoch [71/100] Loss: 548.3009033203125\n",
      "Epoch [72/100] Loss: 545.0726318359375\n",
      "Epoch [73/100] Loss: 541.8653564453125\n",
      "Epoch [74/100] Loss: 538.6785278320312\n",
      "Epoch [75/100] Loss: 535.5121459960938\n",
      "Epoch [76/100] Loss: 532.3658447265625\n",
      "Epoch [77/100] Loss: 529.2394409179688\n",
      "Epoch [78/100] Loss: 526.1326293945312\n",
      "Epoch [79/100] Loss: 523.0452880859375\n",
      "Epoch [80/100] Loss: 519.97705078125\n",
      "Epoch [81/100] Loss: 516.927734375\n",
      "Epoch [82/100] Loss: 513.8971557617188\n",
      "Epoch [83/100] Loss: 510.8851318359375\n",
      "Epoch [84/100] Loss: 507.891357421875\n",
      "Epoch [85/100] Loss: 504.9156494140625\n",
      "Epoch [86/100] Loss: 501.95794677734375\n",
      "Epoch [87/100] Loss: 499.017822265625\n",
      "Epoch [88/100] Loss: 496.09527587890625\n",
      "Epoch [89/100] Loss: 493.18994140625\n",
      "Epoch [90/100] Loss: 490.3018798828125\n",
      "Epoch [91/100] Loss: 487.43072509765625\n",
      "Epoch [92/100] Loss: 484.5763854980469\n",
      "Epoch [93/100] Loss: 481.7386474609375\n",
      "Epoch [94/100] Loss: 478.9172668457031\n",
      "Epoch [95/100] Loss: 476.1122741699219\n",
      "Epoch [96/100] Loss: 473.3233947753906\n",
      "Epoch [97/100] Loss: 470.550537109375\n",
      "Epoch [98/100] Loss: 467.79339599609375\n",
      "Epoch [99/100] Loss: 465.05194091796875\n",
      "Epoch [100/100] Loss: 462.3261413574219\n",
      "Predicted days_remaining for parent_id 366: 9.650755882263184\n",
      "Training for parent_id 367...\n",
      "Epoch [1/100] Loss: 2213.71484375\n",
      "Epoch [2/100] Loss: 2190.25830078125\n",
      "Epoch [3/100] Loss: 2167.137451171875\n",
      "Epoch [4/100] Loss: 2144.518310546875\n",
      "Epoch [5/100] Loss: 2122.523681640625\n",
      "Epoch [6/100] Loss: 2101.2529296875\n",
      "Epoch [7/100] Loss: 2080.7587890625\n",
      "Epoch [8/100] Loss: 2061.03662109375\n",
      "Epoch [9/100] Loss: 2042.046142578125\n",
      "Epoch [10/100] Loss: 2023.7415771484375\n",
      "Epoch [11/100] Loss: 2006.088134765625\n",
      "Epoch [12/100] Loss: 1989.070556640625\n",
      "Epoch [13/100] Loss: 1972.694091796875\n",
      "Epoch [14/100] Loss: 1956.974609375\n",
      "Epoch [15/100] Loss: 1941.931640625\n",
      "Epoch [16/100] Loss: 1927.58251953125\n",
      "Epoch [17/100] Loss: 1913.934326171875\n",
      "Epoch [18/100] Loss: 1900.9815673828125\n",
      "Epoch [19/100] Loss: 1888.7021484375\n",
      "Epoch [20/100] Loss: 1877.059814453125\n",
      "Epoch [21/100] Loss: 1866.006591796875\n",
      "Epoch [22/100] Loss: 1855.4879150390625\n",
      "Epoch [23/100] Loss: 1845.448486328125\n",
      "Epoch [24/100] Loss: 1835.8333740234375\n",
      "Epoch [25/100] Loss: 1826.591796875\n",
      "Epoch [26/100] Loss: 1817.677490234375\n",
      "Epoch [27/100] Loss: 1809.0489501953125\n",
      "Epoch [28/100] Loss: 1800.67041015625\n",
      "Epoch [29/100] Loss: 1792.5113525390625\n",
      "Epoch [30/100] Loss: 1784.54443359375\n",
      "Epoch [31/100] Loss: 1776.7470703125\n",
      "Epoch [32/100] Loss: 1769.100341796875\n",
      "Epoch [33/100] Loss: 1761.588134765625\n",
      "Epoch [34/100] Loss: 1754.196533203125\n",
      "Epoch [35/100] Loss: 1746.9140625\n",
      "Epoch [36/100] Loss: 1739.7322998046875\n",
      "Epoch [37/100] Loss: 1732.6416015625\n",
      "Epoch [38/100] Loss: 1725.63671875\n",
      "Epoch [39/100] Loss: 1718.71142578125\n",
      "Epoch [40/100] Loss: 1711.8609619140625\n",
      "Epoch [41/100] Loss: 1705.081298828125\n",
      "Epoch [42/100] Loss: 1698.36962890625\n",
      "Epoch [43/100] Loss: 1691.722412109375\n",
      "Epoch [44/100] Loss: 1685.136962890625\n",
      "Epoch [45/100] Loss: 1678.611328125\n",
      "Epoch [46/100] Loss: 1672.1431884765625\n",
      "Epoch [47/100] Loss: 1665.72998046875\n",
      "Epoch [48/100] Loss: 1659.3695068359375\n",
      "Epoch [49/100] Loss: 1653.060546875\n",
      "Epoch [50/100] Loss: 1646.8004150390625\n",
      "Epoch [51/100] Loss: 1640.5875244140625\n",
      "Epoch [52/100] Loss: 1634.4195556640625\n",
      "Epoch [53/100] Loss: 1628.2955322265625\n",
      "Epoch [54/100] Loss: 1622.2132568359375\n",
      "Epoch [55/100] Loss: 1616.1715087890625\n",
      "Epoch [56/100] Loss: 1610.169189453125\n",
      "Epoch [57/100] Loss: 1604.2042236328125\n",
      "Epoch [58/100] Loss: 1598.2755126953125\n",
      "Epoch [59/100] Loss: 1592.382568359375\n",
      "Epoch [60/100] Loss: 1586.5238037109375\n",
      "Epoch [61/100] Loss: 1580.6981201171875\n",
      "Epoch [62/100] Loss: 1574.9053955078125\n",
      "Epoch [63/100] Loss: 1569.143798828125\n",
      "Epoch [64/100] Loss: 1563.4132080078125\n",
      "Epoch [65/100] Loss: 1557.712646484375\n",
      "Epoch [66/100] Loss: 1552.04150390625\n",
      "Epoch [67/100] Loss: 1546.39892578125\n",
      "Epoch [68/100] Loss: 1540.78466796875\n",
      "Epoch [69/100] Loss: 1535.19775390625\n",
      "Epoch [70/100] Loss: 1529.6376953125\n",
      "Epoch [71/100] Loss: 1524.10400390625\n",
      "Epoch [72/100] Loss: 1518.5965576171875\n",
      "Epoch [73/100] Loss: 1513.114501953125\n",
      "Epoch [74/100] Loss: 1507.657470703125\n",
      "Epoch [75/100] Loss: 1502.224853515625\n",
      "Epoch [76/100] Loss: 1496.816650390625\n",
      "Epoch [77/100] Loss: 1491.4324951171875\n",
      "Epoch [78/100] Loss: 1486.0714111328125\n",
      "Epoch [79/100] Loss: 1480.7335205078125\n",
      "Epoch [80/100] Loss: 1475.41845703125\n",
      "Epoch [81/100] Loss: 1470.1258544921875\n",
      "Epoch [82/100] Loss: 1464.85546875\n",
      "Epoch [83/100] Loss: 1459.6072998046875\n",
      "Epoch [84/100] Loss: 1454.3802490234375\n",
      "Epoch [85/100] Loss: 1449.1744384765625\n",
      "Epoch [86/100] Loss: 1443.9901123046875\n",
      "Epoch [87/100] Loss: 1438.826171875\n",
      "Epoch [88/100] Loss: 1433.6829833984375\n",
      "Epoch [89/100] Loss: 1428.5601806640625\n",
      "Epoch [90/100] Loss: 1423.45751953125\n",
      "Epoch [91/100] Loss: 1418.374755859375\n",
      "Epoch [92/100] Loss: 1413.3115234375\n",
      "Epoch [93/100] Loss: 1408.267578125\n",
      "Epoch [94/100] Loss: 1403.2430419921875\n",
      "Epoch [95/100] Loss: 1398.237548828125\n",
      "Epoch [96/100] Loss: 1393.2510986328125\n",
      "Epoch [97/100] Loss: 1388.282958984375\n",
      "Epoch [98/100] Loss: 1383.3336181640625\n",
      "Epoch [99/100] Loss: 1378.4027099609375\n",
      "Epoch [100/100] Loss: 1373.48974609375\n",
      "Predicted days_remaining for parent_id 367: 9.951123237609863\n",
      "Training for parent_id 371...\n",
      "Epoch [1/100] Loss: 298.4806213378906\n",
      "Epoch [2/100] Loss: 290.7819519042969\n",
      "Epoch [3/100] Loss: 283.3906555175781\n",
      "Epoch [4/100] Loss: 276.2901611328125\n",
      "Epoch [5/100] Loss: 269.44964599609375\n",
      "Epoch [6/100] Loss: 262.83966064453125\n",
      "Epoch [7/100] Loss: 256.4275817871094\n",
      "Epoch [8/100] Loss: 250.18467712402344\n",
      "Epoch [9/100] Loss: 244.09519958496094\n",
      "Epoch [10/100] Loss: 238.15774536132812\n",
      "Epoch [11/100] Loss: 232.38047790527344\n",
      "Epoch [12/100] Loss: 226.77610778808594\n",
      "Epoch [13/100] Loss: 221.35842895507812\n",
      "Epoch [14/100] Loss: 216.14065551757812\n",
      "Epoch [15/100] Loss: 211.13479614257812\n",
      "Epoch [16/100] Loss: 206.3503875732422\n",
      "Epoch [17/100] Loss: 201.7936248779297\n",
      "Epoch [18/100] Loss: 197.46661376953125\n",
      "Epoch [19/100] Loss: 193.36727905273438\n",
      "Epoch [20/100] Loss: 189.4893035888672\n",
      "Epoch [21/100] Loss: 185.82302856445312\n",
      "Epoch [22/100] Loss: 182.35604858398438\n",
      "Epoch [23/100] Loss: 179.07476806640625\n",
      "Epoch [24/100] Loss: 175.96473693847656\n",
      "Epoch [25/100] Loss: 173.01181030273438\n",
      "Epoch [26/100] Loss: 170.20245361328125\n",
      "Epoch [27/100] Loss: 167.52383422851562\n",
      "Epoch [28/100] Loss: 164.96420288085938\n",
      "Epoch [29/100] Loss: 162.51255798339844\n",
      "Epoch [30/100] Loss: 160.1587371826172\n",
      "Epoch [31/100] Loss: 157.89353942871094\n",
      "Epoch [32/100] Loss: 155.70849609375\n",
      "Epoch [33/100] Loss: 153.5958709716797\n",
      "Epoch [34/100] Loss: 151.54888916015625\n",
      "Epoch [35/100] Loss: 149.56146240234375\n",
      "Epoch [36/100] Loss: 147.62815856933594\n",
      "Epoch [37/100] Loss: 145.7444305419922\n",
      "Epoch [38/100] Loss: 143.90614318847656\n",
      "Epoch [39/100] Loss: 142.1097869873047\n",
      "Epoch [40/100] Loss: 140.35231018066406\n",
      "Epoch [41/100] Loss: 138.6309814453125\n",
      "Epoch [42/100] Loss: 136.94329833984375\n",
      "Epoch [43/100] Loss: 135.2869415283203\n",
      "Epoch [44/100] Loss: 133.6597900390625\n",
      "Epoch [45/100] Loss: 132.059814453125\n",
      "Epoch [46/100] Loss: 130.48524475097656\n",
      "Epoch [47/100] Loss: 128.9344482421875\n",
      "Epoch [48/100] Loss: 127.40614318847656\n",
      "Epoch [49/100] Loss: 125.8991470336914\n",
      "Epoch [50/100] Loss: 124.4126205444336\n",
      "Epoch [51/100] Loss: 122.94583892822266\n",
      "Epoch [52/100] Loss: 121.49823760986328\n",
      "Epoch [53/100] Loss: 120.06930541992188\n",
      "Epoch [54/100] Loss: 118.65856170654297\n",
      "Epoch [55/100] Loss: 117.26543426513672\n",
      "Epoch [56/100] Loss: 115.88941192626953\n",
      "Epoch [57/100] Loss: 114.5298843383789\n",
      "Epoch [58/100] Loss: 113.18619537353516\n",
      "Epoch [59/100] Loss: 111.85779571533203\n",
      "Epoch [60/100] Loss: 110.54419708251953\n",
      "Epoch [61/100] Loss: 109.2449951171875\n",
      "Epoch [62/100] Loss: 107.95983123779297\n",
      "Epoch [63/100] Loss: 106.68853759765625\n",
      "Epoch [64/100] Loss: 105.43097686767578\n",
      "Epoch [65/100] Loss: 104.18709564208984\n",
      "Epoch [66/100] Loss: 102.95687103271484\n",
      "Epoch [67/100] Loss: 101.74031066894531\n",
      "Epoch [68/100] Loss: 100.53746795654297\n",
      "Epoch [69/100] Loss: 99.348388671875\n",
      "Epoch [70/100] Loss: 98.17306518554688\n",
      "Epoch [71/100] Loss: 97.01155090332031\n",
      "Epoch [72/100] Loss: 95.86385345458984\n",
      "Epoch [73/100] Loss: 94.7299575805664\n",
      "Epoch [74/100] Loss: 93.6097412109375\n",
      "Epoch [75/100] Loss: 92.50323486328125\n",
      "Epoch [76/100] Loss: 91.4102783203125\n",
      "Epoch [77/100] Loss: 90.33074188232422\n",
      "Epoch [78/100] Loss: 89.26451110839844\n",
      "Epoch [79/100] Loss: 88.21145629882812\n",
      "Epoch [80/100] Loss: 87.17133331298828\n",
      "Epoch [81/100] Loss: 86.14404296875\n",
      "Epoch [82/100] Loss: 85.12936401367188\n",
      "Epoch [83/100] Loss: 84.12715148925781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100] Loss: 83.13725280761719\n",
      "Epoch [85/100] Loss: 82.15945434570312\n",
      "Epoch [86/100] Loss: 81.19357299804688\n",
      "Epoch [87/100] Loss: 80.2394790649414\n",
      "Epoch [88/100] Loss: 79.29702758789062\n",
      "Epoch [89/100] Loss: 78.3659896850586\n",
      "Epoch [90/100] Loss: 77.44628143310547\n",
      "Epoch [91/100] Loss: 76.53770446777344\n",
      "Epoch [92/100] Loss: 75.64013671875\n",
      "Epoch [93/100] Loss: 74.75336456298828\n",
      "Epoch [94/100] Loss: 73.87733459472656\n",
      "Epoch [95/100] Loss: 73.01184844970703\n",
      "Epoch [96/100] Loss: 72.1567611694336\n",
      "Epoch [97/100] Loss: 71.31201171875\n",
      "Epoch [98/100] Loss: 70.47737121582031\n",
      "Epoch [99/100] Loss: 69.65278625488281\n",
      "Epoch [100/100] Loss: 68.83815002441406\n",
      "Predicted days_remaining for parent_id 371: 9.429085731506348\n",
      "Training for parent_id 373...\n",
      "Epoch [1/100] Loss: 185.20706176757812\n",
      "Epoch [2/100] Loss: 178.7830810546875\n",
      "Epoch [3/100] Loss: 172.45533752441406\n",
      "Epoch [4/100] Loss: 166.264892578125\n",
      "Epoch [5/100] Loss: 160.25424194335938\n",
      "Epoch [6/100] Loss: 154.4536895751953\n",
      "Epoch [7/100] Loss: 148.8740997314453\n",
      "Epoch [8/100] Loss: 143.51478576660156\n",
      "Epoch [9/100] Loss: 138.37716674804688\n",
      "Epoch [10/100] Loss: 133.4694366455078\n",
      "Epoch [11/100] Loss: 128.80453491210938\n",
      "Epoch [12/100] Loss: 124.39610290527344\n",
      "Epoch [13/100] Loss: 120.25471496582031\n",
      "Epoch [14/100] Loss: 116.38520050048828\n",
      "Epoch [15/100] Loss: 112.78511047363281\n",
      "Epoch [16/100] Loss: 109.44464111328125\n",
      "Epoch [17/100] Loss: 106.34776306152344\n",
      "Epoch [18/100] Loss: 103.47463989257812\n",
      "Epoch [19/100] Loss: 100.80406188964844\n",
      "Epoch [20/100] Loss: 98.31561279296875\n",
      "Epoch [21/100] Loss: 95.99063110351562\n",
      "Epoch [22/100] Loss: 93.81270599365234\n",
      "Epoch [23/100] Loss: 91.76758575439453\n",
      "Epoch [24/100] Loss: 89.84286499023438\n",
      "Epoch [25/100] Loss: 88.02727508544922\n",
      "Epoch [26/100] Loss: 86.310546875\n",
      "Epoch [27/100] Loss: 84.68301391601562\n",
      "Epoch [28/100] Loss: 83.13552856445312\n",
      "Epoch [29/100] Loss: 81.65943145751953\n",
      "Epoch [30/100] Loss: 80.24676513671875\n",
      "Epoch [31/100] Loss: 78.8902359008789\n",
      "Epoch [32/100] Loss: 77.5833511352539\n",
      "Epoch [33/100] Loss: 76.32046508789062\n",
      "Epoch [34/100] Loss: 75.0966567993164\n",
      "Epoch [35/100] Loss: 73.90779876708984\n",
      "Epoch [36/100] Loss: 72.75037384033203\n",
      "Epoch [37/100] Loss: 71.62146759033203\n",
      "Epoch [38/100] Loss: 70.51868438720703\n",
      "Epoch [39/100] Loss: 69.43992614746094\n",
      "Epoch [40/100] Loss: 68.38349151611328\n",
      "Epoch [41/100] Loss: 67.34794616699219\n",
      "Epoch [42/100] Loss: 66.33203125\n",
      "Epoch [43/100] Loss: 65.33475494384766\n",
      "Epoch [44/100] Loss: 64.35517883300781\n",
      "Epoch [45/100] Loss: 63.39255905151367\n",
      "Epoch [46/100] Loss: 62.44622802734375\n",
      "Epoch [47/100] Loss: 61.515621185302734\n",
      "Epoch [48/100] Loss: 60.60026931762695\n",
      "Epoch [49/100] Loss: 59.699771881103516\n",
      "Epoch [50/100] Loss: 58.81380081176758\n",
      "Epoch [51/100] Loss: 57.94203186035156\n",
      "Epoch [52/100] Loss: 57.084266662597656\n",
      "Epoch [53/100] Loss: 56.24032211303711\n",
      "Epoch [54/100] Loss: 55.4100227355957\n",
      "Epoch [55/100] Loss: 54.593204498291016\n",
      "Epoch [56/100] Loss: 53.7897834777832\n",
      "Epoch [57/100] Loss: 52.99962615966797\n",
      "Epoch [58/100] Loss: 52.22257995605469\n",
      "Epoch [59/100] Loss: 51.45854949951172\n",
      "Epoch [60/100] Loss: 50.7074089050293\n",
      "Epoch [61/100] Loss: 49.96902084350586\n",
      "Epoch [62/100] Loss: 49.243221282958984\n",
      "Epoch [63/100] Loss: 48.5298957824707\n",
      "Epoch [64/100] Loss: 47.828887939453125\n",
      "Epoch [65/100] Loss: 47.14000701904297\n",
      "Epoch [66/100] Loss: 46.46310806274414\n",
      "Epoch [67/100] Loss: 45.79805374145508\n",
      "Epoch [68/100] Loss: 45.1446647644043\n",
      "Epoch [69/100] Loss: 44.50275421142578\n",
      "Epoch [70/100] Loss: 43.87220001220703\n",
      "Epoch [71/100] Loss: 43.252803802490234\n",
      "Epoch [72/100] Loss: 42.64444351196289\n",
      "Epoch [73/100] Loss: 42.04692077636719\n",
      "Epoch [74/100] Loss: 41.4600944519043\n",
      "Epoch [75/100] Loss: 40.883819580078125\n",
      "Epoch [76/100] Loss: 40.31792449951172\n",
      "Epoch [77/100] Loss: 39.762245178222656\n",
      "Epoch [78/100] Loss: 39.2166748046875\n",
      "Epoch [79/100] Loss: 38.68101501464844\n",
      "Epoch [80/100] Loss: 38.1551628112793\n",
      "Epoch [81/100] Loss: 37.6389274597168\n",
      "Epoch [82/100] Loss: 37.13219451904297\n",
      "Epoch [83/100] Loss: 36.63481903076172\n",
      "Epoch [84/100] Loss: 36.14664840698242\n",
      "Epoch [85/100] Loss: 35.66756820678711\n",
      "Epoch [86/100] Loss: 35.19741439819336\n",
      "Epoch [87/100] Loss: 34.73606491088867\n",
      "Epoch [88/100] Loss: 34.28339767456055\n",
      "Epoch [89/100] Loss: 33.83925247192383\n",
      "Epoch [90/100] Loss: 33.40352249145508\n",
      "Epoch [91/100] Loss: 32.97608184814453\n",
      "Epoch [92/100] Loss: 32.55678176879883\n",
      "Epoch [93/100] Loss: 32.145511627197266\n",
      "Epoch [94/100] Loss: 31.74214744567871\n",
      "Epoch [95/100] Loss: 31.34656524658203\n",
      "Epoch [96/100] Loss: 30.958635330200195\n",
      "Epoch [97/100] Loss: 30.57827377319336\n",
      "Epoch [98/100] Loss: 30.20531463623047\n",
      "Epoch [99/100] Loss: 29.83966827392578\n",
      "Epoch [100/100] Loss: 29.481220245361328\n",
      "Predicted days_remaining for parent_id 373: 8.916940689086914\n",
      "Training for parent_id 376...\n",
      "Epoch [1/100] Loss: 770.88037109375\n",
      "Epoch [2/100] Loss: 758.5783081054688\n",
      "Epoch [3/100] Loss: 746.634765625\n",
      "Epoch [4/100] Loss: 735.0687255859375\n",
      "Epoch [5/100] Loss: 723.8355712890625\n",
      "Epoch [6/100] Loss: 712.904541015625\n",
      "Epoch [7/100] Loss: 702.2772827148438\n",
      "Epoch [8/100] Loss: 691.9724731445312\n",
      "Epoch [9/100] Loss: 682.0108032226562\n",
      "Epoch [10/100] Loss: 672.404541015625\n",
      "Epoch [11/100] Loss: 663.1551513671875\n",
      "Epoch [12/100] Loss: 654.2562866210938\n",
      "Epoch [13/100] Loss: 645.697021484375\n",
      "Epoch [14/100] Loss: 637.46435546875\n",
      "Epoch [15/100] Loss: 629.5459594726562\n",
      "Epoch [16/100] Loss: 621.9307250976562\n",
      "Epoch [17/100] Loss: 614.6090698242188\n",
      "Epoch [18/100] Loss: 607.5733642578125\n",
      "Epoch [19/100] Loss: 600.8162231445312\n",
      "Epoch [20/100] Loss: 594.32958984375\n",
      "Epoch [21/100] Loss: 588.1041870117188\n",
      "Epoch [22/100] Loss: 582.1290283203125\n",
      "Epoch [23/100] Loss: 576.39111328125\n",
      "Epoch [24/100] Loss: 570.8760986328125\n",
      "Epoch [25/100] Loss: 565.568603515625\n",
      "Epoch [26/100] Loss: 560.4530029296875\n",
      "Epoch [27/100] Loss: 555.513427734375\n",
      "Epoch [28/100] Loss: 550.735107421875\n",
      "Epoch [29/100] Loss: 546.103515625\n",
      "Epoch [30/100] Loss: 541.6055297851562\n",
      "Epoch [31/100] Loss: 537.2288818359375\n",
      "Epoch [32/100] Loss: 532.962646484375\n",
      "Epoch [33/100] Loss: 528.7966918945312\n",
      "Epoch [34/100] Loss: 524.7223510742188\n",
      "Epoch [35/100] Loss: 520.7316284179688\n",
      "Epoch [36/100] Loss: 516.8175048828125\n",
      "Epoch [37/100] Loss: 512.9739990234375\n",
      "Epoch [38/100] Loss: 509.1957702636719\n",
      "Epoch [39/100] Loss: 505.4781494140625\n",
      "Epoch [40/100] Loss: 501.8171691894531\n",
      "Epoch [41/100] Loss: 498.2092590332031\n",
      "Epoch [42/100] Loss: 494.6513671875\n",
      "Epoch [43/100] Loss: 491.1404724121094\n",
      "Epoch [44/100] Loss: 487.674072265625\n",
      "Epoch [45/100] Loss: 484.24993896484375\n",
      "Epoch [46/100] Loss: 480.8657531738281\n",
      "Epoch [47/100] Loss: 477.5196838378906\n",
      "Epoch [48/100] Loss: 474.2097473144531\n",
      "Epoch [49/100] Loss: 470.9344787597656\n",
      "Epoch [50/100] Loss: 467.6923828125\n",
      "Epoch [51/100] Loss: 464.4820861816406\n",
      "Epoch [52/100] Loss: 461.3023986816406\n",
      "Epoch [53/100] Loss: 458.1521301269531\n",
      "Epoch [54/100] Loss: 455.0304870605469\n",
      "Epoch [55/100] Loss: 451.9364929199219\n",
      "Epoch [56/100] Loss: 448.8692321777344\n",
      "Epoch [57/100] Loss: 445.828125\n",
      "Epoch [58/100] Loss: 442.8124084472656\n",
      "Epoch [59/100] Loss: 439.8214416503906\n",
      "Epoch [60/100] Loss: 436.85467529296875\n",
      "Epoch [61/100] Loss: 433.9115905761719\n",
      "Epoch [62/100] Loss: 430.9917297363281\n",
      "Epoch [63/100] Loss: 428.09454345703125\n",
      "Epoch [64/100] Loss: 425.2195739746094\n",
      "Epoch [65/100] Loss: 422.3665771484375\n",
      "Epoch [66/100] Loss: 419.53497314453125\n",
      "Epoch [67/100] Loss: 416.7244873046875\n",
      "Epoch [68/100] Loss: 413.9347839355469\n",
      "Epoch [69/100] Loss: 411.16552734375\n",
      "Epoch [70/100] Loss: 408.41632080078125\n",
      "Epoch [71/100] Loss: 405.6868896484375\n",
      "Epoch [72/100] Loss: 402.97705078125\n",
      "Epoch [73/100] Loss: 400.2864990234375\n",
      "Epoch [74/100] Loss: 397.6147766113281\n",
      "Epoch [75/100] Loss: 394.9619140625\n",
      "Epoch [76/100] Loss: 392.32733154296875\n",
      "Epoch [77/100] Loss: 389.7110290527344\n",
      "Epoch [78/100] Loss: 387.11273193359375\n",
      "Epoch [79/100] Loss: 384.5321044921875\n",
      "Epoch [80/100] Loss: 381.96905517578125\n",
      "Epoch [81/100] Loss: 379.42333984375\n",
      "Epoch [82/100] Loss: 376.8946838378906\n",
      "Epoch [83/100] Loss: 374.383056640625\n",
      "Epoch [84/100] Loss: 371.8880310058594\n",
      "Epoch [85/100] Loss: 369.40960693359375\n",
      "Epoch [86/100] Loss: 366.947509765625\n",
      "Epoch [87/100] Loss: 364.5015869140625\n",
      "Epoch [88/100] Loss: 362.07171630859375\n",
      "Epoch [89/100] Loss: 359.65765380859375\n",
      "Epoch [90/100] Loss: 357.25921630859375\n",
      "Epoch [91/100] Loss: 354.8764343261719\n",
      "Epoch [92/100] Loss: 352.5088806152344\n",
      "Epoch [93/100] Loss: 350.1566467285156\n",
      "Epoch [94/100] Loss: 347.8194885253906\n",
      "Epoch [95/100] Loss: 345.4971923828125\n",
      "Epoch [96/100] Loss: 343.18963623046875\n",
      "Epoch [97/100] Loss: 340.8968505859375\n",
      "Epoch [98/100] Loss: 338.6185302734375\n",
      "Epoch [99/100] Loss: 336.3546142578125\n",
      "Epoch [100/100] Loss: 334.1050109863281\n",
      "Predicted days_remaining for parent_id 376: 9.933378219604492\n",
      "Training for parent_id 377...\n",
      "Epoch [1/100] Loss: 110.35912322998047\n",
      "Epoch [2/100] Loss: 106.19380950927734\n",
      "Epoch [3/100] Loss: 102.0809097290039\n",
      "Epoch [4/100] Loss: 98.04156494140625\n",
      "Epoch [5/100] Loss: 94.0965805053711\n",
      "Epoch [6/100] Loss: 90.26087951660156\n",
      "Epoch [7/100] Loss: 86.54418182373047\n",
      "Epoch [8/100] Loss: 82.95509338378906\n",
      "Epoch [9/100] Loss: 79.50337219238281\n",
      "Epoch [10/100] Loss: 76.19966125488281\n",
      "Epoch [11/100] Loss: 73.05354309082031\n",
      "Epoch [12/100] Loss: 70.07169342041016\n",
      "Epoch [13/100] Loss: 67.25687408447266\n",
      "Epoch [14/100] Loss: 64.60807800292969\n",
      "Epoch [15/100] Loss: 62.121315002441406\n",
      "Epoch [16/100] Loss: 59.79054260253906\n",
      "Epoch [17/100] Loss: 57.608551025390625\n",
      "Epoch [18/100] Loss: 55.56767272949219\n",
      "Epoch [19/100] Loss: 53.66004943847656\n",
      "Epoch [20/100] Loss: 51.87776184082031\n",
      "Epoch [21/100] Loss: 50.21283721923828\n",
      "Epoch [22/100] Loss: 48.657257080078125\n",
      "Epoch [23/100] Loss: 47.202972412109375\n",
      "Epoch [24/100] Loss: 45.84203338623047\n",
      "Epoch [25/100] Loss: 44.56660079956055\n",
      "Epoch [26/100] Loss: 43.36919403076172\n",
      "Epoch [27/100] Loss: 42.24266052246094\n",
      "Epoch [28/100] Loss: 41.18034362792969\n",
      "Epoch [29/100] Loss: 40.17609786987305\n",
      "Epoch [30/100] Loss: 39.22438049316406\n",
      "Epoch [31/100] Loss: 38.320167541503906\n",
      "Epoch [32/100] Loss: 37.45900344848633\n",
      "Epoch [33/100] Loss: 36.63697052001953\n",
      "Epoch [34/100] Loss: 35.850616455078125\n",
      "Epoch [35/100] Loss: 35.096954345703125\n",
      "Epoch [36/100] Loss: 34.37338638305664\n",
      "Epoch [37/100] Loss: 33.67764663696289\n",
      "Epoch [38/100] Loss: 33.007774353027344\n",
      "Epoch [39/100] Loss: 32.36205291748047\n",
      "Epoch [40/100] Loss: 31.739002227783203\n",
      "Epoch [41/100] Loss: 31.137325286865234\n",
      "Epoch [42/100] Loss: 30.555875778198242\n",
      "Epoch [43/100] Loss: 29.99363136291504\n",
      "Epoch [44/100] Loss: 29.44969940185547\n",
      "Epoch [45/100] Loss: 28.923274993896484\n",
      "Epoch [46/100] Loss: 28.41362762451172\n",
      "Epoch [47/100] Loss: 27.92011070251465\n",
      "Epoch [48/100] Loss: 27.442121505737305\n",
      "Epoch [49/100] Loss: 26.979089736938477\n",
      "Epoch [50/100] Loss: 26.530508041381836\n",
      "Epoch [51/100] Loss: 26.095901489257812\n",
      "Epoch [52/100] Loss: 25.6748046875\n",
      "Epoch [53/100] Loss: 25.266809463500977\n",
      "Epoch [54/100] Loss: 24.87149429321289\n",
      "Epoch [55/100] Loss: 24.488479614257812\n",
      "Epoch [56/100] Loss: 24.11739730834961\n",
      "Epoch [57/100] Loss: 23.757890701293945\n",
      "Epoch [58/100] Loss: 23.409626007080078\n",
      "Epoch [59/100] Loss: 23.072269439697266\n",
      "Epoch [60/100] Loss: 22.745521545410156\n",
      "Epoch [61/100] Loss: 22.429080963134766\n",
      "Epoch [62/100] Loss: 22.122665405273438\n",
      "Epoch [63/100] Loss: 21.82600975036621\n",
      "Epoch [64/100] Loss: 21.538833618164062\n",
      "Epoch [65/100] Loss: 21.260902404785156\n",
      "Epoch [66/100] Loss: 20.991973876953125\n",
      "Epoch [67/100] Loss: 20.731813430786133\n",
      "Epoch [68/100] Loss: 20.480201721191406\n",
      "Epoch [69/100] Loss: 20.23692512512207\n",
      "Epoch [70/100] Loss: 20.00177574157715\n",
      "Epoch [71/100] Loss: 19.774547576904297\n",
      "Epoch [72/100] Loss: 19.55504608154297\n",
      "Epoch [73/100] Loss: 19.343076705932617\n",
      "Epoch [74/100] Loss: 19.13844871520996\n",
      "Epoch [75/100] Loss: 18.940975189208984\n",
      "Epoch [76/100] Loss: 18.750465393066406\n",
      "Epoch [77/100] Loss: 18.566743850708008\n",
      "Epoch [78/100] Loss: 18.389623641967773\n",
      "Epoch [79/100] Loss: 18.218929290771484\n",
      "Epoch [80/100] Loss: 18.054485321044922\n",
      "Epoch [81/100] Loss: 17.896114349365234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100] Loss: 17.743648529052734\n",
      "Epoch [83/100] Loss: 17.596912384033203\n",
      "Epoch [84/100] Loss: 17.455739974975586\n",
      "Epoch [85/100] Loss: 17.319965362548828\n",
      "Epoch [86/100] Loss: 17.189428329467773\n",
      "Epoch [87/100] Loss: 17.063968658447266\n",
      "Epoch [88/100] Loss: 16.943431854248047\n",
      "Epoch [89/100] Loss: 16.827665328979492\n",
      "Epoch [90/100] Loss: 16.716506958007812\n",
      "Epoch [91/100] Loss: 16.609817504882812\n",
      "Epoch [92/100] Loss: 16.507457733154297\n",
      "Epoch [93/100] Loss: 16.409273147583008\n",
      "Epoch [94/100] Loss: 16.315134048461914\n",
      "Epoch [95/100] Loss: 16.22490119934082\n",
      "Epoch [96/100] Loss: 16.13844108581543\n",
      "Epoch [97/100] Loss: 16.05562973022461\n",
      "Epoch [98/100] Loss: 15.976330757141113\n",
      "Epoch [99/100] Loss: 15.900432586669922\n",
      "Epoch [100/100] Loss: 15.827808380126953\n",
      "Predicted days_remaining for parent_id 377: 8.600720405578613\n",
      "Training for parent_id 380...\n",
      "Epoch [1/100] Loss: 106.14281463623047\n",
      "Epoch [2/100] Loss: 102.22394561767578\n",
      "Epoch [3/100] Loss: 98.44760131835938\n",
      "Epoch [4/100] Loss: 94.80664825439453\n",
      "Epoch [5/100] Loss: 91.29539489746094\n",
      "Epoch [6/100] Loss: 87.9079818725586\n",
      "Epoch [7/100] Loss: 84.63905334472656\n",
      "Epoch [8/100] Loss: 81.48523712158203\n",
      "Epoch [9/100] Loss: 78.4467544555664\n",
      "Epoch [10/100] Loss: 75.52685546875\n",
      "Epoch [11/100] Loss: 72.72989654541016\n",
      "Epoch [12/100] Loss: 70.05960083007812\n",
      "Epoch [13/100] Loss: 67.51788330078125\n",
      "Epoch [14/100] Loss: 65.10491943359375\n",
      "Epoch [15/100] Loss: 62.819393157958984\n",
      "Epoch [16/100] Loss: 60.65899658203125\n",
      "Epoch [17/100] Loss: 58.62077713012695\n",
      "Epoch [18/100] Loss: 56.70136642456055\n",
      "Epoch [19/100] Loss: 54.89700698852539\n",
      "Epoch [20/100] Loss: 53.20356369018555\n",
      "Epoch [21/100] Loss: 51.61628723144531\n",
      "Epoch [22/100] Loss: 50.12979507446289\n",
      "Epoch [23/100] Loss: 48.738094329833984\n",
      "Epoch [24/100] Loss: 47.43458557128906\n",
      "Epoch [25/100] Loss: 46.21234130859375\n",
      "Epoch [26/100] Loss: 45.064239501953125\n",
      "Epoch [27/100] Loss: 43.98324203491211\n",
      "Epoch [28/100] Loss: 42.96262741088867\n",
      "Epoch [29/100] Loss: 41.9961051940918\n",
      "Epoch [30/100] Loss: 41.077964782714844\n",
      "Epoch [31/100] Loss: 40.20310592651367\n",
      "Epoch [32/100] Loss: 39.3670539855957\n",
      "Epoch [33/100] Loss: 38.56592559814453\n",
      "Epoch [34/100] Loss: 37.79637145996094\n",
      "Epoch [35/100] Loss: 37.05554962158203\n",
      "Epoch [36/100] Loss: 36.34104919433594\n",
      "Epoch [37/100] Loss: 35.65081024169922\n",
      "Epoch [38/100] Loss: 34.983089447021484\n",
      "Epoch [39/100] Loss: 34.33641815185547\n",
      "Epoch [40/100] Loss: 33.709537506103516\n",
      "Epoch [41/100] Loss: 33.10137176513672\n",
      "Epoch [42/100] Loss: 32.51099395751953\n",
      "Epoch [43/100] Loss: 31.93758773803711\n",
      "Epoch [44/100] Loss: 31.380477905273438\n",
      "Epoch [45/100] Loss: 30.839021682739258\n",
      "Epoch [46/100] Loss: 30.31268310546875\n",
      "Epoch [47/100] Loss: 29.80095672607422\n",
      "Epoch [48/100] Loss: 29.303396224975586\n",
      "Epoch [49/100] Loss: 28.81960105895996\n",
      "Epoch [50/100] Loss: 28.34917640686035\n",
      "Epoch [51/100] Loss: 27.891759872436523\n",
      "Epoch [52/100] Loss: 27.44704246520996\n",
      "Epoch [53/100] Loss: 27.014686584472656\n",
      "Epoch [54/100] Loss: 26.59440040588379\n",
      "Epoch [55/100] Loss: 26.185895919799805\n",
      "Epoch [56/100] Loss: 25.788896560668945\n",
      "Epoch [57/100] Loss: 25.403133392333984\n",
      "Epoch [58/100] Loss: 25.028339385986328\n",
      "Epoch [59/100] Loss: 24.66427993774414\n",
      "Epoch [60/100] Loss: 24.310699462890625\n",
      "Epoch [61/100] Loss: 23.967357635498047\n",
      "Epoch [62/100] Loss: 23.634017944335938\n",
      "Epoch [63/100] Loss: 23.310461044311523\n",
      "Epoch [64/100] Loss: 22.9964542388916\n",
      "Epoch [65/100] Loss: 22.691776275634766\n",
      "Epoch [66/100] Loss: 22.396209716796875\n",
      "Epoch [67/100] Loss: 22.109548568725586\n",
      "Epoch [68/100] Loss: 21.83157730102539\n",
      "Epoch [69/100] Loss: 21.562091827392578\n",
      "Epoch [70/100] Loss: 21.30088233947754\n",
      "Epoch [71/100] Loss: 21.047761917114258\n",
      "Epoch [72/100] Loss: 20.802536010742188\n",
      "Epoch [73/100] Loss: 20.564998626708984\n",
      "Epoch [74/100] Loss: 20.33496856689453\n",
      "Epoch [75/100] Loss: 20.11225700378418\n",
      "Epoch [76/100] Loss: 19.896686553955078\n",
      "Epoch [77/100] Loss: 19.688068389892578\n",
      "Epoch [78/100] Loss: 19.486228942871094\n",
      "Epoch [79/100] Loss: 19.290992736816406\n",
      "Epoch [80/100] Loss: 19.102190017700195\n",
      "Epoch [81/100] Loss: 18.919654846191406\n",
      "Epoch [82/100] Loss: 18.743213653564453\n",
      "Epoch [83/100] Loss: 18.57271385192871\n",
      "Epoch [84/100] Loss: 18.407991409301758\n",
      "Epoch [85/100] Loss: 18.248889923095703\n",
      "Epoch [86/100] Loss: 18.09525489807129\n",
      "Epoch [87/100] Loss: 17.946937561035156\n",
      "Epoch [88/100] Loss: 17.803789138793945\n",
      "Epoch [89/100] Loss: 17.665664672851562\n",
      "Epoch [90/100] Loss: 17.532421112060547\n",
      "Epoch [91/100] Loss: 17.403919219970703\n",
      "Epoch [92/100] Loss: 17.2800235748291\n",
      "Epoch [93/100] Loss: 17.160598754882812\n",
      "Epoch [94/100] Loss: 17.045516967773438\n",
      "Epoch [95/100] Loss: 16.934646606445312\n",
      "Epoch [96/100] Loss: 16.827861785888672\n",
      "Epoch [97/100] Loss: 16.725040435791016\n",
      "Epoch [98/100] Loss: 16.626056671142578\n",
      "Epoch [99/100] Loss: 16.530807495117188\n",
      "Epoch [100/100] Loss: 16.439163208007812\n",
      "Predicted days_remaining for parent_id 380: 8.366703033447266\n",
      "Training for parent_id 387...\n",
      "Epoch [1/100] Loss: 405.8510437011719\n",
      "Epoch [2/100] Loss: 395.14532470703125\n",
      "Epoch [3/100] Loss: 384.9083557128906\n",
      "Epoch [4/100] Loss: 375.2206115722656\n",
      "Epoch [5/100] Loss: 366.1083068847656\n",
      "Epoch [6/100] Loss: 357.55694580078125\n",
      "Epoch [7/100] Loss: 349.5278625488281\n",
      "Epoch [8/100] Loss: 341.9720458984375\n",
      "Epoch [9/100] Loss: 334.8392028808594\n",
      "Epoch [10/100] Loss: 328.0841979980469\n",
      "Epoch [11/100] Loss: 321.6690368652344\n",
      "Epoch [12/100] Loss: 315.5631408691406\n",
      "Epoch [13/100] Loss: 309.7421875\n",
      "Epoch [14/100] Loss: 304.18701171875\n",
      "Epoch [15/100] Loss: 298.8819580078125\n",
      "Epoch [16/100] Loss: 293.81341552734375\n",
      "Epoch [17/100] Loss: 288.9693908691406\n",
      "Epoch [18/100] Loss: 284.3385925292969\n",
      "Epoch [19/100] Loss: 279.9101257324219\n",
      "Epoch [20/100] Loss: 275.6729736328125\n",
      "Epoch [21/100] Loss: 271.615478515625\n",
      "Epoch [22/100] Loss: 267.7257995605469\n",
      "Epoch [23/100] Loss: 263.991943359375\n",
      "Epoch [24/100] Loss: 260.4024658203125\n",
      "Epoch [25/100] Loss: 256.9466857910156\n",
      "Epoch [26/100] Loss: 253.61476135253906\n",
      "Epoch [27/100] Loss: 250.39776611328125\n",
      "Epoch [28/100] Loss: 247.28729248046875\n",
      "Epoch [29/100] Loss: 244.27552795410156\n",
      "Epoch [30/100] Loss: 241.35494995117188\n",
      "Epoch [31/100] Loss: 238.51803588867188\n",
      "Epoch [32/100] Loss: 235.7576141357422\n",
      "Epoch [33/100] Loss: 233.06631469726562\n",
      "Epoch [34/100] Loss: 230.43731689453125\n",
      "Epoch [35/100] Loss: 227.86410522460938\n",
      "Epoch [36/100] Loss: 225.34080505371094\n",
      "Epoch [37/100] Loss: 222.8621826171875\n",
      "Epoch [38/100] Loss: 220.42381286621094\n",
      "Epoch [39/100] Loss: 218.02197265625\n",
      "Epoch [40/100] Loss: 215.65354919433594\n",
      "Epoch [41/100] Loss: 213.31614685058594\n",
      "Epoch [42/100] Loss: 211.0076446533203\n",
      "Epoch [43/100] Loss: 208.72650146484375\n",
      "Epoch [44/100] Loss: 206.47137451171875\n",
      "Epoch [45/100] Loss: 204.24142456054688\n",
      "Epoch [46/100] Loss: 202.03590393066406\n",
      "Epoch [47/100] Loss: 199.85427856445312\n",
      "Epoch [48/100] Loss: 197.6963348388672\n",
      "Epoch [49/100] Loss: 195.56185913085938\n",
      "Epoch [50/100] Loss: 193.4508514404297\n",
      "Epoch [51/100] Loss: 191.3633575439453\n",
      "Epoch [52/100] Loss: 189.29933166503906\n",
      "Epoch [53/100] Loss: 187.2589111328125\n",
      "Epoch [54/100] Loss: 185.24203491210938\n",
      "Epoch [55/100] Loss: 183.2486114501953\n",
      "Epoch [56/100] Loss: 181.27853393554688\n",
      "Epoch [57/100] Loss: 179.3316650390625\n",
      "Epoch [58/100] Loss: 177.40769958496094\n",
      "Epoch [59/100] Loss: 175.50643920898438\n",
      "Epoch [60/100] Loss: 173.6274871826172\n",
      "Epoch [61/100] Loss: 171.77069091796875\n",
      "Epoch [62/100] Loss: 169.93556213378906\n",
      "Epoch [63/100] Loss: 168.12191772460938\n",
      "Epoch [64/100] Loss: 166.32933044433594\n",
      "Epoch [65/100] Loss: 164.5575714111328\n",
      "Epoch [66/100] Loss: 162.8062744140625\n",
      "Epoch [67/100] Loss: 161.0751495361328\n",
      "Epoch [68/100] Loss: 159.3638916015625\n",
      "Epoch [69/100] Loss: 157.67221069335938\n",
      "Epoch [70/100] Loss: 155.99984741210938\n",
      "Epoch [71/100] Loss: 154.34652709960938\n",
      "Epoch [72/100] Loss: 152.71197509765625\n",
      "Epoch [73/100] Loss: 151.09588623046875\n",
      "Epoch [74/100] Loss: 149.49803161621094\n",
      "Epoch [75/100] Loss: 147.91818237304688\n",
      "Epoch [76/100] Loss: 146.3560333251953\n",
      "Epoch [77/100] Loss: 144.81138610839844\n",
      "Epoch [78/100] Loss: 143.2839813232422\n",
      "Epoch [79/100] Loss: 141.7735595703125\n",
      "Epoch [80/100] Loss: 140.2799530029297\n",
      "Epoch [81/100] Loss: 138.8029022216797\n",
      "Epoch [82/100] Loss: 137.34210205078125\n",
      "Epoch [83/100] Loss: 135.89747619628906\n",
      "Epoch [84/100] Loss: 134.46876525878906\n",
      "Epoch [85/100] Loss: 133.05569458007812\n",
      "Epoch [86/100] Loss: 131.6581268310547\n",
      "Epoch [87/100] Loss: 130.27587890625\n",
      "Epoch [88/100] Loss: 128.90866088867188\n",
      "Epoch [89/100] Loss: 127.55632019042969\n",
      "Epoch [90/100] Loss: 126.21874237060547\n",
      "Epoch [91/100] Loss: 124.89566040039062\n",
      "Epoch [92/100] Loss: 123.58689880371094\n",
      "Epoch [93/100] Loss: 122.29232788085938\n",
      "Epoch [94/100] Loss: 121.01168823242188\n",
      "Epoch [95/100] Loss: 119.74491882324219\n",
      "Epoch [96/100] Loss: 118.49176788330078\n",
      "Epoch [97/100] Loss: 117.2520751953125\n",
      "Epoch [98/100] Loss: 116.02569580078125\n",
      "Epoch [99/100] Loss: 114.81253051757812\n",
      "Epoch [100/100] Loss: 113.61238098144531\n",
      "Predicted days_remaining for parent_id 387: 9.85113525390625\n",
      "Training for parent_id 391...\n",
      "Epoch [1/100] Loss: 445.5580139160156\n",
      "Epoch [2/100] Loss: 436.0566711425781\n",
      "Epoch [3/100] Loss: 426.8977966308594\n",
      "Epoch [4/100] Loss: 418.0626220703125\n",
      "Epoch [5/100] Loss: 409.5443115234375\n",
      "Epoch [6/100] Loss: 401.33709716796875\n",
      "Epoch [7/100] Loss: 393.4287109375\n",
      "Epoch [8/100] Loss: 385.8010559082031\n",
      "Epoch [9/100] Loss: 378.4364318847656\n",
      "Epoch [10/100] Loss: 371.3201599121094\n",
      "Epoch [11/100] Loss: 364.4421081542969\n",
      "Epoch [12/100] Loss: 357.79571533203125\n",
      "Epoch [13/100] Loss: 351.3778076171875\n",
      "Epoch [14/100] Loss: 345.18707275390625\n",
      "Epoch [15/100] Loss: 339.22308349609375\n",
      "Epoch [16/100] Loss: 333.4848327636719\n",
      "Epoch [17/100] Loss: 327.97052001953125\n",
      "Epoch [18/100] Loss: 322.67694091796875\n",
      "Epoch [19/100] Loss: 317.5991516113281\n",
      "Epoch [20/100] Loss: 312.7310485839844\n",
      "Epoch [21/100] Loss: 308.0649719238281\n",
      "Epoch [22/100] Loss: 303.59197998046875\n",
      "Epoch [23/100] Loss: 299.3021545410156\n",
      "Epoch [24/100] Loss: 295.1846618652344\n",
      "Epoch [25/100] Loss: 291.2279357910156\n",
      "Epoch [26/100] Loss: 287.4198303222656\n",
      "Epoch [27/100] Loss: 283.7481689453125\n",
      "Epoch [28/100] Loss: 280.20111083984375\n",
      "Epoch [29/100] Loss: 276.76715087890625\n",
      "Epoch [30/100] Loss: 273.43609619140625\n",
      "Epoch [31/100] Loss: 270.19903564453125\n",
      "Epoch [32/100] Loss: 267.04815673828125\n",
      "Epoch [33/100] Loss: 263.9771728515625\n",
      "Epoch [34/100] Loss: 260.98077392578125\n",
      "Epoch [35/100] Loss: 258.0542297363281\n",
      "Epoch [36/100] Loss: 255.1931915283203\n",
      "Epoch [37/100] Loss: 252.3937530517578\n",
      "Epoch [38/100] Loss: 249.65187072753906\n",
      "Epoch [39/100] Loss: 246.96363830566406\n",
      "Epoch [40/100] Loss: 244.32533264160156\n",
      "Epoch [41/100] Loss: 241.7333984375\n",
      "Epoch [42/100] Loss: 239.1845245361328\n",
      "Epoch [43/100] Loss: 236.6759033203125\n",
      "Epoch [44/100] Loss: 234.20509338378906\n",
      "Epoch [45/100] Loss: 231.77008056640625\n",
      "Epoch [46/100] Loss: 229.36929321289062\n",
      "Epoch [47/100] Loss: 227.0013427734375\n",
      "Epoch [48/100] Loss: 224.66505432128906\n",
      "Epoch [49/100] Loss: 222.35960388183594\n",
      "Epoch [50/100] Loss: 220.0841064453125\n",
      "Epoch [51/100] Loss: 217.8377227783203\n",
      "Epoch [52/100] Loss: 215.61988830566406\n",
      "Epoch [53/100] Loss: 213.42990112304688\n",
      "Epoch [54/100] Loss: 211.26715087890625\n",
      "Epoch [55/100] Loss: 209.13092041015625\n",
      "Epoch [56/100] Loss: 207.02076721191406\n",
      "Epoch [57/100] Loss: 204.93606567382812\n",
      "Epoch [58/100] Loss: 202.87631225585938\n",
      "Epoch [59/100] Loss: 200.8408966064453\n",
      "Epoch [60/100] Loss: 198.8293914794922\n",
      "Epoch [61/100] Loss: 196.84130859375\n",
      "Epoch [62/100] Loss: 194.87611389160156\n",
      "Epoch [63/100] Loss: 192.9333953857422\n",
      "Epoch [64/100] Loss: 191.0126953125\n",
      "Epoch [65/100] Loss: 189.11366271972656\n",
      "Epoch [66/100] Loss: 187.23580932617188\n",
      "Epoch [67/100] Loss: 185.3787841796875\n",
      "Epoch [68/100] Loss: 183.54217529296875\n",
      "Epoch [69/100] Loss: 181.72572326660156\n",
      "Epoch [70/100] Loss: 179.9290313720703\n",
      "Epoch [71/100] Loss: 178.1517791748047\n",
      "Epoch [72/100] Loss: 176.39366149902344\n",
      "Epoch [73/100] Loss: 174.65435791015625\n",
      "Epoch [74/100] Loss: 172.93359375\n",
      "Epoch [75/100] Loss: 171.23109436035156\n",
      "Epoch [76/100] Loss: 169.5465087890625\n",
      "Epoch [77/100] Loss: 167.87965393066406\n",
      "Epoch [78/100] Loss: 166.23025512695312\n",
      "Epoch [79/100] Loss: 164.5980682373047\n",
      "Epoch [80/100] Loss: 162.98280334472656\n",
      "Epoch [81/100] Loss: 161.38424682617188\n",
      "Epoch [82/100] Loss: 159.8022003173828\n",
      "Epoch [83/100] Loss: 158.2363739013672\n",
      "Epoch [84/100] Loss: 156.6865997314453\n",
      "Epoch [85/100] Loss: 155.15260314941406\n",
      "Epoch [86/100] Loss: 153.6342315673828\n",
      "Epoch [87/100] Loss: 152.13121032714844\n",
      "Epoch [88/100] Loss: 150.64340209960938\n",
      "Epoch [89/100] Loss: 149.1706085205078\n",
      "Epoch [90/100] Loss: 147.7125701904297\n",
      "Epoch [91/100] Loss: 146.2691650390625\n",
      "Epoch [92/100] Loss: 144.84019470214844\n",
      "Epoch [93/100] Loss: 143.4254608154297\n",
      "Epoch [94/100] Loss: 142.02479553222656\n",
      "Epoch [95/100] Loss: 140.63803100585938\n",
      "Epoch [96/100] Loss: 139.26499938964844\n",
      "Epoch [97/100] Loss: 137.905517578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100] Loss: 136.5594482421875\n",
      "Epoch [99/100] Loss: 135.22657775878906\n",
      "Epoch [100/100] Loss: 133.90684509277344\n",
      "Predicted days_remaining for parent_id 391: 9.879739761352539\n",
      "Training for parent_id 392...\n",
      "Epoch [1/100] Loss: 131.3476104736328\n",
      "Epoch [2/100] Loss: 126.47150421142578\n",
      "Epoch [3/100] Loss: 121.75531768798828\n",
      "Epoch [4/100] Loss: 117.20198822021484\n",
      "Epoch [5/100] Loss: 112.81210327148438\n",
      "Epoch [6/100] Loss: 108.5772705078125\n",
      "Epoch [7/100] Loss: 104.48373413085938\n",
      "Epoch [8/100] Loss: 100.52092742919922\n",
      "Epoch [9/100] Loss: 96.68421173095703\n",
      "Epoch [10/100] Loss: 92.97355651855469\n",
      "Epoch [11/100] Loss: 89.39199829101562\n",
      "Epoch [12/100] Loss: 85.94498443603516\n",
      "Epoch [13/100] Loss: 82.6400375366211\n",
      "Epoch [14/100] Loss: 79.48588562011719\n",
      "Epoch [15/100] Loss: 76.49091339111328\n",
      "Epoch [16/100] Loss: 73.66163635253906\n",
      "Epoch [17/100] Loss: 71.00102233886719\n",
      "Epoch [18/100] Loss: 68.5079345703125\n",
      "Epoch [19/100] Loss: 66.17703247070312\n",
      "Epoch [20/100] Loss: 63.99970626831055\n",
      "Epoch [21/100] Loss: 61.96549606323242\n",
      "Epoch [22/100] Loss: 60.06314468383789\n",
      "Epoch [23/100] Loss: 58.28136444091797\n",
      "Epoch [24/100] Loss: 56.60935974121094\n",
      "Epoch [25/100] Loss: 55.03709030151367\n",
      "Epoch [26/100] Loss: 53.55534362792969\n",
      "Epoch [27/100] Loss: 52.15583038330078\n",
      "Epoch [28/100] Loss: 50.831085205078125\n",
      "Epoch [29/100] Loss: 49.574424743652344\n",
      "Epoch [30/100] Loss: 48.379817962646484\n",
      "Epoch [31/100] Loss: 47.241905212402344\n",
      "Epoch [32/100] Loss: 46.15583038330078\n",
      "Epoch [33/100] Loss: 45.11726379394531\n",
      "Epoch [34/100] Loss: 44.12230682373047\n",
      "Epoch [35/100] Loss: 43.16749572753906\n",
      "Epoch [36/100] Loss: 42.249752044677734\n",
      "Epoch [37/100] Loss: 41.36631393432617\n",
      "Epoch [38/100] Loss: 40.5147705078125\n",
      "Epoch [39/100] Loss: 39.692970275878906\n",
      "Epoch [40/100] Loss: 38.89900207519531\n",
      "Epoch [41/100] Loss: 38.13119888305664\n",
      "Epoch [42/100] Loss: 37.388057708740234\n",
      "Epoch [43/100] Loss: 36.668251037597656\n",
      "Epoch [44/100] Loss: 35.97059631347656\n",
      "Epoch [45/100] Loss: 35.29403305053711\n",
      "Epoch [46/100] Loss: 34.63759994506836\n",
      "Epoch [47/100] Loss: 34.000431060791016\n",
      "Epoch [48/100] Loss: 33.381752014160156\n",
      "Epoch [49/100] Loss: 32.780845642089844\n",
      "Epoch [50/100] Loss: 32.19704818725586\n",
      "Epoch [51/100] Loss: 31.629756927490234\n",
      "Epoch [52/100] Loss: 31.07844352722168\n",
      "Epoch [53/100] Loss: 30.542583465576172\n",
      "Epoch [54/100] Loss: 30.021717071533203\n",
      "Epoch [55/100] Loss: 29.51539421081543\n",
      "Epoch [56/100] Loss: 29.02322006225586\n",
      "Epoch [57/100] Loss: 28.544815063476562\n",
      "Epoch [58/100] Loss: 28.079811096191406\n",
      "Epoch [59/100] Loss: 27.627870559692383\n",
      "Epoch [60/100] Loss: 27.18866729736328\n",
      "Epoch [61/100] Loss: 26.761905670166016\n",
      "Epoch [62/100] Loss: 26.347280502319336\n",
      "Epoch [63/100] Loss: 25.944496154785156\n",
      "Epoch [64/100] Loss: 25.553287506103516\n",
      "Epoch [65/100] Loss: 25.173370361328125\n",
      "Epoch [66/100] Loss: 24.804479598999023\n",
      "Epoch [67/100] Loss: 24.446374893188477\n",
      "Epoch [68/100] Loss: 24.098783493041992\n",
      "Epoch [69/100] Loss: 23.76146697998047\n",
      "Epoch [70/100] Loss: 23.43417739868164\n",
      "Epoch [71/100] Loss: 23.116682052612305\n",
      "Epoch [72/100] Loss: 22.808738708496094\n",
      "Epoch [73/100] Loss: 22.5101261138916\n",
      "Epoch [74/100] Loss: 22.220603942871094\n",
      "Epoch [75/100] Loss: 21.93996810913086\n",
      "Epoch [76/100] Loss: 21.66798973083496\n",
      "Epoch [77/100] Loss: 21.40445327758789\n",
      "Epoch [78/100] Loss: 21.14914894104004\n",
      "Epoch [79/100] Loss: 20.901878356933594\n",
      "Epoch [80/100] Loss: 20.66242027282715\n",
      "Epoch [81/100] Loss: 20.430591583251953\n",
      "Epoch [82/100] Loss: 20.206186294555664\n",
      "Epoch [83/100] Loss: 19.989009857177734\n",
      "Epoch [84/100] Loss: 19.77887725830078\n",
      "Epoch [85/100] Loss: 19.57560920715332\n",
      "Epoch [86/100] Loss: 19.37902069091797\n",
      "Epoch [87/100] Loss: 19.18891716003418\n",
      "Epoch [88/100] Loss: 19.00514030456543\n",
      "Epoch [89/100] Loss: 18.827516555786133\n",
      "Epoch [90/100] Loss: 18.65587615966797\n",
      "Epoch [91/100] Loss: 18.49004364013672\n",
      "Epoch [92/100] Loss: 18.329870223999023\n",
      "Epoch [93/100] Loss: 18.17519187927246\n",
      "Epoch [94/100] Loss: 18.025859832763672\n",
      "Epoch [95/100] Loss: 17.881710052490234\n",
      "Epoch [96/100] Loss: 17.742599487304688\n",
      "Epoch [97/100] Loss: 17.608386993408203\n",
      "Epoch [98/100] Loss: 17.47892189025879\n",
      "Epoch [99/100] Loss: 17.354066848754883\n",
      "Epoch [100/100] Loss: 17.23369598388672\n",
      "Predicted days_remaining for parent_id 392: 9.112879753112793\n",
      "Training for parent_id 393...\n",
      "Epoch [1/100] Loss: 399.36712646484375\n",
      "Epoch [2/100] Loss: 390.9819030761719\n",
      "Epoch [3/100] Loss: 382.9211120605469\n",
      "Epoch [4/100] Loss: 375.1704406738281\n",
      "Epoch [5/100] Loss: 367.7185363769531\n",
      "Epoch [6/100] Loss: 360.551513671875\n",
      "Epoch [7/100] Loss: 353.6551513671875\n",
      "Epoch [8/100] Loss: 347.0151672363281\n",
      "Epoch [9/100] Loss: 340.6183776855469\n",
      "Epoch [10/100] Loss: 334.45428466796875\n",
      "Epoch [11/100] Loss: 328.5147705078125\n",
      "Epoch [12/100] Loss: 322.79193115234375\n",
      "Epoch [13/100] Loss: 317.2773742675781\n",
      "Epoch [14/100] Loss: 311.96112060546875\n",
      "Epoch [15/100] Loss: 306.8326416015625\n",
      "Epoch [16/100] Loss: 301.8810729980469\n",
      "Epoch [17/100] Loss: 297.0957336425781\n",
      "Epoch [18/100] Loss: 292.4664611816406\n",
      "Epoch [19/100] Loss: 287.9837341308594\n",
      "Epoch [20/100] Loss: 283.63916015625\n",
      "Epoch [21/100] Loss: 279.4253234863281\n",
      "Epoch [22/100] Loss: 275.3363037109375\n",
      "Epoch [23/100] Loss: 271.3671569824219\n",
      "Epoch [24/100] Loss: 267.5138854980469\n",
      "Epoch [25/100] Loss: 263.7730712890625\n",
      "Epoch [26/100] Loss: 260.14141845703125\n",
      "Epoch [27/100] Loss: 256.6154479980469\n",
      "Epoch [28/100] Loss: 253.19149780273438\n",
      "Epoch [29/100] Loss: 249.8653564453125\n",
      "Epoch [30/100] Loss: 246.63262939453125\n",
      "Epoch [31/100] Loss: 243.48841857910156\n",
      "Epoch [32/100] Loss: 240.42791748046875\n",
      "Epoch [33/100] Loss: 237.4461212158203\n",
      "Epoch [34/100] Loss: 234.53829956054688\n",
      "Epoch [35/100] Loss: 231.69992065429688\n",
      "Epoch [36/100] Loss: 228.92666625976562\n",
      "Epoch [37/100] Loss: 226.2147979736328\n",
      "Epoch [38/100] Loss: 223.56077575683594\n",
      "Epoch [39/100] Loss: 220.96168518066406\n",
      "Epoch [40/100] Loss: 218.4148712158203\n",
      "Epoch [41/100] Loss: 215.91802978515625\n",
      "Epoch [42/100] Loss: 213.46925354003906\n",
      "Epoch [43/100] Loss: 211.06668090820312\n",
      "Epoch [44/100] Loss: 208.7086944580078\n",
      "Epoch [45/100] Loss: 206.3936004638672\n",
      "Epoch [46/100] Loss: 204.1198272705078\n",
      "Epoch [47/100] Loss: 201.8857421875\n",
      "Epoch [48/100] Loss: 199.6898651123047\n",
      "Epoch [49/100] Loss: 197.53053283691406\n",
      "Epoch [50/100] Loss: 195.40621948242188\n",
      "Epoch [51/100] Loss: 193.31536865234375\n",
      "Epoch [52/100] Loss: 191.25657653808594\n",
      "Epoch [53/100] Loss: 189.228515625\n",
      "Epoch [54/100] Loss: 187.22982788085938\n",
      "Epoch [55/100] Loss: 185.25930786132812\n",
      "Epoch [56/100] Loss: 183.31594848632812\n",
      "Epoch [57/100] Loss: 181.39869689941406\n",
      "Epoch [58/100] Loss: 179.50657653808594\n",
      "Epoch [59/100] Loss: 177.6388397216797\n",
      "Epoch [60/100] Loss: 175.79466247558594\n",
      "Epoch [61/100] Loss: 173.9733123779297\n",
      "Epoch [62/100] Loss: 172.17420959472656\n",
      "Epoch [63/100] Loss: 170.39675903320312\n",
      "Epoch [64/100] Loss: 168.64041137695312\n",
      "Epoch [65/100] Loss: 166.90457153320312\n",
      "Epoch [66/100] Loss: 165.1888885498047\n",
      "Epoch [67/100] Loss: 163.49290466308594\n",
      "Epoch [68/100] Loss: 161.816162109375\n",
      "Epoch [69/100] Loss: 160.15829467773438\n",
      "Epoch [70/100] Loss: 158.5189971923828\n",
      "Epoch [71/100] Loss: 156.89788818359375\n",
      "Epoch [72/100] Loss: 155.29458618164062\n",
      "Epoch [73/100] Loss: 153.70883178710938\n",
      "Epoch [74/100] Loss: 152.14035034179688\n",
      "Epoch [75/100] Loss: 150.5888214111328\n",
      "Epoch [76/100] Loss: 149.05404663085938\n",
      "Epoch [77/100] Loss: 147.5356903076172\n",
      "Epoch [78/100] Loss: 146.0334930419922\n",
      "Epoch [79/100] Loss: 144.54725646972656\n",
      "Epoch [80/100] Loss: 143.07675170898438\n",
      "Epoch [81/100] Loss: 141.6217498779297\n",
      "Epoch [82/100] Loss: 140.18199157714844\n",
      "Epoch [83/100] Loss: 138.75735473632812\n",
      "Epoch [84/100] Loss: 137.34756469726562\n",
      "Epoch [85/100] Loss: 135.95242309570312\n",
      "Epoch [86/100] Loss: 134.57179260253906\n",
      "Epoch [87/100] Loss: 133.20542907714844\n",
      "Epoch [88/100] Loss: 131.8531494140625\n",
      "Epoch [89/100] Loss: 130.5148162841797\n",
      "Epoch [90/100] Loss: 129.19024658203125\n",
      "Epoch [91/100] Loss: 127.87928771972656\n",
      "Epoch [92/100] Loss: 126.58174896240234\n",
      "Epoch [93/100] Loss: 125.29749298095703\n",
      "Epoch [94/100] Loss: 124.02635955810547\n",
      "Epoch [95/100] Loss: 122.76815795898438\n",
      "Epoch [96/100] Loss: 121.52278900146484\n",
      "Epoch [97/100] Loss: 120.29010009765625\n",
      "Epoch [98/100] Loss: 119.0699234008789\n",
      "Epoch [99/100] Loss: 117.86216735839844\n",
      "Epoch [100/100] Loss: 116.66663360595703\n",
      "Predicted days_remaining for parent_id 393: 9.697847366333008\n",
      "Training for parent_id 395...\n",
      "Epoch [1/100] Loss: 111.05781555175781\n",
      "Epoch [2/100] Loss: 106.81593322753906\n",
      "Epoch [3/100] Loss: 102.78083801269531\n",
      "Epoch [4/100] Loss: 98.98834228515625\n",
      "Epoch [5/100] Loss: 95.42977142333984\n",
      "Epoch [6/100] Loss: 92.07649993896484\n",
      "Epoch [7/100] Loss: 88.90019226074219\n",
      "Epoch [8/100] Loss: 85.87911987304688\n",
      "Epoch [9/100] Loss: 82.99847412109375\n",
      "Epoch [10/100] Loss: 80.2498550415039\n",
      "Epoch [11/100] Loss: 77.62982940673828\n",
      "Epoch [12/100] Loss: 75.13772583007812\n",
      "Epoch [13/100] Loss: 72.77364349365234\n",
      "Epoch [14/100] Loss: 70.53675842285156\n",
      "Epoch [15/100] Loss: 68.42436218261719\n",
      "Epoch [16/100] Loss: 66.43143463134766\n",
      "Epoch [17/100] Loss: 64.55061340332031\n",
      "Epoch [18/100] Loss: 62.772796630859375\n",
      "Epoch [19/100] Loss: 61.08779525756836\n",
      "Epoch [20/100] Loss: 59.48512268066406\n",
      "Epoch [21/100] Loss: 57.95487594604492\n",
      "Epoch [22/100] Loss: 56.48828125\n",
      "Epoch [23/100] Loss: 55.07813262939453\n",
      "Epoch [24/100] Loss: 53.71903991699219\n",
      "Epoch [25/100] Loss: 52.407291412353516\n",
      "Epoch [26/100] Loss: 51.14070129394531\n",
      "Epoch [27/100] Loss: 49.91822052001953\n",
      "Epoch [28/100] Loss: 48.7396354675293\n",
      "Epoch [29/100] Loss: 47.605072021484375\n",
      "Epoch [30/100] Loss: 46.51465606689453\n",
      "Epoch [31/100] Loss: 45.46826934814453\n",
      "Epoch [32/100] Loss: 44.465309143066406\n",
      "Epoch [33/100] Loss: 43.504703521728516\n",
      "Epoch [34/100] Loss: 42.58484649658203\n",
      "Epoch [35/100] Loss: 41.70371627807617\n",
      "Epoch [36/100] Loss: 40.8590202331543\n",
      "Epoch [37/100] Loss: 40.048255920410156\n",
      "Epoch [38/100] Loss: 39.26896667480469\n",
      "Epoch [39/100] Loss: 38.51873016357422\n",
      "Epoch [40/100] Loss: 37.79529571533203\n",
      "Epoch [41/100] Loss: 37.09664535522461\n",
      "Epoch [42/100] Loss: 36.42095184326172\n",
      "Epoch [43/100] Loss: 35.766632080078125\n",
      "Epoch [44/100] Loss: 35.13231658935547\n",
      "Epoch [45/100] Loss: 34.516815185546875\n",
      "Epoch [46/100] Loss: 33.919097900390625\n",
      "Epoch [47/100] Loss: 33.33827209472656\n",
      "Epoch [48/100] Loss: 32.7735595703125\n",
      "Epoch [49/100] Loss: 32.224300384521484\n",
      "Epoch [50/100] Loss: 31.689868927001953\n",
      "Epoch [51/100] Loss: 31.16973304748535\n",
      "Epoch [52/100] Loss: 30.66341781616211\n",
      "Epoch [53/100] Loss: 30.170482635498047\n",
      "Epoch [54/100] Loss: 29.69053077697754\n",
      "Epoch [55/100] Loss: 29.22319221496582\n",
      "Epoch [56/100] Loss: 28.768115997314453\n",
      "Epoch [57/100] Loss: 28.32499885559082\n",
      "Epoch [58/100] Loss: 27.89353370666504\n",
      "Epoch [59/100] Loss: 27.473440170288086\n",
      "Epoch [60/100] Loss: 27.064449310302734\n",
      "Epoch [61/100] Loss: 26.66630744934082\n",
      "Epoch [62/100] Loss: 26.278764724731445\n",
      "Epoch [63/100] Loss: 25.901588439941406\n",
      "Epoch [64/100] Loss: 25.5345516204834\n",
      "Epoch [65/100] Loss: 25.177431106567383\n",
      "Epoch [66/100] Loss: 24.83001708984375\n",
      "Epoch [67/100] Loss: 24.492084503173828\n",
      "Epoch [68/100] Loss: 24.163442611694336\n",
      "Epoch [69/100] Loss: 23.84387969970703\n",
      "Epoch [70/100] Loss: 23.533205032348633\n",
      "Epoch [71/100] Loss: 23.231212615966797\n",
      "Epoch [72/100] Loss: 22.937719345092773\n",
      "Epoch [73/100] Loss: 22.65252685546875\n",
      "Epoch [74/100] Loss: 22.375450134277344\n",
      "Epoch [75/100] Loss: 22.106313705444336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100] Loss: 21.844919204711914\n",
      "Epoch [77/100] Loss: 21.591094970703125\n",
      "Epoch [78/100] Loss: 21.34465980529785\n",
      "Epoch [79/100] Loss: 21.105453491210938\n",
      "Epoch [80/100] Loss: 20.873281478881836\n",
      "Epoch [81/100] Loss: 20.647985458374023\n",
      "Epoch [82/100] Loss: 20.42940902709961\n",
      "Epoch [83/100] Loss: 20.217370986938477\n",
      "Epoch [84/100] Loss: 20.01171875\n",
      "Epoch [85/100] Loss: 19.812299728393555\n",
      "Epoch [86/100] Loss: 19.618955612182617\n",
      "Epoch [87/100] Loss: 19.431537628173828\n",
      "Epoch [88/100] Loss: 19.249897003173828\n",
      "Epoch [89/100] Loss: 19.073890686035156\n",
      "Epoch [90/100] Loss: 18.90337562561035\n",
      "Epoch [91/100] Loss: 18.73821449279785\n",
      "Epoch [92/100] Loss: 18.57827377319336\n",
      "Epoch [93/100] Loss: 18.423425674438477\n",
      "Epoch [94/100] Loss: 18.273529052734375\n",
      "Epoch [95/100] Loss: 18.128467559814453\n",
      "Epoch [96/100] Loss: 17.988107681274414\n",
      "Epoch [97/100] Loss: 17.852333068847656\n",
      "Epoch [98/100] Loss: 17.721025466918945\n",
      "Epoch [99/100] Loss: 17.59406280517578\n",
      "Epoch [100/100] Loss: 17.471332550048828\n",
      "Predicted days_remaining for parent_id 395: 8.042597770690918\n",
      "Training for parent_id 404...\n",
      "Epoch [1/100] Loss: 166.40687561035156\n",
      "Epoch [2/100] Loss: 159.4879150390625\n",
      "Epoch [3/100] Loss: 152.76930236816406\n",
      "Epoch [4/100] Loss: 146.3352508544922\n",
      "Epoch [5/100] Loss: 140.22438049316406\n",
      "Epoch [6/100] Loss: 134.45025634765625\n",
      "Epoch [7/100] Loss: 129.00851440429688\n",
      "Epoch [8/100] Loss: 123.88541412353516\n",
      "Epoch [9/100] Loss: 119.06451416015625\n",
      "Epoch [10/100] Loss: 114.52906799316406\n",
      "Epoch [11/100] Loss: 110.26271057128906\n",
      "Epoch [12/100] Loss: 106.24978637695312\n",
      "Epoch [13/100] Loss: 102.47499084472656\n",
      "Epoch [14/100] Loss: 98.92344665527344\n",
      "Epoch [15/100] Loss: 95.58039855957031\n",
      "Epoch [16/100] Loss: 92.43143463134766\n",
      "Epoch [17/100] Loss: 89.46339416503906\n",
      "Epoch [18/100] Loss: 86.66606903076172\n",
      "Epoch [19/100] Loss: 84.03239440917969\n",
      "Epoch [20/100] Loss: 81.55744934082031\n",
      "Epoch [21/100] Loss: 79.23657989501953\n",
      "Epoch [22/100] Loss: 77.06380462646484\n",
      "Epoch [23/100] Loss: 75.03130340576172\n",
      "Epoch [24/100] Loss: 73.1296615600586\n",
      "Epoch [25/100] Loss: 71.3485107421875\n",
      "Epoch [26/100] Loss: 69.67684936523438\n",
      "Epoch [27/100] Loss: 68.10372161865234\n",
      "Epoch [28/100] Loss: 66.61851501464844\n",
      "Epoch [29/100] Loss: 65.21141052246094\n",
      "Epoch [30/100] Loss: 63.87345504760742\n",
      "Epoch [31/100] Loss: 62.59679412841797\n",
      "Epoch [32/100] Loss: 61.374691009521484\n",
      "Epoch [33/100] Loss: 60.201358795166016\n",
      "Epoch [34/100] Loss: 59.07191848754883\n",
      "Epoch [35/100] Loss: 57.982322692871094\n",
      "Epoch [36/100] Loss: 56.9290885925293\n",
      "Epoch [37/100] Loss: 55.90929412841797\n",
      "Epoch [38/100] Loss: 54.92034149169922\n",
      "Epoch [39/100] Loss: 53.959964752197266\n",
      "Epoch [40/100] Loss: 53.02614212036133\n",
      "Epoch [41/100] Loss: 52.117034912109375\n",
      "Epoch [42/100] Loss: 51.231056213378906\n",
      "Epoch [43/100] Loss: 50.36680603027344\n",
      "Epoch [44/100] Loss: 49.52309036254883\n",
      "Epoch [45/100] Loss: 48.69887161254883\n",
      "Epoch [46/100] Loss: 47.89326477050781\n",
      "Epoch [47/100] Loss: 47.10554885864258\n",
      "Epoch [48/100] Loss: 46.335044860839844\n",
      "Epoch [49/100] Loss: 45.58120346069336\n",
      "Epoch [50/100] Loss: 44.8435173034668\n",
      "Epoch [51/100] Loss: 44.12154006958008\n",
      "Epoch [52/100] Loss: 43.414859771728516\n",
      "Epoch [53/100] Loss: 42.72309112548828\n",
      "Epoch [54/100] Loss: 42.045902252197266\n",
      "Epoch [55/100] Loss: 41.382972717285156\n",
      "Epoch [56/100] Loss: 40.733985900878906\n",
      "Epoch [57/100] Loss: 40.09862518310547\n",
      "Epoch [58/100] Loss: 39.47663879394531\n",
      "Epoch [59/100] Loss: 38.86774444580078\n",
      "Epoch [60/100] Loss: 38.27168273925781\n",
      "Epoch [61/100] Loss: 37.68819808959961\n",
      "Epoch [62/100] Loss: 37.117042541503906\n",
      "Epoch [63/100] Loss: 36.557987213134766\n",
      "Epoch [64/100] Loss: 36.010799407958984\n",
      "Epoch [65/100] Loss: 35.47523498535156\n",
      "Epoch [66/100] Loss: 34.95109939575195\n",
      "Epoch [67/100] Loss: 34.43815231323242\n",
      "Epoch [68/100] Loss: 33.93620300292969\n",
      "Epoch [69/100] Loss: 33.44502639770508\n",
      "Epoch [70/100] Loss: 32.96444320678711\n",
      "Epoch [71/100] Loss: 32.494239807128906\n",
      "Epoch [72/100] Loss: 32.03422927856445\n",
      "Epoch [73/100] Loss: 31.584232330322266\n",
      "Epoch [74/100] Loss: 31.144046783447266\n",
      "Epoch [75/100] Loss: 30.713502883911133\n",
      "Epoch [76/100] Loss: 30.29241943359375\n",
      "Epoch [77/100] Loss: 29.88063621520996\n",
      "Epoch [78/100] Loss: 29.477956771850586\n",
      "Epoch [79/100] Loss: 29.084239959716797\n",
      "Epoch [80/100] Loss: 28.699298858642578\n",
      "Epoch [81/100] Loss: 28.322986602783203\n",
      "Epoch [82/100] Loss: 27.955129623413086\n",
      "Epoch [83/100] Loss: 27.595596313476562\n",
      "Epoch [84/100] Loss: 27.24420166015625\n",
      "Epoch [85/100] Loss: 26.900814056396484\n",
      "Epoch [86/100] Loss: 26.56528091430664\n",
      "Epoch [87/100] Loss: 26.23746681213379\n",
      "Epoch [88/100] Loss: 25.917190551757812\n",
      "Epoch [89/100] Loss: 25.604351043701172\n",
      "Epoch [90/100] Loss: 25.298786163330078\n",
      "Epoch [91/100] Loss: 25.0003604888916\n",
      "Epoch [92/100] Loss: 24.70894432067871\n",
      "Epoch [93/100] Loss: 24.424394607543945\n",
      "Epoch [94/100] Loss: 24.146595001220703\n",
      "Epoch [95/100] Loss: 23.87540054321289\n",
      "Epoch [96/100] Loss: 23.610694885253906\n",
      "Epoch [97/100] Loss: 23.352340698242188\n",
      "Epoch [98/100] Loss: 23.100223541259766\n",
      "Epoch [99/100] Loss: 22.85421371459961\n",
      "Epoch [100/100] Loss: 22.614208221435547\n",
      "Predicted days_remaining for parent_id 404: 8.931745529174805\n",
      "Training for parent_id 406...\n",
      "Epoch [1/100] Loss: 517.4744873046875\n",
      "Epoch [2/100] Loss: 507.6572570800781\n",
      "Epoch [3/100] Loss: 498.04962158203125\n",
      "Epoch [4/100] Loss: 488.60009765625\n",
      "Epoch [5/100] Loss: 479.2917175292969\n",
      "Epoch [6/100] Loss: 470.1293029785156\n",
      "Epoch [7/100] Loss: 461.12371826171875\n",
      "Epoch [8/100] Loss: 452.2891540527344\n",
      "Epoch [9/100] Loss: 443.64111328125\n",
      "Epoch [10/100] Loss: 435.1940002441406\n",
      "Epoch [11/100] Loss: 426.9598388671875\n",
      "Epoch [12/100] Loss: 418.9485778808594\n",
      "Epoch [13/100] Loss: 411.1684265136719\n",
      "Epoch [14/100] Loss: 403.6269226074219\n",
      "Epoch [15/100] Loss: 396.33172607421875\n",
      "Epoch [16/100] Loss: 389.29083251953125\n",
      "Epoch [17/100] Loss: 382.511962890625\n",
      "Epoch [18/100] Loss: 376.0013732910156\n",
      "Epoch [19/100] Loss: 369.7628479003906\n",
      "Epoch [20/100] Loss: 363.79571533203125\n",
      "Epoch [21/100] Loss: 358.0950012207031\n",
      "Epoch [22/100] Loss: 352.65118408203125\n",
      "Epoch [23/100] Loss: 347.45184326171875\n",
      "Epoch [24/100] Loss: 342.4835510253906\n",
      "Epoch [25/100] Loss: 337.7330017089844\n",
      "Epoch [26/100] Loss: 333.1869812011719\n",
      "Epoch [27/100] Loss: 328.8327331542969\n",
      "Epoch [28/100] Loss: 324.6573486328125\n",
      "Epoch [29/100] Loss: 320.64825439453125\n",
      "Epoch [30/100] Loss: 316.7926025390625\n",
      "Epoch [31/100] Loss: 313.0780944824219\n",
      "Epoch [32/100] Loss: 309.4931945800781\n",
      "Epoch [33/100] Loss: 306.027099609375\n",
      "Epoch [34/100] Loss: 302.6703186035156\n",
      "Epoch [35/100] Loss: 299.4145812988281\n",
      "Epoch [36/100] Loss: 296.2527770996094\n",
      "Epoch [37/100] Loss: 293.1787414550781\n",
      "Epoch [38/100] Loss: 290.1871032714844\n",
      "Epoch [39/100] Loss: 287.27294921875\n",
      "Epoch [40/100] Loss: 284.431396484375\n",
      "Epoch [41/100] Loss: 281.65771484375\n",
      "Epoch [42/100] Loss: 278.94732666015625\n",
      "Epoch [43/100] Loss: 276.2955322265625\n",
      "Epoch [44/100] Loss: 273.6978759765625\n",
      "Epoch [45/100] Loss: 271.1501770019531\n",
      "Epoch [46/100] Loss: 268.6484680175781\n",
      "Epoch [47/100] Loss: 266.18914794921875\n",
      "Epoch [48/100] Loss: 263.76910400390625\n",
      "Epoch [49/100] Loss: 261.38555908203125\n",
      "Epoch [50/100] Loss: 259.0360107421875\n",
      "Epoch [51/100] Loss: 256.71826171875\n",
      "Epoch [52/100] Loss: 254.4305419921875\n",
      "Epoch [53/100] Loss: 252.17120361328125\n",
      "Epoch [54/100] Loss: 249.93885803222656\n",
      "Epoch [55/100] Loss: 247.73223876953125\n",
      "Epoch [56/100] Loss: 245.55029296875\n",
      "Epoch [57/100] Loss: 243.39212036132812\n",
      "Epoch [58/100] Loss: 241.25689697265625\n",
      "Epoch [59/100] Loss: 239.14381408691406\n",
      "Epoch [60/100] Loss: 237.0522918701172\n",
      "Epoch [61/100] Loss: 234.98167419433594\n",
      "Epoch [62/100] Loss: 232.9314727783203\n",
      "Epoch [63/100] Loss: 230.9011993408203\n",
      "Epoch [64/100] Loss: 228.89036560058594\n",
      "Epoch [65/100] Loss: 226.8986053466797\n",
      "Epoch [66/100] Loss: 224.9254608154297\n",
      "Epoch [67/100] Loss: 222.97064208984375\n",
      "Epoch [68/100] Loss: 221.03382873535156\n",
      "Epoch [69/100] Loss: 219.1146697998047\n",
      "Epoch [70/100] Loss: 217.2128448486328\n",
      "Epoch [71/100] Loss: 215.32810974121094\n",
      "Epoch [72/100] Loss: 213.460205078125\n",
      "Epoch [73/100] Loss: 211.60887145996094\n",
      "Epoch [74/100] Loss: 209.7738800048828\n",
      "Epoch [75/100] Loss: 207.95497131347656\n",
      "Epoch [76/100] Loss: 206.1519317626953\n",
      "Epoch [77/100] Loss: 204.36456298828125\n",
      "Epoch [78/100] Loss: 202.59262084960938\n",
      "Epoch [79/100] Loss: 200.83596801757812\n",
      "Epoch [80/100] Loss: 199.09442138671875\n",
      "Epoch [81/100] Loss: 197.36767578125\n",
      "Epoch [82/100] Loss: 195.65574645996094\n",
      "Epoch [83/100] Loss: 193.95831298828125\n",
      "Epoch [84/100] Loss: 192.27528381347656\n",
      "Epoch [85/100] Loss: 190.60643005371094\n",
      "Epoch [86/100] Loss: 188.95164489746094\n",
      "Epoch [87/100] Loss: 187.310791015625\n",
      "Epoch [88/100] Loss: 185.6836700439453\n",
      "Epoch [89/100] Loss: 184.0701904296875\n",
      "Epoch [90/100] Loss: 182.4701690673828\n",
      "Epoch [91/100] Loss: 180.88356018066406\n",
      "Epoch [92/100] Loss: 179.31007385253906\n",
      "Epoch [93/100] Loss: 177.74969482421875\n",
      "Epoch [94/100] Loss: 176.20225524902344\n",
      "Epoch [95/100] Loss: 174.66763305664062\n",
      "Epoch [96/100] Loss: 173.14576721191406\n",
      "Epoch [97/100] Loss: 171.63645935058594\n",
      "Epoch [98/100] Loss: 170.13966369628906\n",
      "Epoch [99/100] Loss: 168.65521240234375\n",
      "Epoch [100/100] Loss: 167.18301391601562\n",
      "Predicted days_remaining for parent_id 406: 10.45018482208252\n",
      "Training for parent_id 409...\n",
      "Epoch [1/100] Loss: 105.97648620605469\n",
      "Epoch [2/100] Loss: 102.1537094116211\n",
      "Epoch [3/100] Loss: 98.47237396240234\n",
      "Epoch [4/100] Loss: 94.93297576904297\n",
      "Epoch [5/100] Loss: 91.53536987304688\n",
      "Epoch [6/100] Loss: 88.27948760986328\n",
      "Epoch [7/100] Loss: 85.16437530517578\n",
      "Epoch [8/100] Loss: 82.18702697753906\n",
      "Epoch [9/100] Loss: 79.34261322021484\n",
      "Epoch [10/100] Loss: 76.62511444091797\n",
      "Epoch [11/100] Loss: 74.02821350097656\n",
      "Epoch [12/100] Loss: 71.5459213256836\n",
      "Epoch [13/100] Loss: 69.1729965209961\n",
      "Epoch [14/100] Loss: 66.90476989746094\n",
      "Epoch [15/100] Loss: 64.73714447021484\n",
      "Epoch [16/100] Loss: 62.6662712097168\n",
      "Epoch [17/100] Loss: 60.68842697143555\n",
      "Epoch [18/100] Loss: 58.79995346069336\n",
      "Epoch [19/100] Loss: 56.99724578857422\n",
      "Epoch [20/100] Loss: 55.27677917480469\n",
      "Epoch [21/100] Loss: 53.63520431518555\n",
      "Epoch [22/100] Loss: 52.06932830810547\n",
      "Epoch [23/100] Loss: 50.57608413696289\n",
      "Epoch [24/100] Loss: 49.15251541137695\n",
      "Epoch [25/100] Loss: 47.79570007324219\n",
      "Epoch [26/100] Loss: 46.5026969909668\n",
      "Epoch [27/100] Loss: 45.270484924316406\n",
      "Epoch [28/100] Loss: 44.09600067138672\n",
      "Epoch [29/100] Loss: 42.97610092163086\n",
      "Epoch [30/100] Loss: 41.907596588134766\n",
      "Epoch [31/100] Loss: 40.8873405456543\n",
      "Epoch [32/100] Loss: 39.91223907470703\n",
      "Epoch [33/100] Loss: 38.97928237915039\n",
      "Epoch [34/100] Loss: 38.08555603027344\n",
      "Epoch [35/100] Loss: 37.228355407714844\n",
      "Epoch [36/100] Loss: 36.405128479003906\n",
      "Epoch [37/100] Loss: 35.61351776123047\n",
      "Epoch [38/100] Loss: 34.85141372680664\n",
      "Epoch [39/100] Loss: 34.11688995361328\n",
      "Epoch [40/100] Loss: 33.40825271606445\n",
      "Epoch [41/100] Loss: 32.72399139404297\n",
      "Epoch [42/100] Loss: 32.062801361083984\n",
      "Epoch [43/100] Loss: 31.423492431640625\n",
      "Epoch [44/100] Loss: 30.805065155029297\n",
      "Epoch [45/100] Loss: 30.20659065246582\n",
      "Epoch [46/100] Loss: 29.627262115478516\n",
      "Epoch [47/100] Loss: 29.06637191772461\n",
      "Epoch [48/100] Loss: 28.523258209228516\n",
      "Epoch [49/100] Loss: 27.997337341308594\n",
      "Epoch [50/100] Loss: 27.48805809020996\n",
      "Epoch [51/100] Loss: 26.994943618774414\n",
      "Epoch [52/100] Loss: 26.51751136779785\n",
      "Epoch [53/100] Loss: 26.055360794067383\n",
      "Epoch [54/100] Loss: 25.608076095581055\n",
      "Epoch [55/100] Loss: 25.17527198791504\n",
      "Epoch [56/100] Loss: 24.756587982177734\n",
      "Epoch [57/100] Loss: 24.351669311523438\n",
      "Epoch [58/100] Loss: 23.960168838500977\n",
      "Epoch [59/100] Loss: 23.581750869750977\n",
      "Epoch [60/100] Loss: 23.216081619262695\n",
      "Epoch [61/100] Loss: 22.862838745117188\n",
      "Epoch [62/100] Loss: 22.52169418334961\n",
      "Epoch [63/100] Loss: 22.19231605529785\n",
      "Epoch [64/100] Loss: 21.874406814575195\n",
      "Epoch [65/100] Loss: 21.567636489868164\n",
      "Epoch [66/100] Loss: 21.27170753479004\n",
      "Epoch [67/100] Loss: 20.98630142211914\n",
      "Epoch [68/100] Loss: 20.711124420166016\n",
      "Epoch [69/100] Loss: 20.445877075195312\n",
      "Epoch [70/100] Loss: 20.19026756286621\n",
      "Epoch [71/100] Loss: 19.944015502929688\n",
      "Epoch [72/100] Loss: 19.706846237182617\n",
      "Epoch [73/100] Loss: 19.478473663330078\n",
      "Epoch [74/100] Loss: 19.258636474609375\n",
      "Epoch [75/100] Loss: 19.047075271606445\n",
      "Epoch [76/100] Loss: 18.843534469604492\n",
      "Epoch [77/100] Loss: 18.64775848388672\n",
      "Epoch [78/100] Loss: 18.459514617919922\n",
      "Epoch [79/100] Loss: 18.278553009033203\n",
      "Epoch [80/100] Loss: 18.104644775390625\n",
      "Epoch [81/100] Loss: 17.93756675720215\n",
      "Epoch [82/100] Loss: 17.777095794677734\n",
      "Epoch [83/100] Loss: 17.623014450073242\n",
      "Epoch [84/100] Loss: 17.475112915039062\n",
      "Epoch [85/100] Loss: 17.33318519592285\n",
      "Epoch [86/100] Loss: 17.197032928466797\n",
      "Epoch [87/100] Loss: 17.066465377807617\n",
      "Epoch [88/100] Loss: 16.94127655029297\n",
      "Epoch [89/100] Loss: 16.82130241394043\n",
      "Epoch [90/100] Loss: 16.706348419189453\n",
      "Epoch [91/100] Loss: 16.59624481201172\n",
      "Epoch [92/100] Loss: 16.49081802368164\n",
      "Epoch [93/100] Loss: 16.389902114868164\n",
      "Epoch [94/100] Loss: 16.2933349609375\n",
      "Epoch [95/100] Loss: 16.20096206665039\n",
      "Epoch [96/100] Loss: 16.112628936767578\n",
      "Epoch [97/100] Loss: 16.028186798095703\n",
      "Epoch [98/100] Loss: 15.947490692138672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100] Loss: 15.870403289794922\n",
      "Epoch [100/100] Loss: 15.796786308288574\n",
      "Predicted days_remaining for parent_id 409: 8.614656448364258\n",
      "Training for parent_id 412...\n",
      "Epoch [1/100] Loss: 213.3707733154297\n",
      "Epoch [2/100] Loss: 208.15106201171875\n",
      "Epoch [3/100] Loss: 203.02896118164062\n",
      "Epoch [4/100] Loss: 198.02186584472656\n",
      "Epoch [5/100] Loss: 193.14735412597656\n",
      "Epoch [6/100] Loss: 188.41615295410156\n",
      "Epoch [7/100] Loss: 183.8308868408203\n",
      "Epoch [8/100] Loss: 179.38864135742188\n",
      "Epoch [9/100] Loss: 175.08477783203125\n",
      "Epoch [10/100] Loss: 170.9148406982422\n",
      "Epoch [11/100] Loss: 166.8749542236328\n",
      "Epoch [12/100] Loss: 162.96205139160156\n",
      "Epoch [13/100] Loss: 159.1732940673828\n",
      "Epoch [14/100] Loss: 155.50625610351562\n",
      "Epoch [15/100] Loss: 151.9585418701172\n",
      "Epoch [16/100] Loss: 148.52752685546875\n",
      "Epoch [17/100] Loss: 145.21018981933594\n",
      "Epoch [18/100] Loss: 142.0029296875\n",
      "Epoch [19/100] Loss: 138.90155029296875\n",
      "Epoch [20/100] Loss: 135.90122985839844\n",
      "Epoch [21/100] Loss: 132.99705505371094\n",
      "Epoch [22/100] Loss: 130.1838836669922\n",
      "Epoch [23/100] Loss: 127.45716094970703\n",
      "Epoch [24/100] Loss: 124.81273651123047\n",
      "Epoch [25/100] Loss: 122.24732208251953\n",
      "Epoch [26/100] Loss: 119.75798034667969\n",
      "Epoch [27/100] Loss: 117.34239959716797\n",
      "Epoch [28/100] Loss: 114.99845886230469\n",
      "Epoch [29/100] Loss: 112.72415924072266\n",
      "Epoch [30/100] Loss: 110.51756286621094\n",
      "Epoch [31/100] Loss: 108.37664031982422\n",
      "Epoch [32/100] Loss: 106.29943084716797\n",
      "Epoch [33/100] Loss: 104.28388977050781\n",
      "Epoch [34/100] Loss: 102.32801818847656\n",
      "Epoch [35/100] Loss: 100.42986297607422\n",
      "Epoch [36/100] Loss: 98.58741760253906\n",
      "Epoch [37/100] Loss: 96.79878234863281\n",
      "Epoch [38/100] Loss: 95.06206512451172\n",
      "Epoch [39/100] Loss: 93.37531280517578\n",
      "Epoch [40/100] Loss: 91.73658752441406\n",
      "Epoch [41/100] Loss: 90.14400482177734\n",
      "Epoch [42/100] Loss: 88.59559631347656\n",
      "Epoch [43/100] Loss: 87.08941650390625\n",
      "Epoch [44/100] Loss: 85.6235580444336\n",
      "Epoch [45/100] Loss: 84.19618225097656\n",
      "Epoch [46/100] Loss: 82.80542755126953\n",
      "Epoch [47/100] Loss: 81.44950103759766\n",
      "Epoch [48/100] Loss: 80.12677764892578\n",
      "Epoch [49/100] Loss: 78.83563995361328\n",
      "Epoch [50/100] Loss: 77.5745849609375\n",
      "Epoch [51/100] Loss: 76.34223175048828\n",
      "Epoch [52/100] Loss: 75.13729858398438\n",
      "Epoch [53/100] Loss: 73.95854949951172\n",
      "Epoch [54/100] Loss: 72.80490112304688\n",
      "Epoch [55/100] Loss: 71.67530059814453\n",
      "Epoch [56/100] Loss: 70.56885528564453\n",
      "Epoch [57/100] Loss: 69.48467254638672\n",
      "Epoch [58/100] Loss: 68.42192840576172\n",
      "Epoch [59/100] Loss: 67.37991333007812\n",
      "Epoch [60/100] Loss: 66.3579330444336\n",
      "Epoch [61/100] Loss: 65.35540008544922\n",
      "Epoch [62/100] Loss: 64.37171173095703\n",
      "Epoch [63/100] Loss: 63.40632247924805\n",
      "Epoch [64/100] Loss: 62.45873260498047\n",
      "Epoch [65/100] Loss: 61.52849578857422\n",
      "Epoch [66/100] Loss: 60.61517333984375\n",
      "Epoch [67/100] Loss: 59.71836853027344\n",
      "Epoch [68/100] Loss: 58.837669372558594\n",
      "Epoch [69/100] Loss: 57.97276306152344\n",
      "Epoch [70/100] Loss: 57.12326431274414\n",
      "Epoch [71/100] Loss: 56.288875579833984\n",
      "Epoch [72/100] Loss: 55.46929168701172\n",
      "Epoch [73/100] Loss: 54.66421890258789\n",
      "Epoch [74/100] Loss: 53.873355865478516\n",
      "Epoch [75/100] Loss: 53.09646224975586\n",
      "Epoch [76/100] Loss: 52.3332633972168\n",
      "Epoch [77/100] Loss: 51.58350372314453\n",
      "Epoch [78/100] Loss: 50.846961975097656\n",
      "Epoch [79/100] Loss: 50.12337112426758\n",
      "Epoch [80/100] Loss: 49.412532806396484\n",
      "Epoch [81/100] Loss: 48.7142333984375\n",
      "Epoch [82/100] Loss: 48.02821731567383\n",
      "Epoch [83/100] Loss: 47.35431671142578\n",
      "Epoch [84/100] Loss: 46.69231033325195\n",
      "Epoch [85/100] Loss: 46.0419807434082\n",
      "Epoch [86/100] Loss: 45.403175354003906\n",
      "Epoch [87/100] Loss: 44.77566146850586\n",
      "Epoch [88/100] Loss: 44.15928268432617\n",
      "Epoch [89/100] Loss: 43.55382537841797\n",
      "Epoch [90/100] Loss: 42.95914077758789\n",
      "Epoch [91/100] Loss: 42.375030517578125\n",
      "Epoch [92/100] Loss: 41.80137252807617\n",
      "Epoch [93/100] Loss: 41.23793029785156\n",
      "Epoch [94/100] Loss: 40.684574127197266\n",
      "Epoch [95/100] Loss: 40.141136169433594\n",
      "Epoch [96/100] Loss: 39.607486724853516\n",
      "Epoch [97/100] Loss: 39.083412170410156\n",
      "Epoch [98/100] Loss: 38.56880569458008\n",
      "Epoch [99/100] Loss: 38.063507080078125\n",
      "Epoch [100/100] Loss: 37.56736373901367\n",
      "Predicted days_remaining for parent_id 412: 8.991562843322754\n",
      "Training for parent_id 416...\n",
      "Epoch [1/100] Loss: 517.443115234375\n",
      "Epoch [2/100] Loss: 505.62109375\n",
      "Epoch [3/100] Loss: 494.21502685546875\n",
      "Epoch [4/100] Loss: 483.343505859375\n",
      "Epoch [5/100] Loss: 473.0458068847656\n",
      "Epoch [6/100] Loss: 463.3148498535156\n",
      "Epoch [7/100] Loss: 454.1242980957031\n",
      "Epoch [8/100] Loss: 445.4460144042969\n",
      "Epoch [9/100] Loss: 437.2582092285156\n",
      "Epoch [10/100] Loss: 429.5460205078125\n",
      "Epoch [11/100] Loss: 422.29766845703125\n",
      "Epoch [12/100] Loss: 415.50006103515625\n",
      "Epoch [13/100] Loss: 409.13507080078125\n",
      "Epoch [14/100] Loss: 403.1780700683594\n",
      "Epoch [15/100] Loss: 397.5987243652344\n",
      "Epoch [16/100] Loss: 392.3634033203125\n",
      "Epoch [17/100] Loss: 387.4374084472656\n",
      "Epoch [18/100] Loss: 382.7872314453125\n",
      "Epoch [19/100] Loss: 378.3815612792969\n",
      "Epoch [20/100] Loss: 374.19219970703125\n",
      "Epoch [21/100] Loss: 370.1946105957031\n",
      "Epoch [22/100] Loss: 366.3675231933594\n",
      "Epoch [23/100] Loss: 362.6925964355469\n",
      "Epoch [24/100] Loss: 359.1539611816406\n",
      "Epoch [25/100] Loss: 355.7376403808594\n",
      "Epoch [26/100] Loss: 352.4309997558594\n",
      "Epoch [27/100] Loss: 349.22259521484375\n",
      "Epoch [28/100] Loss: 346.10174560546875\n",
      "Epoch [29/100] Loss: 343.058837890625\n",
      "Epoch [30/100] Loss: 340.08526611328125\n",
      "Epoch [31/100] Loss: 337.1735534667969\n",
      "Epoch [32/100] Loss: 334.31719970703125\n",
      "Epoch [33/100] Loss: 331.51092529296875\n",
      "Epoch [34/100] Loss: 328.75018310546875\n",
      "Epoch [35/100] Loss: 326.0311584472656\n",
      "Epoch [36/100] Loss: 323.3507385253906\n",
      "Epoch [37/100] Loss: 320.7061462402344\n",
      "Epoch [38/100] Loss: 318.0950927734375\n",
      "Epoch [39/100] Loss: 315.51556396484375\n",
      "Epoch [40/100] Loss: 312.965576171875\n",
      "Epoch [41/100] Loss: 310.4436340332031\n",
      "Epoch [42/100] Loss: 307.9481506347656\n",
      "Epoch [43/100] Loss: 305.4777526855469\n",
      "Epoch [44/100] Loss: 303.0312194824219\n",
      "Epoch [45/100] Loss: 300.6074523925781\n",
      "Epoch [46/100] Loss: 298.205322265625\n",
      "Epoch [47/100] Loss: 295.82379150390625\n",
      "Epoch [48/100] Loss: 293.4619445800781\n",
      "Epoch [49/100] Loss: 291.1189270019531\n",
      "Epoch [50/100] Loss: 288.7940368652344\n",
      "Epoch [51/100] Loss: 286.4865417480469\n",
      "Epoch [52/100] Loss: 284.19598388671875\n",
      "Epoch [53/100] Loss: 281.92181396484375\n",
      "Epoch [54/100] Loss: 279.6637878417969\n",
      "Epoch [55/100] Loss: 277.42181396484375\n",
      "Epoch [56/100] Loss: 275.19573974609375\n",
      "Epoch [57/100] Loss: 272.9856872558594\n",
      "Epoch [58/100] Loss: 270.79168701171875\n",
      "Epoch [59/100] Loss: 268.6139221191406\n",
      "Epoch [60/100] Loss: 266.45245361328125\n",
      "Epoch [61/100] Loss: 264.30743408203125\n",
      "Epoch [62/100] Loss: 262.1789245605469\n",
      "Epoch [63/100] Loss: 260.0669860839844\n",
      "Epoch [64/100] Loss: 257.9715576171875\n",
      "Epoch [65/100] Loss: 255.89259338378906\n",
      "Epoch [66/100] Loss: 253.83001708984375\n",
      "Epoch [67/100] Loss: 251.78375244140625\n",
      "Epoch [68/100] Loss: 249.75369262695312\n",
      "Epoch [69/100] Loss: 247.73960876464844\n",
      "Epoch [70/100] Loss: 245.74148559570312\n",
      "Epoch [71/100] Loss: 243.75901794433594\n",
      "Epoch [72/100] Loss: 241.7921905517578\n",
      "Epoch [73/100] Loss: 239.84080505371094\n",
      "Epoch [74/100] Loss: 237.90469360351562\n",
      "Epoch [75/100] Loss: 235.98370361328125\n",
      "Epoch [76/100] Loss: 234.0777130126953\n",
      "Epoch [77/100] Loss: 232.18649291992188\n",
      "Epoch [78/100] Loss: 230.30999755859375\n",
      "Epoch [79/100] Loss: 228.4480438232422\n",
      "Epoch [80/100] Loss: 226.6004638671875\n",
      "Epoch [81/100] Loss: 224.76719665527344\n",
      "Epoch [82/100] Loss: 222.94808959960938\n",
      "Epoch [83/100] Loss: 221.14292907714844\n",
      "Epoch [84/100] Loss: 219.3517303466797\n",
      "Epoch [85/100] Loss: 217.5742645263672\n",
      "Epoch [86/100] Loss: 215.81033325195312\n",
      "Epoch [87/100] Loss: 214.0599822998047\n",
      "Epoch [88/100] Loss: 212.32298278808594\n",
      "Epoch [89/100] Loss: 210.59927368164062\n",
      "Epoch [90/100] Loss: 208.88868713378906\n",
      "Epoch [91/100] Loss: 207.19117736816406\n",
      "Epoch [92/100] Loss: 205.5065460205078\n",
      "Epoch [93/100] Loss: 203.83473205566406\n",
      "Epoch [94/100] Loss: 202.17562866210938\n",
      "Epoch [95/100] Loss: 200.5290985107422\n",
      "Epoch [96/100] Loss: 198.89511108398438\n",
      "Epoch [97/100] Loss: 197.27349853515625\n",
      "Epoch [98/100] Loss: 195.66412353515625\n",
      "Epoch [99/100] Loss: 194.0669708251953\n",
      "Epoch [100/100] Loss: 192.48193359375\n",
      "Predicted days_remaining for parent_id 416: 9.465747833251953\n",
      "Training for parent_id 425...\n",
      "Epoch [1/100] Loss: 1525.0400390625\n",
      "Epoch [2/100] Loss: 1505.3345947265625\n",
      "Epoch [3/100] Loss: 1485.7032470703125\n",
      "Epoch [4/100] Loss: 1466.3370361328125\n",
      "Epoch [5/100] Loss: 1447.39599609375\n",
      "Epoch [6/100] Loss: 1429.0347900390625\n",
      "Epoch [7/100] Loss: 1411.369140625\n",
      "Epoch [8/100] Loss: 1394.4493408203125\n",
      "Epoch [9/100] Loss: 1378.2811279296875\n",
      "Epoch [10/100] Loss: 1362.8526611328125\n",
      "Epoch [11/100] Loss: 1348.143798828125\n",
      "Epoch [12/100] Loss: 1334.1290283203125\n",
      "Epoch [13/100] Loss: 1320.7747802734375\n",
      "Epoch [14/100] Loss: 1308.038330078125\n",
      "Epoch [15/100] Loss: 1295.872314453125\n",
      "Epoch [16/100] Loss: 1284.2301025390625\n",
      "Epoch [17/100] Loss: 1273.07470703125\n",
      "Epoch [18/100] Loss: 1262.379150390625\n",
      "Epoch [19/100] Loss: 1252.1270751953125\n",
      "Epoch [20/100] Loss: 1242.305419921875\n",
      "Epoch [21/100] Loss: 1232.9005126953125\n",
      "Epoch [22/100] Loss: 1223.894775390625\n",
      "Epoch [23/100] Loss: 1215.2666015625\n",
      "Epoch [24/100] Loss: 1206.9908447265625\n",
      "Epoch [25/100] Loss: 1199.0404052734375\n",
      "Epoch [26/100] Loss: 1191.38916015625\n",
      "Epoch [27/100] Loss: 1184.0111083984375\n",
      "Epoch [28/100] Loss: 1176.881591796875\n",
      "Epoch [29/100] Loss: 1169.9788818359375\n",
      "Epoch [30/100] Loss: 1163.2818603515625\n",
      "Epoch [31/100] Loss: 1156.771728515625\n",
      "Epoch [32/100] Loss: 1150.4312744140625\n",
      "Epoch [33/100] Loss: 1144.2440185546875\n",
      "Epoch [34/100] Loss: 1138.1954345703125\n",
      "Epoch [35/100] Loss: 1132.2724609375\n",
      "Epoch [36/100] Loss: 1126.4627685546875\n",
      "Epoch [37/100] Loss: 1120.755615234375\n",
      "Epoch [38/100] Loss: 1115.141845703125\n",
      "Epoch [39/100] Loss: 1109.612548828125\n",
      "Epoch [40/100] Loss: 1104.16064453125\n",
      "Epoch [41/100] Loss: 1098.77978515625\n",
      "Epoch [42/100] Loss: 1093.4647216796875\n",
      "Epoch [43/100] Loss: 1088.2103271484375\n",
      "Epoch [44/100] Loss: 1083.012939453125\n",
      "Epoch [45/100] Loss: 1077.8690185546875\n",
      "Epoch [46/100] Loss: 1072.77587890625\n",
      "Epoch [47/100] Loss: 1067.7303466796875\n",
      "Epoch [48/100] Loss: 1062.73046875\n",
      "Epoch [49/100] Loss: 1057.7740478515625\n",
      "Epoch [50/100] Loss: 1052.859375\n",
      "Epoch [51/100] Loss: 1047.98486328125\n",
      "Epoch [52/100] Loss: 1043.1485595703125\n",
      "Epoch [53/100] Loss: 1038.349609375\n",
      "Epoch [54/100] Loss: 1033.5860595703125\n",
      "Epoch [55/100] Loss: 1028.8570556640625\n",
      "Epoch [56/100] Loss: 1024.1614990234375\n",
      "Epoch [57/100] Loss: 1019.4978637695312\n",
      "Epoch [58/100] Loss: 1014.8653564453125\n",
      "Epoch [59/100] Loss: 1010.262939453125\n",
      "Epoch [60/100] Loss: 1005.6895141601562\n",
      "Epoch [61/100] Loss: 1001.1441650390625\n",
      "Epoch [62/100] Loss: 996.6261596679688\n",
      "Epoch [63/100] Loss: 992.1342163085938\n",
      "Epoch [64/100] Loss: 987.66796875\n",
      "Epoch [65/100] Loss: 983.2263793945312\n",
      "Epoch [66/100] Loss: 978.8086547851562\n",
      "Epoch [67/100] Loss: 974.4139404296875\n",
      "Epoch [68/100] Loss: 970.0415649414062\n",
      "Epoch [69/100] Loss: 965.6908569335938\n",
      "Epoch [70/100] Loss: 961.3612060546875\n",
      "Epoch [71/100] Loss: 957.0519409179688\n",
      "Epoch [72/100] Loss: 952.7625732421875\n",
      "Epoch [73/100] Loss: 948.4922485351562\n",
      "Epoch [74/100] Loss: 944.2412109375\n",
      "Epoch [75/100] Loss: 940.0087280273438\n",
      "Epoch [76/100] Loss: 935.7945556640625\n",
      "Epoch [77/100] Loss: 931.598388671875\n",
      "Epoch [78/100] Loss: 927.42041015625\n",
      "Epoch [79/100] Loss: 923.2604370117188\n",
      "Epoch [80/100] Loss: 919.1182861328125\n",
      "Epoch [81/100] Loss: 914.9943237304688\n",
      "Epoch [82/100] Loss: 910.8880615234375\n",
      "Epoch [83/100] Loss: 906.7999877929688\n",
      "Epoch [84/100] Loss: 902.7297973632812\n",
      "Epoch [85/100] Loss: 898.6778564453125\n",
      "Epoch [86/100] Loss: 894.6439819335938\n",
      "Epoch [87/100] Loss: 890.6279296875\n",
      "Epoch [88/100] Loss: 886.6299438476562\n",
      "Epoch [89/100] Loss: 882.6500244140625\n",
      "Epoch [90/100] Loss: 878.6879272460938\n",
      "Epoch [91/100] Loss: 874.7435302734375\n",
      "Epoch [92/100] Loss: 870.8171997070312\n",
      "Epoch [93/100] Loss: 866.9082641601562\n",
      "Epoch [94/100] Loss: 863.016845703125\n",
      "Epoch [95/100] Loss: 859.1429443359375\n",
      "Epoch [96/100] Loss: 855.2861938476562\n",
      "Epoch [97/100] Loss: 851.4466552734375\n",
      "Epoch [98/100] Loss: 847.6242065429688\n",
      "Epoch [99/100] Loss: 843.8185424804688\n",
      "Epoch [100/100] Loss: 840.0299072265625\n",
      "Predicted days_remaining for parent_id 425: 10.082588195800781\n",
      "Training for parent_id 430...\n",
      "Epoch [1/100] Loss: 155.23924255371094\n",
      "Epoch [2/100] Loss: 150.3480682373047\n",
      "Epoch [3/100] Loss: 145.60464477539062\n",
      "Epoch [4/100] Loss: 141.0357666015625\n",
      "Epoch [5/100] Loss: 136.64608764648438\n",
      "Epoch [6/100] Loss: 132.42532348632812\n",
      "Epoch [7/100] Loss: 128.35516357421875\n",
      "Epoch [8/100] Loss: 124.4134521484375\n",
      "Epoch [9/100] Loss: 120.57955932617188\n",
      "Epoch [10/100] Loss: 116.83882904052734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Loss: 113.18378448486328\n",
      "Epoch [12/100] Loss: 109.61321258544922\n",
      "Epoch [13/100] Loss: 106.13080596923828\n",
      "Epoch [14/100] Loss: 102.74372863769531\n",
      "Epoch [15/100] Loss: 99.46125793457031\n",
      "Epoch [16/100] Loss: 96.29358673095703\n",
      "Epoch [17/100] Loss: 93.2506332397461\n",
      "Epoch [18/100] Loss: 90.34105682373047\n",
      "Epoch [19/100] Loss: 87.57147216796875\n",
      "Epoch [20/100] Loss: 84.94593048095703\n",
      "Epoch [21/100] Loss: 82.46570587158203\n",
      "Epoch [22/100] Loss: 80.12937927246094\n",
      "Epoch [23/100] Loss: 77.93279266357422\n",
      "Epoch [24/100] Loss: 75.86961364746094\n",
      "Epoch [25/100] Loss: 73.93165588378906\n",
      "Epoch [26/100] Loss: 72.10943603515625\n",
      "Epoch [27/100] Loss: 70.39291381835938\n",
      "Epoch [28/100] Loss: 68.77188873291016\n",
      "Epoch [29/100] Loss: 67.23664093017578\n",
      "Epoch [30/100] Loss: 65.7780990600586\n",
      "Epoch [31/100] Loss: 64.38826751708984\n",
      "Epoch [32/100] Loss: 63.06009292602539\n",
      "Epoch [33/100] Loss: 61.787479400634766\n",
      "Epoch [34/100] Loss: 60.565162658691406\n",
      "Epoch [35/100] Loss: 59.38853454589844\n",
      "Epoch [36/100] Loss: 58.253509521484375\n",
      "Epoch [37/100] Loss: 57.15650939941406\n",
      "Epoch [38/100] Loss: 56.094364166259766\n",
      "Epoch [39/100] Loss: 55.064361572265625\n",
      "Epoch [40/100] Loss: 54.06414031982422\n",
      "Epoch [41/100] Loss: 53.091712951660156\n",
      "Epoch [42/100] Loss: 52.145362854003906\n",
      "Epoch [43/100] Loss: 51.22358322143555\n",
      "Epoch [44/100] Loss: 50.325050354003906\n",
      "Epoch [45/100] Loss: 49.448585510253906\n",
      "Epoch [46/100] Loss: 48.593109130859375\n",
      "Epoch [47/100] Loss: 47.75768280029297\n",
      "Epoch [48/100] Loss: 46.94144821166992\n",
      "Epoch [49/100] Loss: 46.1436767578125\n",
      "Epoch [50/100] Loss: 45.36367416381836\n",
      "Epoch [51/100] Loss: 44.600868225097656\n",
      "Epoch [52/100] Loss: 43.85472869873047\n",
      "Epoch [53/100] Loss: 43.12476348876953\n",
      "Epoch [54/100] Loss: 42.41056442260742\n",
      "Epoch [55/100] Loss: 41.71174621582031\n",
      "Epoch [56/100] Loss: 41.02793502807617\n",
      "Epoch [57/100] Loss: 40.35881042480469\n",
      "Epoch [58/100] Loss: 39.70407485961914\n",
      "Epoch [59/100] Loss: 39.06343078613281\n",
      "Epoch [60/100] Loss: 38.43660354614258\n",
      "Epoch [61/100] Loss: 37.823326110839844\n",
      "Epoch [62/100] Loss: 37.223384857177734\n",
      "Epoch [63/100] Loss: 36.636505126953125\n",
      "Epoch [64/100] Loss: 36.062477111816406\n",
      "Epoch [65/100] Loss: 35.50108337402344\n",
      "Epoch [66/100] Loss: 34.952064514160156\n",
      "Epoch [67/100] Loss: 34.41524124145508\n",
      "Epoch [68/100] Loss: 33.89039611816406\n",
      "Epoch [69/100] Loss: 33.377296447753906\n",
      "Epoch [70/100] Loss: 32.87575149536133\n",
      "Epoch [71/100] Loss: 32.385555267333984\n",
      "Epoch [72/100] Loss: 31.906492233276367\n",
      "Epoch [73/100] Loss: 31.438377380371094\n",
      "Epoch [74/100] Loss: 30.980998992919922\n",
      "Epoch [75/100] Loss: 30.534154891967773\n",
      "Epoch [76/100] Loss: 30.0976505279541\n",
      "Epoch [77/100] Loss: 29.671308517456055\n",
      "Epoch [78/100] Loss: 29.25491714477539\n",
      "Epoch [79/100] Loss: 28.848297119140625\n",
      "Epoch [80/100] Loss: 28.451255798339844\n",
      "Epoch [81/100] Loss: 28.063613891601562\n",
      "Epoch [82/100] Loss: 27.685178756713867\n",
      "Epoch [83/100] Loss: 27.315792083740234\n",
      "Epoch [84/100] Loss: 26.95525550842285\n",
      "Epoch [85/100] Loss: 26.603391647338867\n",
      "Epoch [86/100] Loss: 26.26004409790039\n",
      "Epoch [87/100] Loss: 25.925039291381836\n",
      "Epoch [88/100] Loss: 25.59820556640625\n",
      "Epoch [89/100] Loss: 25.27937126159668\n",
      "Epoch [90/100] Loss: 24.96839141845703\n",
      "Epoch [91/100] Loss: 24.66508674621582\n",
      "Epoch [92/100] Loss: 24.36931800842285\n",
      "Epoch [93/100] Loss: 24.080928802490234\n",
      "Epoch [94/100] Loss: 23.799755096435547\n",
      "Epoch [95/100] Loss: 23.525657653808594\n",
      "Epoch [96/100] Loss: 23.258480072021484\n",
      "Epoch [97/100] Loss: 22.998090744018555\n",
      "Epoch [98/100] Loss: 22.744342803955078\n",
      "Epoch [99/100] Loss: 22.497093200683594\n",
      "Epoch [100/100] Loss: 22.256195068359375\n",
      "Predicted days_remaining for parent_id 430: 8.996088981628418\n",
      "Training for parent_id 433...\n",
      "Epoch [1/100] Loss: 512.5875244140625\n",
      "Epoch [2/100] Loss: 503.2513427734375\n",
      "Epoch [3/100] Loss: 494.0148010253906\n",
      "Epoch [4/100] Loss: 484.9298095703125\n",
      "Epoch [5/100] Loss: 476.04364013671875\n",
      "Epoch [6/100] Loss: 467.3822326660156\n",
      "Epoch [7/100] Loss: 458.96087646484375\n",
      "Epoch [8/100] Loss: 450.7962646484375\n",
      "Epoch [9/100] Loss: 442.9072265625\n",
      "Epoch [10/100] Loss: 435.3099060058594\n",
      "Epoch [11/100] Loss: 428.0126037597656\n",
      "Epoch [12/100] Loss: 421.0142822265625\n",
      "Epoch [13/100] Loss: 414.3063049316406\n",
      "Epoch [14/100] Loss: 407.875732421875\n",
      "Epoch [15/100] Loss: 401.7093811035156\n",
      "Epoch [16/100] Loss: 395.79473876953125\n",
      "Epoch [17/100] Loss: 390.1201171875\n",
      "Epoch [18/100] Loss: 384.6737365722656\n",
      "Epoch [19/100] Loss: 379.4433898925781\n",
      "Epoch [20/100] Loss: 374.4158935546875\n",
      "Epoch [21/100] Loss: 369.57745361328125\n",
      "Epoch [22/100] Loss: 364.91436767578125\n",
      "Epoch [23/100] Loss: 360.4136962890625\n",
      "Epoch [24/100] Loss: 356.064208984375\n",
      "Epoch [25/100] Loss: 351.8570861816406\n",
      "Epoch [26/100] Loss: 347.78570556640625\n",
      "Epoch [27/100] Loss: 343.8452453613281\n",
      "Epoch [28/100] Loss: 340.0318603515625\n",
      "Epoch [29/100] Loss: 336.3418884277344\n",
      "Epoch [30/100] Loss: 332.7708740234375\n",
      "Epoch [31/100] Loss: 329.3131408691406\n",
      "Epoch [32/100] Loss: 325.96112060546875\n",
      "Epoch [33/100] Loss: 322.7064208984375\n",
      "Epoch [34/100] Loss: 319.539794921875\n",
      "Epoch [35/100] Loss: 316.4521484375\n",
      "Epoch [36/100] Loss: 313.43499755859375\n",
      "Epoch [37/100] Loss: 310.480712890625\n",
      "Epoch [38/100] Loss: 307.5826721191406\n",
      "Epoch [39/100] Loss: 304.7351989746094\n",
      "Epoch [40/100] Loss: 301.93353271484375\n",
      "Epoch [41/100] Loss: 299.173583984375\n",
      "Epoch [42/100] Loss: 296.4520263671875\n",
      "Epoch [43/100] Loss: 293.7657775878906\n",
      "Epoch [44/100] Loss: 291.1127014160156\n",
      "Epoch [45/100] Loss: 288.49053955078125\n",
      "Epoch [46/100] Loss: 285.89788818359375\n",
      "Epoch [47/100] Loss: 283.3332214355469\n",
      "Epoch [48/100] Loss: 280.7955322265625\n",
      "Epoch [49/100] Loss: 278.2839050292969\n",
      "Epoch [50/100] Loss: 275.79766845703125\n",
      "Epoch [51/100] Loss: 273.3362121582031\n",
      "Epoch [52/100] Loss: 270.899169921875\n",
      "Epoch [53/100] Loss: 268.4862365722656\n",
      "Epoch [54/100] Loss: 266.09698486328125\n",
      "Epoch [55/100] Loss: 263.73114013671875\n",
      "Epoch [56/100] Loss: 261.388671875\n",
      "Epoch [57/100] Loss: 259.069091796875\n",
      "Epoch [58/100] Loss: 256.7723693847656\n",
      "Epoch [59/100] Loss: 254.49827575683594\n",
      "Epoch [60/100] Loss: 252.24655151367188\n",
      "Epoch [61/100] Loss: 250.01702880859375\n",
      "Epoch [62/100] Loss: 247.8094940185547\n",
      "Epoch [63/100] Loss: 245.62367248535156\n",
      "Epoch [64/100] Loss: 243.4593505859375\n",
      "Epoch [65/100] Loss: 241.31629943847656\n",
      "Epoch [66/100] Loss: 239.1941375732422\n",
      "Epoch [67/100] Loss: 237.09278869628906\n",
      "Epoch [68/100] Loss: 235.01185607910156\n",
      "Epoch [69/100] Loss: 232.95094299316406\n",
      "Epoch [70/100] Loss: 230.9099578857422\n",
      "Epoch [71/100] Loss: 228.88845825195312\n",
      "Epoch [72/100] Loss: 226.88626098632812\n",
      "Epoch [73/100] Loss: 224.90293884277344\n",
      "Epoch [74/100] Loss: 222.93829345703125\n",
      "Epoch [75/100] Loss: 220.9919891357422\n",
      "Epoch [76/100] Loss: 219.0637664794922\n",
      "Epoch [77/100] Loss: 217.15335083007812\n",
      "Epoch [78/100] Loss: 215.26048278808594\n",
      "Epoch [79/100] Loss: 213.3849639892578\n",
      "Epoch [80/100] Loss: 211.5264434814453\n",
      "Epoch [81/100] Loss: 209.6846923828125\n",
      "Epoch [82/100] Loss: 207.8594970703125\n",
      "Epoch [83/100] Loss: 206.0507049560547\n",
      "Epoch [84/100] Loss: 204.25796508789062\n",
      "Epoch [85/100] Loss: 202.48109436035156\n",
      "Epoch [86/100] Loss: 200.71995544433594\n",
      "Epoch [87/100] Loss: 198.97433471679688\n",
      "Epoch [88/100] Loss: 197.24398803710938\n",
      "Epoch [89/100] Loss: 195.52874755859375\n",
      "Epoch [90/100] Loss: 193.8284454345703\n",
      "Epoch [91/100] Loss: 192.1428985595703\n",
      "Epoch [92/100] Loss: 190.471923828125\n",
      "Epoch [93/100] Loss: 188.81533813476562\n",
      "Epoch [94/100] Loss: 187.17298889160156\n",
      "Epoch [95/100] Loss: 185.54476928710938\n",
      "Epoch [96/100] Loss: 183.93048095703125\n",
      "Epoch [97/100] Loss: 182.32989501953125\n",
      "Epoch [98/100] Loss: 180.74301147460938\n",
      "Epoch [99/100] Loss: 179.16966247558594\n",
      "Epoch [100/100] Loss: 177.6095428466797\n",
      "Predicted days_remaining for parent_id 433: 10.036808967590332\n",
      "Training for parent_id 434...\n",
      "Epoch [1/100] Loss: 144.7507781982422\n",
      "Epoch [2/100] Loss: 139.23231506347656\n",
      "Epoch [3/100] Loss: 133.87530517578125\n",
      "Epoch [4/100] Loss: 128.68295288085938\n",
      "Epoch [5/100] Loss: 123.66546630859375\n",
      "Epoch [6/100] Loss: 118.83649444580078\n",
      "Epoch [7/100] Loss: 114.20693969726562\n",
      "Epoch [8/100] Loss: 109.7822494506836\n",
      "Epoch [9/100] Loss: 105.56317901611328\n",
      "Epoch [10/100] Loss: 101.5476303100586\n",
      "Epoch [11/100] Loss: 97.7325668334961\n",
      "Epoch [12/100] Loss: 94.11488342285156\n",
      "Epoch [13/100] Loss: 90.6914291381836\n",
      "Epoch [14/100] Loss: 87.4583740234375\n",
      "Epoch [15/100] Loss: 84.41071319580078\n",
      "Epoch [16/100] Loss: 81.54202270507812\n",
      "Epoch [17/100] Loss: 78.84455871582031\n",
      "Epoch [18/100] Loss: 76.30933380126953\n",
      "Epoch [19/100] Loss: 73.92650604248047\n",
      "Epoch [20/100] Loss: 71.685791015625\n",
      "Epoch [21/100] Loss: 69.57667541503906\n",
      "Epoch [22/100] Loss: 67.58871459960938\n",
      "Epoch [23/100] Loss: 65.71187591552734\n",
      "Epoch [24/100] Loss: 63.93657302856445\n",
      "Epoch [25/100] Loss: 62.254024505615234\n",
      "Epoch [26/100] Loss: 60.65610885620117\n",
      "Epoch [27/100] Loss: 59.135597229003906\n",
      "Epoch [28/100] Loss: 57.68598937988281\n",
      "Epoch [29/100] Loss: 56.30149459838867\n",
      "Epoch [30/100] Loss: 54.97694778442383\n",
      "Epoch [31/100] Loss: 53.707767486572266\n",
      "Epoch [32/100] Loss: 52.489871978759766\n",
      "Epoch [33/100] Loss: 51.31959533691406\n",
      "Epoch [34/100] Loss: 50.193695068359375\n",
      "Epoch [35/100] Loss: 49.1092643737793\n",
      "Epoch [36/100] Loss: 48.06365203857422\n",
      "Epoch [37/100] Loss: 47.05455017089844\n",
      "Epoch [38/100] Loss: 46.0798454284668\n",
      "Epoch [39/100] Loss: 45.13762664794922\n",
      "Epoch [40/100] Loss: 44.226158142089844\n",
      "Epoch [41/100] Loss: 43.34391784667969\n",
      "Epoch [42/100] Loss: 42.48945236206055\n",
      "Epoch [43/100] Loss: 41.66145706176758\n",
      "Epoch [44/100] Loss: 40.858726501464844\n",
      "Epoch [45/100] Loss: 40.080196380615234\n",
      "Epoch [46/100] Loss: 39.32481002807617\n",
      "Epoch [47/100] Loss: 38.59165954589844\n",
      "Epoch [48/100] Loss: 37.87983703613281\n",
      "Epoch [49/100] Loss: 37.18856430053711\n",
      "Epoch [50/100] Loss: 36.517059326171875\n",
      "Epoch [51/100] Loss: 35.86460494995117\n",
      "Epoch [52/100] Loss: 35.23052215576172\n",
      "Epoch [53/100] Loss: 34.61418151855469\n",
      "Epoch [54/100] Loss: 34.014984130859375\n",
      "Epoch [55/100] Loss: 33.43235397338867\n",
      "Epoch [56/100] Loss: 32.865753173828125\n",
      "Epoch [57/100] Loss: 32.314674377441406\n",
      "Epoch [58/100] Loss: 31.778627395629883\n",
      "Epoch [59/100] Loss: 31.25715446472168\n",
      "Epoch [60/100] Loss: 30.749828338623047\n",
      "Epoch [61/100] Loss: 30.2562313079834\n",
      "Epoch [62/100] Loss: 29.775970458984375\n",
      "Epoch [63/100] Loss: 29.308673858642578\n",
      "Epoch [64/100] Loss: 28.853988647460938\n",
      "Epoch [65/100] Loss: 28.411584854125977\n",
      "Epoch [66/100] Loss: 27.981117248535156\n",
      "Epoch [67/100] Loss: 27.56230926513672\n",
      "Epoch [68/100] Loss: 27.154821395874023\n",
      "Epoch [69/100] Loss: 26.758413314819336\n",
      "Epoch [70/100] Loss: 26.372774124145508\n",
      "Epoch [71/100] Loss: 25.997665405273438\n",
      "Epoch [72/100] Loss: 25.632814407348633\n",
      "Epoch [73/100] Loss: 25.277963638305664\n",
      "Epoch [74/100] Loss: 24.932899475097656\n",
      "Epoch [75/100] Loss: 24.59735870361328\n",
      "Epoch [76/100] Loss: 24.27113151550293\n",
      "Epoch [77/100] Loss: 23.953983306884766\n",
      "Epoch [78/100] Loss: 23.64569091796875\n",
      "Epoch [79/100] Loss: 23.346067428588867\n",
      "Epoch [80/100] Loss: 23.054889678955078\n",
      "Epoch [81/100] Loss: 22.771963119506836\n",
      "Epoch [82/100] Loss: 22.497068405151367\n",
      "Epoch [83/100] Loss: 22.230031967163086\n",
      "Epoch [84/100] Loss: 21.97065544128418\n",
      "Epoch [85/100] Loss: 21.718772888183594\n",
      "Epoch [86/100] Loss: 21.474172592163086\n",
      "Epoch [87/100] Loss: 21.236703872680664\n",
      "Epoch [88/100] Loss: 21.006187438964844\n",
      "Epoch [89/100] Loss: 20.782438278198242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100] Loss: 20.565309524536133\n",
      "Epoch [91/100] Loss: 20.35462188720703\n",
      "Epoch [92/100] Loss: 20.150222778320312\n",
      "Epoch [93/100] Loss: 19.951950073242188\n",
      "Epoch [94/100] Loss: 19.759660720825195\n",
      "Epoch [95/100] Loss: 19.57320213317871\n",
      "Epoch [96/100] Loss: 19.392412185668945\n",
      "Epoch [97/100] Loss: 19.21717071533203\n",
      "Epoch [98/100] Loss: 19.04732322692871\n",
      "Epoch [99/100] Loss: 18.882732391357422\n",
      "Epoch [100/100] Loss: 18.723251342773438\n",
      "Predicted days_remaining for parent_id 434: 9.717447280883789\n",
      "Training for parent_id 437...\n",
      "Epoch [1/100] Loss: 636.625\n",
      "Epoch [2/100] Loss: 624.7950439453125\n",
      "Epoch [3/100] Loss: 613.23046875\n",
      "Epoch [4/100] Loss: 601.998046875\n",
      "Epoch [5/100] Loss: 591.1469116210938\n",
      "Epoch [6/100] Loss: 580.7047119140625\n",
      "Epoch [7/100] Loss: 570.6776123046875\n",
      "Epoch [8/100] Loss: 561.0465698242188\n",
      "Epoch [9/100] Loss: 551.780029296875\n",
      "Epoch [10/100] Loss: 542.849365234375\n",
      "Epoch [11/100] Loss: 534.2334594726562\n",
      "Epoch [12/100] Loss: 525.9178466796875\n",
      "Epoch [13/100] Loss: 517.8926391601562\n",
      "Epoch [14/100] Loss: 510.1495361328125\n",
      "Epoch [15/100] Loss: 502.68212890625\n",
      "Epoch [16/100] Loss: 495.48486328125\n",
      "Epoch [17/100] Loss: 488.55340576171875\n",
      "Epoch [18/100] Loss: 481.88421630859375\n",
      "Epoch [19/100] Loss: 475.47406005859375\n",
      "Epoch [20/100] Loss: 469.31927490234375\n",
      "Epoch [21/100] Loss: 463.41546630859375\n",
      "Epoch [22/100] Loss: 457.7568054199219\n",
      "Epoch [23/100] Loss: 452.3363037109375\n",
      "Epoch [24/100] Loss: 447.1458435058594\n",
      "Epoch [25/100] Loss: 442.1761169433594\n",
      "Epoch [26/100] Loss: 437.416748046875\n",
      "Epoch [27/100] Loss: 432.85614013671875\n",
      "Epoch [28/100] Loss: 428.48187255859375\n",
      "Epoch [29/100] Loss: 424.28033447265625\n",
      "Epoch [30/100] Loss: 420.23779296875\n",
      "Epoch [31/100] Loss: 416.34002685546875\n",
      "Epoch [32/100] Loss: 412.57330322265625\n",
      "Epoch [33/100] Loss: 408.9243469238281\n",
      "Epoch [34/100] Loss: 405.38092041015625\n",
      "Epoch [35/100] Loss: 401.93182373046875\n",
      "Epoch [36/100] Loss: 398.56689453125\n",
      "Epoch [37/100] Loss: 395.2773132324219\n",
      "Epoch [38/100] Loss: 392.05535888671875\n",
      "Epoch [39/100] Loss: 388.8944091796875\n",
      "Epoch [40/100] Loss: 385.78863525390625\n",
      "Epoch [41/100] Loss: 382.7331237792969\n",
      "Epoch [42/100] Loss: 379.72369384765625\n",
      "Epoch [43/100] Loss: 376.7567443847656\n",
      "Epoch [44/100] Loss: 373.8290710449219\n",
      "Epoch [45/100] Loss: 370.93829345703125\n",
      "Epoch [46/100] Loss: 368.0817565917969\n",
      "Epoch [47/100] Loss: 365.2577819824219\n",
      "Epoch [48/100] Loss: 362.46453857421875\n",
      "Epoch [49/100] Loss: 359.70074462890625\n",
      "Epoch [50/100] Loss: 356.9648132324219\n",
      "Epoch [51/100] Loss: 354.2558898925781\n",
      "Epoch [52/100] Loss: 351.5728759765625\n",
      "Epoch [53/100] Loss: 348.9149169921875\n",
      "Epoch [54/100] Loss: 346.2811279296875\n",
      "Epoch [55/100] Loss: 343.67095947265625\n",
      "Epoch [56/100] Loss: 341.0837097167969\n",
      "Epoch [57/100] Loss: 338.51885986328125\n",
      "Epoch [58/100] Loss: 335.97589111328125\n",
      "Epoch [59/100] Loss: 333.4543151855469\n",
      "Epoch [60/100] Loss: 330.95367431640625\n",
      "Epoch [61/100] Loss: 328.4736022949219\n",
      "Epoch [62/100] Loss: 326.01373291015625\n",
      "Epoch [63/100] Loss: 323.5736389160156\n",
      "Epoch [64/100] Loss: 321.15313720703125\n",
      "Epoch [65/100] Loss: 318.7517395019531\n",
      "Epoch [66/100] Loss: 316.3692321777344\n",
      "Epoch [67/100] Loss: 314.00537109375\n",
      "Epoch [68/100] Loss: 311.65985107421875\n",
      "Epoch [69/100] Loss: 309.33233642578125\n",
      "Epoch [70/100] Loss: 307.022705078125\n",
      "Epoch [71/100] Loss: 304.73065185546875\n",
      "Epoch [72/100] Loss: 302.45599365234375\n",
      "Epoch [73/100] Loss: 300.1984558105469\n",
      "Epoch [74/100] Loss: 297.9579162597656\n",
      "Epoch [75/100] Loss: 295.73406982421875\n",
      "Epoch [76/100] Loss: 293.52679443359375\n",
      "Epoch [77/100] Loss: 291.3357238769531\n",
      "Epoch [78/100] Loss: 289.16094970703125\n",
      "Epoch [79/100] Loss: 287.0020446777344\n",
      "Epoch [80/100] Loss: 284.8590393066406\n",
      "Epoch [81/100] Loss: 282.7315979003906\n",
      "Epoch [82/100] Loss: 280.61962890625\n",
      "Epoch [83/100] Loss: 278.5229187011719\n",
      "Epoch [84/100] Loss: 276.44140625\n",
      "Epoch [85/100] Loss: 274.37481689453125\n",
      "Epoch [86/100] Loss: 272.3230895996094\n",
      "Epoch [87/100] Loss: 270.2860107421875\n",
      "Epoch [88/100] Loss: 268.2635192871094\n",
      "Epoch [89/100] Loss: 266.2554626464844\n",
      "Epoch [90/100] Loss: 264.2615966796875\n",
      "Epoch [91/100] Loss: 262.2819519042969\n",
      "Epoch [92/100] Loss: 260.3161926269531\n",
      "Epoch [93/100] Loss: 258.3644104003906\n",
      "Epoch [94/100] Loss: 256.4263610839844\n",
      "Epoch [95/100] Loss: 254.50192260742188\n",
      "Epoch [96/100] Loss: 252.59104919433594\n",
      "Epoch [97/100] Loss: 250.69357299804688\n",
      "Epoch [98/100] Loss: 248.80935668945312\n",
      "Epoch [99/100] Loss: 246.9382781982422\n",
      "Epoch [100/100] Loss: 245.08035278320312\n",
      "Predicted days_remaining for parent_id 437: 9.62393569946289\n",
      "Training for parent_id 445...\n",
      "Epoch [1/100] Loss: 361.89892578125\n",
      "Epoch [2/100] Loss: 354.23211669921875\n",
      "Epoch [3/100] Loss: 346.9147033691406\n",
      "Epoch [4/100] Loss: 339.9732971191406\n",
      "Epoch [5/100] Loss: 333.3961486816406\n",
      "Epoch [6/100] Loss: 327.13525390625\n",
      "Epoch [7/100] Loss: 321.1276550292969\n",
      "Epoch [8/100] Loss: 315.310791015625\n",
      "Epoch [9/100] Loss: 309.6302185058594\n",
      "Epoch [10/100] Loss: 304.0445251464844\n",
      "Epoch [11/100] Loss: 298.5276184082031\n",
      "Epoch [12/100] Loss: 293.0675048828125\n",
      "Epoch [13/100] Loss: 287.6631164550781\n",
      "Epoch [14/100] Loss: 282.3211364746094\n",
      "Epoch [15/100] Loss: 277.0531005859375\n",
      "Epoch [16/100] Loss: 271.87298583984375\n",
      "Epoch [17/100] Loss: 266.7958679199219\n",
      "Epoch [18/100] Loss: 261.8371887207031\n",
      "Epoch [19/100] Loss: 257.0114440917969\n",
      "Epoch [20/100] Loss: 252.33157348632812\n",
      "Epoch [21/100] Loss: 247.8080291748047\n",
      "Epoch [22/100] Loss: 243.44810485839844\n",
      "Epoch [23/100] Loss: 239.2555389404297\n",
      "Epoch [24/100] Loss: 235.2303009033203\n",
      "Epoch [25/100] Loss: 231.3692626953125\n",
      "Epoch [26/100] Loss: 227.66651916503906\n",
      "Epoch [27/100] Loss: 224.11439514160156\n",
      "Epoch [28/100] Loss: 220.70407104492188\n",
      "Epoch [29/100] Loss: 217.4258575439453\n",
      "Epoch [30/100] Loss: 214.26995849609375\n",
      "Epoch [31/100] Loss: 211.2267303466797\n",
      "Epoch [32/100] Loss: 208.28665161132812\n",
      "Epoch [33/100] Loss: 205.44091796875\n",
      "Epoch [34/100] Loss: 202.68121337890625\n",
      "Epoch [35/100] Loss: 200.0000762939453\n",
      "Epoch [36/100] Loss: 197.390625\n",
      "Epoch [37/100] Loss: 194.84693908691406\n",
      "Epoch [38/100] Loss: 192.36367797851562\n",
      "Epoch [39/100] Loss: 189.93630981445312\n",
      "Epoch [40/100] Loss: 187.56089782714844\n",
      "Epoch [41/100] Loss: 185.234130859375\n",
      "Epoch [42/100] Loss: 182.95303344726562\n",
      "Epoch [43/100] Loss: 180.71524047851562\n",
      "Epoch [44/100] Loss: 178.51840209960938\n",
      "Epoch [45/100] Loss: 176.3606414794922\n",
      "Epoch [46/100] Loss: 174.24012756347656\n",
      "Epoch [47/100] Loss: 172.15528869628906\n",
      "Epoch [48/100] Loss: 170.10464477539062\n",
      "Epoch [49/100] Loss: 168.0868682861328\n",
      "Epoch [50/100] Loss: 166.10073852539062\n",
      "Epoch [51/100] Loss: 164.14517211914062\n",
      "Epoch [52/100] Loss: 162.2190704345703\n",
      "Epoch [53/100] Loss: 160.32162475585938\n",
      "Epoch [54/100] Loss: 158.4519500732422\n",
      "Epoch [55/100] Loss: 156.60922241210938\n",
      "Epoch [56/100] Loss: 154.79273986816406\n",
      "Epoch [57/100] Loss: 153.0018310546875\n",
      "Epoch [58/100] Loss: 151.23583984375\n",
      "Epoch [59/100] Loss: 149.49411010742188\n",
      "Epoch [60/100] Loss: 147.77609252929688\n",
      "Epoch [61/100] Loss: 146.08126831054688\n",
      "Epoch [62/100] Loss: 144.4091033935547\n",
      "Epoch [63/100] Loss: 142.7590789794922\n",
      "Epoch [64/100] Loss: 141.13075256347656\n",
      "Epoch [65/100] Loss: 139.52366638183594\n",
      "Epoch [66/100] Loss: 137.93739318847656\n",
      "Epoch [67/100] Loss: 136.37158203125\n",
      "Epoch [68/100] Loss: 134.82574462890625\n",
      "Epoch [69/100] Loss: 133.29957580566406\n",
      "Epoch [70/100] Loss: 131.79269409179688\n",
      "Epoch [71/100] Loss: 130.30477905273438\n",
      "Epoch [72/100] Loss: 128.83543395996094\n",
      "Epoch [73/100] Loss: 127.38434600830078\n",
      "Epoch [74/100] Loss: 125.95126342773438\n",
      "Epoch [75/100] Loss: 124.53580474853516\n",
      "Epoch [76/100] Loss: 123.13773345947266\n",
      "Epoch [77/100] Loss: 121.75672149658203\n",
      "Epoch [78/100] Loss: 120.39253997802734\n",
      "Epoch [79/100] Loss: 119.04486083984375\n",
      "Epoch [80/100] Loss: 117.71343994140625\n",
      "Epoch [81/100] Loss: 116.39800262451172\n",
      "Epoch [82/100] Loss: 115.0983657836914\n",
      "Epoch [83/100] Loss: 113.81425476074219\n",
      "Epoch [84/100] Loss: 112.54540252685547\n",
      "Epoch [85/100] Loss: 111.29164123535156\n",
      "Epoch [86/100] Loss: 110.05269622802734\n",
      "Epoch [87/100] Loss: 108.82842254638672\n",
      "Epoch [88/100] Loss: 107.61849212646484\n",
      "Epoch [89/100] Loss: 106.42282104492188\n",
      "Epoch [90/100] Loss: 105.24116516113281\n",
      "Epoch [91/100] Loss: 104.07331085205078\n",
      "Epoch [92/100] Loss: 102.91913604736328\n",
      "Epoch [93/100] Loss: 101.7783432006836\n",
      "Epoch [94/100] Loss: 100.65084838867188\n",
      "Epoch [95/100] Loss: 99.53646087646484\n",
      "Epoch [96/100] Loss: 98.43498229980469\n",
      "Epoch [97/100] Loss: 97.34629821777344\n",
      "Epoch [98/100] Loss: 96.27020263671875\n",
      "Epoch [99/100] Loss: 95.20650482177734\n",
      "Epoch [100/100] Loss: 94.15515899658203\n",
      "Predicted days_remaining for parent_id 445: 9.879914283752441\n",
      "Training for parent_id 454...\n",
      "Epoch [1/100] Loss: 505.6855163574219\n",
      "Epoch [2/100] Loss: 497.01361083984375\n",
      "Epoch [3/100] Loss: 488.5107727050781\n",
      "Epoch [4/100] Loss: 480.1728210449219\n",
      "Epoch [5/100] Loss: 471.994384765625\n",
      "Epoch [6/100] Loss: 463.9652099609375\n",
      "Epoch [7/100] Loss: 456.0694580078125\n",
      "Epoch [8/100] Loss: 448.29010009765625\n",
      "Epoch [9/100] Loss: 440.61083984375\n",
      "Epoch [10/100] Loss: 433.01727294921875\n",
      "Epoch [11/100] Loss: 425.4991149902344\n",
      "Epoch [12/100] Loss: 418.052978515625\n",
      "Epoch [13/100] Loss: 410.6846618652344\n",
      "Epoch [14/100] Loss: 403.40924072265625\n",
      "Epoch [15/100] Loss: 396.25\n",
      "Epoch [16/100] Loss: 389.23583984375\n",
      "Epoch [17/100] Loss: 382.396728515625\n",
      "Epoch [18/100] Loss: 375.76068115234375\n",
      "Epoch [19/100] Loss: 369.3494567871094\n",
      "Epoch [20/100] Loss: 363.1781311035156\n",
      "Epoch [21/100] Loss: 357.25372314453125\n",
      "Epoch [22/100] Loss: 351.5760803222656\n",
      "Epoch [23/100] Loss: 346.13946533203125\n",
      "Epoch [24/100] Loss: 340.9335632324219\n",
      "Epoch [25/100] Loss: 335.946044921875\n",
      "Epoch [26/100] Loss: 331.163818359375\n",
      "Epoch [27/100] Loss: 326.57415771484375\n",
      "Epoch [28/100] Loss: 322.1649169921875\n",
      "Epoch [29/100] Loss: 317.9246826171875\n",
      "Epoch [30/100] Loss: 313.8424072265625\n",
      "Epoch [31/100] Loss: 309.90692138671875\n",
      "Epoch [32/100] Loss: 306.107177734375\n",
      "Epoch [33/100] Loss: 302.431884765625\n",
      "Epoch [34/100] Loss: 298.8702697753906\n",
      "Epoch [35/100] Loss: 295.41217041015625\n",
      "Epoch [36/100] Loss: 292.0480041503906\n",
      "Epoch [37/100] Loss: 288.7695617675781\n",
      "Epoch [38/100] Loss: 285.56964111328125\n",
      "Epoch [39/100] Loss: 282.4420166015625\n",
      "Epoch [40/100] Loss: 279.3814697265625\n",
      "Epoch [41/100] Loss: 276.383544921875\n",
      "Epoch [42/100] Loss: 273.444580078125\n",
      "Epoch [43/100] Loss: 270.56121826171875\n",
      "Epoch [44/100] Loss: 267.73065185546875\n",
      "Epoch [45/100] Loss: 264.95013427734375\n",
      "Epoch [46/100] Loss: 262.217529296875\n",
      "Epoch [47/100] Loss: 259.53045654296875\n",
      "Epoch [48/100] Loss: 256.8869934082031\n",
      "Epoch [49/100] Loss: 254.2852325439453\n",
      "Epoch [50/100] Loss: 251.72344970703125\n",
      "Epoch [51/100] Loss: 249.20008850097656\n",
      "Epoch [52/100] Loss: 246.71365356445312\n",
      "Epoch [53/100] Loss: 244.2628173828125\n",
      "Epoch [54/100] Loss: 241.84634399414062\n",
      "Epoch [55/100] Loss: 239.46307373046875\n",
      "Epoch [56/100] Loss: 237.1119842529297\n",
      "Epoch [57/100] Loss: 234.79208374023438\n",
      "Epoch [58/100] Loss: 232.50247192382812\n",
      "Epoch [59/100] Loss: 230.2423095703125\n",
      "Epoch [60/100] Loss: 228.0107421875\n",
      "Epoch [61/100] Loss: 225.80703735351562\n",
      "Epoch [62/100] Loss: 223.63046264648438\n",
      "Epoch [63/100] Loss: 221.48031616210938\n",
      "Epoch [64/100] Loss: 219.35598754882812\n",
      "Epoch [65/100] Loss: 217.2567596435547\n",
      "Epoch [66/100] Loss: 215.1820526123047\n",
      "Epoch [67/100] Loss: 213.13128662109375\n",
      "Epoch [68/100] Loss: 211.10391235351562\n",
      "Epoch [69/100] Loss: 209.099365234375\n",
      "Epoch [70/100] Loss: 207.11717224121094\n",
      "Epoch [71/100] Loss: 205.15679931640625\n",
      "Epoch [72/100] Loss: 203.21783447265625\n",
      "Epoch [73/100] Loss: 201.29971313476562\n",
      "Epoch [74/100] Loss: 199.40211486816406\n",
      "Epoch [75/100] Loss: 197.5246124267578\n",
      "Epoch [76/100] Loss: 195.66676330566406\n",
      "Epoch [77/100] Loss: 193.82827758789062\n",
      "Epoch [78/100] Loss: 192.0086669921875\n",
      "Epoch [79/100] Loss: 190.2077178955078\n",
      "Epoch [80/100] Loss: 188.42506408691406\n",
      "Epoch [81/100] Loss: 186.66033935546875\n",
      "Epoch [82/100] Loss: 184.91334533691406\n",
      "Epoch [83/100] Loss: 183.18368530273438\n",
      "Epoch [84/100] Loss: 181.47113037109375\n",
      "Epoch [85/100] Loss: 179.7754364013672\n",
      "Epoch [86/100] Loss: 178.0963592529297\n",
      "Epoch [87/100] Loss: 176.43357849121094\n",
      "Epoch [88/100] Loss: 174.78689575195312\n",
      "Epoch [89/100] Loss: 173.15609741210938\n",
      "Epoch [90/100] Loss: 171.5409698486328\n",
      "Epoch [91/100] Loss: 169.9412841796875\n",
      "Epoch [92/100] Loss: 168.35682678222656\n",
      "Epoch [93/100] Loss: 166.78741455078125\n",
      "Epoch [94/100] Loss: 165.2328643798828\n",
      "Epoch [95/100] Loss: 163.69288635253906\n",
      "Epoch [96/100] Loss: 162.16741943359375\n",
      "Epoch [97/100] Loss: 160.65626525878906\n",
      "Epoch [98/100] Loss: 159.15928649902344\n",
      "Epoch [99/100] Loss: 157.67616271972656\n",
      "Epoch [100/100] Loss: 156.20689392089844\n",
      "Predicted days_remaining for parent_id 454: 9.90459156036377\n",
      "Training for parent_id 458...\n",
      "Epoch [1/100] Loss: 1148.87451171875\n",
      "Epoch [2/100] Loss: 1129.84326171875\n",
      "Epoch [3/100] Loss: 1111.0616455078125\n",
      "Epoch [4/100] Loss: 1092.7545166015625\n",
      "Epoch [5/100] Loss: 1075.1002197265625\n",
      "Epoch [6/100] Loss: 1058.2481689453125\n",
      "Epoch [7/100] Loss: 1042.302001953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Loss: 1027.3035888671875\n",
      "Epoch [9/100] Loss: 1013.244140625\n",
      "Epoch [10/100] Loss: 1000.0895385742188\n",
      "Epoch [11/100] Loss: 987.8002319335938\n",
      "Epoch [12/100] Loss: 976.3355102539062\n",
      "Epoch [13/100] Loss: 965.6482543945312\n",
      "Epoch [14/100] Loss: 955.6840209960938\n",
      "Epoch [15/100] Loss: 946.3790893554688\n",
      "Epoch [16/100] Loss: 937.66357421875\n",
      "Epoch [17/100] Loss: 929.4641723632812\n",
      "Epoch [18/100] Loss: 921.7071533203125\n",
      "Epoch [19/100] Loss: 914.32080078125\n",
      "Epoch [20/100] Loss: 907.23828125\n",
      "Epoch [21/100] Loss: 900.4012451171875\n",
      "Epoch [22/100] Loss: 893.7631225585938\n",
      "Epoch [23/100] Loss: 887.291748046875\n",
      "Epoch [24/100] Loss: 880.9722290039062\n",
      "Epoch [25/100] Loss: 874.80712890625\n",
      "Epoch [26/100] Loss: 868.8118286132812\n",
      "Epoch [27/100] Loss: 863.0084228515625\n",
      "Epoch [28/100] Loss: 857.4158325195312\n",
      "Epoch [29/100] Loss: 852.0435791015625\n",
      "Epoch [30/100] Loss: 846.8873291015625\n",
      "Epoch [31/100] Loss: 841.931396484375\n",
      "Epoch [32/100] Loss: 837.1516723632812\n",
      "Epoch [33/100] Loss: 832.5223388671875\n",
      "Epoch [34/100] Loss: 828.0180053710938\n",
      "Epoch [35/100] Loss: 823.6170654296875\n",
      "Epoch [36/100] Loss: 819.3016357421875\n",
      "Epoch [37/100] Loss: 815.0570068359375\n",
      "Epoch [38/100] Loss: 810.8713989257812\n",
      "Epoch [39/100] Loss: 806.7357177734375\n",
      "Epoch [40/100] Loss: 802.64208984375\n",
      "Epoch [41/100] Loss: 798.5850219726562\n",
      "Epoch [42/100] Loss: 794.5596313476562\n",
      "Epoch [43/100] Loss: 790.56298828125\n",
      "Epoch [44/100] Loss: 786.5925903320312\n",
      "Epoch [45/100] Loss: 782.646728515625\n",
      "Epoch [46/100] Loss: 778.7246704101562\n",
      "Epoch [47/100] Loss: 774.8256225585938\n",
      "Epoch [48/100] Loss: 770.9494018554688\n",
      "Epoch [49/100] Loss: 767.0953979492188\n",
      "Epoch [50/100] Loss: 763.263916015625\n",
      "Epoch [51/100] Loss: 759.4542236328125\n",
      "Epoch [52/100] Loss: 755.6660766601562\n",
      "Epoch [53/100] Loss: 751.8992919921875\n",
      "Epoch [54/100] Loss: 748.1531982421875\n",
      "Epoch [55/100] Loss: 744.4274291992188\n",
      "Epoch [56/100] Loss: 740.721435546875\n",
      "Epoch [57/100] Loss: 737.0347290039062\n",
      "Epoch [58/100] Loss: 733.3667602539062\n",
      "Epoch [59/100] Loss: 729.7169799804688\n",
      "Epoch [60/100] Loss: 726.0845947265625\n",
      "Epoch [61/100] Loss: 722.4690551757812\n",
      "Epoch [62/100] Loss: 718.8699951171875\n",
      "Epoch [63/100] Loss: 715.2864990234375\n",
      "Epoch [64/100] Loss: 711.7178344726562\n",
      "Epoch [65/100] Loss: 708.16357421875\n",
      "Epoch [66/100] Loss: 704.6231079101562\n",
      "Epoch [67/100] Loss: 701.0958862304688\n",
      "Epoch [68/100] Loss: 697.5816650390625\n",
      "Epoch [69/100] Loss: 694.0799560546875\n",
      "Epoch [70/100] Loss: 690.5910034179688\n",
      "Epoch [71/100] Loss: 687.1145629882812\n",
      "Epoch [72/100] Loss: 683.6510620117188\n",
      "Epoch [73/100] Loss: 680.2006225585938\n",
      "Epoch [74/100] Loss: 676.7637329101562\n",
      "Epoch [75/100] Loss: 673.3404541015625\n",
      "Epoch [76/100] Loss: 669.9315185546875\n",
      "Epoch [77/100] Loss: 666.5370483398438\n",
      "Epoch [78/100] Loss: 663.1575317382812\n",
      "Epoch [79/100] Loss: 659.7928466796875\n",
      "Epoch [80/100] Loss: 656.4432983398438\n",
      "Epoch [81/100] Loss: 653.1091918945312\n",
      "Epoch [82/100] Loss: 649.7901611328125\n",
      "Epoch [83/100] Loss: 646.4866333007812\n",
      "Epoch [84/100] Loss: 643.1983642578125\n",
      "Epoch [85/100] Loss: 639.92529296875\n",
      "Epoch [86/100] Loss: 636.6674194335938\n",
      "Epoch [87/100] Loss: 633.4246215820312\n",
      "Epoch [88/100] Loss: 630.1971435546875\n",
      "Epoch [89/100] Loss: 626.9844970703125\n",
      "Epoch [90/100] Loss: 623.7869873046875\n",
      "Epoch [91/100] Loss: 620.6043701171875\n",
      "Epoch [92/100] Loss: 617.4367065429688\n",
      "Epoch [93/100] Loss: 614.2836303710938\n",
      "Epoch [94/100] Loss: 611.1455078125\n",
      "Epoch [95/100] Loss: 608.0220947265625\n",
      "Epoch [96/100] Loss: 604.9132080078125\n",
      "Epoch [97/100] Loss: 601.8189086914062\n",
      "Epoch [98/100] Loss: 598.7391357421875\n",
      "Epoch [99/100] Loss: 595.6737060546875\n",
      "Epoch [100/100] Loss: 592.6227416992188\n",
      "Predicted days_remaining for parent_id 458: 9.767745971679688\n",
      "Training for parent_id 460...\n",
      "Epoch [1/100] Loss: 389.9720458984375\n",
      "Epoch [2/100] Loss: 381.2200927734375\n",
      "Epoch [3/100] Loss: 372.6309814453125\n",
      "Epoch [4/100] Loss: 364.207763671875\n",
      "Epoch [5/100] Loss: 355.9665832519531\n",
      "Epoch [6/100] Loss: 347.9312438964844\n",
      "Epoch [7/100] Loss: 340.1272277832031\n",
      "Epoch [8/100] Loss: 332.5748596191406\n",
      "Epoch [9/100] Loss: 325.28692626953125\n",
      "Epoch [10/100] Loss: 318.2709655761719\n",
      "Epoch [11/100] Loss: 311.5311279296875\n",
      "Epoch [12/100] Loss: 305.0697937011719\n",
      "Epoch [13/100] Loss: 298.88775634765625\n",
      "Epoch [14/100] Loss: 292.9840087890625\n",
      "Epoch [15/100] Loss: 287.35498046875\n",
      "Epoch [16/100] Loss: 281.9948425292969\n",
      "Epoch [17/100] Loss: 276.8957824707031\n",
      "Epoch [18/100] Loss: 272.0482482910156\n",
      "Epoch [19/100] Loss: 267.44183349609375\n",
      "Epoch [20/100] Loss: 263.0658264160156\n",
      "Epoch [21/100] Loss: 258.9091796875\n",
      "Epoch [22/100] Loss: 254.96087646484375\n",
      "Epoch [23/100] Loss: 251.2093505859375\n",
      "Epoch [24/100] Loss: 247.64263916015625\n",
      "Epoch [25/100] Loss: 244.24807739257812\n",
      "Epoch [26/100] Loss: 241.01260375976562\n",
      "Epoch [27/100] Loss: 237.9226837158203\n",
      "Epoch [28/100] Loss: 234.96482849121094\n",
      "Epoch [29/100] Loss: 232.125732421875\n",
      "Epoch [30/100] Loss: 229.3930206298828\n",
      "Epoch [31/100] Loss: 226.7550048828125\n",
      "Epoch [32/100] Loss: 224.2011260986328\n",
      "Epoch [33/100] Loss: 221.7220916748047\n",
      "Epoch [34/100] Loss: 219.30963134765625\n",
      "Epoch [35/100] Loss: 216.95655822753906\n",
      "Epoch [36/100] Loss: 214.6568145751953\n",
      "Epoch [37/100] Loss: 212.40518188476562\n",
      "Epoch [38/100] Loss: 210.19705200195312\n",
      "Epoch [39/100] Loss: 208.0287628173828\n",
      "Epoch [40/100] Loss: 205.8968963623047\n",
      "Epoch [41/100] Loss: 203.79888916015625\n",
      "Epoch [42/100] Loss: 201.7322235107422\n",
      "Epoch [43/100] Loss: 199.6949462890625\n",
      "Epoch [44/100] Loss: 197.68540954589844\n",
      "Epoch [45/100] Loss: 195.7020263671875\n",
      "Epoch [46/100] Loss: 193.7435760498047\n",
      "Epoch [47/100] Loss: 191.80897521972656\n",
      "Epoch [48/100] Loss: 189.8972930908203\n",
      "Epoch [49/100] Loss: 188.00767517089844\n",
      "Epoch [50/100] Loss: 186.13941955566406\n",
      "Epoch [51/100] Loss: 184.29193115234375\n",
      "Epoch [52/100] Loss: 182.4646453857422\n",
      "Epoch [53/100] Loss: 180.6570587158203\n",
      "Epoch [54/100] Loss: 178.86875915527344\n",
      "Epoch [55/100] Loss: 177.09922790527344\n",
      "Epoch [56/100] Loss: 175.34829711914062\n",
      "Epoch [57/100] Loss: 173.6155548095703\n",
      "Epoch [58/100] Loss: 171.9006805419922\n",
      "Epoch [59/100] Loss: 170.203369140625\n",
      "Epoch [60/100] Loss: 168.5233917236328\n",
      "Epoch [61/100] Loss: 166.8604736328125\n",
      "Epoch [62/100] Loss: 165.21437072753906\n",
      "Epoch [63/100] Loss: 163.58485412597656\n",
      "Epoch [64/100] Loss: 161.97164916992188\n",
      "Epoch [65/100] Loss: 160.37461853027344\n",
      "Epoch [66/100] Loss: 158.79348754882812\n",
      "Epoch [67/100] Loss: 157.22813415527344\n",
      "Epoch [68/100] Loss: 155.6782684326172\n",
      "Epoch [69/100] Loss: 154.14366149902344\n",
      "Epoch [70/100] Loss: 152.624267578125\n",
      "Epoch [71/100] Loss: 151.11978149414062\n",
      "Epoch [72/100] Loss: 149.6300811767578\n",
      "Epoch [73/100] Loss: 148.1549530029297\n",
      "Epoch [74/100] Loss: 146.69427490234375\n",
      "Epoch [75/100] Loss: 145.247802734375\n",
      "Epoch [76/100] Loss: 143.8154296875\n",
      "Epoch [77/100] Loss: 142.3970184326172\n",
      "Epoch [78/100] Loss: 140.9923095703125\n",
      "Epoch [79/100] Loss: 139.60125732421875\n",
      "Epoch [80/100] Loss: 138.22369384765625\n",
      "Epoch [81/100] Loss: 136.85939025878906\n",
      "Epoch [82/100] Loss: 135.50830078125\n",
      "Epoch [83/100] Loss: 134.17025756835938\n",
      "Epoch [84/100] Loss: 132.84512329101562\n",
      "Epoch [85/100] Loss: 131.53271484375\n",
      "Epoch [86/100] Loss: 130.2329559326172\n",
      "Epoch [87/100] Loss: 128.94569396972656\n",
      "Epoch [88/100] Loss: 127.67082214355469\n",
      "Epoch [89/100] Loss: 126.40818786621094\n",
      "Epoch [90/100] Loss: 125.15769958496094\n",
      "Epoch [91/100] Loss: 123.91923522949219\n",
      "Epoch [92/100] Loss: 122.69267272949219\n",
      "Epoch [93/100] Loss: 121.47787475585938\n",
      "Epoch [94/100] Loss: 120.2747802734375\n",
      "Epoch [95/100] Loss: 119.08320617675781\n",
      "Epoch [96/100] Loss: 117.9030990600586\n",
      "Epoch [97/100] Loss: 116.73435974121094\n",
      "Epoch [98/100] Loss: 115.57684326171875\n",
      "Epoch [99/100] Loss: 114.43048858642578\n",
      "Epoch [100/100] Loss: 113.29512023925781\n",
      "Predicted days_remaining for parent_id 460: 9.863988876342773\n",
      "Training for parent_id 464...\n",
      "Epoch [1/100] Loss: 839.3311157226562\n",
      "Epoch [2/100] Loss: 826.5357055664062\n",
      "Epoch [3/100] Loss: 813.908935546875\n",
      "Epoch [4/100] Loss: 801.45458984375\n",
      "Epoch [5/100] Loss: 789.2113037109375\n",
      "Epoch [6/100] Loss: 777.2217407226562\n",
      "Epoch [7/100] Loss: 765.5265502929688\n",
      "Epoch [8/100] Loss: 754.16650390625\n",
      "Epoch [9/100] Loss: 743.173828125\n",
      "Epoch [10/100] Loss: 732.5625610351562\n",
      "Epoch [11/100] Loss: 722.3276977539062\n",
      "Epoch [12/100] Loss: 712.4552612304688\n",
      "Epoch [13/100] Loss: 702.9338989257812\n",
      "Epoch [14/100] Loss: 693.7609252929688\n",
      "Epoch [15/100] Loss: 684.9406127929688\n",
      "Epoch [16/100] Loss: 676.4796752929688\n",
      "Epoch [17/100] Loss: 668.3820190429688\n",
      "Epoch [18/100] Loss: 660.6455688476562\n",
      "Epoch [19/100] Loss: 653.2633666992188\n",
      "Epoch [20/100] Loss: 646.2232055664062\n",
      "Epoch [21/100] Loss: 639.5098266601562\n",
      "Epoch [22/100] Loss: 633.1048583984375\n",
      "Epoch [23/100] Loss: 626.9883422851562\n",
      "Epoch [24/100] Loss: 621.138916015625\n",
      "Epoch [25/100] Loss: 615.5352783203125\n",
      "Epoch [26/100] Loss: 610.1565551757812\n",
      "Epoch [27/100] Loss: 604.98291015625\n",
      "Epoch [28/100] Loss: 599.996337890625\n",
      "Epoch [29/100] Loss: 595.1797485351562\n",
      "Epoch [30/100] Loss: 590.5176391601562\n",
      "Epoch [31/100] Loss: 585.99560546875\n",
      "Epoch [32/100] Loss: 581.6001586914062\n",
      "Epoch [33/100] Loss: 577.3194580078125\n",
      "Epoch [34/100] Loss: 573.1421508789062\n",
      "Epoch [35/100] Loss: 569.0582275390625\n",
      "Epoch [36/100] Loss: 565.05859375\n",
      "Epoch [37/100] Loss: 561.1348876953125\n",
      "Epoch [38/100] Loss: 557.280029296875\n",
      "Epoch [39/100] Loss: 553.4873046875\n",
      "Epoch [40/100] Loss: 549.751220703125\n",
      "Epoch [41/100] Loss: 546.06640625\n",
      "Epoch [42/100] Loss: 542.4284057617188\n",
      "Epoch [43/100] Loss: 538.8330688476562\n",
      "Epoch [44/100] Loss: 535.2767944335938\n",
      "Epoch [45/100] Loss: 531.756591796875\n",
      "Epoch [46/100] Loss: 528.2698364257812\n",
      "Epoch [47/100] Loss: 524.8143310546875\n",
      "Epoch [48/100] Loss: 521.388427734375\n",
      "Epoch [49/100] Loss: 517.990966796875\n",
      "Epoch [50/100] Loss: 514.6207275390625\n",
      "Epoch [51/100] Loss: 511.27703857421875\n",
      "Epoch [52/100] Loss: 507.9593200683594\n",
      "Epoch [53/100] Loss: 504.667236328125\n",
      "Epoch [54/100] Loss: 501.4002990722656\n",
      "Epoch [55/100] Loss: 498.1582336425781\n",
      "Epoch [56/100] Loss: 494.9407958984375\n",
      "Epoch [57/100] Loss: 491.74761962890625\n",
      "Epoch [58/100] Loss: 488.57843017578125\n",
      "Epoch [59/100] Loss: 485.4330139160156\n",
      "Epoch [60/100] Loss: 482.31085205078125\n",
      "Epoch [61/100] Loss: 479.2117614746094\n",
      "Epoch [62/100] Loss: 476.13519287109375\n",
      "Epoch [63/100] Loss: 473.0807800292969\n",
      "Epoch [64/100] Loss: 470.0478820800781\n",
      "Epoch [65/100] Loss: 467.0361022949219\n",
      "Epoch [66/100] Loss: 464.044677734375\n",
      "Epoch [67/100] Loss: 461.07342529296875\n",
      "Epoch [68/100] Loss: 458.12158203125\n",
      "Epoch [69/100] Loss: 455.1888122558594\n",
      "Epoch [70/100] Loss: 452.2746276855469\n",
      "Epoch [71/100] Loss: 449.3787536621094\n",
      "Epoch [72/100] Loss: 446.5007629394531\n",
      "Epoch [73/100] Loss: 443.64056396484375\n",
      "Epoch [74/100] Loss: 440.7976379394531\n",
      "Epoch [75/100] Loss: 437.9722595214844\n",
      "Epoch [76/100] Loss: 435.1639709472656\n",
      "Epoch [77/100] Loss: 432.3727722167969\n",
      "Epoch [78/100] Loss: 429.5986633300781\n",
      "Epoch [79/100] Loss: 426.8415222167969\n",
      "Epoch [80/100] Loss: 424.1014099121094\n",
      "Epoch [81/100] Loss: 421.3782043457031\n",
      "Epoch [82/100] Loss: 418.67181396484375\n",
      "Epoch [83/100] Loss: 415.98223876953125\n",
      "Epoch [84/100] Loss: 413.3094482421875\n",
      "Epoch [85/100] Loss: 410.6532287597656\n",
      "Epoch [86/100] Loss: 408.0135803222656\n",
      "Epoch [87/100] Loss: 405.3904113769531\n",
      "Epoch [88/100] Loss: 402.7836608886719\n",
      "Epoch [89/100] Loss: 400.19317626953125\n",
      "Epoch [90/100] Loss: 397.6187744140625\n",
      "Epoch [91/100] Loss: 395.0602722167969\n",
      "Epoch [92/100] Loss: 392.5176086425781\n",
      "Epoch [93/100] Loss: 389.99090576171875\n",
      "Epoch [94/100] Loss: 387.4797058105469\n",
      "Epoch [95/100] Loss: 384.9839172363281\n",
      "Epoch [96/100] Loss: 382.50360107421875\n",
      "Epoch [97/100] Loss: 380.0384521484375\n",
      "Epoch [98/100] Loss: 377.5885009765625\n",
      "Epoch [99/100] Loss: 375.1535949707031\n",
      "Epoch [100/100] Loss: 372.7333984375\n",
      "Predicted days_remaining for parent_id 464: 9.88493537902832\n",
      "Training for parent_id 466...\n",
      "Epoch [1/100] Loss: 690.9436645507812\n",
      "Epoch [2/100] Loss: 678.13671875\n",
      "Epoch [3/100] Loss: 665.7152099609375\n",
      "Epoch [4/100] Loss: 653.6965942382812\n",
      "Epoch [5/100] Loss: 642.0875244140625\n",
      "Epoch [6/100] Loss: 630.876708984375\n",
      "Epoch [7/100] Loss: 620.047607421875\n",
      "Epoch [8/100] Loss: 609.5892944335938\n",
      "Epoch [9/100] Loss: 599.5003662109375\n",
      "Epoch [10/100] Loss: 589.7838134765625\n",
      "Epoch [11/100] Loss: 580.44482421875\n",
      "Epoch [12/100] Loss: 571.4869995117188\n",
      "Epoch [13/100] Loss: 562.9118041992188\n",
      "Epoch [14/100] Loss: 554.718017578125\n",
      "Epoch [15/100] Loss: 546.9015502929688\n",
      "Epoch [16/100] Loss: 539.4551391601562\n",
      "Epoch [17/100] Loss: 532.3685913085938\n",
      "Epoch [18/100] Loss: 525.6280517578125\n",
      "Epoch [19/100] Loss: 519.2169189453125\n",
      "Epoch [20/100] Loss: 513.1151123046875\n",
      "Epoch [21/100] Loss: 507.30078125\n",
      "Epoch [22/100] Loss: 501.75067138671875\n",
      "Epoch [23/100] Loss: 496.4416809082031\n",
      "Epoch [24/100] Loss: 491.351806640625\n",
      "Epoch [25/100] Loss: 486.4607238769531\n",
      "Epoch [26/100] Loss: 481.7500305175781\n",
      "Epoch [27/100] Loss: 477.2033386230469\n",
      "Epoch [28/100] Loss: 472.8058166503906\n",
      "Epoch [29/100] Loss: 468.54412841796875\n",
      "Epoch [30/100] Loss: 464.40618896484375\n",
      "Epoch [31/100] Loss: 460.3808288574219\n",
      "Epoch [32/100] Loss: 456.4580383300781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100] Loss: 452.6283264160156\n",
      "Epoch [34/100] Loss: 448.88348388671875\n",
      "Epoch [35/100] Loss: 445.2156677246094\n",
      "Epoch [36/100] Loss: 441.6180114746094\n",
      "Epoch [37/100] Loss: 438.084228515625\n",
      "Epoch [38/100] Loss: 434.6087341308594\n",
      "Epoch [39/100] Loss: 431.18658447265625\n",
      "Epoch [40/100] Loss: 427.81298828125\n",
      "Epoch [41/100] Loss: 424.4842224121094\n",
      "Epoch [42/100] Loss: 421.1963806152344\n",
      "Epoch [43/100] Loss: 417.94622802734375\n",
      "Epoch [44/100] Loss: 414.73077392578125\n",
      "Epoch [45/100] Loss: 411.5474548339844\n",
      "Epoch [46/100] Loss: 408.39373779296875\n",
      "Epoch [47/100] Loss: 405.2677001953125\n",
      "Epoch [48/100] Loss: 402.16741943359375\n",
      "Epoch [49/100] Loss: 399.091552734375\n",
      "Epoch [50/100] Loss: 396.0385437011719\n",
      "Epoch [51/100] Loss: 393.0079040527344\n",
      "Epoch [52/100] Loss: 389.99908447265625\n",
      "Epoch [53/100] Loss: 387.01177978515625\n",
      "Epoch [54/100] Loss: 384.0460510253906\n",
      "Epoch [55/100] Loss: 381.102294921875\n",
      "Epoch [56/100] Loss: 378.180908203125\n",
      "Epoch [57/100] Loss: 375.2823486328125\n",
      "Epoch [58/100] Loss: 372.4070129394531\n",
      "Epoch [59/100] Loss: 369.55517578125\n",
      "Epoch [60/100] Loss: 366.72698974609375\n",
      "Epoch [61/100] Loss: 363.92254638671875\n",
      "Epoch [62/100] Loss: 361.1416320800781\n",
      "Epoch [63/100] Loss: 358.384033203125\n",
      "Epoch [64/100] Loss: 355.6493835449219\n",
      "Epoch [65/100] Loss: 352.9375305175781\n",
      "Epoch [66/100] Loss: 350.2480163574219\n",
      "Epoch [67/100] Loss: 347.58074951171875\n",
      "Epoch [68/100] Loss: 344.9354248046875\n",
      "Epoch [69/100] Loss: 342.311767578125\n",
      "Epoch [70/100] Loss: 339.7095947265625\n",
      "Epoch [71/100] Loss: 337.1287841796875\n",
      "Epoch [72/100] Loss: 334.56915283203125\n",
      "Epoch [73/100] Loss: 332.0305480957031\n",
      "Epoch [74/100] Loss: 329.5125732421875\n",
      "Epoch [75/100] Loss: 327.01531982421875\n",
      "Epoch [76/100] Loss: 324.538330078125\n",
      "Epoch [77/100] Loss: 322.08148193359375\n",
      "Epoch [78/100] Loss: 319.6445617675781\n",
      "Epoch [79/100] Loss: 317.2271423339844\n",
      "Epoch [80/100] Loss: 314.8292541503906\n",
      "Epoch [81/100] Loss: 312.4504089355469\n",
      "Epoch [82/100] Loss: 310.0904235839844\n",
      "Epoch [83/100] Loss: 307.7491149902344\n",
      "Epoch [84/100] Loss: 305.4262390136719\n",
      "Epoch [85/100] Loss: 303.1214599609375\n",
      "Epoch [86/100] Loss: 300.83465576171875\n",
      "Epoch [87/100] Loss: 298.565673828125\n",
      "Epoch [88/100] Loss: 296.31414794921875\n",
      "Epoch [89/100] Loss: 294.08001708984375\n",
      "Epoch [90/100] Loss: 291.8630676269531\n",
      "Epoch [91/100] Loss: 289.6630554199219\n",
      "Epoch [92/100] Loss: 287.4799499511719\n",
      "Epoch [93/100] Loss: 285.3133544921875\n",
      "Epoch [94/100] Loss: 283.16326904296875\n",
      "Epoch [95/100] Loss: 281.02935791015625\n",
      "Epoch [96/100] Loss: 278.91156005859375\n",
      "Epoch [97/100] Loss: 276.8096618652344\n",
      "Epoch [98/100] Loss: 274.7234802246094\n",
      "Epoch [99/100] Loss: 272.65283203125\n",
      "Epoch [100/100] Loss: 270.59759521484375\n",
      "Predicted days_remaining for parent_id 466: 9.808853149414062\n",
      "Training for parent_id 481...\n",
      "Epoch [1/100] Loss: 974.7008666992188\n",
      "Epoch [2/100] Loss: 953.3301391601562\n",
      "Epoch [3/100] Loss: 932.2813110351562\n",
      "Epoch [4/100] Loss: 911.831298828125\n",
      "Epoch [5/100] Loss: 892.1924438476562\n",
      "Epoch [6/100] Loss: 873.57958984375\n",
      "Epoch [7/100] Loss: 856.1925659179688\n",
      "Epoch [8/100] Loss: 840.1436767578125\n",
      "Epoch [9/100] Loss: 825.44287109375\n",
      "Epoch [10/100] Loss: 812.0218505859375\n",
      "Epoch [11/100] Loss: 799.7659912109375\n",
      "Epoch [12/100] Loss: 788.5416259765625\n",
      "Epoch [13/100] Loss: 778.21728515625\n",
      "Epoch [14/100] Loss: 768.6757202148438\n",
      "Epoch [15/100] Loss: 759.818603515625\n",
      "Epoch [16/100] Loss: 751.56689453125\n",
      "Epoch [17/100] Loss: 743.8585815429688\n",
      "Epoch [18/100] Loss: 736.6431274414062\n",
      "Epoch [19/100] Loss: 729.876220703125\n",
      "Epoch [20/100] Loss: 723.51611328125\n",
      "Epoch [21/100] Loss: 717.5210571289062\n",
      "Epoch [22/100] Loss: 711.8493041992188\n",
      "Epoch [23/100] Loss: 706.4602661132812\n",
      "Epoch [24/100] Loss: 701.3163452148438\n",
      "Epoch [25/100] Loss: 696.383056640625\n",
      "Epoch [26/100] Loss: 691.6301879882812\n",
      "Epoch [27/100] Loss: 687.032470703125\n",
      "Epoch [28/100] Loss: 682.568603515625\n",
      "Epoch [29/100] Loss: 678.2208251953125\n",
      "Epoch [30/100] Loss: 673.9744873046875\n",
      "Epoch [31/100] Loss: 669.8175048828125\n",
      "Epoch [32/100] Loss: 665.7396850585938\n",
      "Epoch [33/100] Loss: 661.73193359375\n",
      "Epoch [34/100] Loss: 657.7865600585938\n",
      "Epoch [35/100] Loss: 653.8966064453125\n",
      "Epoch [36/100] Loss: 650.0562744140625\n",
      "Epoch [37/100] Loss: 646.2601318359375\n",
      "Epoch [38/100] Loss: 642.5035400390625\n",
      "Epoch [39/100] Loss: 638.783203125\n",
      "Epoch [40/100] Loss: 635.0956420898438\n",
      "Epoch [41/100] Loss: 631.4386596679688\n",
      "Epoch [42/100] Loss: 627.8101196289062\n",
      "Epoch [43/100] Loss: 624.2085571289062\n",
      "Epoch [44/100] Loss: 620.6329345703125\n",
      "Epoch [45/100] Loss: 617.0823364257812\n",
      "Epoch [46/100] Loss: 613.5559692382812\n",
      "Epoch [47/100] Loss: 610.0532836914062\n",
      "Epoch [48/100] Loss: 606.5740966796875\n",
      "Epoch [49/100] Loss: 603.1177368164062\n",
      "Epoch [50/100] Loss: 599.6837768554688\n",
      "Epoch [51/100] Loss: 596.2720947265625\n",
      "Epoch [52/100] Loss: 592.8817749023438\n",
      "Epoch [53/100] Loss: 589.5128173828125\n",
      "Epoch [54/100] Loss: 586.1644287109375\n",
      "Epoch [55/100] Loss: 582.836181640625\n",
      "Epoch [56/100] Loss: 579.5272827148438\n",
      "Epoch [57/100] Loss: 576.2373046875\n",
      "Epoch [58/100] Loss: 572.965576171875\n",
      "Epoch [59/100] Loss: 569.7114868164062\n",
      "Epoch [60/100] Loss: 566.4746704101562\n",
      "Epoch [61/100] Loss: 563.2545166015625\n",
      "Epoch [62/100] Loss: 560.0507202148438\n",
      "Epoch [63/100] Loss: 556.8632202148438\n",
      "Epoch [64/100] Loss: 553.6914672851562\n",
      "Epoch [65/100] Loss: 550.5357055664062\n",
      "Epoch [66/100] Loss: 547.3958740234375\n",
      "Epoch [67/100] Loss: 544.2720947265625\n",
      "Epoch [68/100] Loss: 541.164306640625\n",
      "Epoch [69/100] Loss: 538.0725708007812\n",
      "Epoch [70/100] Loss: 534.9970703125\n",
      "Epoch [71/100] Loss: 531.9376831054688\n",
      "Epoch [72/100] Loss: 528.89453125\n",
      "Epoch [73/100] Loss: 525.8674926757812\n",
      "Epoch [74/100] Loss: 522.8563842773438\n",
      "Epoch [75/100] Loss: 519.8612670898438\n",
      "Epoch [76/100] Loss: 516.8818969726562\n",
      "Epoch [77/100] Loss: 513.91845703125\n",
      "Epoch [78/100] Loss: 510.9704895019531\n",
      "Epoch [79/100] Loss: 508.0379333496094\n",
      "Epoch [80/100] Loss: 505.1211242675781\n",
      "Epoch [81/100] Loss: 502.2196350097656\n",
      "Epoch [82/100] Loss: 499.33343505859375\n",
      "Epoch [83/100] Loss: 496.4626159667969\n",
      "Epoch [84/100] Loss: 493.6069030761719\n",
      "Epoch [85/100] Loss: 490.76629638671875\n",
      "Epoch [86/100] Loss: 487.9407958984375\n",
      "Epoch [87/100] Loss: 485.1304931640625\n",
      "Epoch [88/100] Loss: 482.33489990234375\n",
      "Epoch [89/100] Loss: 479.5542907714844\n",
      "Epoch [90/100] Loss: 476.7884521484375\n",
      "Epoch [91/100] Loss: 474.0372619628906\n",
      "Epoch [92/100] Loss: 471.3009033203125\n",
      "Epoch [93/100] Loss: 468.578857421875\n",
      "Epoch [94/100] Loss: 465.8714599609375\n",
      "Epoch [95/100] Loss: 463.1783142089844\n",
      "Epoch [96/100] Loss: 460.4994201660156\n",
      "Epoch [97/100] Loss: 457.8348388671875\n",
      "Epoch [98/100] Loss: 455.18438720703125\n",
      "Epoch [99/100] Loss: 452.54791259765625\n",
      "Epoch [100/100] Loss: 449.9253845214844\n",
      "Predicted days_remaining for parent_id 481: 9.94425106048584\n",
      "Training for parent_id 486...\n",
      "Epoch [1/100] Loss: 427.6656494140625\n",
      "Epoch [2/100] Loss: 418.401123046875\n",
      "Epoch [3/100] Loss: 409.0102233886719\n",
      "Epoch [4/100] Loss: 399.56549072265625\n",
      "Epoch [5/100] Loss: 390.1590881347656\n",
      "Epoch [6/100] Loss: 380.8986511230469\n",
      "Epoch [7/100] Loss: 371.8926086425781\n",
      "Epoch [8/100] Loss: 363.236572265625\n",
      "Epoch [9/100] Loss: 355.0037536621094\n",
      "Epoch [10/100] Loss: 347.24102783203125\n",
      "Epoch [11/100] Loss: 339.9677734375\n",
      "Epoch [12/100] Loss: 333.17974853515625\n",
      "Epoch [13/100] Loss: 326.8536071777344\n",
      "Epoch [14/100] Loss: 320.95367431640625\n",
      "Epoch [15/100] Loss: 315.4386901855469\n",
      "Epoch [16/100] Loss: 310.2669677734375\n",
      "Epoch [17/100] Loss: 305.39923095703125\n",
      "Epoch [18/100] Loss: 300.80059814453125\n",
      "Epoch [19/100] Loss: 296.4407958984375\n",
      "Epoch [20/100] Loss: 292.29425048828125\n",
      "Epoch [21/100] Loss: 288.33953857421875\n",
      "Epoch [22/100] Loss: 284.5584716796875\n",
      "Epoch [23/100] Loss: 280.9357604980469\n",
      "Epoch [24/100] Loss: 277.45782470703125\n",
      "Epoch [25/100] Loss: 274.1125793457031\n",
      "Epoch [26/100] Loss: 270.88897705078125\n",
      "Epoch [27/100] Loss: 267.7767333984375\n",
      "Epoch [28/100] Loss: 264.7665100097656\n",
      "Epoch [29/100] Loss: 261.84979248046875\n",
      "Epoch [30/100] Loss: 259.0191955566406\n",
      "Epoch [31/100] Loss: 256.2679138183594\n",
      "Epoch [32/100] Loss: 253.5897979736328\n",
      "Epoch [33/100] Loss: 250.9794158935547\n",
      "Epoch [34/100] Loss: 248.43153381347656\n",
      "Epoch [35/100] Loss: 245.94102478027344\n",
      "Epoch [36/100] Loss: 243.50302124023438\n",
      "Epoch [37/100] Loss: 241.11282348632812\n",
      "Epoch [38/100] Loss: 238.76560974121094\n",
      "Epoch [39/100] Loss: 236.45709228515625\n",
      "Epoch [40/100] Loss: 234.18313598632812\n",
      "Epoch [41/100] Loss: 231.94024658203125\n",
      "Epoch [42/100] Loss: 229.7252655029297\n",
      "Epoch [43/100] Loss: 227.53564453125\n",
      "Epoch [44/100] Loss: 225.369384765625\n",
      "Epoch [45/100] Loss: 223.22486877441406\n",
      "Epoch [46/100] Loss: 221.10093688964844\n",
      "Epoch [47/100] Loss: 218.9967803955078\n",
      "Epoch [48/100] Loss: 216.91175842285156\n",
      "Epoch [49/100] Loss: 214.84555053710938\n",
      "Epoch [50/100] Loss: 212.79782104492188\n",
      "Epoch [51/100] Loss: 210.76853942871094\n",
      "Epoch [52/100] Loss: 208.75753784179688\n",
      "Epoch [53/100] Loss: 206.76473999023438\n",
      "Epoch [54/100] Loss: 204.79017639160156\n",
      "Epoch [55/100] Loss: 202.8336944580078\n",
      "Epoch [56/100] Loss: 200.89532470703125\n",
      "Epoch [57/100] Loss: 198.9750213623047\n",
      "Epoch [58/100] Loss: 197.07275390625\n",
      "Epoch [59/100] Loss: 195.18844604492188\n",
      "Epoch [60/100] Loss: 193.322021484375\n",
      "Epoch [61/100] Loss: 191.47337341308594\n",
      "Epoch [62/100] Loss: 189.64247131347656\n",
      "Epoch [63/100] Loss: 187.8291778564453\n",
      "Epoch [64/100] Loss: 186.03343200683594\n",
      "Epoch [65/100] Loss: 184.2550048828125\n",
      "Epoch [66/100] Loss: 182.49386596679688\n",
      "Epoch [67/100] Loss: 180.7498321533203\n",
      "Epoch [68/100] Loss: 179.02276611328125\n",
      "Epoch [69/100] Loss: 177.31243896484375\n",
      "Epoch [70/100] Loss: 175.6187744140625\n",
      "Epoch [71/100] Loss: 173.94151306152344\n",
      "Epoch [72/100] Loss: 172.28053283691406\n",
      "Epoch [73/100] Loss: 170.6356201171875\n",
      "Epoch [74/100] Loss: 169.00662231445312\n",
      "Epoch [75/100] Loss: 167.39337158203125\n",
      "Epoch [76/100] Loss: 165.79566955566406\n",
      "Epoch [77/100] Loss: 164.2133331298828\n",
      "Epoch [78/100] Loss: 162.6461944580078\n",
      "Epoch [79/100] Loss: 161.09405517578125\n",
      "Epoch [80/100] Loss: 159.55674743652344\n",
      "Epoch [81/100] Loss: 158.0341339111328\n",
      "Epoch [82/100] Loss: 156.5260009765625\n",
      "Epoch [83/100] Loss: 155.03224182128906\n",
      "Epoch [84/100] Loss: 153.55259704589844\n",
      "Epoch [85/100] Loss: 152.08702087402344\n",
      "Epoch [86/100] Loss: 150.63525390625\n",
      "Epoch [87/100] Loss: 149.19723510742188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100] Loss: 147.77276611328125\n",
      "Epoch [89/100] Loss: 146.36170959472656\n",
      "Epoch [90/100] Loss: 144.96388244628906\n",
      "Epoch [91/100] Loss: 143.57920837402344\n",
      "Epoch [92/100] Loss: 142.20755004882812\n",
      "Epoch [93/100] Loss: 140.8486785888672\n",
      "Epoch [94/100] Loss: 139.50253295898438\n",
      "Epoch [95/100] Loss: 138.16897583007812\n",
      "Epoch [96/100] Loss: 136.84791564941406\n",
      "Epoch [97/100] Loss: 135.5391845703125\n",
      "Epoch [98/100] Loss: 134.24261474609375\n",
      "Epoch [99/100] Loss: 132.9581756591797\n",
      "Epoch [100/100] Loss: 131.68572998046875\n",
      "Predicted days_remaining for parent_id 486: 9.980244636535645\n",
      "Training for parent_id 511...\n",
      "Epoch [1/100] Loss: 172.58827209472656\n",
      "Epoch [2/100] Loss: 165.46456909179688\n",
      "Epoch [3/100] Loss: 158.57730102539062\n",
      "Epoch [4/100] Loss: 152.00473022460938\n",
      "Epoch [5/100] Loss: 145.7820587158203\n",
      "Epoch [6/100] Loss: 139.91676330566406\n",
      "Epoch [7/100] Loss: 134.408203125\n",
      "Epoch [8/100] Loss: 129.25645446777344\n",
      "Epoch [9/100] Loss: 124.4591293334961\n",
      "Epoch [10/100] Loss: 120.0075912475586\n",
      "Epoch [11/100] Loss: 115.88681030273438\n",
      "Epoch [12/100] Loss: 112.07698822021484\n",
      "Epoch [13/100] Loss: 108.55543518066406\n",
      "Epoch [14/100] Loss: 105.29826354980469\n",
      "Epoch [15/100] Loss: 102.28153228759766\n",
      "Epoch [16/100] Loss: 99.48200988769531\n",
      "Epoch [17/100] Loss: 96.87744903564453\n",
      "Epoch [18/100] Loss: 94.44684600830078\n",
      "Epoch [19/100] Loss: 92.17073822021484\n",
      "Epoch [20/100] Loss: 90.03153991699219\n",
      "Epoch [21/100] Loss: 88.01380920410156\n",
      "Epoch [22/100] Loss: 86.1042709350586\n",
      "Epoch [23/100] Loss: 84.29190826416016\n",
      "Epoch [24/100] Loss: 82.56774139404297\n",
      "Epoch [25/100] Loss: 80.92445373535156\n",
      "Epoch [26/100] Loss: 79.35596466064453\n",
      "Epoch [27/100] Loss: 77.85688781738281\n",
      "Epoch [28/100] Loss: 76.42217254638672\n",
      "Epoch [29/100] Loss: 75.04690551757812\n",
      "Epoch [30/100] Loss: 73.72615814208984\n",
      "Epoch [31/100] Loss: 72.45510864257812\n",
      "Epoch [32/100] Loss: 71.2291259765625\n",
      "Epoch [33/100] Loss: 70.04396057128906\n",
      "Epoch [34/100] Loss: 68.89572143554688\n",
      "Epoch [35/100] Loss: 67.7810287475586\n",
      "Epoch [36/100] Loss: 66.69696044921875\n",
      "Epoch [37/100] Loss: 65.64096069335938\n",
      "Epoch [38/100] Loss: 64.61087799072266\n",
      "Epoch [39/100] Loss: 63.60490417480469\n",
      "Epoch [40/100] Loss: 62.62144470214844\n",
      "Epoch [41/100] Loss: 61.65915298461914\n",
      "Epoch [42/100] Loss: 60.71690368652344\n",
      "Epoch [43/100] Loss: 59.79362869262695\n",
      "Epoch [44/100] Loss: 58.88847351074219\n",
      "Epoch [45/100] Loss: 58.000648498535156\n",
      "Epoch [46/100] Loss: 57.12946319580078\n",
      "Epoch [47/100] Loss: 56.27431869506836\n",
      "Epoch [48/100] Loss: 55.434635162353516\n",
      "Epoch [49/100] Loss: 54.60990905761719\n",
      "Epoch [50/100] Loss: 53.799720764160156\n",
      "Epoch [51/100] Loss: 53.0036506652832\n",
      "Epoch [52/100] Loss: 52.22133255004883\n",
      "Epoch [53/100] Loss: 51.45243835449219\n",
      "Epoch [54/100] Loss: 50.6966552734375\n",
      "Epoch [55/100] Loss: 49.9537239074707\n",
      "Epoch [56/100] Loss: 49.223419189453125\n",
      "Epoch [57/100] Loss: 48.505496978759766\n",
      "Epoch [58/100] Loss: 47.79975891113281\n",
      "Epoch [59/100] Loss: 47.10603332519531\n",
      "Epoch [60/100] Loss: 46.42414474487305\n",
      "Epoch [61/100] Loss: 45.753936767578125\n",
      "Epoch [62/100] Loss: 45.09524917602539\n",
      "Epoch [63/100] Loss: 44.447975158691406\n",
      "Epoch [64/100] Loss: 43.81193542480469\n",
      "Epoch [65/100] Loss: 43.1870231628418\n",
      "Epoch [66/100] Loss: 42.57310485839844\n",
      "Epoch [67/100] Loss: 41.970035552978516\n",
      "Epoch [68/100] Loss: 41.3776741027832\n",
      "Epoch [69/100] Loss: 40.795894622802734\n",
      "Epoch [70/100] Loss: 40.22456359863281\n",
      "Epoch [71/100] Loss: 39.66353225708008\n",
      "Epoch [72/100] Loss: 39.11267852783203\n",
      "Epoch [73/100] Loss: 38.57184600830078\n",
      "Epoch [74/100] Loss: 38.040897369384766\n",
      "Epoch [75/100] Loss: 37.51969909667969\n",
      "Epoch [76/100] Loss: 37.00810623168945\n",
      "Epoch [77/100] Loss: 36.506004333496094\n",
      "Epoch [78/100] Loss: 36.01322937011719\n",
      "Epoch [79/100] Loss: 35.5296745300293\n",
      "Epoch [80/100] Loss: 35.05516052246094\n",
      "Epoch [81/100] Loss: 34.58961868286133\n",
      "Epoch [82/100] Loss: 34.132843017578125\n",
      "Epoch [83/100] Loss: 33.68476104736328\n",
      "Epoch [84/100] Loss: 33.24522399902344\n",
      "Epoch [85/100] Loss: 32.81410217285156\n",
      "Epoch [86/100] Loss: 32.391273498535156\n",
      "Epoch [87/100] Loss: 31.97661018371582\n",
      "Epoch [88/100] Loss: 31.56998062133789\n",
      "Epoch [89/100] Loss: 31.171289443969727\n",
      "Epoch [90/100] Loss: 30.780391693115234\n",
      "Epoch [91/100] Loss: 30.397174835205078\n",
      "Epoch [92/100] Loss: 30.021522521972656\n",
      "Epoch [93/100] Loss: 29.65330696105957\n",
      "Epoch [94/100] Loss: 29.292436599731445\n",
      "Epoch [95/100] Loss: 28.938764572143555\n",
      "Epoch [96/100] Loss: 28.59220314025879\n",
      "Epoch [97/100] Loss: 28.25262451171875\n",
      "Epoch [98/100] Loss: 27.91993522644043\n",
      "Epoch [99/100] Loss: 27.594018936157227\n",
      "Epoch [100/100] Loss: 27.274768829345703\n",
      "Predicted days_remaining for parent_id 511: 9.210994720458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Define the data extraction function for each patient\n",
    "def get_data_for_parent(df, parent_id):\n",
    "    data = df[df['parent_id'] == parent_id]\n",
    "    features = data.drop(columns=['parent_id', 'days_remaining'])\n",
    "    target = data['days_remaining'].values\n",
    "    features_tensor = torch.tensor(features.values).float().unsqueeze(0)  # Add batch dimension\n",
    "    target_tensor = torch.tensor(target).float().unsqueeze(0)  # Add batch dimension\n",
    "    return features_tensor, target_tensor\n",
    "\n",
    "# Initialize the RNN model\n",
    "input_size = 80  # Number of input features\n",
    "hidden_size = 64\n",
    "output_size = 1  # We are predicting a single value (e.g., days_remaining)\n",
    "model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 100  # You can adjust this number\n",
    "\n",
    "# Loop through each parent_id and train the model\n",
    "unique_parent_ids = df['parent_id'].unique()\n",
    "\n",
    "# Save the models\n",
    "models = []\n",
    "\n",
    "for parent_id in unique_parent_ids:\n",
    "    print(f\"Training for parent_id {parent_id}...\")\n",
    "    \n",
    "    # Get the data for the current parent_id\n",
    "    features, target = get_data_for_parent(df, parent_id)\n",
    "    \n",
    "    # Initialize a new model for each parent_id\n",
    "    model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop for each parent_id\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item()}\")\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = f\"Models/model_parent_{parent_id}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    models.append(model)  # Add model to the list\n",
    "\n",
    "    # Optionally, evaluate the model or predict on test data after training\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "        print(f\"Predicted days_remaining for parent_id {parent_id}: {predictions.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "07433b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each model and create the ensemble prediction function\n",
    "def ensemble_prediction(models, input_data):\n",
    "    predictions = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model in models:\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            output = model(input_data)\n",
    "            predictions.append(output.item())  # Store the prediction\n",
    "        \n",
    "    # Average the predictions to form the final prediction\n",
    "    return sum(predictions) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "77d4890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model MSE on test data: 690.8509070225107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## preparing the test data\n",
    "test_data = df.drop(\"parent_id\",axis=1)\n",
    "X = test_data.drop(\"days_remaining\",axis=1)\n",
    "y= test_data.days_remaining\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Evaluate ensemble model on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    # Prepare each test sample with the correct dimensions\n",
    "    test_sample = torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0)  # Shape (1, 1, input_size)\n",
    "    true_value = y_test.iloc[i]  # Actual value for the test sample\n",
    "    \n",
    "    # Get the ensemble prediction\n",
    "    prediction = ensemble_prediction(models, test_sample)\n",
    "    \n",
    "    # Append to lists for MSE calculation\n",
    "    y_true.append(true_value)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Ensemble Model MSE on test data: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8a3b998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ced34ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 80])\n"
     ]
    }
   ],
   "source": [
    "a,b = get_data_for_parent(df,522)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d9c11e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output without batch dimension: torch.Size([1, 8, 64])\n",
      "Output with batch dimension: torch.Size([1, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example sequence data\n",
    "sequence_data = torch.randn(8, 80)  # Shape: [8, 80] (1 sequence, 8 time steps, 80 features)\n",
    "sequence_data_with_batch = sequence_data.unsqueeze(0)  # Shape: [1, 8, 80] (1 batch, 8 time steps, 80 features)\n",
    "\n",
    "# RNN model\n",
    "rnn = nn.RNN(input_size=80, hidden_size=64, num_layers=1)\n",
    "\n",
    "# Forward pass with data without batch dimension\n",
    "output_without_batch, _ = rnn(sequence_data.unsqueeze(0))  # Shape: [1, 8, 64], hidden state is discarded\n",
    "print(f\"Output without batch dimension: {output_without_batch.shape}\")\n",
    "\n",
    "# Forward pass with data with batch dimension\n",
    "output_with_batch, _ = rnn(sequence_data_with_batch)  # Shape: [1, 8, 64], hidden state is discarded\n",
    "print(f\"Output with batch dimension: {output_with_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bca1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
