{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2185cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c5e9f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_id</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>highest_mean_arterial_pressure</th>\n",
       "      <th>lowest_mean_arterial_pressure</th>\n",
       "      <th>highest_heart_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Bilateral Ground Glass</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Unilateral Consolidation</th>\n",
       "      <th>Bilateral Ground Glass Opacities</th>\n",
       "      <th>Bilateral consolidationinfiltration</th>\n",
       "      <th>Pulmonary Embolism</th>\n",
       "      <th>Scarring or Fibrosis</th>\n",
       "      <th>days_remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>85.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>67.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>36.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>79.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>121.333333</td>\n",
       "      <td>61.833333</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>72.333333</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    parent_id  systolic_blood_pressure  diastolic_blood_pressure  heart_rate  \\\n",
       "0           6               127.000000                 76.000000        68.0   \n",
       "1           6                97.000000                 60.000000        68.0   \n",
       "2           6               140.000000                 68.000000        72.0   \n",
       "3           6               108.000000                 63.000000        98.0   \n",
       "4           6               126.000000                 77.000000        68.0   \n",
       "5           6               120.000000                 82.000000        76.0   \n",
       "6           6               128.000000                 62.000000        73.0   \n",
       "7           6               116.000000                 74.000000        82.0   \n",
       "8          14               107.000000                 64.000000        60.0   \n",
       "9          14               153.000000                 72.000000        56.0   \n",
       "10         14               168.000000                 66.000000        58.0   \n",
       "11         14               160.000000                 60.000000        58.0   \n",
       "12         14               188.000000                 75.000000        60.0   \n",
       "13         14               172.000000                 65.000000        68.0   \n",
       "14         14               153.000000                 60.000000        62.0   \n",
       "15         14               132.000000                 79.000000        83.0   \n",
       "16         15               157.000000                100.000000        89.0   \n",
       "17         15               168.000000                 93.000000        85.0   \n",
       "18         15               161.000000                100.000000        88.0   \n",
       "19         15               139.000000                 87.000000        91.0   \n",
       "20         15               139.000000                 89.000000        67.0   \n",
       "21         15               131.000000                 89.000000        73.0   \n",
       "22         15               133.000000                 85.000000        67.0   \n",
       "23         15               103.000000                 66.000000        66.0   \n",
       "24         25               105.000000                 56.000000        84.0   \n",
       "25         25               118.000000                 66.000000        60.0   \n",
       "26         25               115.000000                 61.000000        79.0   \n",
       "27         25               121.333333                 61.833333        90.0   \n",
       "28         25               133.000000                 71.000000        91.0   \n",
       "29         25               116.000000                 59.000000        92.0   \n",
       "\n",
       "    respiratory_rate  oxygen_saturation  temperature  \\\n",
       "0               19.0               95.0         36.6   \n",
       "1               22.0               98.0         36.4   \n",
       "2               22.0               99.0         36.5   \n",
       "3               22.0               95.0         36.5   \n",
       "4               24.0               98.0         36.5   \n",
       "5               24.0               93.0         36.6   \n",
       "6               26.0               93.0         36.8   \n",
       "7               20.0               96.0         37.6   \n",
       "8               20.0               94.0         36.9   \n",
       "9               18.0               93.0         36.7   \n",
       "10              18.0               94.0         36.3   \n",
       "11              18.0               93.0         36.4   \n",
       "12              20.0               93.0         36.6   \n",
       "13              18.0               99.0         36.5   \n",
       "14              18.0               95.0         36.8   \n",
       "15              20.0               97.0         37.0   \n",
       "16              18.0               95.0         36.8   \n",
       "17              20.0               93.0         37.6   \n",
       "18              18.0               93.0         37.1   \n",
       "19              17.0               84.0         36.9   \n",
       "20              18.0               92.0         36.8   \n",
       "21              19.0               93.0         36.5   \n",
       "22              19.0               93.0         36.9   \n",
       "23              20.0               95.0         37.3   \n",
       "24              22.0               88.0         37.7   \n",
       "25              30.0               97.0         36.7   \n",
       "26              34.0               95.0         36.3   \n",
       "27              30.0               96.0         37.3   \n",
       "28              30.0               98.0         37.0   \n",
       "29              30.0               96.0         37.2   \n",
       "\n",
       "    highest_mean_arterial_pressure  lowest_mean_arterial_pressure  \\\n",
       "0                        92.000000                      80.000000   \n",
       "1                        71.000000                      67.000000   \n",
       "2                        84.000000                      84.000000   \n",
       "3                        77.000000                      77.000000   \n",
       "4                        92.000000                      92.000000   \n",
       "5                        93.000000                      93.000000   \n",
       "6                        93.000000                      81.000000   \n",
       "7                        86.000000                      66.000000   \n",
       "8                         0.000000                       0.000000   \n",
       "9                         0.000000                       0.000000   \n",
       "10                        0.000000                       0.000000   \n",
       "11                        0.000000                       0.000000   \n",
       "12                        0.000000                       0.000000   \n",
       "13                        0.000000                       0.000000   \n",
       "14                        0.000000                       0.000000   \n",
       "15                        0.000000                       0.000000   \n",
       "16                        0.000000                       0.000000   \n",
       "17                        0.000000                       0.000000   \n",
       "18                        0.000000                       0.000000   \n",
       "19                        0.000000                       0.000000   \n",
       "20                        0.000000                       0.000000   \n",
       "21                        0.000000                       0.000000   \n",
       "22                        0.000000                       0.000000   \n",
       "23                        0.000000                       0.000000   \n",
       "24                       78.000000                      71.000000   \n",
       "25                      107.000000                      72.000000   \n",
       "26                      122.000000                      71.000000   \n",
       "27                       93.166667                      72.333333   \n",
       "28                       91.000000                      75.000000   \n",
       "29                       82.000000                      76.000000   \n",
       "\n",
       "    highest_heart_rate  ...  Bilateral Ground Glass  Cardiomegaly  Edema  \\\n",
       "0                 77.0  ...                       0             0      0   \n",
       "1                 77.0  ...                       0             0      0   \n",
       "2                 97.0  ...                       0             0      0   \n",
       "3                107.0  ...                       0             0      0   \n",
       "4                 77.0  ...                       0             0      0   \n",
       "5                 82.0  ...                       0             0      0   \n",
       "6                 78.0  ...                       0             0      0   \n",
       "7                 94.0  ...                       0             0      0   \n",
       "8                 64.0  ...                       0             0      0   \n",
       "9                 83.0  ...                       0             0      0   \n",
       "10                58.0  ...                       0             0      0   \n",
       "11                60.0  ...                       0             0      0   \n",
       "12                81.0  ...                       0             0      0   \n",
       "13                70.0  ...                       0             0      0   \n",
       "14                73.0  ...                       0             0      0   \n",
       "15                83.0  ...                       0             0      0   \n",
       "16                89.0  ...                       0             0      0   \n",
       "17                94.0  ...                       0             0      0   \n",
       "18                92.0  ...                       0             0      0   \n",
       "19                91.0  ...                       0             0      0   \n",
       "20                75.0  ...                       0             0      0   \n",
       "21                73.0  ...                       0             0      0   \n",
       "22                73.0  ...                       0             0      0   \n",
       "23                78.0  ...                       0             0      0   \n",
       "24                92.0  ...                       0             1      0   \n",
       "25               114.0  ...                       0             1      0   \n",
       "26                90.0  ...                       0             1      0   \n",
       "27                98.0  ...                       0             1      0   \n",
       "28               101.0  ...                       0             0      0   \n",
       "29                99.0  ...                       0             1      0   \n",
       "\n",
       "    Effusion  Unilateral Consolidation  Bilateral Ground Glass Opacities  \\\n",
       "0          0                         0                                 0   \n",
       "1          0                         0                                 0   \n",
       "2          0                         0                                 0   \n",
       "3          1                         0                                 0   \n",
       "4          0                         0                                 1   \n",
       "5          0                         0                                 0   \n",
       "6          0                         0                                 0   \n",
       "7          0                         0                                 0   \n",
       "8          0                         0                                 0   \n",
       "9          0                         0                                 0   \n",
       "10         0                         0                                 0   \n",
       "11         0                         0                                 0   \n",
       "12         0                         0                                 0   \n",
       "13         0                         0                                 0   \n",
       "14         0                         0                                 0   \n",
       "15         0                         0                                 0   \n",
       "16         0                         0                                 0   \n",
       "17         0                         0                                 0   \n",
       "18         0                         0                                 0   \n",
       "19         0                         0                                 0   \n",
       "20         0                         0                                 0   \n",
       "21         0                         0                                 0   \n",
       "22         1                         1                                 0   \n",
       "23         0                         0                                 0   \n",
       "24         0                         0                                 0   \n",
       "25         0                         0                                 0   \n",
       "26         0                         0                                 0   \n",
       "27         0                         0                                 0   \n",
       "28         0                         0                                 0   \n",
       "29         0                         0                                 0   \n",
       "\n",
       "    Bilateral consolidationinfiltration  Pulmonary Embolism  \\\n",
       "0                                     0                   0   \n",
       "1                                     0                   0   \n",
       "2                                     0                   0   \n",
       "3                                     0                   0   \n",
       "4                                     1                   0   \n",
       "5                                     0                   0   \n",
       "6                                     0                   0   \n",
       "7                                     0                   0   \n",
       "8                                     0                   0   \n",
       "9                                     0                   0   \n",
       "10                                    0                   0   \n",
       "11                                    0                   0   \n",
       "12                                    0                   0   \n",
       "13                                    0                   0   \n",
       "14                                    0                   0   \n",
       "15                                    0                   0   \n",
       "16                                    0                   0   \n",
       "17                                    0                   0   \n",
       "18                                    0                   0   \n",
       "19                                    0                   0   \n",
       "20                                    0                   0   \n",
       "21                                    0                   0   \n",
       "22                                    0                   0   \n",
       "23                                    0                   0   \n",
       "24                                    0                   0   \n",
       "25                                    0                   0   \n",
       "26                                    0                   0   \n",
       "27                                    0                   0   \n",
       "28                                    0                   0   \n",
       "29                                    0                   0   \n",
       "\n",
       "    Scarring or Fibrosis  days_remaining  \n",
       "0                      0              31  \n",
       "1                      0              30  \n",
       "2                      0              29  \n",
       "3                      0              28  \n",
       "4                      0              27  \n",
       "5                      0              26  \n",
       "6                      0              25  \n",
       "7                      0              18  \n",
       "8                      0              32  \n",
       "9                      0              31  \n",
       "10                     0              30  \n",
       "11                     0              29  \n",
       "12                     0              28  \n",
       "13                     0              27  \n",
       "14                     0              26  \n",
       "15                     0              19  \n",
       "16                     0              33  \n",
       "17                     1              32  \n",
       "18                     0              31  \n",
       "19                     0              30  \n",
       "20                     0              29  \n",
       "21                     0              28  \n",
       "22                     0              27  \n",
       "23                     0              20  \n",
       "24                     0              71  \n",
       "25                     0              70  \n",
       "26                     0              69  \n",
       "27                     0              68  \n",
       "28                     0              67  \n",
       "29                     0              66  \n",
       "\n",
       "[30 rows x 67 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/df_over_14.csv\")\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "df['days_remaining'] = df['hospital_length_of_stay']-df['day']\n",
    "df = df.drop(['hospital_length_of_stay','day'],axis=1)\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588c299",
   "metadata": {},
   "source": [
    "### Standard scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11ce369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the dataset: 784\n",
      "Train dataset rows: 584\n",
      "Test dataset rows: 200\n",
      "Parent ID: 14, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 15, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 25, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 40, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 41, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 50, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 51, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 61, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 74, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 80, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 82, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 88, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 91, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 99, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 103, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 105, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 113, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 123, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 124, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 144, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 150, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 166, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 168, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 179, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 199, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 211, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 215, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 218, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 234, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 238, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 256, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 265, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 277, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 281, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 289, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 302, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 310, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 312, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 315, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 316, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 317, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 320, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 321, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 326, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 332, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 340, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 346, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 348, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 360, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 362, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 366, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 371, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 376, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 377, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 387, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 392, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 393, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 395, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 404, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 406, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 412, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 425, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 430, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 433, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 434, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 437, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 445, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 458, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 460, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 464, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 481, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 486, Features Shape: (8, 65), Target Shape: (8,)\n",
      "Parent ID: 511, Features Shape: (8, 65), Target Shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example function to process parent_id data\n",
    "def get_data_for_parent(df, parent_id):\n",
    "    # Filter rows belonging to the specific parent_id\n",
    "    parent_data = df[df['parent_id'] == parent_id]\n",
    "    \n",
    "    # Example: Separate features and target\n",
    "    features = parent_data.drop(columns=['parent_id', 'days_remaining'])\n",
    "    target = parent_data['days_remaining']\n",
    "    \n",
    "    return features, target\n",
    "\n",
    "# Assuming `df` is already loaded into your environment\n",
    "# Replace this with the actual dataset loading code if necessary\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# List of binary columns\n",
    "columns_binary = [\n",
    "    'intubated', 'cardiac_arrest', 'arrested_time', 'major_cardiac_events', \n",
    "    'clinically_diagnosed_infections', 'mechanical_ventilation', 'antiarrhythmic_therapies', \n",
    "    'renal_replacement_therapy_dialysis', 'cardiovascular_mechanical_support', 'echocardiogram', \n",
    "    'chest_x_ray', 'chest_ct', 'head_ct', 'antimicrobial', 'anticoagulation', 'steroid',\n",
    "    'Bilateral Consolidation', 'Bilateral Ground Glass', 'Cardiomegaly', 'Edema', 'Effusion', \n",
    "    'Pneumothorax', 'Unilateral Consolidation', 'Unilateral Ground Glass', 'Bilateral Ground Glass Opacities',\n",
    "    'Bilateral consolidationinfiltration', 'Subarachnoid Hemorrhage', 'Subdural Hemorrhage',\n",
    "    'Emphysematous or Bronchiectasis changes', 'Emphysematous or Bronchiectatic changes', \n",
    "    'Pulmonary Embolism', 'Scarring or Fibrosis', 'Unilateral Ground Glass Opacities', \n",
    "    'Unilateral consolidationinfiltration'\n",
    "]\n",
    "\n",
    "# Define columns to exclude from scaling\n",
    "columns_to_exclude = ['parent_id', 'days_remaining'] + columns_binary\n",
    "\n",
    "# Select the columns to scale\n",
    "columns_to_scale = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform only the columns that need scaling\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Step 2: Split based on unique parent_id\n",
    "unique_parent_ids = df['parent_id'].unique()\n",
    "\n",
    "# Randomly shuffle and split into 75% train and 25% test\n",
    "train_parent_ids, test_parent_ids = train_test_split(unique_parent_ids, test_size=0.25, random_state=42)\n",
    "\n",
    "# Filter the dataset based on the split\n",
    "train_df = df[df['parent_id'].isin(train_parent_ids)]\n",
    "test_df = df[df['parent_id'].isin(test_parent_ids)]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total rows in the dataset: {len(df)}\")\n",
    "print(f\"Train dataset rows: {len(train_df)}\")\n",
    "print(f\"Test dataset rows: {len(test_df)}\")\n",
    "\n",
    "# Example: Pass 75% (train_df) to the next function\n",
    "for parent_id in train_df['parent_id'].unique():\n",
    "    features, target = get_data_for_parent(train_df, parent_id)\n",
    "    # Example: Print the features and target shapes\n",
    "    print(f\"Parent ID: {parent_id}, Features Shape: {features.shape}, Target Shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b4486",
   "metadata": {},
   "source": [
    "### Constant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6b3698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleRNN class for many-to-many prediction\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RNN outputs all timesteps, not just the last one\n",
    "        out, _ = self.rnn(x)  # RNN returns output for all timesteps\n",
    "        out = self.fc(out)  # Pass through the fully connected layer for each timestep\n",
    "        return out\n",
    "\n",
    "# Define the data extraction function (many-to-many)\n",
    "def get_data_for_parent(df, parent_id):\n",
    "    data = df[df['parent_id'] == parent_id]\n",
    "    features = data.drop(columns=['parent_id', 'days_remaining'])\n",
    "    target = data['days_remaining'].values\n",
    "    \n",
    "    # Convert to tensors\n",
    "    features_tensor = torch.tensor(features.values).float().unsqueeze(0)  # Add batch dimension\n",
    "    target_tensor = torch.tensor(target).float().unsqueeze(0)  # Add batch dimension\n",
    "    return features_tensor, target_tensor\n",
    "\n",
    "# Define the ensemble prediction function\n",
    "def ensemble_prediction(models, input_data):\n",
    "    predictions = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for model in models:\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            output = model(input_data)\n",
    "            predictions.append(output.squeeze().tolist())  # Store the predictions for each timestep\n",
    "        \n",
    "    # Average the predictions to form the final prediction at each timestep\n",
    "    return torch.mean(torch.tensor(predictions), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeab56",
   "metadata": {},
   "source": [
    "### RNN model with standard scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b73628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for parent_id 14...\n",
      "Epoch [10/1000], Loss: 685.3375\n",
      "Epoch [20/1000], Loss: 595.1287\n",
      "Epoch [30/1000], Loss: 537.0505\n",
      "Epoch [40/1000], Loss: 496.5052\n",
      "Epoch [50/1000], Loss: 461.9673\n",
      "Epoch [60/1000], Loss: 430.5992\n",
      "Epoch [70/1000], Loss: 401.6237\n",
      "Epoch [80/1000], Loss: 374.6862\n",
      "Epoch [90/1000], Loss: 349.5580\n",
      "Epoch [100/1000], Loss: 326.0681\n",
      "Epoch [110/1000], Loss: 304.0797\n",
      "Epoch [120/1000], Loss: 283.4786\n",
      "Epoch [130/1000], Loss: 264.1672\n",
      "Epoch [140/1000], Loss: 246.0609\n",
      "Epoch [150/1000], Loss: 229.0853\n",
      "Epoch [160/1000], Loss: 213.1735\n",
      "Epoch [170/1000], Loss: 198.2652\n",
      "Epoch [180/1000], Loss: 184.3047\n",
      "Epoch [190/1000], Loss: 171.2409\n",
      "Epoch [200/1000], Loss: 159.0253\n",
      "Epoch [210/1000], Loss: 147.6130\n",
      "Epoch [220/1000], Loss: 136.9610\n",
      "Epoch [230/1000], Loss: 127.0288\n",
      "Epoch [240/1000], Loss: 117.7773\n",
      "Epoch [250/1000], Loss: 109.1698\n",
      "Epoch [260/1000], Loss: 101.1706\n",
      "Epoch [270/1000], Loss: 93.7458\n",
      "Epoch [280/1000], Loss: 86.8627\n",
      "Epoch [290/1000], Loss: 80.4900\n",
      "Epoch [300/1000], Loss: 74.5976\n",
      "Epoch [310/1000], Loss: 69.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/1000], Loss: 64.1398\n",
      "Epoch [330/1000], Loss: 59.5201\n",
      "Epoch [340/1000], Loss: 55.2725\n",
      "Epoch [350/1000], Loss: 51.3726\n",
      "Epoch [360/1000], Loss: 47.7973\n",
      "Epoch [370/1000], Loss: 44.5245\n",
      "Epoch [380/1000], Loss: 41.5332\n",
      "Epoch [390/1000], Loss: 38.8035\n",
      "Epoch [400/1000], Loss: 36.3162\n",
      "Epoch [410/1000], Loss: 34.0535\n",
      "Epoch [420/1000], Loss: 31.9984\n",
      "Epoch [430/1000], Loss: 30.1347\n",
      "Epoch [440/1000], Loss: 28.4476\n",
      "Epoch [450/1000], Loss: 26.9226\n",
      "Epoch [460/1000], Loss: 25.5465\n",
      "Epoch [470/1000], Loss: 24.3068\n",
      "Epoch [480/1000], Loss: 23.1919\n",
      "Epoch [490/1000], Loss: 22.1909\n",
      "Epoch [500/1000], Loss: 21.2936\n",
      "Epoch [510/1000], Loss: 20.4906\n",
      "Epoch [520/1000], Loss: 19.7734\n",
      "Epoch [530/1000], Loss: 19.1337\n",
      "Epoch [540/1000], Loss: 18.5643\n",
      "Epoch [550/1000], Loss: 18.0582\n",
      "Epoch [560/1000], Loss: 17.6092\n",
      "Epoch [570/1000], Loss: 17.2115\n",
      "Epoch [580/1000], Loss: 16.8599\n",
      "Epoch [590/1000], Loss: 16.5495\n",
      "Epoch [600/1000], Loss: 16.2761\n",
      "Epoch [610/1000], Loss: 16.0356\n",
      "Epoch [620/1000], Loss: 15.8244\n",
      "Epoch [630/1000], Loss: 15.6394\n",
      "Epoch [640/1000], Loss: 15.4774\n",
      "Epoch [650/1000], Loss: 15.3360\n",
      "Epoch [660/1000], Loss: 15.2127\n",
      "Epoch [670/1000], Loss: 15.1054\n",
      "Epoch [680/1000], Loss: 15.0121\n",
      "Epoch [690/1000], Loss: 14.9312\n",
      "Epoch [700/1000], Loss: 14.8612\n",
      "Epoch [710/1000], Loss: 14.8007\n",
      "Epoch [720/1000], Loss: 14.7485\n",
      "Epoch [730/1000], Loss: 14.7035\n",
      "Epoch [740/1000], Loss: 14.6649\n",
      "Epoch [750/1000], Loss: 14.6317\n",
      "Epoch [760/1000], Loss: 14.6033\n",
      "Epoch [770/1000], Loss: 14.5790\n",
      "Epoch [780/1000], Loss: 14.5583\n",
      "Epoch [790/1000], Loss: 14.5406\n",
      "Epoch [800/1000], Loss: 14.5255\n",
      "Epoch [810/1000], Loss: 14.5128\n",
      "Epoch [820/1000], Loss: 14.5020\n",
      "Epoch [830/1000], Loss: 14.4928\n",
      "Epoch [840/1000], Loss: 14.4851\n",
      "Epoch [850/1000], Loss: 14.4785\n",
      "Epoch [860/1000], Loss: 14.4730\n",
      "Epoch [870/1000], Loss: 14.4684\n",
      "Epoch [880/1000], Loss: 14.4646\n",
      "Epoch [890/1000], Loss: 14.4613\n",
      "Epoch [900/1000], Loss: 14.4586\n",
      "Epoch [910/1000], Loss: 14.4563\n",
      "Epoch [920/1000], Loss: 14.4544\n",
      "Epoch [930/1000], Loss: 14.4529\n",
      "Epoch [940/1000], Loss: 14.4516\n",
      "Epoch [950/1000], Loss: 14.4505\n",
      "Epoch [960/1000], Loss: 14.4496\n",
      "Epoch [970/1000], Loss: 14.4488\n",
      "Epoch [980/1000], Loss: 14.4482\n",
      "Epoch [990/1000], Loss: 14.4477\n",
      "Epoch [1000/1000], Loss: 14.4473\n",
      "Predicted days_remaining for parent_id 14: [27.47165870666504, 27.74399757385254, 27.744779586791992, 27.744253158569336, 27.742090225219727, 27.74391746520996, 27.744327545166016, 27.73830223083496]\n",
      "Training for parent_id 15...\n",
      "Epoch [10/1000], Loss: 732.0754\n",
      "Epoch [20/1000], Loss: 650.6321\n",
      "Epoch [30/1000], Loss: 598.4760\n",
      "Epoch [40/1000], Loss: 558.9742\n",
      "Epoch [50/1000], Loss: 524.4089\n",
      "Epoch [60/1000], Loss: 492.4570\n",
      "Epoch [70/1000], Loss: 462.4687\n",
      "Epoch [80/1000], Loss: 434.2377\n",
      "Epoch [90/1000], Loss: 407.6326\n",
      "Epoch [100/1000], Loss: 382.5493\n",
      "Epoch [110/1000], Loss: 358.8910\n",
      "Epoch [120/1000], Loss: 336.5674\n",
      "Epoch [130/1000], Loss: 315.4974\n",
      "Epoch [140/1000], Loss: 295.6092\n",
      "Epoch [150/1000], Loss: 276.8384\n",
      "Epoch [160/1000], Loss: 259.1270\n",
      "Epoch [170/1000], Loss: 242.4215\n",
      "Epoch [180/1000], Loss: 226.6728\n",
      "Epoch [190/1000], Loss: 211.8348\n",
      "Epoch [200/1000], Loss: 197.8643\n",
      "Epoch [210/1000], Loss: 184.7202\n",
      "Epoch [220/1000], Loss: 172.3637\n",
      "Epoch [230/1000], Loss: 160.7572\n",
      "Epoch [240/1000], Loss: 149.8652\n",
      "Epoch [250/1000], Loss: 139.6534\n",
      "Epoch [260/1000], Loss: 130.0885\n",
      "Epoch [270/1000], Loss: 121.1388\n",
      "Epoch [280/1000], Loss: 112.7735\n",
      "Epoch [290/1000], Loss: 104.9629\n",
      "Epoch [300/1000], Loss: 97.6782\n",
      "Epoch [310/1000], Loss: 90.8919\n",
      "Epoch [320/1000], Loss: 84.5771\n",
      "Epoch [330/1000], Loss: 78.7081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [340/1000], Loss: 73.2599\n",
      "Epoch [350/1000], Loss: 68.2087\n",
      "Epoch [360/1000], Loss: 63.5313\n",
      "Epoch [370/1000], Loss: 59.2056\n",
      "Epoch [380/1000], Loss: 55.2103\n",
      "Epoch [390/1000], Loss: 51.5250\n",
      "Epoch [400/1000], Loss: 48.1301\n",
      "Epoch [410/1000], Loss: 45.0069\n",
      "Epoch [420/1000], Loss: 42.1375\n",
      "Epoch [430/1000], Loss: 39.5049\n",
      "Epoch [440/1000], Loss: 37.0930\n",
      "Epoch [450/1000], Loss: 34.8862\n",
      "Epoch [460/1000], Loss: 32.8699\n",
      "Epoch [470/1000], Loss: 31.0304\n",
      "Epoch [480/1000], Loss: 29.3545\n",
      "Epoch [490/1000], Loss: 27.8298\n",
      "Epoch [500/1000], Loss: 26.4447\n",
      "Epoch [510/1000], Loss: 25.1882\n",
      "Epoch [520/1000], Loss: 24.0501\n",
      "Epoch [530/1000], Loss: 23.0207\n",
      "Epoch [540/1000], Loss: 22.0910\n",
      "Epoch [550/1000], Loss: 21.2526\n",
      "Epoch [560/1000], Loss: 20.4976\n",
      "Epoch [570/1000], Loss: 19.8188\n",
      "Epoch [580/1000], Loss: 19.2094\n",
      "Epoch [590/1000], Loss: 18.6631\n",
      "Epoch [600/1000], Loss: 18.1741\n",
      "Epoch [610/1000], Loss: 17.7371\n",
      "Epoch [620/1000], Loss: 17.3471\n",
      "Epoch [630/1000], Loss: 16.9996\n",
      "Epoch [640/1000], Loss: 16.6904\n",
      "Epoch [650/1000], Loss: 16.4158\n",
      "Epoch [660/1000], Loss: 16.1722\n",
      "Epoch [670/1000], Loss: 15.9565\n",
      "Epoch [680/1000], Loss: 15.7657\n",
      "Epoch [690/1000], Loss: 15.5973\n",
      "Epoch [700/1000], Loss: 15.4489\n",
      "Epoch [710/1000], Loss: 15.3182\n",
      "Epoch [720/1000], Loss: 15.2034\n",
      "Epoch [730/1000], Loss: 15.1026\n",
      "Epoch [740/1000], Loss: 15.0144\n",
      "Epoch [750/1000], Loss: 14.9371\n",
      "Epoch [760/1000], Loss: 14.8697\n",
      "Epoch [770/1000], Loss: 14.8109\n",
      "Epoch [780/1000], Loss: 14.7597\n",
      "Epoch [790/1000], Loss: 14.7152\n",
      "Epoch [800/1000], Loss: 14.6766\n",
      "Epoch [810/1000], Loss: 14.6431\n",
      "Epoch [820/1000], Loss: 14.6141\n",
      "Epoch [830/1000], Loss: 14.5891\n",
      "Epoch [840/1000], Loss: 14.5675\n",
      "Epoch [850/1000], Loss: 14.5489\n",
      "Epoch [860/1000], Loss: 14.5329\n",
      "Epoch [870/1000], Loss: 14.5192\n",
      "Epoch [880/1000], Loss: 14.5075\n",
      "Epoch [890/1000], Loss: 14.4974\n",
      "Epoch [900/1000], Loss: 14.4888\n",
      "Epoch [910/1000], Loss: 14.4815\n",
      "Epoch [920/1000], Loss: 14.4752\n",
      "Epoch [930/1000], Loss: 14.4699\n",
      "Epoch [940/1000], Loss: 14.4654\n",
      "Epoch [950/1000], Loss: 14.4616\n",
      "Epoch [960/1000], Loss: 14.4584\n",
      "Epoch [970/1000], Loss: 14.4556\n",
      "Epoch [980/1000], Loss: 14.4533\n",
      "Epoch [990/1000], Loss: 14.4514\n",
      "Epoch [1000/1000], Loss: 14.4497\n",
      "Predicted days_remaining for parent_id 15: [28.49004364013672, 28.687084197998047, 28.68719482421875, 28.686628341674805, 28.685230255126953, 28.68478012084961, 28.685232162475586, 28.682287216186523]\n",
      "Training for parent_id 25...\n",
      "Epoch [10/1000], Loss: 4200.3330\n",
      "Epoch [20/1000], Loss: 3999.2378\n",
      "Epoch [30/1000], Loss: 3860.1152\n",
      "Epoch [40/1000], Loss: 3750.4458\n",
      "Epoch [50/1000], Loss: 3652.2896\n",
      "Epoch [60/1000], Loss: 3559.3628\n",
      "Epoch [70/1000], Loss: 3470.3064\n",
      "Epoch [80/1000], Loss: 3384.6221\n",
      "Epoch [90/1000], Loss: 3301.9302\n",
      "Epoch [100/1000], Loss: 3221.8948\n",
      "Epoch [110/1000], Loss: 3144.2341\n",
      "Epoch [120/1000], Loss: 3068.7214\n",
      "Epoch [130/1000], Loss: 2995.1775\n",
      "Epoch [140/1000], Loss: 2923.4607\n",
      "Epoch [150/1000], Loss: 2853.4568\n",
      "Epoch [160/1000], Loss: 2785.0708\n",
      "Epoch [170/1000], Loss: 2718.2249\n",
      "Epoch [180/1000], Loss: 2652.8530\n",
      "Epoch [190/1000], Loss: 2588.8975\n",
      "Epoch [200/1000], Loss: 2526.3088\n",
      "Epoch [210/1000], Loss: 2465.0425\n",
      "Epoch [220/1000], Loss: 2405.0598\n",
      "Epoch [230/1000], Loss: 2346.3250\n",
      "Epoch [240/1000], Loss: 2288.8062\n",
      "Epoch [250/1000], Loss: 2232.4727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000], Loss: 2177.2979\n",
      "Epoch [270/1000], Loss: 2123.2559\n",
      "Epoch [280/1000], Loss: 2070.3230\n",
      "Epoch [290/1000], Loss: 2018.4760\n",
      "Epoch [300/1000], Loss: 1967.6941\n",
      "Epoch [310/1000], Loss: 1917.9567\n",
      "Epoch [320/1000], Loss: 1869.2439\n",
      "Epoch [330/1000], Loss: 1821.5378\n",
      "Epoch [340/1000], Loss: 1774.8198\n",
      "Epoch [350/1000], Loss: 1729.0731\n",
      "Epoch [360/1000], Loss: 1684.2803\n",
      "Epoch [370/1000], Loss: 1640.4254\n",
      "Epoch [380/1000], Loss: 1597.4928\n",
      "Epoch [390/1000], Loss: 1555.4663\n",
      "Epoch [400/1000], Loss: 1514.3318\n",
      "Epoch [410/1000], Loss: 1474.0741\n",
      "Epoch [420/1000], Loss: 1434.6787\n",
      "Epoch [430/1000], Loss: 1396.1318\n",
      "Epoch [440/1000], Loss: 1358.4194\n",
      "Epoch [450/1000], Loss: 1321.5281\n",
      "Epoch [460/1000], Loss: 1285.4443\n",
      "Epoch [470/1000], Loss: 1250.1549\n",
      "Epoch [480/1000], Loss: 1215.6472\n",
      "Epoch [490/1000], Loss: 1181.9081\n",
      "Epoch [500/1000], Loss: 1148.9254\n",
      "Epoch [510/1000], Loss: 1116.6863\n",
      "Epoch [520/1000], Loss: 1085.1790\n",
      "Epoch [530/1000], Loss: 1054.3911\n",
      "Epoch [540/1000], Loss: 1024.3109\n",
      "Epoch [550/1000], Loss: 994.9266\n",
      "Epoch [560/1000], Loss: 966.2264\n",
      "Epoch [570/1000], Loss: 938.1989\n",
      "Epoch [580/1000], Loss: 910.8329\n",
      "Epoch [590/1000], Loss: 884.1166\n",
      "Epoch [600/1000], Loss: 858.0396\n",
      "Epoch [610/1000], Loss: 832.5904\n",
      "Epoch [620/1000], Loss: 807.7582\n",
      "Epoch [630/1000], Loss: 783.5323\n",
      "Epoch [640/1000], Loss: 759.9019\n",
      "Epoch [650/1000], Loss: 736.8564\n",
      "Epoch [660/1000], Loss: 714.3857\n",
      "Epoch [670/1000], Loss: 692.4794\n",
      "Epoch [680/1000], Loss: 671.1268\n",
      "Epoch [690/1000], Loss: 650.3180\n",
      "Epoch [700/1000], Loss: 630.0435\n",
      "Epoch [710/1000], Loss: 610.2925\n",
      "Epoch [720/1000], Loss: 591.0557\n",
      "Epoch [730/1000], Loss: 572.3232\n",
      "Epoch [740/1000], Loss: 554.0856\n",
      "Epoch [750/1000], Loss: 536.3330\n",
      "Epoch [760/1000], Loss: 519.0562\n",
      "Epoch [770/1000], Loss: 502.2458\n",
      "Epoch [780/1000], Loss: 485.8930\n",
      "Epoch [790/1000], Loss: 469.9881\n",
      "Epoch [800/1000], Loss: 454.5223\n",
      "Epoch [810/1000], Loss: 439.4867\n",
      "Epoch [820/1000], Loss: 424.8723\n",
      "Epoch [830/1000], Loss: 410.6705\n",
      "Epoch [840/1000], Loss: 396.8729\n",
      "Epoch [850/1000], Loss: 383.4706\n",
      "Epoch [860/1000], Loss: 370.4554\n",
      "Epoch [870/1000], Loss: 357.8189\n",
      "Epoch [880/1000], Loss: 345.5529\n",
      "Epoch [890/1000], Loss: 333.6493\n",
      "Epoch [900/1000], Loss: 322.0999\n",
      "Epoch [910/1000], Loss: 310.8970\n",
      "Epoch [920/1000], Loss: 300.0326\n",
      "Epoch [930/1000], Loss: 289.4991\n",
      "Epoch [940/1000], Loss: 279.2888\n",
      "Epoch [950/1000], Loss: 269.3941\n",
      "Epoch [960/1000], Loss: 259.8079\n",
      "Epoch [970/1000], Loss: 250.5225\n",
      "Epoch [980/1000], Loss: 241.5310\n",
      "Epoch [990/1000], Loss: 232.8261\n",
      "Epoch [1000/1000], Loss: 224.4010\n",
      "Predicted days_remaining for parent_id 25: [52.24020767211914, 52.29594421386719, 52.29522705078125, 52.29603576660156, 52.294761657714844, 52.29607009887695, 52.294593811035156, 52.294952392578125]\n",
      "Training for parent_id 40...\n",
      "Epoch [10/1000], Loss: 184.6973\n",
      "Epoch [20/1000], Loss: 140.9374\n",
      "Epoch [30/1000], Loss: 112.5984\n",
      "Epoch [40/1000], Loss: 94.8803\n",
      "Epoch [50/1000], Loss: 81.4769\n",
      "Epoch [60/1000], Loss: 70.3550\n",
      "Epoch [70/1000], Loss: 60.9029\n",
      "Epoch [80/1000], Loss: 52.8574\n",
      "Epoch [90/1000], Loss: 46.0308\n",
      "Epoch [100/1000], Loss: 40.2645\n",
      "Epoch [110/1000], Loss: 35.4196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000], Loss: 31.3726\n",
      "Epoch [130/1000], Loss: 28.0137\n",
      "Epoch [140/1000], Loss: 25.2452\n",
      "Epoch [150/1000], Loss: 22.9799\n",
      "Epoch [160/1000], Loss: 21.1405\n",
      "Epoch [170/1000], Loss: 19.6590\n",
      "Epoch [180/1000], Loss: 18.4754\n",
      "Epoch [190/1000], Loss: 17.5379\n",
      "Epoch [200/1000], Loss: 16.8018\n",
      "Epoch [210/1000], Loss: 16.2288\n",
      "Epoch [220/1000], Loss: 15.7868\n",
      "Epoch [230/1000], Loss: 15.4488\n",
      "Epoch [240/1000], Loss: 15.1927\n",
      "Epoch [250/1000], Loss: 15.0004\n",
      "Epoch [260/1000], Loss: 14.8572\n",
      "Epoch [270/1000], Loss: 14.7516\n",
      "Epoch [280/1000], Loss: 14.6743\n",
      "Epoch [290/1000], Loss: 14.6182\n",
      "Epoch [300/1000], Loss: 14.5777\n",
      "Epoch [310/1000], Loss: 14.5488\n",
      "Epoch [320/1000], Loss: 14.5283\n",
      "Epoch [330/1000], Loss: 14.5137\n",
      "Epoch [340/1000], Loss: 14.5034\n",
      "Epoch [350/1000], Loss: 14.4961\n",
      "Epoch [360/1000], Loss: 14.4910\n",
      "Epoch [370/1000], Loss: 14.4873\n",
      "Epoch [380/1000], Loss: 14.4845\n",
      "Epoch [390/1000], Loss: 14.4825\n",
      "Epoch [400/1000], Loss: 14.4809\n",
      "Epoch [410/1000], Loss: 14.4796\n",
      "Epoch [420/1000], Loss: 14.4786\n",
      "Epoch [430/1000], Loss: 14.4777\n",
      "Epoch [440/1000], Loss: 14.4768\n",
      "Epoch [450/1000], Loss: 14.4761\n",
      "Epoch [460/1000], Loss: 14.4753\n",
      "Epoch [470/1000], Loss: 14.4747\n",
      "Epoch [480/1000], Loss: 14.4740\n",
      "Epoch [490/1000], Loss: 14.4734\n",
      "Epoch [500/1000], Loss: 14.4728\n",
      "Epoch [510/1000], Loss: 14.4722\n",
      "Epoch [520/1000], Loss: 14.4716\n",
      "Epoch [530/1000], Loss: 14.4710\n",
      "Epoch [540/1000], Loss: 14.4704\n",
      "Epoch [550/1000], Loss: 14.4699\n",
      "Epoch [560/1000], Loss: 14.4694\n",
      "Epoch [570/1000], Loss: 14.4688\n",
      "Epoch [580/1000], Loss: 14.4683\n",
      "Epoch [590/1000], Loss: 14.4678\n",
      "Epoch [600/1000], Loss: 14.4673\n",
      "Epoch [610/1000], Loss: 14.4669\n",
      "Epoch [620/1000], Loss: 14.4664\n",
      "Epoch [630/1000], Loss: 14.4659\n",
      "Epoch [640/1000], Loss: 14.4655\n",
      "Epoch [650/1000], Loss: 14.4650\n",
      "Epoch [660/1000], Loss: 14.4646\n",
      "Epoch [670/1000], Loss: 14.4642\n",
      "Epoch [680/1000], Loss: 14.4638\n",
      "Epoch [690/1000], Loss: 14.4634\n",
      "Epoch [700/1000], Loss: 14.4630\n",
      "Epoch [710/1000], Loss: 14.4626\n",
      "Epoch [720/1000], Loss: 14.4622\n",
      "Epoch [730/1000], Loss: 14.4618\n",
      "Epoch [740/1000], Loss: 14.4615\n",
      "Epoch [750/1000], Loss: 14.4611\n",
      "Epoch [760/1000], Loss: 14.4608\n",
      "Epoch [770/1000], Loss: 14.4604\n",
      "Epoch [780/1000], Loss: 14.4601\n",
      "Epoch [790/1000], Loss: 14.4598\n",
      "Epoch [800/1000], Loss: 14.4594\n",
      "Epoch [810/1000], Loss: 14.4591\n",
      "Epoch [820/1000], Loss: 14.4588\n",
      "Epoch [830/1000], Loss: 14.4585\n",
      "Epoch [840/1000], Loss: 14.4582\n",
      "Epoch [850/1000], Loss: 14.4579\n",
      "Epoch [860/1000], Loss: 14.4576\n",
      "Epoch [870/1000], Loss: 14.4573\n",
      "Epoch [880/1000], Loss: 14.4571\n",
      "Epoch [890/1000], Loss: 14.4568\n",
      "Epoch [900/1000], Loss: 14.4565\n",
      "Epoch [910/1000], Loss: 14.4563\n",
      "Epoch [920/1000], Loss: 14.4560\n",
      "Epoch [930/1000], Loss: 14.4558\n",
      "Epoch [940/1000], Loss: 14.4555\n",
      "Epoch [950/1000], Loss: 14.4553\n",
      "Epoch [960/1000], Loss: 14.4551\n",
      "Epoch [970/1000], Loss: 14.4548\n",
      "Epoch [980/1000], Loss: 14.4546\n",
      "Epoch [990/1000], Loss: 14.4544\n",
      "Epoch [1000/1000], Loss: 14.4542\n",
      "Predicted days_remaining for parent_id 40: [14.4088773727417, 14.7999849319458, 14.797624588012695, 14.798199653625488, 14.801937103271484, 14.801739692687988, 14.801495552062988, 14.789543151855469]\n",
      "Training for parent_id 41...\n",
      "Epoch [10/1000], Loss: 978.8786\n",
      "Epoch [20/1000], Loss: 878.7805\n",
      "Epoch [30/1000], Loss: 813.0870\n",
      "Epoch [40/1000], Loss: 764.2104\n",
      "Epoch [50/1000], Loss: 721.2754\n",
      "Epoch [60/1000], Loss: 681.2618\n",
      "Epoch [70/1000], Loss: 643.2634\n",
      "Epoch [80/1000], Loss: 607.3246\n",
      "Epoch [90/1000], Loss: 573.4797\n",
      "Epoch [100/1000], Loss: 541.5812\n",
      "Epoch [110/1000], Loss: 511.4740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000], Loss: 483.0133\n",
      "Epoch [130/1000], Loss: 456.0718\n",
      "Epoch [140/1000], Loss: 430.5411\n",
      "Epoch [150/1000], Loss: 406.3288\n",
      "Epoch [160/1000], Loss: 383.3558\n",
      "Epoch [170/1000], Loss: 361.5527\n",
      "Epoch [180/1000], Loss: 340.8584\n",
      "Epoch [190/1000], Loss: 321.2177\n",
      "Epoch [200/1000], Loss: 302.5804\n",
      "Epoch [210/1000], Loss: 284.9005\n",
      "Epoch [220/1000], Loss: 268.1346\n",
      "Epoch [230/1000], Loss: 252.2424\n",
      "Epoch [240/1000], Loss: 237.1861\n",
      "Epoch [250/1000], Loss: 222.9294\n",
      "Epoch [260/1000], Loss: 209.4377\n",
      "Epoch [270/1000], Loss: 196.6784\n",
      "Epoch [280/1000], Loss: 184.6196\n",
      "Epoch [290/1000], Loss: 173.2311\n",
      "Epoch [300/1000], Loss: 162.4832\n",
      "Epoch [310/1000], Loss: 152.3480\n",
      "Epoch [320/1000], Loss: 142.7978\n",
      "Epoch [330/1000], Loss: 133.8063\n",
      "Epoch [340/1000], Loss: 125.3479\n",
      "Epoch [350/1000], Loss: 117.3977\n",
      "Epoch [360/1000], Loss: 109.9318\n",
      "Epoch [370/1000], Loss: 102.9270\n",
      "Epoch [380/1000], Loss: 96.3608\n",
      "Epoch [390/1000], Loss: 90.2114\n",
      "Epoch [400/1000], Loss: 84.4578\n",
      "Epoch [410/1000], Loss: 79.0798\n",
      "Epoch [420/1000], Loss: 74.0576\n",
      "Epoch [430/1000], Loss: 69.3725\n",
      "Epoch [440/1000], Loss: 65.0062\n",
      "Epoch [450/1000], Loss: 60.9410\n",
      "Epoch [460/1000], Loss: 57.1602\n",
      "Epoch [470/1000], Loss: 53.6474\n",
      "Epoch [480/1000], Loss: 50.3872\n",
      "Epoch [490/1000], Loss: 47.3646\n",
      "Epoch [500/1000], Loss: 44.5652\n",
      "Epoch [510/1000], Loss: 41.9754\n",
      "Epoch [520/1000], Loss: 39.5821\n",
      "Epoch [530/1000], Loss: 37.3729\n",
      "Epoch [540/1000], Loss: 35.3357\n",
      "Epoch [550/1000], Loss: 33.4594\n",
      "Epoch [560/1000], Loss: 31.7331\n",
      "Epoch [570/1000], Loss: 30.1467\n",
      "Epoch [580/1000], Loss: 28.6905\n",
      "Epoch [590/1000], Loss: 27.3553\n",
      "Epoch [600/1000], Loss: 26.1325\n",
      "Epoch [610/1000], Loss: 25.0139\n",
      "Epoch [620/1000], Loss: 23.9919\n",
      "Epoch [630/1000], Loss: 23.0591\n",
      "Epoch [640/1000], Loss: 22.2088\n",
      "Epoch [650/1000], Loss: 21.4346\n",
      "Epoch [660/1000], Loss: 20.7306\n",
      "Epoch [670/1000], Loss: 20.0911\n",
      "Epoch [680/1000], Loss: 19.5109\n",
      "Epoch [690/1000], Loss: 18.9851\n",
      "Epoch [700/1000], Loss: 18.5093\n",
      "Epoch [710/1000], Loss: 18.0791\n",
      "Epoch [720/1000], Loss: 17.6907\n",
      "Epoch [730/1000], Loss: 17.3404\n",
      "Epoch [740/1000], Loss: 17.0249\n",
      "Epoch [750/1000], Loss: 16.7411\n",
      "Epoch [760/1000], Loss: 16.4861\n",
      "Epoch [770/1000], Loss: 16.2573\n",
      "Epoch [780/1000], Loss: 16.0522\n",
      "Epoch [790/1000], Loss: 15.8685\n",
      "Epoch [800/1000], Loss: 15.7043\n",
      "Epoch [810/1000], Loss: 15.5577\n",
      "Epoch [820/1000], Loss: 15.4270\n",
      "Epoch [830/1000], Loss: 15.3105\n",
      "Epoch [840/1000], Loss: 15.2068\n",
      "Epoch [850/1000], Loss: 15.1147\n",
      "Epoch [860/1000], Loss: 15.0330\n",
      "Epoch [870/1000], Loss: 14.9606\n",
      "Epoch [880/1000], Loss: 14.8964\n",
      "Epoch [890/1000], Loss: 14.8398\n",
      "Epoch [900/1000], Loss: 14.7897\n",
      "Epoch [910/1000], Loss: 14.7455\n",
      "Epoch [920/1000], Loss: 14.7066\n",
      "Epoch [930/1000], Loss: 14.6724\n",
      "Epoch [940/1000], Loss: 14.6424\n",
      "Epoch [950/1000], Loss: 14.6160\n",
      "Epoch [960/1000], Loss: 14.5929\n",
      "Epoch [970/1000], Loss: 14.5727\n",
      "Epoch [980/1000], Loss: 14.5550\n",
      "Epoch [990/1000], Loss: 14.5396\n",
      "Epoch [1000/1000], Loss: 14.5261\n",
      "Predicted days_remaining for parent_id 41: [32.30625915527344, 32.483402252197266, 32.48186111450195, 32.482975006103516, 32.483882904052734, 32.48272705078125, 32.484153747558594, 32.476409912109375]\n",
      "Training for parent_id 50...\n",
      "Epoch [10/1000], Loss: 995.6984\n",
      "Epoch [20/1000], Loss: 891.3024\n",
      "Epoch [30/1000], Loss: 814.1157\n",
      "Epoch [40/1000], Loss: 758.8296\n",
      "Epoch [50/1000], Loss: 713.4080\n",
      "Epoch [60/1000], Loss: 672.5437\n",
      "Epoch [70/1000], Loss: 634.6783\n",
      "Epoch [80/1000], Loss: 599.2430\n",
      "Epoch [90/1000], Loss: 565.9152\n",
      "Epoch [100/1000], Loss: 534.4726\n",
      "Epoch [110/1000], Loss: 504.7479\n",
      "Epoch [120/1000], Loss: 476.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000], Loss: 449.9378\n",
      "Epoch [140/1000], Loss: 424.6461\n",
      "Epoch [150/1000], Loss: 400.6494\n",
      "Epoch [160/1000], Loss: 377.8755\n",
      "Epoch [170/1000], Loss: 356.2599\n",
      "Epoch [180/1000], Loss: 335.7447\n",
      "Epoch [190/1000], Loss: 316.2769\n",
      "Epoch [200/1000], Loss: 297.8077\n",
      "Epoch [210/1000], Loss: 280.2919\n",
      "Epoch [220/1000], Loss: 263.6871\n",
      "Epoch [230/1000], Loss: 247.9534\n",
      "Epoch [240/1000], Loss: 233.0529\n",
      "Epoch [250/1000], Loss: 218.9496\n",
      "Epoch [260/1000], Loss: 205.6091\n",
      "Epoch [270/1000], Loss: 192.9986\n",
      "Epoch [280/1000], Loss: 181.0862\n",
      "Epoch [290/1000], Loss: 169.8414\n",
      "Epoch [300/1000], Loss: 159.2349\n",
      "Epoch [310/1000], Loss: 149.2383\n",
      "Epoch [320/1000], Loss: 139.8240\n",
      "Epoch [330/1000], Loss: 130.9654\n",
      "Epoch [340/1000], Loss: 122.6370\n",
      "Epoch [350/1000], Loss: 114.8137\n",
      "Epoch [360/1000], Loss: 107.4714\n",
      "Epoch [370/1000], Loss: 100.5869\n",
      "Epoch [380/1000], Loss: 94.1376\n",
      "Epoch [390/1000], Loss: 88.1018\n",
      "Epoch [400/1000], Loss: 82.4583\n",
      "Epoch [410/1000], Loss: 77.1867\n",
      "Epoch [420/1000], Loss: 72.2675\n",
      "Epoch [430/1000], Loss: 67.6817\n",
      "Epoch [440/1000], Loss: 63.4111\n",
      "Epoch [450/1000], Loss: 59.4380\n",
      "Epoch [460/1000], Loss: 55.7457\n",
      "Epoch [470/1000], Loss: 52.3179\n",
      "Epoch [480/1000], Loss: 49.1390\n",
      "Epoch [490/1000], Loss: 46.1942\n",
      "Epoch [500/1000], Loss: 43.4691\n",
      "Epoch [510/1000], Loss: 40.9502\n",
      "Epoch [520/1000], Loss: 38.6244\n",
      "Epoch [530/1000], Loss: 36.4793\n",
      "Epoch [540/1000], Loss: 34.5031\n",
      "Epoch [550/1000], Loss: 32.6845\n",
      "Epoch [560/1000], Loss: 31.0129\n",
      "Epoch [570/1000], Loss: 29.4782\n",
      "Epoch [580/1000], Loss: 28.0707\n",
      "Epoch [590/1000], Loss: 26.7815\n",
      "Epoch [600/1000], Loss: 25.6020\n",
      "Epoch [610/1000], Loss: 24.5240\n",
      "Epoch [620/1000], Loss: 23.5401\n",
      "Epoch [630/1000], Loss: 22.6430\n",
      "Epoch [640/1000], Loss: 21.8261\n",
      "Epoch [650/1000], Loss: 21.0832\n",
      "Epoch [660/1000], Loss: 20.4082\n",
      "Epoch [670/1000], Loss: 19.7958\n",
      "Epoch [680/1000], Loss: 19.2407\n",
      "Epoch [690/1000], Loss: 18.7384\n",
      "Epoch [700/1000], Loss: 18.2842\n",
      "Epoch [710/1000], Loss: 17.8741\n",
      "Epoch [720/1000], Loss: 17.5042\n",
      "Epoch [730/1000], Loss: 17.1711\n",
      "Epoch [740/1000], Loss: 16.8714\n",
      "Epoch [750/1000], Loss: 16.6021\n",
      "Epoch [760/1000], Loss: 16.3604\n",
      "Epoch [770/1000], Loss: 16.1437\n",
      "Epoch [780/1000], Loss: 15.9498\n",
      "Epoch [790/1000], Loss: 15.7764\n",
      "Epoch [800/1000], Loss: 15.6216\n",
      "Epoch [810/1000], Loss: 15.4835\n",
      "Epoch [820/1000], Loss: 15.3605\n",
      "Epoch [830/1000], Loss: 15.2511\n",
      "Epoch [840/1000], Loss: 15.1539\n",
      "Epoch [850/1000], Loss: 15.0676\n",
      "Epoch [860/1000], Loss: 14.9911\n",
      "Epoch [870/1000], Loss: 14.9234\n",
      "Epoch [880/1000], Loss: 14.8636\n",
      "Epoch [890/1000], Loss: 14.8108\n",
      "Epoch [900/1000], Loss: 14.7642\n",
      "Epoch [910/1000], Loss: 14.7232\n",
      "Epoch [920/1000], Loss: 14.6871\n",
      "Epoch [930/1000], Loss: 14.6554\n",
      "Epoch [940/1000], Loss: 14.6276\n",
      "Epoch [950/1000], Loss: 14.6032\n",
      "Epoch [960/1000], Loss: 14.5819\n",
      "Epoch [970/1000], Loss: 14.5633\n",
      "Epoch [980/1000], Loss: 14.5470\n",
      "Epoch [990/1000], Loss: 14.5328\n",
      "Epoch [1000/1000], Loss: 14.5205\n",
      "Predicted days_remaining for parent_id 50: [32.272247314453125, 32.50425720214844, 32.50443649291992, 32.502227783203125, 32.50423049926758, 32.50259017944336, 32.499996185302734, 32.504058837890625]\n",
      "Training for parent_id 51...\n",
      "Epoch [10/1000], Loss: 131.3364\n",
      "Epoch [20/1000], Loss: 95.1722\n",
      "Epoch [30/1000], Loss: 74.3624\n",
      "Epoch [40/1000], Loss: 61.7118\n",
      "Epoch [50/1000], Loss: 52.1729\n",
      "Epoch [60/1000], Loss: 44.4669\n",
      "Epoch [70/1000], Loss: 38.1741\n",
      "Epoch [80/1000], Loss: 33.0466\n",
      "Epoch [90/1000], Loss: 28.8937\n",
      "Epoch [100/1000], Loss: 25.5572\n",
      "Epoch [110/1000], Loss: 22.9019\n",
      "Epoch [120/1000], Loss: 20.8107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000], Loss: 19.1826\n",
      "Epoch [140/1000], Loss: 17.9302\n",
      "Epoch [150/1000], Loss: 16.9791\n",
      "Epoch [160/1000], Loss: 16.2661\n",
      "Epoch [170/1000], Loss: 15.7389\n",
      "Epoch [180/1000], Loss: 15.3543\n",
      "Epoch [190/1000], Loss: 15.0776\n",
      "Epoch [200/1000], Loss: 14.8814\n",
      "Epoch [210/1000], Loss: 14.7441\n",
      "Epoch [220/1000], Loss: 14.6495\n",
      "Epoch [230/1000], Loss: 14.5851\n",
      "Epoch [240/1000], Loss: 14.5419\n",
      "Epoch [250/1000], Loss: 14.5133\n",
      "Epoch [260/1000], Loss: 14.4946\n",
      "Epoch [270/1000], Loss: 14.4824\n",
      "Epoch [280/1000], Loss: 14.4746\n",
      "Epoch [290/1000], Loss: 14.4696\n",
      "Epoch [300/1000], Loss: 14.4664\n",
      "Epoch [310/1000], Loss: 14.4643\n",
      "Epoch [320/1000], Loss: 14.4628\n",
      "Epoch [330/1000], Loss: 14.4618\n",
      "Epoch [340/1000], Loss: 14.4611\n",
      "Epoch [350/1000], Loss: 14.4605\n",
      "Epoch [360/1000], Loss: 14.4600\n",
      "Epoch [370/1000], Loss: 14.4595\n",
      "Epoch [380/1000], Loss: 14.4591\n",
      "Epoch [390/1000], Loss: 14.4587\n",
      "Epoch [400/1000], Loss: 14.4583\n",
      "Epoch [410/1000], Loss: 14.4580\n",
      "Epoch [420/1000], Loss: 14.4576\n",
      "Epoch [430/1000], Loss: 14.4573\n",
      "Epoch [440/1000], Loss: 14.4569\n",
      "Epoch [450/1000], Loss: 14.4566\n",
      "Epoch [460/1000], Loss: 14.4563\n",
      "Epoch [470/1000], Loss: 14.4559\n",
      "Epoch [480/1000], Loss: 14.4556\n",
      "Epoch [490/1000], Loss: 14.4553\n",
      "Epoch [500/1000], Loss: 14.4550\n",
      "Epoch [510/1000], Loss: 14.4547\n",
      "Epoch [520/1000], Loss: 14.4544\n",
      "Epoch [530/1000], Loss: 14.4541\n",
      "Epoch [540/1000], Loss: 14.4539\n",
      "Epoch [550/1000], Loss: 14.4536\n",
      "Epoch [560/1000], Loss: 14.4533\n",
      "Epoch [570/1000], Loss: 14.4531\n",
      "Epoch [580/1000], Loss: 14.4528\n",
      "Epoch [590/1000], Loss: 14.4526\n",
      "Epoch [600/1000], Loss: 14.4523\n",
      "Epoch [610/1000], Loss: 14.4521\n",
      "Epoch [620/1000], Loss: 14.4519\n",
      "Epoch [630/1000], Loss: 14.4516\n",
      "Epoch [640/1000], Loss: 14.4514\n",
      "Epoch [650/1000], Loss: 14.4512\n",
      "Epoch [660/1000], Loss: 14.4510\n",
      "Epoch [670/1000], Loss: 14.4508\n",
      "Epoch [680/1000], Loss: 14.4506\n",
      "Epoch [690/1000], Loss: 14.4504\n",
      "Epoch [700/1000], Loss: 14.4502\n",
      "Epoch [710/1000], Loss: 14.4500\n",
      "Epoch [720/1000], Loss: 14.4498\n",
      "Epoch [730/1000], Loss: 14.4496\n",
      "Epoch [740/1000], Loss: 14.4494\n",
      "Epoch [750/1000], Loss: 14.4492\n",
      "Epoch [760/1000], Loss: 14.4491\n",
      "Epoch [770/1000], Loss: 14.4489\n",
      "Epoch [780/1000], Loss: 14.4487\n",
      "Epoch [790/1000], Loss: 14.4485\n",
      "Epoch [800/1000], Loss: 14.4484\n",
      "Epoch [810/1000], Loss: 14.4482\n",
      "Epoch [820/1000], Loss: 14.4481\n",
      "Epoch [830/1000], Loss: 14.4479\n",
      "Epoch [840/1000], Loss: 14.4478\n",
      "Epoch [850/1000], Loss: 14.4476\n",
      "Epoch [860/1000], Loss: 14.4475\n",
      "Epoch [870/1000], Loss: 14.4473\n",
      "Epoch [880/1000], Loss: 14.4472\n",
      "Epoch [890/1000], Loss: 14.4471\n",
      "Epoch [900/1000], Loss: 14.4469\n",
      "Epoch [910/1000], Loss: 14.4468\n",
      "Epoch [920/1000], Loss: 14.4467\n",
      "Epoch [930/1000], Loss: 14.4466\n",
      "Epoch [940/1000], Loss: 14.4464\n",
      "Epoch [950/1000], Loss: 14.4463\n",
      "Epoch [960/1000], Loss: 14.4462\n",
      "Epoch [970/1000], Loss: 14.4461\n",
      "Epoch [980/1000], Loss: 14.4460\n",
      "Epoch [990/1000], Loss: 14.4459\n",
      "Epoch [1000/1000], Loss: 14.4457\n",
      "Predicted days_remaining for parent_id 51: [12.510236740112305, 12.78317928314209, 12.782581329345703, 12.790207862854004, 12.784416198730469, 12.792098999023438, 12.772464752197266, 12.784232139587402]\n",
      "Training for parent_id 61...\n",
      "Epoch [10/1000], Loss: 280.0597\n",
      "Epoch [20/1000], Loss: 223.0708\n",
      "Epoch [30/1000], Loss: 184.7789\n",
      "Epoch [40/1000], Loss: 160.6187\n",
      "Epoch [50/1000], Loss: 141.8858\n",
      "Epoch [60/1000], Loss: 125.7184\n",
      "Epoch [70/1000], Loss: 111.2431\n",
      "Epoch [80/1000], Loss: 98.3325\n",
      "Epoch [90/1000], Loss: 86.9593\n",
      "Epoch [100/1000], Loss: 76.9626\n",
      "Epoch [110/1000], Loss: 68.1920\n",
      "Epoch [120/1000], Loss: 60.5124\n",
      "Epoch [130/1000], Loss: 53.8018\n",
      "Epoch [140/1000], Loss: 47.9518\n",
      "Epoch [150/1000], Loss: 42.8659\n",
      "Epoch [160/1000], Loss: 38.4584\n",
      "Epoch [170/1000], Loss: 34.6524\n",
      "Epoch [180/1000], Loss: 31.3785\n",
      "Epoch [190/1000], Loss: 28.5739\n",
      "Epoch [200/1000], Loss: 26.1818\n",
      "Epoch [210/1000], Loss: 24.1509\n",
      "Epoch [220/1000], Loss: 22.4347\n",
      "Epoch [230/1000], Loss: 20.9915\n",
      "Epoch [240/1000], Loss: 19.7839\n",
      "Epoch [250/1000], Loss: 18.7786\n",
      "Epoch [260/1000], Loss: 17.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 17.2598\n",
      "Epoch [280/1000], Loss: 16.6975\n",
      "Epoch [290/1000], Loss: 16.2390\n",
      "Epoch [300/1000], Loss: 15.8672\n",
      "Epoch [310/1000], Loss: 15.5673\n",
      "Epoch [320/1000], Loss: 15.3267\n",
      "Epoch [330/1000], Loss: 15.1346\n",
      "Epoch [340/1000], Loss: 14.9822\n",
      "Epoch [350/1000], Loss: 14.8619\n",
      "Epoch [360/1000], Loss: 14.7674\n",
      "Epoch [370/1000], Loss: 14.6936\n",
      "Epoch [380/1000], Loss: 14.6362\n",
      "Epoch [390/1000], Loss: 14.5918\n",
      "Epoch [400/1000], Loss: 14.5578\n",
      "Epoch [410/1000], Loss: 14.5317\n",
      "Epoch [420/1000], Loss: 14.5118\n",
      "Epoch [430/1000], Loss: 14.4968\n",
      "Epoch [440/1000], Loss: 14.4854\n",
      "Epoch [450/1000], Loss: 14.4769\n",
      "Epoch [460/1000], Loss: 14.4705\n",
      "Epoch [470/1000], Loss: 14.4658\n",
      "Epoch [480/1000], Loss: 14.4622\n",
      "Epoch [490/1000], Loss: 14.4596\n",
      "Epoch [500/1000], Loss: 14.4577\n",
      "Epoch [510/1000], Loss: 14.4562\n",
      "Epoch [520/1000], Loss: 14.4551\n",
      "Epoch [530/1000], Loss: 14.4543\n",
      "Epoch [540/1000], Loss: 14.4537\n",
      "Epoch [550/1000], Loss: 14.4532\n",
      "Epoch [560/1000], Loss: 14.4529\n",
      "Epoch [570/1000], Loss: 14.4525\n",
      "Epoch [580/1000], Loss: 14.4523\n",
      "Epoch [590/1000], Loss: 14.4521\n",
      "Epoch [600/1000], Loss: 14.4519\n",
      "Epoch [610/1000], Loss: 14.4517\n",
      "Epoch [620/1000], Loss: 14.4516\n",
      "Epoch [630/1000], Loss: 14.4514\n",
      "Epoch [640/1000], Loss: 14.4513\n",
      "Epoch [650/1000], Loss: 14.4511\n",
      "Epoch [660/1000], Loss: 14.4510\n",
      "Epoch [670/1000], Loss: 14.4509\n",
      "Epoch [680/1000], Loss: 14.4507\n",
      "Epoch [690/1000], Loss: 14.4506\n",
      "Epoch [700/1000], Loss: 14.4505\n",
      "Epoch [710/1000], Loss: 14.4503\n",
      "Epoch [720/1000], Loss: 14.4502\n",
      "Epoch [730/1000], Loss: 14.4501\n",
      "Epoch [740/1000], Loss: 14.4500\n",
      "Epoch [750/1000], Loss: 14.4499\n",
      "Epoch [760/1000], Loss: 14.4497\n",
      "Epoch [770/1000], Loss: 14.4496\n",
      "Epoch [780/1000], Loss: 14.4495\n",
      "Epoch [790/1000], Loss: 14.4494\n",
      "Epoch [800/1000], Loss: 14.4493\n",
      "Epoch [810/1000], Loss: 14.4492\n",
      "Epoch [820/1000], Loss: 14.4491\n",
      "Epoch [830/1000], Loss: 14.4489\n",
      "Epoch [840/1000], Loss: 14.4488\n",
      "Epoch [850/1000], Loss: 14.4487\n",
      "Epoch [860/1000], Loss: 14.4486\n",
      "Epoch [870/1000], Loss: 14.4485\n",
      "Epoch [880/1000], Loss: 14.4484\n",
      "Epoch [890/1000], Loss: 14.4483\n",
      "Epoch [900/1000], Loss: 14.4482\n",
      "Epoch [910/1000], Loss: 14.4481\n",
      "Epoch [920/1000], Loss: 14.4480\n",
      "Epoch [930/1000], Loss: 14.4479\n",
      "Epoch [940/1000], Loss: 14.4478\n",
      "Epoch [950/1000], Loss: 14.4477\n",
      "Epoch [960/1000], Loss: 14.4476\n",
      "Epoch [970/1000], Loss: 14.4475\n",
      "Epoch [980/1000], Loss: 14.4474\n",
      "Epoch [990/1000], Loss: 14.4473\n",
      "Epoch [1000/1000], Loss: 14.4472\n",
      "Predicted days_remaining for parent_id 61: [17.49025535583496, 17.79140853881836, 17.790302276611328, 17.79030990600586, 17.78653335571289, 17.79067611694336, 17.796186447143555, 17.766395568847656]\n",
      "Training for parent_id 74...\n",
      "Epoch [10/1000], Loss: 76.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 52.2131\n",
      "Epoch [30/1000], Loss: 38.6449\n",
      "Epoch [40/1000], Loss: 30.9227\n",
      "Epoch [50/1000], Loss: 25.7113\n",
      "Epoch [60/1000], Loss: 21.9924\n",
      "Epoch [70/1000], Loss: 19.3776\n",
      "Epoch [80/1000], Loss: 17.5867\n",
      "Epoch [90/1000], Loss: 16.3984\n",
      "Epoch [100/1000], Loss: 15.6372\n",
      "Epoch [110/1000], Loss: 15.1674\n",
      "Epoch [120/1000], Loss: 14.8883\n",
      "Epoch [130/1000], Loss: 14.7284\n",
      "Epoch [140/1000], Loss: 14.6397\n",
      "Epoch [150/1000], Loss: 14.5915\n",
      "Epoch [160/1000], Loss: 14.5653\n",
      "Epoch [170/1000], Loss: 14.5504\n",
      "Epoch [180/1000], Loss: 14.5411\n",
      "Epoch [190/1000], Loss: 14.5345\n",
      "Epoch [200/1000], Loss: 14.5293\n",
      "Epoch [210/1000], Loss: 14.5247\n",
      "Epoch [220/1000], Loss: 14.5206\n",
      "Epoch [230/1000], Loss: 14.5168\n",
      "Epoch [240/1000], Loss: 14.5132\n",
      "Epoch [250/1000], Loss: 14.5099\n",
      "Epoch [260/1000], Loss: 14.5068\n",
      "Epoch [270/1000], Loss: 14.5038\n",
      "Epoch [280/1000], Loss: 14.5011\n",
      "Epoch [290/1000], Loss: 14.4985\n",
      "Epoch [300/1000], Loss: 14.4961\n",
      "Epoch [310/1000], Loss: 14.4938\n",
      "Epoch [320/1000], Loss: 14.4917\n",
      "Epoch [330/1000], Loss: 14.4896\n",
      "Epoch [340/1000], Loss: 14.4877\n",
      "Epoch [350/1000], Loss: 14.4859\n",
      "Epoch [360/1000], Loss: 14.4842\n",
      "Epoch [370/1000], Loss: 14.4826\n",
      "Epoch [380/1000], Loss: 14.4810\n",
      "Epoch [390/1000], Loss: 14.4795\n",
      "Epoch [400/1000], Loss: 14.4782\n",
      "Epoch [410/1000], Loss: 14.4768\n",
      "Epoch [420/1000], Loss: 14.4756\n",
      "Epoch [430/1000], Loss: 14.4744\n",
      "Epoch [440/1000], Loss: 14.4732\n",
      "Epoch [450/1000], Loss: 14.4721\n",
      "Epoch [460/1000], Loss: 14.4711\n",
      "Epoch [470/1000], Loss: 14.4701\n",
      "Epoch [480/1000], Loss: 14.4692\n",
      "Epoch [490/1000], Loss: 14.4683\n",
      "Epoch [500/1000], Loss: 14.4674\n",
      "Epoch [510/1000], Loss: 14.4666\n",
      "Epoch [520/1000], Loss: 14.4658\n",
      "Epoch [530/1000], Loss: 14.4650\n",
      "Epoch [540/1000], Loss: 14.4643\n",
      "Epoch [550/1000], Loss: 14.4636\n",
      "Epoch [560/1000], Loss: 14.4629\n",
      "Epoch [570/1000], Loss: 14.4622\n",
      "Epoch [580/1000], Loss: 14.4616\n",
      "Epoch [590/1000], Loss: 14.4610\n",
      "Epoch [600/1000], Loss: 14.4604\n",
      "Epoch [610/1000], Loss: 14.4599\n",
      "Epoch [620/1000], Loss: 14.4593\n",
      "Epoch [630/1000], Loss: 14.4588\n",
      "Epoch [640/1000], Loss: 14.4583\n",
      "Epoch [650/1000], Loss: 14.4578\n",
      "Epoch [660/1000], Loss: 14.4574\n",
      "Epoch [670/1000], Loss: 14.4569\n",
      "Epoch [680/1000], Loss: 14.4565\n",
      "Epoch [690/1000], Loss: 14.4560\n",
      "Epoch [700/1000], Loss: 14.4556\n",
      "Epoch [710/1000], Loss: 14.4552\n",
      "Epoch [720/1000], Loss: 14.4549\n",
      "Epoch [730/1000], Loss: 14.4545\n",
      "Epoch [740/1000], Loss: 14.4541\n",
      "Epoch [750/1000], Loss: 14.4538\n",
      "Epoch [760/1000], Loss: 14.4535\n",
      "Epoch [770/1000], Loss: 14.4531\n",
      "Epoch [780/1000], Loss: 14.4528\n",
      "Epoch [790/1000], Loss: 14.4525\n",
      "Epoch [800/1000], Loss: 14.4522\n",
      "Epoch [810/1000], Loss: 14.4519\n",
      "Epoch [820/1000], Loss: 14.4516\n",
      "Epoch [830/1000], Loss: 14.4514\n",
      "Epoch [840/1000], Loss: 14.4511\n",
      "Epoch [850/1000], Loss: 14.4508\n",
      "Epoch [860/1000], Loss: 14.4506\n",
      "Epoch [870/1000], Loss: 14.4503\n",
      "Epoch [880/1000], Loss: 14.4501\n",
      "Epoch [890/1000], Loss: 14.4499\n",
      "Epoch [900/1000], Loss: 14.4497\n",
      "Epoch [910/1000], Loss: 14.4494\n",
      "Epoch [920/1000], Loss: 14.4492\n",
      "Epoch [930/1000], Loss: 14.4490\n",
      "Epoch [940/1000], Loss: 14.4488\n",
      "Epoch [950/1000], Loss: 14.4486\n",
      "Epoch [960/1000], Loss: 14.4484\n",
      "Epoch [970/1000], Loss: 14.4482\n",
      "Epoch [980/1000], Loss: 14.4481\n",
      "Epoch [990/1000], Loss: 14.4479\n",
      "Epoch [1000/1000], Loss: 14.4477\n",
      "Predicted days_remaining for parent_id 74: [9.484309196472168, 9.800031661987305, 9.79350757598877, 9.779879570007324, 9.793707847595215, 9.760600090026855, 9.781943321228027, 9.800543785095215]\n",
      "Training for parent_id 80...\n",
      "Epoch [10/1000], Loss: 108.8217\n",
      "Epoch [20/1000], Loss: 80.5067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 65.0979\n",
      "Epoch [40/1000], Loss: 54.5623\n",
      "Epoch [50/1000], Loss: 46.1805\n",
      "Epoch [60/1000], Loss: 39.3024\n",
      "Epoch [70/1000], Loss: 33.6926\n",
      "Epoch [80/1000], Loss: 29.1728\n",
      "Epoch [90/1000], Loss: 25.5761\n",
      "Epoch [100/1000], Loss: 22.7518\n",
      "Epoch [110/1000], Loss: 20.5649\n",
      "Epoch [120/1000], Loss: 18.8959\n",
      "Epoch [130/1000], Loss: 17.6412\n",
      "Epoch [140/1000], Loss: 16.7125\n",
      "Epoch [150/1000], Loss: 16.0358\n",
      "Epoch [160/1000], Loss: 15.5506\n",
      "Epoch [170/1000], Loss: 15.2083\n",
      "Epoch [180/1000], Loss: 14.9707\n",
      "Epoch [190/1000], Loss: 14.8085\n",
      "Epoch [200/1000], Loss: 14.6994\n",
      "Epoch [210/1000], Loss: 14.6270\n",
      "Epoch [220/1000], Loss: 14.5797\n",
      "Epoch [230/1000], Loss: 14.5491\n",
      "Epoch [240/1000], Loss: 14.5293\n",
      "Epoch [250/1000], Loss: 14.5166\n",
      "Epoch [260/1000], Loss: 14.5083\n",
      "Epoch [270/1000], Loss: 14.5028\n",
      "Epoch [280/1000], Loss: 14.4989\n",
      "Epoch [290/1000], Loss: 14.4960\n",
      "Epoch [300/1000], Loss: 14.4938\n",
      "Epoch [310/1000], Loss: 14.4919\n",
      "Epoch [320/1000], Loss: 14.4902\n",
      "Epoch [330/1000], Loss: 14.4887\n",
      "Epoch [340/1000], Loss: 14.4873\n",
      "Epoch [350/1000], Loss: 14.4860\n",
      "Epoch [360/1000], Loss: 14.4847\n",
      "Epoch [370/1000], Loss: 14.4835\n",
      "Epoch [380/1000], Loss: 14.4824\n",
      "Epoch [390/1000], Loss: 14.4812\n",
      "Epoch [400/1000], Loss: 14.4801\n",
      "Epoch [410/1000], Loss: 14.4791\n",
      "Epoch [420/1000], Loss: 14.4781\n",
      "Epoch [430/1000], Loss: 14.4771\n",
      "Epoch [440/1000], Loss: 14.4762\n",
      "Epoch [450/1000], Loss: 14.4753\n",
      "Epoch [460/1000], Loss: 14.4744\n",
      "Epoch [470/1000], Loss: 14.4735\n",
      "Epoch [480/1000], Loss: 14.4727\n",
      "Epoch [490/1000], Loss: 14.4719\n",
      "Epoch [500/1000], Loss: 14.4711\n",
      "Epoch [510/1000], Loss: 14.4704\n",
      "Epoch [520/1000], Loss: 14.4697\n",
      "Epoch [530/1000], Loss: 14.4690\n",
      "Epoch [540/1000], Loss: 14.4683\n",
      "Epoch [550/1000], Loss: 14.4676\n",
      "Epoch [560/1000], Loss: 14.4670\n",
      "Epoch [570/1000], Loss: 14.4664\n",
      "Epoch [580/1000], Loss: 14.4658\n",
      "Epoch [590/1000], Loss: 14.4652\n",
      "Epoch [600/1000], Loss: 14.4646\n",
      "Epoch [610/1000], Loss: 14.4641\n",
      "Epoch [620/1000], Loss: 14.4635\n",
      "Epoch [630/1000], Loss: 14.4630\n",
      "Epoch [640/1000], Loss: 14.4625\n",
      "Epoch [650/1000], Loss: 14.4620\n",
      "Epoch [660/1000], Loss: 14.4616\n",
      "Epoch [670/1000], Loss: 14.4611\n",
      "Epoch [680/1000], Loss: 14.4607\n",
      "Epoch [690/1000], Loss: 14.4602\n",
      "Epoch [700/1000], Loss: 14.4598\n",
      "Epoch [710/1000], Loss: 14.4594\n",
      "Epoch [720/1000], Loss: 14.4590\n",
      "Epoch [730/1000], Loss: 14.4586\n",
      "Epoch [740/1000], Loss: 14.4582\n",
      "Epoch [750/1000], Loss: 14.4579\n",
      "Epoch [760/1000], Loss: 14.4575\n",
      "Epoch [770/1000], Loss: 14.4572\n",
      "Epoch [780/1000], Loss: 14.4568\n",
      "Epoch [790/1000], Loss: 14.4565\n",
      "Epoch [800/1000], Loss: 14.4562\n",
      "Epoch [810/1000], Loss: 14.4558\n",
      "Epoch [820/1000], Loss: 14.4555\n",
      "Epoch [830/1000], Loss: 14.4552\n",
      "Epoch [840/1000], Loss: 14.4549\n",
      "Epoch [850/1000], Loss: 14.4547\n",
      "Epoch [860/1000], Loss: 14.4544\n",
      "Epoch [870/1000], Loss: 14.4541\n",
      "Epoch [880/1000], Loss: 14.4538\n",
      "Epoch [890/1000], Loss: 14.4536\n",
      "Epoch [900/1000], Loss: 14.4533\n",
      "Epoch [910/1000], Loss: 14.4531\n",
      "Epoch [920/1000], Loss: 14.4528\n",
      "Epoch [930/1000], Loss: 14.4526\n",
      "Epoch [940/1000], Loss: 14.4524\n",
      "Epoch [950/1000], Loss: 14.4521\n",
      "Epoch [960/1000], Loss: 14.4519\n",
      "Epoch [970/1000], Loss: 14.4517\n",
      "Epoch [980/1000], Loss: 14.4515\n",
      "Epoch [990/1000], Loss: 14.4513\n",
      "Epoch [1000/1000], Loss: 14.4511\n",
      "Predicted days_remaining for parent_id 80: [11.441851615905762, 11.793597221374512, 11.799330711364746, 11.795297622680664, 11.796870231628418, 11.796584129333496, 11.781447410583496, 11.791780471801758]\n",
      "Training for parent_id 82...\n",
      "Epoch [10/1000], Loss: 238.4707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 192.1291\n",
      "Epoch [30/1000], Loss: 163.6747\n",
      "Epoch [40/1000], Loss: 143.3538\n",
      "Epoch [50/1000], Loss: 127.0202\n",
      "Epoch [60/1000], Loss: 112.8491\n",
      "Epoch [70/1000], Loss: 100.2507\n",
      "Epoch [80/1000], Loss: 89.0285\n",
      "Epoch [90/1000], Loss: 79.0846\n",
      "Epoch [100/1000], Loss: 70.3004\n",
      "Epoch [110/1000], Loss: 62.5582\n",
      "Epoch [120/1000], Loss: 55.7500\n",
      "Epoch [130/1000], Loss: 49.7781\n",
      "Epoch [140/1000], Loss: 44.5551\n",
      "Epoch [150/1000], Loss: 40.0023\n",
      "Epoch [160/1000], Loss: 36.0485\n",
      "Epoch [170/1000], Loss: 32.6285\n",
      "Epoch [180/1000], Loss: 29.6828\n",
      "Epoch [190/1000], Loss: 27.1571\n",
      "Epoch [200/1000], Loss: 25.0015\n",
      "Epoch [210/1000], Loss: 23.1705\n",
      "Epoch [220/1000], Loss: 21.6230\n",
      "Epoch [230/1000], Loss: 20.3216\n",
      "Epoch [240/1000], Loss: 19.2327\n",
      "Epoch [250/1000], Loss: 18.3265\n",
      "Epoch [260/1000], Loss: 17.5762\n",
      "Epoch [270/1000], Loss: 16.9582\n",
      "Epoch [280/1000], Loss: 16.4521\n",
      "Epoch [290/1000], Loss: 16.0397\n",
      "Epoch [300/1000], Loss: 15.7055\n",
      "Epoch [310/1000], Loss: 15.4361\n",
      "Epoch [320/1000], Loss: 15.2202\n",
      "Epoch [330/1000], Loss: 15.0481\n",
      "Epoch [340/1000], Loss: 14.9116\n",
      "Epoch [350/1000], Loss: 14.8040\n",
      "Epoch [360/1000], Loss: 14.7196\n",
      "Epoch [370/1000], Loss: 14.6537\n",
      "Epoch [380/1000], Loss: 14.6026\n",
      "Epoch [390/1000], Loss: 14.5632\n",
      "Epoch [400/1000], Loss: 14.5330\n",
      "Epoch [410/1000], Loss: 14.5099\n",
      "Epoch [420/1000], Loss: 14.4923\n",
      "Epoch [430/1000], Loss: 14.4791\n",
      "Epoch [440/1000], Loss: 14.4692\n",
      "Epoch [450/1000], Loss: 14.4617\n",
      "Epoch [460/1000], Loss: 14.4562\n",
      "Epoch [470/1000], Loss: 14.4521\n",
      "Epoch [480/1000], Loss: 14.4491\n",
      "Epoch [490/1000], Loss: 14.4469\n",
      "Epoch [500/1000], Loss: 14.4453\n",
      "Epoch [510/1000], Loss: 14.4442\n",
      "Epoch [520/1000], Loss: 14.4433\n",
      "Epoch [530/1000], Loss: 14.4427\n",
      "Epoch [540/1000], Loss: 14.4423\n",
      "Epoch [550/1000], Loss: 14.4420\n",
      "Epoch [560/1000], Loss: 14.4417\n",
      "Epoch [570/1000], Loss: 14.4416\n",
      "Epoch [580/1000], Loss: 14.4415\n",
      "Epoch [590/1000], Loss: 14.4414\n",
      "Epoch [600/1000], Loss: 14.4413\n",
      "Epoch [610/1000], Loss: 14.4413\n",
      "Epoch [620/1000], Loss: 14.4412\n",
      "Epoch [630/1000], Loss: 14.4412\n",
      "Epoch [640/1000], Loss: 14.4412\n",
      "Epoch [650/1000], Loss: 14.4411\n",
      "Epoch [660/1000], Loss: 14.4411\n",
      "Epoch [670/1000], Loss: 14.4411\n",
      "Epoch [680/1000], Loss: 14.4411\n",
      "Epoch [690/1000], Loss: 14.4411\n",
      "Epoch [700/1000], Loss: 14.4410\n",
      "Epoch [710/1000], Loss: 14.4410\n",
      "Epoch [720/1000], Loss: 14.4410\n",
      "Epoch [730/1000], Loss: 14.4410\n",
      "Epoch [740/1000], Loss: 14.4410\n",
      "Epoch [750/1000], Loss: 14.4409\n",
      "Epoch [760/1000], Loss: 14.4409\n",
      "Epoch [770/1000], Loss: 14.4409\n",
      "Epoch [780/1000], Loss: 14.4409\n",
      "Epoch [790/1000], Loss: 14.4409\n",
      "Epoch [800/1000], Loss: 14.4409\n",
      "Epoch [810/1000], Loss: 14.4408\n",
      "Epoch [820/1000], Loss: 14.4408\n",
      "Epoch [830/1000], Loss: 14.4408\n",
      "Epoch [840/1000], Loss: 14.4408\n",
      "Epoch [850/1000], Loss: 14.4408\n",
      "Epoch [860/1000], Loss: 14.4407\n",
      "Epoch [870/1000], Loss: 14.4407\n",
      "Epoch [880/1000], Loss: 14.4407\n",
      "Epoch [890/1000], Loss: 14.4407\n",
      "Epoch [900/1000], Loss: 14.4407\n",
      "Epoch [910/1000], Loss: 14.4407\n",
      "Epoch [920/1000], Loss: 14.4406\n",
      "Epoch [930/1000], Loss: 14.4406\n",
      "Epoch [940/1000], Loss: 14.4406\n",
      "Epoch [950/1000], Loss: 14.4406\n",
      "Epoch [960/1000], Loss: 14.4406\n",
      "Epoch [970/1000], Loss: 14.4406\n",
      "Epoch [980/1000], Loss: 14.4405\n",
      "Epoch [990/1000], Loss: 14.4405\n",
      "Epoch [1000/1000], Loss: 14.4405\n",
      "Predicted days_remaining for parent_id 82: [16.605327606201172, 16.76846694946289, 16.772727966308594, 16.776315689086914, 16.76618766784668, 16.768375396728516, 16.771711349487305, 16.77186393737793]\n",
      "Training for parent_id 88...\n",
      "Epoch [10/1000], Loss: 111.7103\n",
      "Epoch [20/1000], Loss: 83.5644\n",
      "Epoch [30/1000], Loss: 66.7704\n",
      "Epoch [40/1000], Loss: 55.6503\n",
      "Epoch [50/1000], Loss: 47.0734\n",
      "Epoch [60/1000], Loss: 40.1334\n",
      "Epoch [70/1000], Loss: 34.4895\n",
      "Epoch [80/1000], Loss: 29.9260\n",
      "Epoch [90/1000], Loss: 26.2702\n",
      "Epoch [100/1000], Loss: 23.3736\n",
      "Epoch [110/1000], Loss: 21.1063\n",
      "Epoch [120/1000], Loss: 19.3542\n",
      "Epoch [130/1000], Loss: 18.0187\n",
      "Epoch [140/1000], Loss: 17.0151\n",
      "Epoch [150/1000], Loss: 16.2719\n",
      "Epoch [160/1000], Loss: 15.7298\n",
      "Epoch [170/1000], Loss: 15.3403\n",
      "Epoch [180/1000], Loss: 15.0648\n",
      "Epoch [190/1000], Loss: 14.8729\n",
      "Epoch [200/1000], Loss: 14.7413\n",
      "Epoch [210/1000], Loss: 14.6523\n",
      "Epoch [220/1000], Loss: 14.5930\n",
      "Epoch [230/1000], Loss: 14.5540\n",
      "Epoch [240/1000], Loss: 14.5286\n",
      "Epoch [250/1000], Loss: 14.5121\n",
      "Epoch [260/1000], Loss: 14.5015\n",
      "Epoch [270/1000], Loss: 14.4946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/1000], Loss: 14.4900\n",
      "Epoch [290/1000], Loss: 14.4869\n",
      "Epoch [300/1000], Loss: 14.4846\n",
      "Epoch [310/1000], Loss: 14.4829\n",
      "Epoch [320/1000], Loss: 14.4815\n",
      "Epoch [330/1000], Loss: 14.4802\n",
      "Epoch [340/1000], Loss: 14.4791\n",
      "Epoch [350/1000], Loss: 14.4781\n",
      "Epoch [360/1000], Loss: 14.4771\n",
      "Epoch [370/1000], Loss: 14.4762\n",
      "Epoch [380/1000], Loss: 14.4753\n",
      "Epoch [390/1000], Loss: 14.4744\n",
      "Epoch [400/1000], Loss: 14.4736\n",
      "Epoch [410/1000], Loss: 14.4728\n",
      "Epoch [420/1000], Loss: 14.4720\n",
      "Epoch [430/1000], Loss: 14.4712\n",
      "Epoch [440/1000], Loss: 14.4705\n",
      "Epoch [450/1000], Loss: 14.4698\n",
      "Epoch [460/1000], Loss: 14.4691\n",
      "Epoch [470/1000], Loss: 14.4684\n",
      "Epoch [480/1000], Loss: 14.4677\n",
      "Epoch [490/1000], Loss: 14.4671\n",
      "Epoch [500/1000], Loss: 14.4665\n",
      "Epoch [510/1000], Loss: 14.4659\n",
      "Epoch [520/1000], Loss: 14.4653\n",
      "Epoch [530/1000], Loss: 14.4647\n",
      "Epoch [540/1000], Loss: 14.4642\n",
      "Epoch [550/1000], Loss: 14.4637\n",
      "Epoch [560/1000], Loss: 14.4631\n",
      "Epoch [570/1000], Loss: 14.4626\n",
      "Epoch [580/1000], Loss: 14.4621\n",
      "Epoch [590/1000], Loss: 14.4617\n",
      "Epoch [600/1000], Loss: 14.4612\n",
      "Epoch [610/1000], Loss: 14.4607\n",
      "Epoch [620/1000], Loss: 14.4603\n",
      "Epoch [630/1000], Loss: 14.4599\n",
      "Epoch [640/1000], Loss: 14.4595\n",
      "Epoch [650/1000], Loss: 14.4591\n",
      "Epoch [660/1000], Loss: 14.4587\n",
      "Epoch [670/1000], Loss: 14.4583\n",
      "Epoch [680/1000], Loss: 14.4579\n",
      "Epoch [690/1000], Loss: 14.4575\n",
      "Epoch [700/1000], Loss: 14.4572\n",
      "Epoch [710/1000], Loss: 14.4568\n",
      "Epoch [720/1000], Loss: 14.4565\n",
      "Epoch [730/1000], Loss: 14.4562\n",
      "Epoch [740/1000], Loss: 14.4558\n",
      "Epoch [750/1000], Loss: 14.4555\n",
      "Epoch [760/1000], Loss: 14.4552\n",
      "Epoch [770/1000], Loss: 14.4549\n",
      "Epoch [780/1000], Loss: 14.4546\n",
      "Epoch [790/1000], Loss: 14.4544\n",
      "Epoch [800/1000], Loss: 14.4541\n",
      "Epoch [810/1000], Loss: 14.4538\n",
      "Epoch [820/1000], Loss: 14.4535\n",
      "Epoch [830/1000], Loss: 14.4533\n",
      "Epoch [840/1000], Loss: 14.4530\n",
      "Epoch [850/1000], Loss: 14.4528\n",
      "Epoch [860/1000], Loss: 14.4525\n",
      "Epoch [870/1000], Loss: 14.4523\n",
      "Epoch [880/1000], Loss: 14.4521\n",
      "Epoch [890/1000], Loss: 14.4519\n",
      "Epoch [900/1000], Loss: 14.4516\n",
      "Epoch [910/1000], Loss: 14.4514\n",
      "Epoch [920/1000], Loss: 14.4512\n",
      "Epoch [930/1000], Loss: 14.4510\n",
      "Epoch [940/1000], Loss: 14.4508\n",
      "Epoch [950/1000], Loss: 14.4506\n",
      "Epoch [960/1000], Loss: 14.4504\n",
      "Epoch [970/1000], Loss: 14.4502\n",
      "Epoch [980/1000], Loss: 14.4500\n",
      "Epoch [990/1000], Loss: 14.4499\n",
      "Epoch [1000/1000], Loss: 14.4497\n",
      "Predicted days_remaining for parent_id 88: [11.45798397064209, 11.788606643676758, 11.792072296142578, 11.794013977050781, 11.789409637451172, 11.791671752929688, 11.792275428771973, 11.791633605957031]\n",
      "Training for parent_id 91...\n",
      "Epoch [10/1000], Loss: 996.3688\n",
      "Epoch [20/1000], Loss: 891.4482\n",
      "Epoch [30/1000], Loss: 821.3643\n",
      "Epoch [40/1000], Loss: 770.7784\n",
      "Epoch [50/1000], Loss: 727.1298\n",
      "Epoch [60/1000], Loss: 687.1703\n",
      "Epoch [70/1000], Loss: 649.9741\n",
      "Epoch [80/1000], Loss: 615.0868\n",
      "Epoch [90/1000], Loss: 582.2080\n",
      "Epoch [100/1000], Loss: 551.1186\n",
      "Epoch [110/1000], Loss: 521.6520\n",
      "Epoch [120/1000], Loss: 493.6777\n",
      "Epoch [130/1000], Loss: 467.0903\n",
      "Epoch [140/1000], Loss: 441.8031\n",
      "Epoch [150/1000], Loss: 417.7419\n",
      "Epoch [160/1000], Loss: 394.8427\n",
      "Epoch [170/1000], Loss: 373.0488\n",
      "Epoch [180/1000], Loss: 352.3088\n",
      "Epoch [190/1000], Loss: 332.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 313.8076\n",
      "Epoch [210/1000], Loss: 295.9628\n",
      "Epoch [220/1000], Loss: 279.0039\n",
      "Epoch [230/1000], Loss: 262.8946\n",
      "Epoch [240/1000], Loss: 247.6010\n",
      "Epoch [250/1000], Loss: 233.0900\n",
      "Epoch [260/1000], Loss: 219.3303\n",
      "Epoch [270/1000], Loss: 206.2914\n",
      "Epoch [280/1000], Loss: 193.9439\n",
      "Epoch [290/1000], Loss: 182.2596\n",
      "Epoch [300/1000], Loss: 171.2109\n",
      "Epoch [310/1000], Loss: 160.7712\n",
      "Epoch [320/1000], Loss: 150.9145\n",
      "Epoch [330/1000], Loss: 141.6159\n",
      "Epoch [340/1000], Loss: 132.8507\n",
      "Epoch [350/1000], Loss: 124.5955\n",
      "Epoch [360/1000], Loss: 116.8272\n",
      "Epoch [370/1000], Loss: 109.5233\n",
      "Epoch [380/1000], Loss: 102.6622\n",
      "Epoch [390/1000], Loss: 96.2229\n",
      "Epoch [400/1000], Loss: 90.1850\n",
      "Epoch [410/1000], Loss: 84.5288\n",
      "Epoch [420/1000], Loss: 79.2350\n",
      "Epoch [430/1000], Loss: 74.2853\n",
      "Epoch [440/1000], Loss: 69.6617\n",
      "Epoch [450/1000], Loss: 65.3470\n",
      "Epoch [460/1000], Loss: 61.3245\n",
      "Epoch [470/1000], Loss: 57.5782\n",
      "Epoch [480/1000], Loss: 54.0927\n",
      "Epoch [490/1000], Loss: 50.8531\n",
      "Epoch [500/1000], Loss: 47.8452\n",
      "Epoch [510/1000], Loss: 45.0553\n",
      "Epoch [520/1000], Loss: 42.4704\n",
      "Epoch [530/1000], Loss: 40.0778\n",
      "Epoch [540/1000], Loss: 37.8658\n",
      "Epoch [550/1000], Loss: 35.8227\n",
      "Epoch [560/1000], Loss: 33.9379\n",
      "Epoch [570/1000], Loss: 32.2008\n",
      "Epoch [580/1000], Loss: 30.6018\n",
      "Epoch [590/1000], Loss: 29.1314\n",
      "Epoch [600/1000], Loss: 27.7807\n",
      "Epoch [610/1000], Loss: 26.5415\n",
      "Epoch [620/1000], Loss: 25.4058\n",
      "Epoch [630/1000], Loss: 24.3661\n",
      "Epoch [640/1000], Loss: 23.4154\n",
      "Epoch [650/1000], Loss: 22.5470\n",
      "Epoch [660/1000], Loss: 21.7548\n",
      "Epoch [670/1000], Loss: 21.0328\n",
      "Epoch [680/1000], Loss: 20.3756\n",
      "Epoch [690/1000], Loss: 19.7782\n",
      "Epoch [700/1000], Loss: 19.2356\n",
      "Epoch [710/1000], Loss: 18.7434\n",
      "Epoch [720/1000], Loss: 18.2974\n",
      "Epoch [730/1000], Loss: 17.8939\n",
      "Epoch [740/1000], Loss: 17.5291\n",
      "Epoch [750/1000], Loss: 17.1998\n",
      "Epoch [760/1000], Loss: 16.9028\n",
      "Epoch [770/1000], Loss: 16.6353\n",
      "Epoch [780/1000], Loss: 16.3946\n",
      "Epoch [790/1000], Loss: 16.1784\n",
      "Epoch [800/1000], Loss: 15.9843\n",
      "Epoch [810/1000], Loss: 15.8103\n",
      "Epoch [820/1000], Loss: 15.6545\n",
      "Epoch [830/1000], Loss: 15.5152\n",
      "Epoch [840/1000], Loss: 15.3907\n",
      "Epoch [850/1000], Loss: 15.2797\n",
      "Epoch [860/1000], Loss: 15.1808\n",
      "Epoch [870/1000], Loss: 15.0927\n",
      "Epoch [880/1000], Loss: 15.0145\n",
      "Epoch [890/1000], Loss: 14.9450\n",
      "Epoch [900/1000], Loss: 14.8834\n",
      "Epoch [910/1000], Loss: 14.8288\n",
      "Epoch [920/1000], Loss: 14.7805\n",
      "Epoch [930/1000], Loss: 14.7379\n",
      "Epoch [940/1000], Loss: 14.7002\n",
      "Epoch [950/1000], Loss: 14.6671\n",
      "Epoch [960/1000], Loss: 14.6379\n",
      "Epoch [970/1000], Loss: 14.6122\n",
      "Epoch [980/1000], Loss: 14.5897\n",
      "Epoch [990/1000], Loss: 14.5699\n",
      "Epoch [1000/1000], Loss: 14.5526\n",
      "Predicted days_remaining for parent_id 91: [33.3150749206543, 33.430213928222656, 33.43195343017578, 33.43219757080078, 33.42987060546875, 33.421592712402344, 33.42995071411133, 33.43128204345703]\n",
      "Training for parent_id 99...\n",
      "Epoch [10/1000], Loss: 101.4257\n",
      "Epoch [20/1000], Loss: 66.8245\n",
      "Epoch [30/1000], Loss: 47.3257\n",
      "Epoch [40/1000], Loss: 37.2880\n",
      "Epoch [50/1000], Loss: 30.6805\n",
      "Epoch [60/1000], Loss: 25.9065\n",
      "Epoch [70/1000], Loss: 22.4233\n",
      "Epoch [80/1000], Loss: 19.9106\n",
      "Epoch [90/1000], Loss: 18.1331\n",
      "Epoch [100/1000], Loss: 16.9051\n",
      "Epoch [110/1000], Loss: 16.0787\n",
      "Epoch [120/1000], Loss: 15.5377\n",
      "Epoch [130/1000], Loss: 15.1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 14.9787\n",
      "Epoch [150/1000], Loss: 14.8480\n",
      "Epoch [160/1000], Loss: 14.7690\n",
      "Epoch [170/1000], Loss: 14.7206\n",
      "Epoch [180/1000], Loss: 14.6900\n",
      "Epoch [190/1000], Loss: 14.6694\n",
      "Epoch [200/1000], Loss: 14.6542\n",
      "Epoch [210/1000], Loss: 14.6421\n",
      "Epoch [220/1000], Loss: 14.6318\n",
      "Epoch [230/1000], Loss: 14.6227\n",
      "Epoch [240/1000], Loss: 14.6143\n",
      "Epoch [250/1000], Loss: 14.6066\n",
      "Epoch [260/1000], Loss: 14.5994\n",
      "Epoch [270/1000], Loss: 14.5927\n",
      "Epoch [280/1000], Loss: 14.5864\n",
      "Epoch [290/1000], Loss: 14.5805\n",
      "Epoch [300/1000], Loss: 14.5749\n",
      "Epoch [310/1000], Loss: 14.5697\n",
      "Epoch [320/1000], Loss: 14.5647\n",
      "Epoch [330/1000], Loss: 14.5601\n",
      "Epoch [340/1000], Loss: 14.5557\n",
      "Epoch [350/1000], Loss: 14.5515\n",
      "Epoch [360/1000], Loss: 14.5476\n",
      "Epoch [370/1000], Loss: 14.5438\n",
      "Epoch [380/1000], Loss: 14.5403\n",
      "Epoch [390/1000], Loss: 14.5369\n",
      "Epoch [400/1000], Loss: 14.5337\n",
      "Epoch [410/1000], Loss: 14.5307\n",
      "Epoch [420/1000], Loss: 14.5278\n",
      "Epoch [430/1000], Loss: 14.5250\n",
      "Epoch [440/1000], Loss: 14.5224\n",
      "Epoch [450/1000], Loss: 14.5199\n",
      "Epoch [460/1000], Loss: 14.5175\n",
      "Epoch [470/1000], Loss: 14.5152\n",
      "Epoch [480/1000], Loss: 14.5130\n",
      "Epoch [490/1000], Loss: 14.5109\n",
      "Epoch [500/1000], Loss: 14.5089\n",
      "Epoch [510/1000], Loss: 14.5069\n",
      "Epoch [520/1000], Loss: 14.5051\n",
      "Epoch [530/1000], Loss: 14.5033\n",
      "Epoch [540/1000], Loss: 14.5016\n",
      "Epoch [550/1000], Loss: 14.5000\n",
      "Epoch [560/1000], Loss: 14.4984\n",
      "Epoch [570/1000], Loss: 14.4969\n",
      "Epoch [580/1000], Loss: 14.4954\n",
      "Epoch [590/1000], Loss: 14.4940\n",
      "Epoch [600/1000], Loss: 14.4927\n",
      "Epoch [610/1000], Loss: 14.4914\n",
      "Epoch [620/1000], Loss: 14.4901\n",
      "Epoch [630/1000], Loss: 14.4889\n",
      "Epoch [640/1000], Loss: 14.4877\n",
      "Epoch [650/1000], Loss: 14.4866\n",
      "Epoch [660/1000], Loss: 14.4855\n",
      "Epoch [670/1000], Loss: 14.4844\n",
      "Epoch [680/1000], Loss: 14.4834\n",
      "Epoch [690/1000], Loss: 14.4824\n",
      "Epoch [700/1000], Loss: 14.4815\n",
      "Epoch [710/1000], Loss: 14.4805\n",
      "Epoch [720/1000], Loss: 14.4797\n",
      "Epoch [730/1000], Loss: 14.4788\n",
      "Epoch [740/1000], Loss: 14.4779\n",
      "Epoch [750/1000], Loss: 14.4771\n",
      "Epoch [760/1000], Loss: 14.4763\n",
      "Epoch [770/1000], Loss: 14.4756\n",
      "Epoch [780/1000], Loss: 14.4748\n",
      "Epoch [790/1000], Loss: 14.4741\n",
      "Epoch [800/1000], Loss: 14.4734\n",
      "Epoch [810/1000], Loss: 14.4727\n",
      "Epoch [820/1000], Loss: 14.4720\n",
      "Epoch [830/1000], Loss: 14.4714\n",
      "Epoch [840/1000], Loss: 14.4708\n",
      "Epoch [850/1000], Loss: 14.4702\n",
      "Epoch [860/1000], Loss: 14.4696\n",
      "Epoch [870/1000], Loss: 14.4690\n",
      "Epoch [880/1000], Loss: 14.4684\n",
      "Epoch [890/1000], Loss: 14.4679\n",
      "Epoch [900/1000], Loss: 14.4674\n",
      "Epoch [910/1000], Loss: 14.4668\n",
      "Epoch [920/1000], Loss: 14.4663\n",
      "Epoch [930/1000], Loss: 14.4658\n",
      "Epoch [940/1000], Loss: 14.4654\n",
      "Epoch [950/1000], Loss: 14.4649\n",
      "Epoch [960/1000], Loss: 14.4644\n",
      "Epoch [970/1000], Loss: 14.4640\n",
      "Epoch [980/1000], Loss: 14.4636\n",
      "Epoch [990/1000], Loss: 14.4631\n",
      "Epoch [1000/1000], Loss: 14.4627\n",
      "Predicted days_remaining for parent_id 99: [10.328932762145996, 10.79549789428711, 10.813558578491211, 10.808329582214355, 10.814799308776855, 10.812945365905762, 10.814817428588867, 10.798115730285645]\n",
      "Training for parent_id 103...\n",
      "Epoch [10/1000], Loss: 143.2936\n",
      "Epoch [20/1000], Loss: 106.0765\n",
      "Epoch [30/1000], Loss: 82.2301\n",
      "Epoch [40/1000], Loss: 67.7596\n",
      "Epoch [50/1000], Loss: 57.3676\n",
      "Epoch [60/1000], Loss: 48.9880\n",
      "Epoch [70/1000], Loss: 42.0490\n",
      "Epoch [80/1000], Loss: 36.3224\n",
      "Epoch [90/1000], Loss: 31.6322\n",
      "Epoch [100/1000], Loss: 27.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 24.7584\n",
      "Epoch [120/1000], Loss: 22.3181\n",
      "Epoch [130/1000], Loss: 20.3959\n",
      "Epoch [140/1000], Loss: 18.8990\n",
      "Epoch [150/1000], Loss: 17.7470\n",
      "Epoch [160/1000], Loss: 16.8712\n",
      "Epoch [170/1000], Loss: 16.2135\n",
      "Epoch [180/1000], Loss: 15.7258\n",
      "Epoch [190/1000], Loss: 15.3688\n",
      "Epoch [200/1000], Loss: 15.1106\n",
      "Epoch [210/1000], Loss: 14.9262\n",
      "Epoch [220/1000], Loss: 14.7961\n",
      "Epoch [230/1000], Loss: 14.7053\n",
      "Epoch [240/1000], Loss: 14.6426\n",
      "Epoch [250/1000], Loss: 14.5996\n",
      "Epoch [260/1000], Loss: 14.5704\n",
      "Epoch [270/1000], Loss: 14.5505\n",
      "Epoch [280/1000], Loss: 14.5369\n",
      "Epoch [290/1000], Loss: 14.5276\n",
      "Epoch [300/1000], Loss: 14.5210\n",
      "Epoch [310/1000], Loss: 14.5162\n",
      "Epoch [320/1000], Loss: 14.5126\n",
      "Epoch [330/1000], Loss: 14.5097\n",
      "Epoch [340/1000], Loss: 14.5073\n",
      "Epoch [350/1000], Loss: 14.5052\n",
      "Epoch [360/1000], Loss: 14.5034\n",
      "Epoch [370/1000], Loss: 14.5016\n",
      "Epoch [380/1000], Loss: 14.5000\n",
      "Epoch [390/1000], Loss: 14.4985\n",
      "Epoch [400/1000], Loss: 14.4970\n",
      "Epoch [410/1000], Loss: 14.4955\n",
      "Epoch [420/1000], Loss: 14.4942\n",
      "Epoch [430/1000], Loss: 14.4928\n",
      "Epoch [440/1000], Loss: 14.4916\n",
      "Epoch [450/1000], Loss: 14.4903\n",
      "Epoch [460/1000], Loss: 14.4891\n",
      "Epoch [470/1000], Loss: 14.4879\n",
      "Epoch [480/1000], Loss: 14.4868\n",
      "Epoch [490/1000], Loss: 14.4857\n",
      "Epoch [500/1000], Loss: 14.4847\n",
      "Epoch [510/1000], Loss: 14.4836\n",
      "Epoch [520/1000], Loss: 14.4826\n",
      "Epoch [530/1000], Loss: 14.4817\n",
      "Epoch [540/1000], Loss: 14.4807\n",
      "Epoch [550/1000], Loss: 14.4798\n",
      "Epoch [560/1000], Loss: 14.4790\n",
      "Epoch [570/1000], Loss: 14.4781\n",
      "Epoch [580/1000], Loss: 14.4773\n",
      "Epoch [590/1000], Loss: 14.4765\n",
      "Epoch [600/1000], Loss: 14.4757\n",
      "Epoch [610/1000], Loss: 14.4749\n",
      "Epoch [620/1000], Loss: 14.4742\n",
      "Epoch [630/1000], Loss: 14.4735\n",
      "Epoch [640/1000], Loss: 14.4728\n",
      "Epoch [650/1000], Loss: 14.4721\n",
      "Epoch [660/1000], Loss: 14.4715\n",
      "Epoch [670/1000], Loss: 14.4708\n",
      "Epoch [680/1000], Loss: 14.4702\n",
      "Epoch [690/1000], Loss: 14.4696\n",
      "Epoch [700/1000], Loss: 14.4690\n",
      "Epoch [710/1000], Loss: 14.4684\n",
      "Epoch [720/1000], Loss: 14.4679\n",
      "Epoch [730/1000], Loss: 14.4673\n",
      "Epoch [740/1000], Loss: 14.4668\n",
      "Epoch [750/1000], Loss: 14.4663\n",
      "Epoch [760/1000], Loss: 14.4658\n",
      "Epoch [770/1000], Loss: 14.4653\n",
      "Epoch [780/1000], Loss: 14.4648\n",
      "Epoch [790/1000], Loss: 14.4643\n",
      "Epoch [800/1000], Loss: 14.4639\n",
      "Epoch [810/1000], Loss: 14.4634\n",
      "Epoch [820/1000], Loss: 14.4630\n",
      "Epoch [830/1000], Loss: 14.4626\n",
      "Epoch [840/1000], Loss: 14.4622\n",
      "Epoch [850/1000], Loss: 14.4618\n",
      "Epoch [860/1000], Loss: 14.4614\n",
      "Epoch [870/1000], Loss: 14.4610\n",
      "Epoch [880/1000], Loss: 14.4606\n",
      "Epoch [890/1000], Loss: 14.4603\n",
      "Epoch [900/1000], Loss: 14.4599\n",
      "Epoch [910/1000], Loss: 14.4595\n",
      "Epoch [920/1000], Loss: 14.4592\n",
      "Epoch [930/1000], Loss: 14.4589\n",
      "Epoch [940/1000], Loss: 14.4585\n",
      "Epoch [950/1000], Loss: 14.4582\n",
      "Epoch [960/1000], Loss: 14.4579\n",
      "Epoch [970/1000], Loss: 14.4576\n",
      "Epoch [980/1000], Loss: 14.4573\n",
      "Epoch [990/1000], Loss: 14.4570\n",
      "Epoch [1000/1000], Loss: 14.4567\n",
      "Predicted days_remaining for parent_id 103: [12.383042335510254, 12.805684089660645, 12.80249309539795, 12.792360305786133, 12.804411888122559, 12.801358222961426, 12.805741310119629, 12.801051139831543]\n",
      "Training for parent_id 105...\n",
      "Epoch [10/1000], Loss: 249.3179\n",
      "Epoch [20/1000], Loss: 201.4006\n",
      "Epoch [30/1000], Loss: 171.0729\n",
      "Epoch [40/1000], Loss: 150.2231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 133.0710\n",
      "Epoch [60/1000], Loss: 118.1015\n",
      "Epoch [70/1000], Loss: 104.8800\n",
      "Epoch [80/1000], Loss: 93.1705\n",
      "Epoch [90/1000], Loss: 82.7979\n",
      "Epoch [100/1000], Loss: 73.6181\n",
      "Epoch [110/1000], Loss: 65.5079\n",
      "Epoch [120/1000], Loss: 58.3590\n",
      "Epoch [130/1000], Loss: 52.0748\n",
      "Epoch [140/1000], Loss: 46.5680\n",
      "Epoch [150/1000], Loss: 41.7594\n",
      "Epoch [160/1000], Loss: 37.5762\n",
      "Epoch [170/1000], Loss: 33.9517\n",
      "Epoch [180/1000], Loss: 30.8247\n",
      "Epoch [190/1000], Loss: 28.1387\n",
      "Epoch [200/1000], Loss: 25.8421\n",
      "Epoch [210/1000], Loss: 23.8877\n",
      "Epoch [220/1000], Loss: 22.2326\n",
      "Epoch [230/1000], Loss: 20.8378\n",
      "Epoch [240/1000], Loss: 19.6683\n",
      "Epoch [250/1000], Loss: 18.6928\n",
      "Epoch [260/1000], Loss: 17.8832\n",
      "Epoch [270/1000], Loss: 17.2148\n",
      "Epoch [280/1000], Loss: 16.6658\n",
      "Epoch [290/1000], Loss: 16.2174\n",
      "Epoch [300/1000], Loss: 15.8530\n",
      "Epoch [310/1000], Loss: 15.5585\n",
      "Epoch [320/1000], Loss: 15.3217\n",
      "Epoch [330/1000], Loss: 15.1324\n",
      "Epoch [340/1000], Loss: 14.9818\n",
      "Epoch [350/1000], Loss: 14.8626\n",
      "Epoch [360/1000], Loss: 14.7688\n",
      "Epoch [370/1000], Loss: 14.6954\n",
      "Epoch [380/1000], Loss: 14.6382\n",
      "Epoch [390/1000], Loss: 14.5938\n",
      "Epoch [400/1000], Loss: 14.5597\n",
      "Epoch [410/1000], Loss: 14.5335\n",
      "Epoch [420/1000], Loss: 14.5135\n",
      "Epoch [430/1000], Loss: 14.4983\n",
      "Epoch [440/1000], Loss: 14.4868\n",
      "Epoch [450/1000], Loss: 14.4782\n",
      "Epoch [460/1000], Loss: 14.4717\n",
      "Epoch [470/1000], Loss: 14.4668\n",
      "Epoch [480/1000], Loss: 14.4632\n",
      "Epoch [490/1000], Loss: 14.4605\n",
      "Epoch [500/1000], Loss: 14.4585\n",
      "Epoch [510/1000], Loss: 14.4570\n",
      "Epoch [520/1000], Loss: 14.4559\n",
      "Epoch [530/1000], Loss: 14.4551\n",
      "Epoch [540/1000], Loss: 14.4544\n",
      "Epoch [550/1000], Loss: 14.4539\n",
      "Epoch [560/1000], Loss: 14.4535\n",
      "Epoch [570/1000], Loss: 14.4532\n",
      "Epoch [580/1000], Loss: 14.4530\n",
      "Epoch [590/1000], Loss: 14.4527\n",
      "Epoch [600/1000], Loss: 14.4525\n",
      "Epoch [610/1000], Loss: 14.4524\n",
      "Epoch [620/1000], Loss: 14.4522\n",
      "Epoch [630/1000], Loss: 14.4520\n",
      "Epoch [640/1000], Loss: 14.4519\n",
      "Epoch [650/1000], Loss: 14.4518\n",
      "Epoch [660/1000], Loss: 14.4516\n",
      "Epoch [670/1000], Loss: 14.4515\n",
      "Epoch [680/1000], Loss: 14.4513\n",
      "Epoch [690/1000], Loss: 14.4512\n",
      "Epoch [700/1000], Loss: 14.4511\n",
      "Epoch [710/1000], Loss: 14.4510\n",
      "Epoch [720/1000], Loss: 14.4508\n",
      "Epoch [730/1000], Loss: 14.4507\n",
      "Epoch [740/1000], Loss: 14.4506\n",
      "Epoch [750/1000], Loss: 14.4505\n",
      "Epoch [760/1000], Loss: 14.4503\n",
      "Epoch [770/1000], Loss: 14.4502\n",
      "Epoch [780/1000], Loss: 14.4501\n",
      "Epoch [790/1000], Loss: 14.4500\n",
      "Epoch [800/1000], Loss: 14.4499\n",
      "Epoch [810/1000], Loss: 14.4497\n",
      "Epoch [820/1000], Loss: 14.4496\n",
      "Epoch [830/1000], Loss: 14.4495\n",
      "Epoch [840/1000], Loss: 14.4494\n",
      "Epoch [850/1000], Loss: 14.4493\n",
      "Epoch [860/1000], Loss: 14.4492\n",
      "Epoch [870/1000], Loss: 14.4491\n",
      "Epoch [880/1000], Loss: 14.4490\n",
      "Epoch [890/1000], Loss: 14.4489\n",
      "Epoch [900/1000], Loss: 14.4487\n",
      "Epoch [910/1000], Loss: 14.4486\n",
      "Epoch [920/1000], Loss: 14.4485\n",
      "Epoch [930/1000], Loss: 14.4484\n",
      "Epoch [940/1000], Loss: 14.4483\n",
      "Epoch [950/1000], Loss: 14.4482\n",
      "Epoch [960/1000], Loss: 14.4481\n",
      "Epoch [970/1000], Loss: 14.4480\n",
      "Epoch [980/1000], Loss: 14.4479\n",
      "Epoch [990/1000], Loss: 14.4478\n",
      "Epoch [1000/1000], Loss: 14.4477\n",
      "Predicted days_remaining for parent_id 105: [16.48287010192871, 16.79559326171875, 16.79410171508789, 16.790180206298828, 16.77943992614746, 16.786680221557617, 16.785959243774414, 16.787288665771484]\n",
      "Training for parent_id 113...\n",
      "Epoch [10/1000], Loss: 528.9026\n",
      "Epoch [20/1000], Loss: 449.6440\n",
      "Epoch [30/1000], Loss: 400.0406\n",
      "Epoch [40/1000], Loss: 365.6644\n",
      "Epoch [50/1000], Loss: 336.9613\n",
      "Epoch [60/1000], Loss: 311.1339\n",
      "Epoch [70/1000], Loss: 287.1988\n",
      "Epoch [80/1000], Loss: 264.9723\n",
      "Epoch [90/1000], Loss: 244.4631\n",
      "Epoch [100/1000], Loss: 225.5160\n",
      "Epoch [110/1000], Loss: 207.9869\n",
      "Epoch [120/1000], Loss: 191.7548\n",
      "Epoch [130/1000], Loss: 176.7172\n",
      "Epoch [140/1000], Loss: 162.7857\n",
      "Epoch [150/1000], Loss: 149.8830\n",
      "Epoch [160/1000], Loss: 137.9402\n",
      "Epoch [170/1000], Loss: 126.8947\n",
      "Epoch [180/1000], Loss: 116.6894\n",
      "Epoch [190/1000], Loss: 107.2710\n",
      "Epoch [200/1000], Loss: 98.5898\n",
      "Epoch [210/1000], Loss: 90.5992\n",
      "Epoch [220/1000], Loss: 83.2550\n",
      "Epoch [230/1000], Loss: 76.5153\n",
      "Epoch [240/1000], Loss: 70.3402\n",
      "Epoch [250/1000], Loss: 64.6922\n",
      "Epoch [260/1000], Loss: 59.5351\n",
      "Epoch [270/1000], Loss: 54.8346\n",
      "Epoch [280/1000], Loss: 50.5583\n",
      "Epoch [290/1000], Loss: 46.6751\n",
      "Epoch [300/1000], Loss: 43.1557\n",
      "Epoch [310/1000], Loss: 39.9722\n",
      "Epoch [320/1000], Loss: 37.0983\n",
      "Epoch [330/1000], Loss: 34.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [340/1000], Loss: 32.1811\n",
      "Epoch [350/1000], Loss: 30.0925\n",
      "Epoch [360/1000], Loss: 28.2224\n",
      "Epoch [370/1000], Loss: 26.5516\n",
      "Epoch [380/1000], Loss: 25.0620\n",
      "Epoch [390/1000], Loss: 23.7368\n",
      "Epoch [400/1000], Loss: 22.5605\n",
      "Epoch [410/1000], Loss: 21.5186\n",
      "Epoch [420/1000], Loss: 20.5977\n",
      "Epoch [430/1000], Loss: 19.7856\n",
      "Epoch [440/1000], Loss: 19.0711\n",
      "Epoch [450/1000], Loss: 18.4438\n",
      "Epoch [460/1000], Loss: 17.8943\n",
      "Epoch [470/1000], Loss: 17.4141\n",
      "Epoch [480/1000], Loss: 16.9953\n",
      "Epoch [490/1000], Loss: 16.6309\n",
      "Epoch [500/1000], Loss: 16.3145\n",
      "Epoch [510/1000], Loss: 16.0406\n",
      "Epoch [520/1000], Loss: 15.8038\n",
      "Epoch [530/1000], Loss: 15.5997\n",
      "Epoch [540/1000], Loss: 15.4240\n",
      "Epoch [550/1000], Loss: 15.2733\n",
      "Epoch [560/1000], Loss: 15.1442\n",
      "Epoch [570/1000], Loss: 15.0340\n",
      "Epoch [580/1000], Loss: 14.9400\n",
      "Epoch [590/1000], Loss: 14.8600\n",
      "Epoch [600/1000], Loss: 14.7922\n",
      "Epoch [610/1000], Loss: 14.7347\n",
      "Epoch [620/1000], Loss: 14.6862\n",
      "Epoch [630/1000], Loss: 14.6453\n",
      "Epoch [640/1000], Loss: 14.6109\n",
      "Epoch [650/1000], Loss: 14.5821\n",
      "Epoch [660/1000], Loss: 14.5579\n",
      "Epoch [670/1000], Loss: 14.5378\n",
      "Epoch [680/1000], Loss: 14.5210\n",
      "Epoch [690/1000], Loss: 14.5070\n",
      "Epoch [700/1000], Loss: 14.4954\n",
      "Epoch [710/1000], Loss: 14.4858\n",
      "Epoch [720/1000], Loss: 14.4779\n",
      "Epoch [730/1000], Loss: 14.4714\n",
      "Epoch [740/1000], Loss: 14.4660\n",
      "Epoch [750/1000], Loss: 14.4616\n",
      "Epoch [760/1000], Loss: 14.4580\n",
      "Epoch [770/1000], Loss: 14.4551\n",
      "Epoch [780/1000], Loss: 14.4527\n",
      "Epoch [790/1000], Loss: 14.4508\n",
      "Epoch [800/1000], Loss: 14.4492\n",
      "Epoch [810/1000], Loss: 14.4479\n",
      "Epoch [820/1000], Loss: 14.4469\n",
      "Epoch [830/1000], Loss: 14.4460\n",
      "Epoch [840/1000], Loss: 14.4454\n",
      "Epoch [850/1000], Loss: 14.4448\n",
      "Epoch [860/1000], Loss: 14.4444\n",
      "Epoch [870/1000], Loss: 14.4440\n",
      "Epoch [880/1000], Loss: 14.4438\n",
      "Epoch [890/1000], Loss: 14.4435\n",
      "Epoch [900/1000], Loss: 14.4433\n",
      "Epoch [910/1000], Loss: 14.4432\n",
      "Epoch [920/1000], Loss: 14.4431\n",
      "Epoch [930/1000], Loss: 14.4430\n",
      "Epoch [940/1000], Loss: 14.4429\n",
      "Epoch [950/1000], Loss: 14.4428\n",
      "Epoch [960/1000], Loss: 14.4428\n",
      "Epoch [970/1000], Loss: 14.4427\n",
      "Epoch [980/1000], Loss: 14.4427\n",
      "Epoch [990/1000], Loss: 14.4427\n",
      "Epoch [1000/1000], Loss: 14.4426\n",
      "Predicted days_remaining for parent_id 113: [24.554889678955078, 24.770076751708984, 24.77085304260254, 24.769908905029297, 24.772289276123047, 24.769718170166016, 24.772083282470703, 24.768741607666016]\n",
      "Training for parent_id 123...\n",
      "Epoch [10/1000], Loss: 684.9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 603.6503\n",
      "Epoch [30/1000], Loss: 544.8244\n",
      "Epoch [40/1000], Loss: 502.6698\n",
      "Epoch [50/1000], Loss: 467.3780\n",
      "Epoch [60/1000], Loss: 435.4860\n",
      "Epoch [70/1000], Loss: 406.1066\n",
      "Epoch [80/1000], Loss: 378.8685\n",
      "Epoch [90/1000], Loss: 353.5233\n",
      "Epoch [100/1000], Loss: 329.8733\n",
      "Epoch [110/1000], Loss: 307.7590\n",
      "Epoch [120/1000], Loss: 287.0506\n",
      "Epoch [130/1000], Loss: 267.6412\n",
      "Epoch [140/1000], Loss: 249.4406\n",
      "Epoch [150/1000], Loss: 232.3710\n",
      "Epoch [160/1000], Loss: 216.3641\n",
      "Epoch [170/1000], Loss: 201.3586\n",
      "Epoch [180/1000], Loss: 187.2988\n",
      "Epoch [190/1000], Loss: 174.1331\n",
      "Epoch [200/1000], Loss: 161.8138\n",
      "Epoch [210/1000], Loss: 150.2961\n",
      "Epoch [220/1000], Loss: 139.5376\n",
      "Epoch [230/1000], Loss: 129.4979\n",
      "Epoch [240/1000], Loss: 120.1389\n",
      "Epoch [250/1000], Loss: 111.4240\n",
      "Epoch [260/1000], Loss: 103.3180\n",
      "Epoch [270/1000], Loss: 95.7873\n",
      "Epoch [280/1000], Loss: 88.7998\n",
      "Epoch [290/1000], Loss: 82.3244\n",
      "Epoch [300/1000], Loss: 76.3314\n",
      "Epoch [310/1000], Loss: 70.7923\n",
      "Epoch [320/1000], Loss: 65.6796\n",
      "Epoch [330/1000], Loss: 60.9670\n",
      "Epoch [340/1000], Loss: 56.6294\n",
      "Epoch [350/1000], Loss: 52.6426\n",
      "Epoch [360/1000], Loss: 48.9838\n",
      "Epoch [370/1000], Loss: 45.6308\n",
      "Epoch [380/1000], Loss: 42.5626\n",
      "Epoch [390/1000], Loss: 39.7595\n",
      "Epoch [400/1000], Loss: 37.2024\n",
      "Epoch [410/1000], Loss: 34.8733\n",
      "Epoch [420/1000], Loss: 32.7553\n",
      "Epoch [430/1000], Loss: 30.8323\n",
      "Epoch [440/1000], Loss: 29.0890\n",
      "Epoch [450/1000], Loss: 27.5114\n",
      "Epoch [460/1000], Loss: 26.0858\n",
      "Epoch [470/1000], Loss: 24.7998\n",
      "Epoch [480/1000], Loss: 23.6417\n",
      "Epoch [490/1000], Loss: 22.6003\n",
      "Epoch [500/1000], Loss: 21.6656\n",
      "Epoch [510/1000], Loss: 20.8280\n",
      "Epoch [520/1000], Loss: 20.0786\n",
      "Epoch [530/1000], Loss: 19.4093\n",
      "Epoch [540/1000], Loss: 18.8125\n",
      "Epoch [550/1000], Loss: 18.2813\n",
      "Epoch [560/1000], Loss: 17.8093\n",
      "Epoch [570/1000], Loss: 17.3905\n",
      "Epoch [580/1000], Loss: 17.0197\n",
      "Epoch [590/1000], Loss: 16.6919\n",
      "Epoch [600/1000], Loss: 16.4026\n",
      "Epoch [610/1000], Loss: 16.1477\n",
      "Epoch [620/1000], Loss: 15.9235\n",
      "Epoch [630/1000], Loss: 15.7266\n",
      "Epoch [640/1000], Loss: 15.5541\n",
      "Epoch [650/1000], Loss: 15.4032\n",
      "Epoch [660/1000], Loss: 15.2713\n",
      "Epoch [670/1000], Loss: 15.1563\n",
      "Epoch [680/1000], Loss: 15.0563\n",
      "Epoch [690/1000], Loss: 14.9693\n",
      "Epoch [700/1000], Loss: 14.8939\n",
      "Epoch [710/1000], Loss: 14.8286\n",
      "Epoch [720/1000], Loss: 14.7721\n",
      "Epoch [730/1000], Loss: 14.7234\n",
      "Epoch [740/1000], Loss: 14.6815\n",
      "Epoch [750/1000], Loss: 14.6454\n",
      "Epoch [760/1000], Loss: 14.6144\n",
      "Epoch [770/1000], Loss: 14.5879\n",
      "Epoch [780/1000], Loss: 14.5652\n",
      "Epoch [790/1000], Loss: 14.5458\n",
      "Epoch [800/1000], Loss: 14.5293\n",
      "Epoch [810/1000], Loss: 14.5153\n",
      "Epoch [820/1000], Loss: 14.5033\n",
      "Epoch [830/1000], Loss: 14.4932\n",
      "Epoch [840/1000], Loss: 14.4847\n",
      "Epoch [850/1000], Loss: 14.4774\n",
      "Epoch [860/1000], Loss: 14.4713\n",
      "Epoch [870/1000], Loss: 14.4662\n",
      "Epoch [880/1000], Loss: 14.4619\n",
      "Epoch [890/1000], Loss: 14.4582\n",
      "Epoch [900/1000], Loss: 14.4552\n",
      "Epoch [910/1000], Loss: 14.4527\n",
      "Epoch [920/1000], Loss: 14.4506\n",
      "Epoch [930/1000], Loss: 14.4488\n",
      "Epoch [940/1000], Loss: 14.4473\n",
      "Epoch [950/1000], Loss: 14.4461\n",
      "Epoch [960/1000], Loss: 14.4451\n",
      "Epoch [970/1000], Loss: 14.4443\n",
      "Epoch [980/1000], Loss: 14.4436\n",
      "Epoch [990/1000], Loss: 14.4430\n",
      "Epoch [1000/1000], Loss: 14.4426\n",
      "Predicted days_remaining for parent_id 123: [27.560331344604492, 27.73151206970215, 27.7292423248291, 27.72437858581543, 27.726350784301758, 27.729095458984375, 27.729747772216797, 27.71424102783203]\n",
      "Training for parent_id 124...\n",
      "Epoch [10/1000], Loss: 178.2493\n",
      "Epoch [20/1000], Loss: 141.5943\n",
      "Epoch [30/1000], Loss: 118.5830\n",
      "Epoch [40/1000], Loss: 102.2719\n",
      "Epoch [50/1000], Loss: 88.6812\n",
      "Epoch [60/1000], Loss: 77.0220\n",
      "Epoch [70/1000], Loss: 66.9975\n",
      "Epoch [80/1000], Loss: 58.3916\n",
      "Epoch [90/1000], Loss: 51.0258\n",
      "Epoch [100/1000], Loss: 44.7438\n",
      "Epoch [110/1000], Loss: 39.4077\n",
      "Epoch [120/1000], Loss: 34.8956\n",
      "Epoch [130/1000], Loss: 31.0998\n",
      "Epoch [140/1000], Loss: 27.9246\n",
      "Epoch [150/1000], Loss: 25.2849\n",
      "Epoch [160/1000], Loss: 23.1047\n",
      "Epoch [170/1000], Loss: 21.3163\n",
      "Epoch [180/1000], Loss: 19.8598\n",
      "Epoch [190/1000], Loss: 18.6825\n",
      "Epoch [200/1000], Loss: 17.7380\n",
      "Epoch [210/1000], Loss: 16.9861\n",
      "Epoch [220/1000], Loss: 16.3923\n",
      "Epoch [230/1000], Loss: 15.9271\n",
      "Epoch [240/1000], Loss: 15.5656\n",
      "Epoch [250/1000], Loss: 15.2868\n",
      "Epoch [260/1000], Loss: 15.0737\n",
      "Epoch [270/1000], Loss: 14.9120\n",
      "Epoch [280/1000], Loss: 14.7904\n",
      "Epoch [290/1000], Loss: 14.6996\n",
      "Epoch [300/1000], Loss: 14.6324\n",
      "Epoch [310/1000], Loss: 14.5830\n",
      "Epoch [320/1000], Loss: 14.5470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [330/1000], Loss: 14.5209\n",
      "Epoch [340/1000], Loss: 14.5022\n",
      "Epoch [350/1000], Loss: 14.4888\n",
      "Epoch [360/1000], Loss: 14.4793\n",
      "Epoch [370/1000], Loss: 14.4726\n",
      "Epoch [380/1000], Loss: 14.4679\n",
      "Epoch [390/1000], Loss: 14.4645\n",
      "Epoch [400/1000], Loss: 14.4622\n",
      "Epoch [410/1000], Loss: 14.4605\n",
      "Epoch [420/1000], Loss: 14.4593\n",
      "Epoch [430/1000], Loss: 14.4584\n",
      "Epoch [440/1000], Loss: 14.4577\n",
      "Epoch [450/1000], Loss: 14.4572\n",
      "Epoch [460/1000], Loss: 14.4567\n",
      "Epoch [470/1000], Loss: 14.4564\n",
      "Epoch [480/1000], Loss: 14.4560\n",
      "Epoch [490/1000], Loss: 14.4557\n",
      "Epoch [500/1000], Loss: 14.4554\n",
      "Epoch [510/1000], Loss: 14.4552\n",
      "Epoch [520/1000], Loss: 14.4549\n",
      "Epoch [530/1000], Loss: 14.4547\n",
      "Epoch [540/1000], Loss: 14.4544\n",
      "Epoch [550/1000], Loss: 14.4542\n",
      "Epoch [560/1000], Loss: 14.4540\n",
      "Epoch [570/1000], Loss: 14.4537\n",
      "Epoch [580/1000], Loss: 14.4535\n",
      "Epoch [590/1000], Loss: 14.4533\n",
      "Epoch [600/1000], Loss: 14.4531\n",
      "Epoch [610/1000], Loss: 14.4529\n",
      "Epoch [620/1000], Loss: 14.4526\n",
      "Epoch [630/1000], Loss: 14.4524\n",
      "Epoch [640/1000], Loss: 14.4522\n",
      "Epoch [650/1000], Loss: 14.4520\n",
      "Epoch [660/1000], Loss: 14.4518\n",
      "Epoch [670/1000], Loss: 14.4516\n",
      "Epoch [680/1000], Loss: 14.4515\n",
      "Epoch [690/1000], Loss: 14.4513\n",
      "Epoch [700/1000], Loss: 14.4511\n",
      "Epoch [710/1000], Loss: 14.4509\n",
      "Epoch [720/1000], Loss: 14.4507\n",
      "Epoch [730/1000], Loss: 14.4506\n",
      "Epoch [740/1000], Loss: 14.4504\n",
      "Epoch [750/1000], Loss: 14.4502\n",
      "Epoch [760/1000], Loss: 14.4500\n",
      "Epoch [770/1000], Loss: 14.4499\n",
      "Epoch [780/1000], Loss: 14.4497\n",
      "Epoch [790/1000], Loss: 14.4496\n",
      "Epoch [800/1000], Loss: 14.4494\n",
      "Epoch [810/1000], Loss: 14.4493\n",
      "Epoch [820/1000], Loss: 14.4491\n",
      "Epoch [830/1000], Loss: 14.4490\n",
      "Epoch [840/1000], Loss: 14.4488\n",
      "Epoch [850/1000], Loss: 14.4487\n",
      "Epoch [860/1000], Loss: 14.4485\n",
      "Epoch [870/1000], Loss: 14.4484\n",
      "Epoch [880/1000], Loss: 14.4483\n",
      "Epoch [890/1000], Loss: 14.4481\n",
      "Epoch [900/1000], Loss: 14.4480\n",
      "Epoch [910/1000], Loss: 14.4479\n",
      "Epoch [920/1000], Loss: 14.4477\n",
      "Epoch [930/1000], Loss: 14.4476\n",
      "Epoch [940/1000], Loss: 14.4475\n",
      "Epoch [950/1000], Loss: 14.4474\n",
      "Epoch [960/1000], Loss: 14.4472\n",
      "Epoch [970/1000], Loss: 14.4471\n",
      "Epoch [980/1000], Loss: 14.4470\n",
      "Epoch [990/1000], Loss: 14.4469\n",
      "Epoch [1000/1000], Loss: 14.4468\n",
      "Predicted days_remaining for parent_id 124: [14.496295928955078, 14.793034553527832, 14.793663024902344, 14.79365348815918, 14.79203987121582, 14.775798797607422, 14.783819198608398, 14.772655487060547]\n",
      "Training for parent_id 144...\n",
      "Epoch [10/1000], Loss: 97.2058\n",
      "Epoch [20/1000], Loss: 73.4153\n",
      "Epoch [30/1000], Loss: 58.5239\n",
      "Epoch [40/1000], Loss: 48.4404\n",
      "Epoch [50/1000], Loss: 40.7401\n",
      "Epoch [60/1000], Loss: 34.5960\n",
      "Epoch [70/1000], Loss: 29.6988\n",
      "Epoch [80/1000], Loss: 25.8397\n",
      "Epoch [90/1000], Loss: 22.8395\n",
      "Epoch [100/1000], Loss: 20.5404\n",
      "Epoch [110/1000], Loss: 18.8051\n",
      "Epoch [120/1000], Loss: 17.5162\n",
      "Epoch [130/1000], Loss: 16.5749\n",
      "Epoch [140/1000], Loss: 15.8995\n",
      "Epoch [150/1000], Loss: 15.4235\n",
      "Epoch [160/1000], Loss: 15.0944\n",
      "Epoch [170/1000], Loss: 14.8711\n",
      "Epoch [180/1000], Loss: 14.7224\n",
      "Epoch [190/1000], Loss: 14.6254\n",
      "Epoch [200/1000], Loss: 14.5632\n",
      "Epoch [210/1000], Loss: 14.5241\n",
      "Epoch [220/1000], Loss: 14.4999\n",
      "Epoch [230/1000], Loss: 14.4851\n",
      "Epoch [240/1000], Loss: 14.4762\n",
      "Epoch [250/1000], Loss: 14.4708\n",
      "Epoch [260/1000], Loss: 14.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 14.4655\n",
      "Epoch [280/1000], Loss: 14.4641\n",
      "Epoch [290/1000], Loss: 14.4632\n",
      "Epoch [300/1000], Loss: 14.4624\n",
      "Epoch [310/1000], Loss: 14.4618\n",
      "Epoch [320/1000], Loss: 14.4612\n",
      "Epoch [330/1000], Loss: 14.4607\n",
      "Epoch [340/1000], Loss: 14.4601\n",
      "Epoch [350/1000], Loss: 14.4596\n",
      "Epoch [360/1000], Loss: 14.4592\n",
      "Epoch [370/1000], Loss: 14.4587\n",
      "Epoch [380/1000], Loss: 14.4583\n",
      "Epoch [390/1000], Loss: 14.4578\n",
      "Epoch [400/1000], Loss: 14.4574\n",
      "Epoch [410/1000], Loss: 14.4570\n",
      "Epoch [420/1000], Loss: 14.4566\n",
      "Epoch [430/1000], Loss: 14.4562\n",
      "Epoch [440/1000], Loss: 14.4558\n",
      "Epoch [450/1000], Loss: 14.4554\n",
      "Epoch [460/1000], Loss: 14.4551\n",
      "Epoch [470/1000], Loss: 14.4547\n",
      "Epoch [480/1000], Loss: 14.4544\n",
      "Epoch [490/1000], Loss: 14.4540\n",
      "Epoch [500/1000], Loss: 14.4537\n",
      "Epoch [510/1000], Loss: 14.4534\n",
      "Epoch [520/1000], Loss: 14.4531\n",
      "Epoch [530/1000], Loss: 14.4528\n",
      "Epoch [540/1000], Loss: 14.4525\n",
      "Epoch [550/1000], Loss: 14.4522\n",
      "Epoch [560/1000], Loss: 14.4520\n",
      "Epoch [570/1000], Loss: 14.4517\n",
      "Epoch [580/1000], Loss: 14.4514\n",
      "Epoch [590/1000], Loss: 14.4512\n",
      "Epoch [600/1000], Loss: 14.4509\n",
      "Epoch [610/1000], Loss: 14.4507\n",
      "Epoch [620/1000], Loss: 14.4504\n",
      "Epoch [630/1000], Loss: 14.4502\n",
      "Epoch [640/1000], Loss: 14.4500\n",
      "Epoch [650/1000], Loss: 14.4498\n",
      "Epoch [660/1000], Loss: 14.4496\n",
      "Epoch [670/1000], Loss: 14.4493\n",
      "Epoch [680/1000], Loss: 14.4491\n",
      "Epoch [690/1000], Loss: 14.4489\n",
      "Epoch [700/1000], Loss: 14.4487\n",
      "Epoch [710/1000], Loss: 14.4486\n",
      "Epoch [720/1000], Loss: 14.4484\n",
      "Epoch [730/1000], Loss: 14.4482\n",
      "Epoch [740/1000], Loss: 14.4480\n",
      "Epoch [750/1000], Loss: 14.4478\n",
      "Epoch [760/1000], Loss: 14.4477\n",
      "Epoch [770/1000], Loss: 14.4475\n",
      "Epoch [780/1000], Loss: 14.4474\n",
      "Epoch [790/1000], Loss: 14.4472\n",
      "Epoch [800/1000], Loss: 14.4470\n",
      "Epoch [810/1000], Loss: 14.4469\n",
      "Epoch [820/1000], Loss: 14.4467\n",
      "Epoch [830/1000], Loss: 14.4466\n",
      "Epoch [840/1000], Loss: 14.4465\n",
      "Epoch [850/1000], Loss: 14.4463\n",
      "Epoch [860/1000], Loss: 14.4462\n",
      "Epoch [870/1000], Loss: 14.4461\n",
      "Epoch [880/1000], Loss: 14.4459\n",
      "Epoch [890/1000], Loss: 14.4458\n",
      "Epoch [900/1000], Loss: 14.4457\n",
      "Epoch [910/1000], Loss: 14.4456\n",
      "Epoch [920/1000], Loss: 14.4454\n",
      "Epoch [930/1000], Loss: 14.4453\n",
      "Epoch [940/1000], Loss: 14.4452\n",
      "Epoch [950/1000], Loss: 14.4451\n",
      "Epoch [960/1000], Loss: 14.4450\n",
      "Epoch [970/1000], Loss: 14.4449\n",
      "Epoch [980/1000], Loss: 14.4448\n",
      "Epoch [990/1000], Loss: 14.4447\n",
      "Epoch [1000/1000], Loss: 14.4446\n",
      "Predicted days_remaining for parent_id 144: [10.527392387390137, 10.784249305725098, 10.78221321105957, 10.778311729431152, 10.786765098571777, 10.782596588134766, 10.775833129882812, 10.781333923339844]\n",
      "Training for parent_id 150...\n",
      "Epoch [10/1000], Loss: 168.6824\n",
      "Epoch [20/1000], Loss: 132.1411\n",
      "Epoch [30/1000], Loss: 106.1832\n",
      "Epoch [40/1000], Loss: 89.1948\n",
      "Epoch [50/1000], Loss: 76.3333\n",
      "Epoch [60/1000], Loss: 65.6776\n",
      "Epoch [70/1000], Loss: 56.6610\n",
      "Epoch [80/1000], Loss: 49.0292\n",
      "Epoch [90/1000], Loss: 42.5992\n",
      "Epoch [100/1000], Loss: 37.2117\n",
      "Epoch [110/1000], Loss: 32.7260\n",
      "Epoch [120/1000], Loss: 29.0168\n",
      "Epoch [130/1000], Loss: 25.9725\n",
      "Epoch [140/1000], Loss: 23.4935\n",
      "Epoch [150/1000], Loss: 21.4917\n",
      "Epoch [160/1000], Loss: 19.8890\n",
      "Epoch [170/1000], Loss: 18.6175\n",
      "Epoch [180/1000], Loss: 17.6179\n",
      "Epoch [190/1000], Loss: 16.8395\n",
      "Epoch [200/1000], Loss: 16.2392\n",
      "Epoch [210/1000], Loss: 15.7806\n",
      "Epoch [220/1000], Loss: 15.4338\n",
      "Epoch [230/1000], Loss: 15.1740\n",
      "Epoch [240/1000], Loss: 14.9812\n",
      "Epoch [250/1000], Loss: 14.8397\n",
      "Epoch [260/1000], Loss: 14.7366\n",
      "Epoch [270/1000], Loss: 14.6623\n",
      "Epoch [280/1000], Loss: 14.6092\n",
      "Epoch [290/1000], Loss: 14.5715\n",
      "Epoch [300/1000], Loss: 14.5449\n",
      "Epoch [310/1000], Loss: 14.5263\n",
      "Epoch [320/1000], Loss: 14.5133\n",
      "Epoch [330/1000], Loss: 14.5042\n",
      "Epoch [340/1000], Loss: 14.4978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/1000], Loss: 14.4933\n",
      "Epoch [360/1000], Loss: 14.4900\n",
      "Epoch [370/1000], Loss: 14.4876\n",
      "Epoch [380/1000], Loss: 14.4858\n",
      "Epoch [390/1000], Loss: 14.4843\n",
      "Epoch [400/1000], Loss: 14.4830\n",
      "Epoch [410/1000], Loss: 14.4819\n",
      "Epoch [420/1000], Loss: 14.4810\n",
      "Epoch [430/1000], Loss: 14.4801\n",
      "Epoch [440/1000], Loss: 14.4793\n",
      "Epoch [450/1000], Loss: 14.4785\n",
      "Epoch [460/1000], Loss: 14.4777\n",
      "Epoch [470/1000], Loss: 14.4769\n",
      "Epoch [480/1000], Loss: 14.4762\n",
      "Epoch [490/1000], Loss: 14.4755\n",
      "Epoch [500/1000], Loss: 14.4748\n",
      "Epoch [510/1000], Loss: 14.4742\n",
      "Epoch [520/1000], Loss: 14.4735\n",
      "Epoch [530/1000], Loss: 14.4729\n",
      "Epoch [540/1000], Loss: 14.4723\n",
      "Epoch [550/1000], Loss: 14.4717\n",
      "Epoch [560/1000], Loss: 14.4711\n",
      "Epoch [570/1000], Loss: 14.4705\n",
      "Epoch [580/1000], Loss: 14.4699\n",
      "Epoch [590/1000], Loss: 14.4694\n",
      "Epoch [600/1000], Loss: 14.4688\n",
      "Epoch [610/1000], Loss: 14.4683\n",
      "Epoch [620/1000], Loss: 14.4678\n",
      "Epoch [630/1000], Loss: 14.4673\n",
      "Epoch [640/1000], Loss: 14.4668\n",
      "Epoch [650/1000], Loss: 14.4663\n",
      "Epoch [660/1000], Loss: 14.4659\n",
      "Epoch [670/1000], Loss: 14.4654\n",
      "Epoch [680/1000], Loss: 14.4650\n",
      "Epoch [690/1000], Loss: 14.4645\n",
      "Epoch [700/1000], Loss: 14.4641\n",
      "Epoch [710/1000], Loss: 14.4637\n",
      "Epoch [720/1000], Loss: 14.4633\n",
      "Epoch [730/1000], Loss: 14.4629\n",
      "Epoch [740/1000], Loss: 14.4625\n",
      "Epoch [750/1000], Loss: 14.4621\n",
      "Epoch [760/1000], Loss: 14.4617\n",
      "Epoch [770/1000], Loss: 14.4613\n",
      "Epoch [780/1000], Loss: 14.4610\n",
      "Epoch [790/1000], Loss: 14.4606\n",
      "Epoch [800/1000], Loss: 14.4603\n",
      "Epoch [810/1000], Loss: 14.4599\n",
      "Epoch [820/1000], Loss: 14.4596\n",
      "Epoch [830/1000], Loss: 14.4593\n",
      "Epoch [840/1000], Loss: 14.4590\n",
      "Epoch [850/1000], Loss: 14.4587\n",
      "Epoch [860/1000], Loss: 14.4584\n",
      "Epoch [870/1000], Loss: 14.4581\n",
      "Epoch [880/1000], Loss: 14.4578\n",
      "Epoch [890/1000], Loss: 14.4575\n",
      "Epoch [900/1000], Loss: 14.4572\n",
      "Epoch [910/1000], Loss: 14.4569\n",
      "Epoch [920/1000], Loss: 14.4566\n",
      "Epoch [930/1000], Loss: 14.4564\n",
      "Epoch [940/1000], Loss: 14.4561\n",
      "Epoch [950/1000], Loss: 14.4559\n",
      "Epoch [960/1000], Loss: 14.4556\n",
      "Epoch [970/1000], Loss: 14.4554\n",
      "Epoch [980/1000], Loss: 14.4551\n",
      "Epoch [990/1000], Loss: 14.4549\n",
      "Epoch [1000/1000], Loss: 14.4547\n",
      "Predicted days_remaining for parent_id 150: [13.403507232666016, 13.796555519104004, 13.804237365722656, 13.801071166992188, 13.800962448120117, 13.793395042419434, 13.797765731811523, 13.800871849060059]\n",
      "Training for parent_id 166...\n",
      "Epoch [10/1000], Loss: 2527.3154\n",
      "Epoch [20/1000], Loss: 2347.1990\n",
      "Epoch [30/1000], Loss: 2218.2373\n",
      "Epoch [40/1000], Loss: 2126.2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 2050.7612\n",
      "Epoch [60/1000], Loss: 1981.5614\n",
      "Epoch [70/1000], Loss: 1915.9568\n",
      "Epoch [80/1000], Loss: 1853.3186\n",
      "Epoch [90/1000], Loss: 1793.2903\n",
      "Epoch [100/1000], Loss: 1735.5592\n",
      "Epoch [110/1000], Loss: 1679.8784\n",
      "Epoch [120/1000], Loss: 1626.0623\n",
      "Epoch [130/1000], Loss: 1573.9677\n",
      "Epoch [140/1000], Loss: 1523.4817\n",
      "Epoch [150/1000], Loss: 1474.5120\n",
      "Epoch [160/1000], Loss: 1426.9821\n",
      "Epoch [170/1000], Loss: 1380.8265\n",
      "Epoch [180/1000], Loss: 1335.9897\n",
      "Epoch [190/1000], Loss: 1292.4219\n",
      "Epoch [200/1000], Loss: 1250.0791\n",
      "Epoch [210/1000], Loss: 1208.9220\n",
      "Epoch [220/1000], Loss: 1168.9144\n",
      "Epoch [230/1000], Loss: 1130.0229\n",
      "Epoch [240/1000], Loss: 1092.2168\n",
      "Epoch [250/1000], Loss: 1055.4673\n",
      "Epoch [260/1000], Loss: 1019.7473\n",
      "Epoch [270/1000], Loss: 985.0308\n",
      "Epoch [280/1000], Loss: 951.2934\n",
      "Epoch [290/1000], Loss: 918.5118\n",
      "Epoch [300/1000], Loss: 886.6635\n",
      "Epoch [310/1000], Loss: 855.7267\n",
      "Epoch [320/1000], Loss: 825.6804\n",
      "Epoch [330/1000], Loss: 796.5044\n",
      "Epoch [340/1000], Loss: 768.1791\n",
      "Epoch [350/1000], Loss: 740.6851\n",
      "Epoch [360/1000], Loss: 714.0035\n",
      "Epoch [370/1000], Loss: 688.1162\n",
      "Epoch [380/1000], Loss: 663.0053\n",
      "Epoch [390/1000], Loss: 638.6534\n",
      "Epoch [400/1000], Loss: 615.0430\n",
      "Epoch [410/1000], Loss: 592.1574\n",
      "Epoch [420/1000], Loss: 569.9800\n",
      "Epoch [430/1000], Loss: 548.4946\n",
      "Epoch [440/1000], Loss: 527.6850\n",
      "Epoch [450/1000], Loss: 507.5357\n",
      "Epoch [460/1000], Loss: 488.0311\n",
      "Epoch [470/1000], Loss: 469.1560\n",
      "Epoch [480/1000], Loss: 450.8951\n",
      "Epoch [490/1000], Loss: 433.2341\n",
      "Epoch [500/1000], Loss: 416.1581\n",
      "Epoch [510/1000], Loss: 399.6529\n",
      "Epoch [520/1000], Loss: 383.7041\n",
      "Epoch [530/1000], Loss: 368.2982\n",
      "Epoch [540/1000], Loss: 353.4212\n",
      "Epoch [550/1000], Loss: 339.0595\n",
      "Epoch [560/1000], Loss: 325.2001\n",
      "Epoch [570/1000], Loss: 311.8296\n",
      "Epoch [580/1000], Loss: 298.9353\n",
      "Epoch [590/1000], Loss: 286.5044\n",
      "Epoch [600/1000], Loss: 274.5243\n",
      "Epoch [610/1000], Loss: 262.9826\n",
      "Epoch [620/1000], Loss: 251.8675\n",
      "Epoch [630/1000], Loss: 241.1669\n",
      "Epoch [640/1000], Loss: 230.8690\n",
      "Epoch [650/1000], Loss: 220.9624\n",
      "Epoch [660/1000], Loss: 211.4356\n",
      "Epoch [670/1000], Loss: 202.2776\n",
      "Epoch [680/1000], Loss: 193.4772\n",
      "Epoch [690/1000], Loss: 185.0239\n",
      "Epoch [700/1000], Loss: 176.9072\n",
      "Epoch [710/1000], Loss: 169.1164\n",
      "Epoch [720/1000], Loss: 161.6417\n",
      "Epoch [730/1000], Loss: 154.4729\n",
      "Epoch [740/1000], Loss: 147.6002\n",
      "Epoch [750/1000], Loss: 141.0141\n",
      "Epoch [760/1000], Loss: 134.7053\n",
      "Epoch [770/1000], Loss: 128.6646\n",
      "Epoch [780/1000], Loss: 122.8828\n",
      "Epoch [790/1000], Loss: 117.3512\n",
      "Epoch [800/1000], Loss: 112.0613\n",
      "Epoch [810/1000], Loss: 107.0047\n",
      "Epoch [820/1000], Loss: 102.1731\n",
      "Epoch [830/1000], Loss: 97.5584\n",
      "Epoch [840/1000], Loss: 93.1530\n",
      "Epoch [850/1000], Loss: 88.9490\n",
      "Epoch [860/1000], Loss: 84.9391\n",
      "Epoch [870/1000], Loss: 81.1161\n",
      "Epoch [880/1000], Loss: 77.4728\n",
      "Epoch [890/1000], Loss: 74.0023\n",
      "Epoch [900/1000], Loss: 70.6980\n",
      "Epoch [910/1000], Loss: 67.5533\n",
      "Epoch [920/1000], Loss: 64.5620\n",
      "Epoch [930/1000], Loss: 61.7178\n",
      "Epoch [940/1000], Loss: 59.0148\n",
      "Epoch [950/1000], Loss: 56.4472\n",
      "Epoch [960/1000], Loss: 54.0093\n",
      "Epoch [970/1000], Loss: 51.6957\n",
      "Epoch [980/1000], Loss: 49.5010\n",
      "Epoch [990/1000], Loss: 47.4203\n",
      "Epoch [1000/1000], Loss: 45.4486\n",
      "Predicted days_remaining for parent_id 166: [46.15172576904297, 46.20663070678711, 46.20511245727539, 46.206199645996094, 46.204498291015625, 46.205413818359375, 46.20368957519531, 46.2045783996582]\n",
      "Training for parent_id 168...\n",
      "Epoch [10/1000], Loss: 333.3152\n",
      "Epoch [20/1000], Loss: 275.6768\n",
      "Epoch [30/1000], Loss: 240.9230\n",
      "Epoch [40/1000], Loss: 216.1091\n",
      "Epoch [50/1000], Loss: 195.0569\n",
      "Epoch [60/1000], Loss: 176.2266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000], Loss: 159.2415\n",
      "Epoch [80/1000], Loss: 143.8889\n",
      "Epoch [90/1000], Loss: 130.0011\n",
      "Epoch [100/1000], Loss: 117.4341\n",
      "Epoch [110/1000], Loss: 106.0635\n",
      "Epoch [120/1000], Loss: 95.7816\n",
      "Epoch [130/1000], Loss: 86.4944\n",
      "Epoch [140/1000], Loss: 78.1181\n",
      "Epoch [150/1000], Loss: 70.5771\n",
      "Epoch [160/1000], Loss: 63.8022\n",
      "Epoch [170/1000], Loss: 57.7295\n",
      "Epoch [180/1000], Loss: 52.2999\n",
      "Epoch [190/1000], Loss: 47.4579\n",
      "Epoch [200/1000], Loss: 43.1522\n",
      "Epoch [210/1000], Loss: 39.3344\n",
      "Epoch [220/1000], Loss: 35.9595\n",
      "Epoch [230/1000], Loss: 32.9853\n",
      "Epoch [240/1000], Loss: 30.3727\n",
      "Epoch [250/1000], Loss: 28.0851\n",
      "Epoch [260/1000], Loss: 26.0890\n",
      "Epoch [270/1000], Loss: 24.3529\n",
      "Epoch [280/1000], Loss: 22.8482\n",
      "Epoch [290/1000], Loss: 21.5487\n",
      "Epoch [300/1000], Loss: 20.4303\n",
      "Epoch [310/1000], Loss: 19.4713\n",
      "Epoch [320/1000], Loss: 18.6517\n",
      "Epoch [330/1000], Loss: 17.9540\n",
      "Epoch [340/1000], Loss: 17.3621\n",
      "Epoch [350/1000], Loss: 16.8618\n",
      "Epoch [360/1000], Loss: 16.4405\n",
      "Epoch [370/1000], Loss: 16.0870\n",
      "Epoch [380/1000], Loss: 15.7915\n",
      "Epoch [390/1000], Loss: 15.5454\n",
      "Epoch [400/1000], Loss: 15.3412\n",
      "Epoch [410/1000], Loss: 15.1724\n",
      "Epoch [420/1000], Loss: 15.0333\n",
      "Epoch [430/1000], Loss: 14.9192\n",
      "Epoch [440/1000], Loss: 14.8259\n",
      "Epoch [450/1000], Loss: 14.7499\n",
      "Epoch [460/1000], Loss: 14.6882\n",
      "Epoch [470/1000], Loss: 14.6383\n",
      "Epoch [480/1000], Loss: 14.5981\n",
      "Epoch [490/1000], Loss: 14.5659\n",
      "Epoch [500/1000], Loss: 14.5401\n",
      "Epoch [510/1000], Loss: 14.5195\n",
      "Epoch [520/1000], Loss: 14.5032\n",
      "Epoch [530/1000], Loss: 14.4902\n",
      "Epoch [540/1000], Loss: 14.4801\n",
      "Epoch [550/1000], Loss: 14.4721\n",
      "Epoch [560/1000], Loss: 14.4658\n",
      "Epoch [570/1000], Loss: 14.4609\n",
      "Epoch [580/1000], Loss: 14.4571\n",
      "Epoch [590/1000], Loss: 14.4541\n",
      "Epoch [600/1000], Loss: 14.4519\n",
      "Epoch [610/1000], Loss: 14.4501\n",
      "Epoch [620/1000], Loss: 14.4488\n",
      "Epoch [630/1000], Loss: 14.4477\n",
      "Epoch [640/1000], Loss: 14.4469\n",
      "Epoch [650/1000], Loss: 14.4463\n",
      "Epoch [660/1000], Loss: 14.4458\n",
      "Epoch [670/1000], Loss: 14.4455\n",
      "Epoch [680/1000], Loss: 14.4452\n",
      "Epoch [690/1000], Loss: 14.4450\n",
      "Epoch [700/1000], Loss: 14.4448\n",
      "Epoch [710/1000], Loss: 14.4447\n",
      "Epoch [720/1000], Loss: 14.4446\n",
      "Epoch [730/1000], Loss: 14.4445\n",
      "Epoch [740/1000], Loss: 14.4444\n",
      "Epoch [750/1000], Loss: 14.4444\n",
      "Epoch [760/1000], Loss: 14.4443\n",
      "Epoch [770/1000], Loss: 14.4442\n",
      "Epoch [780/1000], Loss: 14.4442\n",
      "Epoch [790/1000], Loss: 14.4442\n",
      "Epoch [800/1000], Loss: 14.4441\n",
      "Epoch [810/1000], Loss: 14.4441\n",
      "Epoch [820/1000], Loss: 14.4440\n",
      "Epoch [830/1000], Loss: 14.4440\n",
      "Epoch [840/1000], Loss: 14.4440\n",
      "Epoch [850/1000], Loss: 14.4439\n",
      "Epoch [860/1000], Loss: 14.4439\n",
      "Epoch [870/1000], Loss: 14.4439\n",
      "Epoch [880/1000], Loss: 14.4438\n",
      "Epoch [890/1000], Loss: 14.4438\n",
      "Epoch [900/1000], Loss: 14.4438\n",
      "Epoch [910/1000], Loss: 14.4437\n",
      "Epoch [920/1000], Loss: 14.4437\n",
      "Epoch [930/1000], Loss: 14.4437\n",
      "Epoch [940/1000], Loss: 14.4436\n",
      "Epoch [950/1000], Loss: 14.4436\n",
      "Epoch [960/1000], Loss: 14.4436\n",
      "Epoch [970/1000], Loss: 14.4435\n",
      "Epoch [980/1000], Loss: 14.4435\n",
      "Epoch [990/1000], Loss: 14.4435\n",
      "Epoch [1000/1000], Loss: 14.4434\n",
      "Predicted days_remaining for parent_id 168: [19.547067642211914, 19.782995223999023, 19.78359603881836, 19.783018112182617, 19.782136917114258, 19.780786514282227, 19.77731704711914, 19.764387130737305]\n",
      "Training for parent_id 179...\n",
      "Epoch [10/1000], Loss: 160.5871\n",
      "Epoch [20/1000], Loss: 123.4245\n",
      "Epoch [30/1000], Loss: 101.7624\n",
      "Epoch [40/1000], Loss: 86.8148\n",
      "Epoch [50/1000], Loss: 74.5734\n",
      "Epoch [60/1000], Loss: 64.2604\n",
      "Epoch [70/1000], Loss: 55.5008\n",
      "Epoch [80/1000], Loss: 48.0683\n",
      "Epoch [90/1000], Loss: 41.8119\n",
      "Epoch [100/1000], Loss: 36.5756\n",
      "Epoch [110/1000], Loss: 32.2189\n",
      "Epoch [120/1000], Loss: 28.6175\n",
      "Epoch [130/1000], Loss: 25.6614\n",
      "Epoch [140/1000], Loss: 23.2534\n",
      "Epoch [150/1000], Loss: 21.3077\n",
      "Epoch [160/1000], Loss: 19.7487\n",
      "Epoch [170/1000], Loss: 18.5105\n",
      "Epoch [180/1000], Loss: 17.5361\n",
      "Epoch [190/1000], Loss: 16.7764\n",
      "Epoch [200/1000], Loss: 16.1897\n",
      "Epoch [210/1000], Loss: 15.7410\n",
      "Epoch [220/1000], Loss: 15.4011\n",
      "Epoch [230/1000], Loss: 15.1462\n",
      "Epoch [240/1000], Loss: 14.9568\n",
      "Epoch [250/1000], Loss: 14.8175\n",
      "Epoch [260/1000], Loss: 14.7160\n",
      "Epoch [270/1000], Loss: 14.6428\n",
      "Epoch [280/1000], Loss: 14.5904\n",
      "Epoch [290/1000], Loss: 14.5533\n",
      "Epoch [300/1000], Loss: 14.5271\n",
      "Epoch [310/1000], Loss: 14.5089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/1000], Loss: 14.4962\n",
      "Epoch [330/1000], Loss: 14.4874\n",
      "Epoch [340/1000], Loss: 14.4813\n",
      "Epoch [350/1000], Loss: 14.4771\n",
      "Epoch [360/1000], Loss: 14.4741\n",
      "Epoch [370/1000], Loss: 14.4720\n",
      "Epoch [380/1000], Loss: 14.4704\n",
      "Epoch [390/1000], Loss: 14.4692\n",
      "Epoch [400/1000], Loss: 14.4683\n",
      "Epoch [410/1000], Loss: 14.4675\n",
      "Epoch [420/1000], Loss: 14.4668\n",
      "Epoch [430/1000], Loss: 14.4662\n",
      "Epoch [440/1000], Loss: 14.4656\n",
      "Epoch [450/1000], Loss: 14.4651\n",
      "Epoch [460/1000], Loss: 14.4646\n",
      "Epoch [470/1000], Loss: 14.4641\n",
      "Epoch [480/1000], Loss: 14.4636\n",
      "Epoch [490/1000], Loss: 14.4631\n",
      "Epoch [500/1000], Loss: 14.4627\n",
      "Epoch [510/1000], Loss: 14.4622\n",
      "Epoch [520/1000], Loss: 14.4618\n",
      "Epoch [530/1000], Loss: 14.4614\n",
      "Epoch [540/1000], Loss: 14.4610\n",
      "Epoch [550/1000], Loss: 14.4606\n",
      "Epoch [560/1000], Loss: 14.4602\n",
      "Epoch [570/1000], Loss: 14.4598\n",
      "Epoch [580/1000], Loss: 14.4594\n",
      "Epoch [590/1000], Loss: 14.4590\n",
      "Epoch [600/1000], Loss: 14.4587\n",
      "Epoch [610/1000], Loss: 14.4583\n",
      "Epoch [620/1000], Loss: 14.4580\n",
      "Epoch [630/1000], Loss: 14.4576\n",
      "Epoch [640/1000], Loss: 14.4573\n",
      "Epoch [650/1000], Loss: 14.4570\n",
      "Epoch [660/1000], Loss: 14.4567\n",
      "Epoch [670/1000], Loss: 14.4564\n",
      "Epoch [680/1000], Loss: 14.4561\n",
      "Epoch [690/1000], Loss: 14.4558\n",
      "Epoch [700/1000], Loss: 14.4555\n",
      "Epoch [710/1000], Loss: 14.4552\n",
      "Epoch [720/1000], Loss: 14.4549\n",
      "Epoch [730/1000], Loss: 14.4547\n",
      "Epoch [740/1000], Loss: 14.4544\n",
      "Epoch [750/1000], Loss: 14.4541\n",
      "Epoch [760/1000], Loss: 14.4539\n",
      "Epoch [770/1000], Loss: 14.4536\n",
      "Epoch [780/1000], Loss: 14.4534\n",
      "Epoch [790/1000], Loss: 14.4531\n",
      "Epoch [800/1000], Loss: 14.4529\n",
      "Epoch [810/1000], Loss: 14.4527\n",
      "Epoch [820/1000], Loss: 14.4524\n",
      "Epoch [830/1000], Loss: 14.4522\n",
      "Epoch [840/1000], Loss: 14.4520\n",
      "Epoch [850/1000], Loss: 14.4518\n",
      "Epoch [860/1000], Loss: 14.4516\n",
      "Epoch [870/1000], Loss: 14.4514\n",
      "Epoch [880/1000], Loss: 14.4512\n",
      "Epoch [890/1000], Loss: 14.4510\n",
      "Epoch [900/1000], Loss: 14.4508\n",
      "Epoch [910/1000], Loss: 14.4506\n",
      "Epoch [920/1000], Loss: 14.4504\n",
      "Epoch [930/1000], Loss: 14.4503\n",
      "Epoch [940/1000], Loss: 14.4501\n",
      "Epoch [950/1000], Loss: 14.4499\n",
      "Epoch [960/1000], Loss: 14.4497\n",
      "Epoch [970/1000], Loss: 14.4496\n",
      "Epoch [980/1000], Loss: 14.4494\n",
      "Epoch [990/1000], Loss: 14.4492\n",
      "Epoch [1000/1000], Loss: 14.4491\n",
      "Predicted days_remaining for parent_id 179: [13.465865135192871, 13.789920806884766, 13.789685249328613, 13.79535961151123, 13.799137115478516, 13.795580863952637, 13.784600257873535, 13.779780387878418]\n",
      "Training for parent_id 199...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 288.2629\n",
      "Epoch [20/1000], Loss: 242.6016\n",
      "Epoch [30/1000], Loss: 209.1379\n",
      "Epoch [40/1000], Loss: 185.0251\n",
      "Epoch [50/1000], Loss: 165.0816\n",
      "Epoch [60/1000], Loss: 147.5531\n",
      "Epoch [70/1000], Loss: 131.9401\n",
      "Epoch [80/1000], Loss: 118.0023\n",
      "Epoch [90/1000], Loss: 105.5574\n",
      "Epoch [100/1000], Loss: 94.4457\n",
      "Epoch [110/1000], Loss: 84.5286\n",
      "Epoch [120/1000], Loss: 75.6862\n",
      "Epoch [130/1000], Loss: 67.8140\n",
      "Epoch [140/1000], Loss: 60.8192\n",
      "Epoch [150/1000], Loss: 54.6183\n",
      "Epoch [160/1000], Loss: 49.1358\n",
      "Epoch [170/1000], Loss: 44.3027\n",
      "Epoch [180/1000], Loss: 40.0554\n",
      "Epoch [190/1000], Loss: 36.3355\n",
      "Epoch [200/1000], Loss: 33.0889\n",
      "Epoch [210/1000], Loss: 30.2657\n",
      "Epoch [220/1000], Loss: 27.8201\n",
      "Epoch [230/1000], Loss: 25.7098\n",
      "Epoch [240/1000], Loss: 23.8961\n",
      "Epoch [250/1000], Loss: 22.3436\n",
      "Epoch [260/1000], Loss: 21.0204\n",
      "Epoch [270/1000], Loss: 19.8972\n",
      "Epoch [280/1000], Loss: 18.9478\n",
      "Epoch [290/1000], Loss: 18.1489\n",
      "Epoch [300/1000], Loss: 17.4795\n",
      "Epoch [310/1000], Loss: 16.9210\n",
      "Epoch [320/1000], Loss: 16.4572\n",
      "Epoch [330/1000], Loss: 16.0737\n",
      "Epoch [340/1000], Loss: 15.7579\n",
      "Epoch [350/1000], Loss: 15.4991\n",
      "Epoch [360/1000], Loss: 15.2879\n",
      "Epoch [370/1000], Loss: 15.1163\n",
      "Epoch [380/1000], Loss: 14.9776\n",
      "Epoch [390/1000], Loss: 14.8659\n",
      "Epoch [400/1000], Loss: 14.7763\n",
      "Epoch [410/1000], Loss: 14.7049\n",
      "Epoch [420/1000], Loss: 14.6481\n",
      "Epoch [430/1000], Loss: 14.6031\n",
      "Epoch [440/1000], Loss: 14.5678\n",
      "Epoch [450/1000], Loss: 14.5400\n",
      "Epoch [460/1000], Loss: 14.5184\n",
      "Epoch [470/1000], Loss: 14.5015\n",
      "Epoch [480/1000], Loss: 14.4885\n",
      "Epoch [490/1000], Loss: 14.4784\n",
      "Epoch [500/1000], Loss: 14.4707\n",
      "Epoch [510/1000], Loss: 14.4648\n",
      "Epoch [520/1000], Loss: 14.4603\n",
      "Epoch [530/1000], Loss: 14.4569\n",
      "Epoch [540/1000], Loss: 14.4543\n",
      "Epoch [550/1000], Loss: 14.4523\n",
      "Epoch [560/1000], Loss: 14.4509\n",
      "Epoch [570/1000], Loss: 14.4497\n",
      "Epoch [580/1000], Loss: 14.4489\n",
      "Epoch [590/1000], Loss: 14.4483\n",
      "Epoch [600/1000], Loss: 14.4478\n",
      "Epoch [610/1000], Loss: 14.4474\n",
      "Epoch [620/1000], Loss: 14.4471\n",
      "Epoch [630/1000], Loss: 14.4469\n",
      "Epoch [640/1000], Loss: 14.4467\n",
      "Epoch [650/1000], Loss: 14.4466\n",
      "Epoch [660/1000], Loss: 14.4465\n",
      "Epoch [670/1000], Loss: 14.4464\n",
      "Epoch [680/1000], Loss: 14.4463\n",
      "Epoch [690/1000], Loss: 14.4462\n",
      "Epoch [700/1000], Loss: 14.4461\n",
      "Epoch [710/1000], Loss: 14.4461\n",
      "Epoch [720/1000], Loss: 14.4460\n",
      "Epoch [730/1000], Loss: 14.4459\n",
      "Epoch [740/1000], Loss: 14.4459\n",
      "Epoch [750/1000], Loss: 14.4458\n",
      "Epoch [760/1000], Loss: 14.4458\n",
      "Epoch [770/1000], Loss: 14.4457\n",
      "Epoch [780/1000], Loss: 14.4456\n",
      "Epoch [790/1000], Loss: 14.4456\n",
      "Epoch [800/1000], Loss: 14.4455\n",
      "Epoch [810/1000], Loss: 14.4455\n",
      "Epoch [820/1000], Loss: 14.4454\n",
      "Epoch [830/1000], Loss: 14.4453\n",
      "Epoch [840/1000], Loss: 14.4453\n",
      "Epoch [850/1000], Loss: 14.4452\n",
      "Epoch [860/1000], Loss: 14.4452\n",
      "Epoch [870/1000], Loss: 14.4451\n",
      "Epoch [880/1000], Loss: 14.4451\n",
      "Epoch [890/1000], Loss: 14.4450\n",
      "Epoch [900/1000], Loss: 14.4450\n",
      "Epoch [910/1000], Loss: 14.4449\n",
      "Epoch [920/1000], Loss: 14.4449\n",
      "Epoch [930/1000], Loss: 14.4448\n",
      "Epoch [940/1000], Loss: 14.4448\n",
      "Epoch [950/1000], Loss: 14.4447\n",
      "Epoch [960/1000], Loss: 14.4446\n",
      "Epoch [970/1000], Loss: 14.4446\n",
      "Epoch [980/1000], Loss: 14.4445\n",
      "Epoch [990/1000], Loss: 14.4445\n",
      "Epoch [1000/1000], Loss: 14.4444\n",
      "Predicted days_remaining for parent_id 199: [17.52998924255371, 17.78291893005371, 17.78407096862793, 17.7796630859375, 17.780351638793945, 17.782079696655273, 17.777585983276367, 17.785261154174805]\n",
      "Training for parent_id 211...\n",
      "Epoch [10/1000], Loss: 202.4556\n",
      "Epoch [20/1000], Loss: 158.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 131.0500\n",
      "Epoch [40/1000], Loss: 112.5002\n",
      "Epoch [50/1000], Loss: 97.8593\n",
      "Epoch [60/1000], Loss: 85.4560\n",
      "Epoch [70/1000], Loss: 74.7548\n",
      "Epoch [80/1000], Loss: 65.4875\n",
      "Epoch [90/1000], Loss: 57.4672\n",
      "Epoch [100/1000], Loss: 50.5435\n",
      "Epoch [110/1000], Loss: 44.5875\n",
      "Epoch [120/1000], Loss: 39.4857\n",
      "Epoch [130/1000], Loss: 35.1366\n",
      "Epoch [140/1000], Loss: 31.4483\n",
      "Epoch [150/1000], Loss: 28.3381\n",
      "Epoch [160/1000], Loss: 25.7307\n",
      "Epoch [170/1000], Loss: 23.5584\n",
      "Epoch [180/1000], Loss: 21.7602\n",
      "Epoch [190/1000], Loss: 20.2815\n",
      "Epoch [200/1000], Loss: 19.0739\n",
      "Epoch [210/1000], Loss: 18.0944\n",
      "Epoch [220/1000], Loss: 17.3055\n",
      "Epoch [230/1000], Loss: 16.6747\n",
      "Epoch [240/1000], Loss: 16.1739\n",
      "Epoch [250/1000], Loss: 15.7792\n",
      "Epoch [260/1000], Loss: 15.4703\n",
      "Epoch [270/1000], Loss: 15.2303\n",
      "Epoch [280/1000], Loss: 15.0453\n",
      "Epoch [290/1000], Loss: 14.9035\n",
      "Epoch [300/1000], Loss: 14.7958\n",
      "Epoch [310/1000], Loss: 14.7144\n",
      "Epoch [320/1000], Loss: 14.6533\n",
      "Epoch [330/1000], Loss: 14.6078\n",
      "Epoch [340/1000], Loss: 14.5740\n",
      "Epoch [350/1000], Loss: 14.5491\n",
      "Epoch [360/1000], Loss: 14.5309\n",
      "Epoch [370/1000], Loss: 14.5175\n",
      "Epoch [380/1000], Loss: 14.5077\n",
      "Epoch [390/1000], Loss: 14.5006\n",
      "Epoch [400/1000], Loss: 14.4953\n",
      "Epoch [410/1000], Loss: 14.4915\n",
      "Epoch [420/1000], Loss: 14.4886\n",
      "Epoch [430/1000], Loss: 14.4864\n",
      "Epoch [440/1000], Loss: 14.4846\n",
      "Epoch [450/1000], Loss: 14.4833\n",
      "Epoch [460/1000], Loss: 14.4821\n",
      "Epoch [470/1000], Loss: 14.4811\n",
      "Epoch [480/1000], Loss: 14.4803\n",
      "Epoch [490/1000], Loss: 14.4795\n",
      "Epoch [500/1000], Loss: 14.4788\n",
      "Epoch [510/1000], Loss: 14.4781\n",
      "Epoch [520/1000], Loss: 14.4775\n",
      "Epoch [530/1000], Loss: 14.4768\n",
      "Epoch [540/1000], Loss: 14.4762\n",
      "Epoch [550/1000], Loss: 14.4757\n",
      "Epoch [560/1000], Loss: 14.4751\n",
      "Epoch [570/1000], Loss: 14.4745\n",
      "Epoch [580/1000], Loss: 14.4740\n",
      "Epoch [590/1000], Loss: 14.4734\n",
      "Epoch [600/1000], Loss: 14.4729\n",
      "Epoch [610/1000], Loss: 14.4724\n",
      "Epoch [620/1000], Loss: 14.4719\n",
      "Epoch [630/1000], Loss: 14.4714\n",
      "Epoch [640/1000], Loss: 14.4709\n",
      "Epoch [650/1000], Loss: 14.4704\n",
      "Epoch [660/1000], Loss: 14.4699\n",
      "Epoch [670/1000], Loss: 14.4695\n",
      "Epoch [680/1000], Loss: 14.4690\n",
      "Epoch [690/1000], Loss: 14.4686\n",
      "Epoch [700/1000], Loss: 14.4682\n",
      "Epoch [710/1000], Loss: 14.4677\n",
      "Epoch [720/1000], Loss: 14.4673\n",
      "Epoch [730/1000], Loss: 14.4669\n",
      "Epoch [740/1000], Loss: 14.4665\n",
      "Epoch [750/1000], Loss: 14.4661\n",
      "Epoch [760/1000], Loss: 14.4657\n",
      "Epoch [770/1000], Loss: 14.4653\n",
      "Epoch [780/1000], Loss: 14.4649\n",
      "Epoch [790/1000], Loss: 14.4646\n",
      "Epoch [800/1000], Loss: 14.4642\n",
      "Epoch [810/1000], Loss: 14.4638\n",
      "Epoch [820/1000], Loss: 14.4635\n",
      "Epoch [830/1000], Loss: 14.4631\n",
      "Epoch [840/1000], Loss: 14.4628\n",
      "Epoch [850/1000], Loss: 14.4625\n",
      "Epoch [860/1000], Loss: 14.4621\n",
      "Epoch [870/1000], Loss: 14.4618\n",
      "Epoch [880/1000], Loss: 14.4615\n",
      "Epoch [890/1000], Loss: 14.4612\n",
      "Epoch [900/1000], Loss: 14.4609\n",
      "Epoch [910/1000], Loss: 14.4606\n",
      "Epoch [920/1000], Loss: 14.4603\n",
      "Epoch [930/1000], Loss: 14.4600\n",
      "Epoch [940/1000], Loss: 14.4597\n",
      "Epoch [950/1000], Loss: 14.4594\n",
      "Epoch [960/1000], Loss: 14.4591\n",
      "Epoch [970/1000], Loss: 14.4589\n",
      "Epoch [980/1000], Loss: 14.4586\n",
      "Epoch [990/1000], Loss: 14.4584\n",
      "Epoch [1000/1000], Loss: 14.4581\n",
      "Predicted days_remaining for parent_id 211: [15.370612144470215, 15.801596641540527, 15.804192543029785, 15.808073997497559, 15.805508613586426, 15.801544189453125, 15.805359840393066, 15.803227424621582]\n",
      "Training for parent_id 215...\n",
      "Epoch [10/1000], Loss: 333.1344\n",
      "Epoch [20/1000], Loss: 279.7355\n",
      "Epoch [30/1000], Loss: 232.0201\n",
      "Epoch [40/1000], Loss: 200.5584\n",
      "Epoch [50/1000], Loss: 177.0779\n",
      "Epoch [60/1000], Loss: 157.2161\n",
      "Epoch [70/1000], Loss: 139.7122\n",
      "Epoch [80/1000], Loss: 124.2302\n",
      "Epoch [90/1000], Loss: 110.5226\n",
      "Epoch [100/1000], Loss: 98.3731\n",
      "Epoch [110/1000], Loss: 87.6080\n",
      "Epoch [120/1000], Loss: 78.0781\n",
      "Epoch [130/1000], Loss: 69.6516\n",
      "Epoch [140/1000], Loss: 62.2115\n",
      "Epoch [150/1000], Loss: 55.6546\n",
      "Epoch [160/1000], Loss: 49.8886\n",
      "Epoch [170/1000], Loss: 44.8313\n",
      "Epoch [180/1000], Loss: 40.4083\n",
      "Epoch [190/1000], Loss: 36.5523\n",
      "Epoch [200/1000], Loss: 33.2020\n",
      "Epoch [210/1000], Loss: 30.3014\n",
      "Epoch [220/1000], Loss: 27.7995\n",
      "Epoch [230/1000], Loss: 25.6499\n",
      "Epoch [240/1000], Loss: 23.8104\n",
      "Epoch [250/1000], Loss: 22.2426\n",
      "Epoch [260/1000], Loss: 20.9121\n",
      "Epoch [270/1000], Loss: 19.7878\n",
      "Epoch [280/1000], Loss: 18.8417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/1000], Loss: 18.0492\n",
      "Epoch [300/1000], Loss: 17.3882\n",
      "Epoch [310/1000], Loss: 16.8394\n",
      "Epoch [320/1000], Loss: 16.3857\n",
      "Epoch [330/1000], Loss: 16.0124\n",
      "Epoch [340/1000], Loss: 15.7066\n",
      "Epoch [350/1000], Loss: 15.4572\n",
      "Epoch [360/1000], Loss: 15.2547\n",
      "Epoch [370/1000], Loss: 15.0911\n",
      "Epoch [380/1000], Loss: 14.9594\n",
      "Epoch [390/1000], Loss: 14.8540\n",
      "Epoch [400/1000], Loss: 14.7699\n",
      "Epoch [410/1000], Loss: 14.7031\n",
      "Epoch [420/1000], Loss: 14.6503\n",
      "Epoch [430/1000], Loss: 14.6088\n",
      "Epoch [440/1000], Loss: 14.5762\n",
      "Epoch [450/1000], Loss: 14.5508\n",
      "Epoch [460/1000], Loss: 14.5310\n",
      "Epoch [470/1000], Loss: 14.5157\n",
      "Epoch [480/1000], Loss: 14.5038\n",
      "Epoch [490/1000], Loss: 14.4947\n",
      "Epoch [500/1000], Loss: 14.4877\n",
      "Epoch [510/1000], Loss: 14.4823\n",
      "Epoch [520/1000], Loss: 14.4782\n",
      "Epoch [530/1000], Loss: 14.4750\n",
      "Epoch [540/1000], Loss: 14.4725\n",
      "Epoch [550/1000], Loss: 14.4707\n",
      "Epoch [560/1000], Loss: 14.4692\n",
      "Epoch [570/1000], Loss: 14.4680\n",
      "Epoch [580/1000], Loss: 14.4671\n",
      "Epoch [590/1000], Loss: 14.4664\n",
      "Epoch [600/1000], Loss: 14.4658\n",
      "Epoch [610/1000], Loss: 14.4652\n",
      "Epoch [620/1000], Loss: 14.4648\n",
      "Epoch [630/1000], Loss: 14.4644\n",
      "Epoch [640/1000], Loss: 14.4640\n",
      "Epoch [650/1000], Loss: 14.4637\n",
      "Epoch [660/1000], Loss: 14.4634\n",
      "Epoch [670/1000], Loss: 14.4631\n",
      "Epoch [680/1000], Loss: 14.4628\n",
      "Epoch [690/1000], Loss: 14.4626\n",
      "Epoch [700/1000], Loss: 14.4623\n",
      "Epoch [710/1000], Loss: 14.4621\n",
      "Epoch [720/1000], Loss: 14.4618\n",
      "Epoch [730/1000], Loss: 14.4616\n",
      "Epoch [740/1000], Loss: 14.4613\n",
      "Epoch [750/1000], Loss: 14.4611\n",
      "Epoch [760/1000], Loss: 14.4608\n",
      "Epoch [770/1000], Loss: 14.4606\n",
      "Epoch [780/1000], Loss: 14.4604\n",
      "Epoch [790/1000], Loss: 14.4602\n",
      "Epoch [800/1000], Loss: 14.4599\n",
      "Epoch [810/1000], Loss: 14.4597\n",
      "Epoch [820/1000], Loss: 14.4595\n",
      "Epoch [830/1000], Loss: 14.4593\n",
      "Epoch [840/1000], Loss: 14.4591\n",
      "Epoch [850/1000], Loss: 14.4588\n",
      "Epoch [860/1000], Loss: 14.4586\n",
      "Epoch [870/1000], Loss: 14.4584\n",
      "Epoch [880/1000], Loss: 14.4582\n",
      "Epoch [890/1000], Loss: 14.4580\n",
      "Epoch [900/1000], Loss: 14.4578\n",
      "Epoch [910/1000], Loss: 14.4576\n",
      "Epoch [920/1000], Loss: 14.4574\n",
      "Epoch [930/1000], Loss: 14.4572\n",
      "Epoch [940/1000], Loss: 14.4570\n",
      "Epoch [950/1000], Loss: 14.4568\n",
      "Epoch [960/1000], Loss: 14.4566\n",
      "Epoch [970/1000], Loss: 14.4565\n",
      "Epoch [980/1000], Loss: 14.4563\n",
      "Epoch [990/1000], Loss: 14.4561\n",
      "Epoch [1000/1000], Loss: 14.4559\n",
      "Predicted days_remaining for parent_id 215: [18.39156723022461, 18.8045597076416, 18.7989501953125, 18.802711486816406, 18.804990768432617, 18.80410385131836, 18.798603057861328, 18.79761505126953]\n",
      "Training for parent_id 218...\n",
      "Epoch [10/1000], Loss: 814.7577\n",
      "Epoch [20/1000], Loss: 724.2657\n",
      "Epoch [30/1000], Loss: 659.6805\n",
      "Epoch [40/1000], Loss: 612.9465\n",
      "Epoch [50/1000], Loss: 573.9005\n",
      "Epoch [60/1000], Loss: 538.1912\n",
      "Epoch [70/1000], Loss: 504.8367\n",
      "Epoch [80/1000], Loss: 473.7885\n",
      "Epoch [90/1000], Loss: 444.7878\n",
      "Epoch [100/1000], Loss: 417.6082\n",
      "Epoch [110/1000], Loss: 392.0708\n",
      "Epoch [120/1000], Loss: 368.0324\n",
      "Epoch [130/1000], Loss: 345.3770\n",
      "Epoch [140/1000], Loss: 324.0081\n",
      "Epoch [150/1000], Loss: 303.8439\n",
      "Epoch [160/1000], Loss: 284.8135\n",
      "Epoch [170/1000], Loss: 266.8541\n",
      "Epoch [180/1000], Loss: 249.9091\n",
      "Epoch [190/1000], Loss: 233.9269\n",
      "Epoch [200/1000], Loss: 218.8597\n",
      "Epoch [210/1000], Loss: 204.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 191.2955\n",
      "Epoch [230/1000], Loss: 178.7171\n",
      "Epoch [240/1000], Loss: 166.8904\n",
      "Epoch [250/1000], Loss: 155.7795\n",
      "Epoch [260/1000], Loss: 145.3501\n",
      "Epoch [270/1000], Loss: 135.5691\n",
      "Epoch [280/1000], Loss: 126.4049\n",
      "Epoch [290/1000], Loss: 117.8270\n",
      "Epoch [300/1000], Loss: 109.8058\n",
      "Epoch [310/1000], Loss: 102.3129\n",
      "Epoch [320/1000], Loss: 95.3210\n",
      "Epoch [330/1000], Loss: 88.8037\n",
      "Epoch [340/1000], Loss: 82.7356\n",
      "Epoch [350/1000], Loss: 77.0919\n",
      "Epoch [360/1000], Loss: 71.8491\n",
      "Epoch [370/1000], Loss: 66.9845\n",
      "Epoch [380/1000], Loss: 62.4761\n",
      "Epoch [390/1000], Loss: 58.3029\n",
      "Epoch [400/1000], Loss: 54.4447\n",
      "Epoch [410/1000], Loss: 50.8822\n",
      "Epoch [420/1000], Loss: 47.5968\n",
      "Epoch [430/1000], Loss: 44.5709\n",
      "Epoch [440/1000], Loss: 41.7874\n",
      "Epoch [450/1000], Loss: 39.2305\n",
      "Epoch [460/1000], Loss: 36.8846\n",
      "Epoch [470/1000], Loss: 34.7352\n",
      "Epoch [480/1000], Loss: 32.7685\n",
      "Epoch [490/1000], Loss: 30.9713\n",
      "Epoch [500/1000], Loss: 29.3313\n",
      "Epoch [510/1000], Loss: 27.8368\n",
      "Epoch [520/1000], Loss: 26.4767\n",
      "Epoch [530/1000], Loss: 25.2406\n",
      "Epoch [540/1000], Loss: 24.1189\n",
      "Epoch [550/1000], Loss: 23.1022\n",
      "Epoch [560/1000], Loss: 22.1822\n",
      "Epoch [570/1000], Loss: 21.3507\n",
      "Epoch [580/1000], Loss: 20.6003\n",
      "Epoch [590/1000], Loss: 19.9241\n",
      "Epoch [600/1000], Loss: 19.3156\n",
      "Epoch [610/1000], Loss: 18.7688\n",
      "Epoch [620/1000], Loss: 18.2781\n",
      "Epoch [630/1000], Loss: 17.8384\n",
      "Epoch [640/1000], Loss: 17.4450\n",
      "Epoch [650/1000], Loss: 17.0936\n",
      "Epoch [660/1000], Loss: 16.7800\n",
      "Epoch [670/1000], Loss: 16.5007\n",
      "Epoch [680/1000], Loss: 16.2522\n",
      "Epoch [690/1000], Loss: 16.0315\n",
      "Epoch [700/1000], Loss: 15.8357\n",
      "Epoch [710/1000], Loss: 15.6623\n",
      "Epoch [720/1000], Loss: 15.5090\n",
      "Epoch [730/1000], Loss: 15.3736\n",
      "Epoch [740/1000], Loss: 15.2542\n",
      "Epoch [750/1000], Loss: 15.1491\n",
      "Epoch [760/1000], Loss: 15.0566\n",
      "Epoch [770/1000], Loss: 14.9755\n",
      "Epoch [780/1000], Loss: 14.9044\n",
      "Epoch [790/1000], Loss: 14.8421\n",
      "Epoch [800/1000], Loss: 14.7877\n",
      "Epoch [810/1000], Loss: 14.7403\n",
      "Epoch [820/1000], Loss: 14.6989\n",
      "Epoch [830/1000], Loss: 14.6629\n",
      "Epoch [840/1000], Loss: 14.6316\n",
      "Epoch [850/1000], Loss: 14.6045\n",
      "Epoch [860/1000], Loss: 14.5810\n",
      "Epoch [870/1000], Loss: 14.5607\n",
      "Epoch [880/1000], Loss: 14.5431\n",
      "Epoch [890/1000], Loss: 14.5280\n",
      "Epoch [900/1000], Loss: 14.5150\n",
      "Epoch [910/1000], Loss: 14.5038\n",
      "Epoch [920/1000], Loss: 14.4942\n",
      "Epoch [930/1000], Loss: 14.4859\n",
      "Epoch [940/1000], Loss: 14.4789\n",
      "Epoch [950/1000], Loss: 14.4729\n",
      "Epoch [960/1000], Loss: 14.4677\n",
      "Epoch [970/1000], Loss: 14.4634\n",
      "Epoch [980/1000], Loss: 14.4596\n",
      "Epoch [990/1000], Loss: 14.4565\n",
      "Epoch [1000/1000], Loss: 14.4538\n",
      "Predicted days_remaining for parent_id 218: [29.505687713623047, 29.649423599243164, 29.650344848632812, 29.65146255493164, 29.652345657348633, 29.65088653564453, 29.6508846282959, 29.65078353881836]\n",
      "Training for parent_id 234...\n",
      "Epoch [10/1000], Loss: 1196.2329\n",
      "Epoch [20/1000], Loss: 1076.2340\n",
      "Epoch [30/1000], Loss: 997.6210\n",
      "Epoch [40/1000], Loss: 941.6545\n",
      "Epoch [50/1000], Loss: 893.7179\n",
      "Epoch [60/1000], Loss: 849.5850\n",
      "Epoch [70/1000], Loss: 808.1232\n",
      "Epoch [80/1000], Loss: 768.6911\n",
      "Epoch [90/1000], Loss: 731.0701\n",
      "Epoch [100/1000], Loss: 695.2315\n",
      "Epoch [110/1000], Loss: 661.0789\n",
      "Epoch [120/1000], Loss: 628.5087\n",
      "Epoch [130/1000], Loss: 597.4322\n",
      "Epoch [140/1000], Loss: 567.7704\n",
      "Epoch [150/1000], Loss: 539.4489\n",
      "Epoch [160/1000], Loss: 512.3981\n",
      "Epoch [170/1000], Loss: 486.5551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000], Loss: 461.8627\n",
      "Epoch [190/1000], Loss: 438.2695\n",
      "Epoch [200/1000], Loss: 415.7285\n",
      "Epoch [210/1000], Loss: 394.1964\n",
      "Epoch [220/1000], Loss: 373.6327\n",
      "Epoch [230/1000], Loss: 353.9999\n",
      "Epoch [240/1000], Loss: 335.2621\n",
      "Epoch [250/1000], Loss: 317.3854\n",
      "Epoch [260/1000], Loss: 300.3376\n",
      "Epoch [270/1000], Loss: 284.0878\n",
      "Epoch [280/1000], Loss: 268.6062\n",
      "Epoch [290/1000], Loss: 253.8641\n",
      "Epoch [300/1000], Loss: 239.8338\n",
      "Epoch [310/1000], Loss: 226.4886\n",
      "Epoch [320/1000], Loss: 213.8024\n",
      "Epoch [330/1000], Loss: 201.7501\n",
      "Epoch [340/1000], Loss: 190.3072\n",
      "Epoch [350/1000], Loss: 179.4498\n",
      "Epoch [360/1000], Loss: 169.1548\n",
      "Epoch [370/1000], Loss: 159.3997\n",
      "Epoch [380/1000], Loss: 150.1626\n",
      "Epoch [390/1000], Loss: 141.4220\n",
      "Epoch [400/1000], Loss: 133.1574\n",
      "Epoch [410/1000], Loss: 125.3484\n",
      "Epoch [420/1000], Loss: 117.9756\n",
      "Epoch [430/1000], Loss: 111.0196\n",
      "Epoch [440/1000], Loss: 104.4622\n",
      "Epoch [450/1000], Loss: 98.2852\n",
      "Epoch [460/1000], Loss: 92.4711\n",
      "Epoch [470/1000], Loss: 87.0031\n",
      "Epoch [480/1000], Loss: 81.8647\n",
      "Epoch [490/1000], Loss: 77.0400\n",
      "Epoch [500/1000], Loss: 72.5136\n",
      "Epoch [510/1000], Loss: 68.2707\n",
      "Epoch [520/1000], Loss: 64.2968\n",
      "Epoch [530/1000], Loss: 60.5781\n",
      "Epoch [540/1000], Loss: 57.1012\n",
      "Epoch [550/1000], Loss: 53.8532\n",
      "Epoch [560/1000], Loss: 50.8217\n",
      "Epoch [570/1000], Loss: 47.9949\n",
      "Epoch [580/1000], Loss: 45.3612\n",
      "Epoch [590/1000], Loss: 42.9097\n",
      "Epoch [600/1000], Loss: 40.6298\n",
      "Epoch [610/1000], Loss: 38.5115\n",
      "Epoch [620/1000], Loss: 36.5451\n",
      "Epoch [630/1000], Loss: 34.7214\n",
      "Epoch [640/1000], Loss: 33.0317\n",
      "Epoch [650/1000], Loss: 31.4676\n",
      "Epoch [660/1000], Loss: 30.0211\n",
      "Epoch [670/1000], Loss: 28.6846\n",
      "Epoch [680/1000], Loss: 27.4510\n",
      "Epoch [690/1000], Loss: 26.3134\n",
      "Epoch [700/1000], Loss: 25.2654\n",
      "Epoch [710/1000], Loss: 24.3008\n",
      "Epoch [720/1000], Loss: 23.4139\n",
      "Epoch [730/1000], Loss: 22.5992\n",
      "Epoch [740/1000], Loss: 21.8516\n",
      "Epoch [750/1000], Loss: 21.1662\n",
      "Epoch [760/1000], Loss: 20.5384\n",
      "Epoch [770/1000], Loss: 19.9641\n",
      "Epoch [780/1000], Loss: 19.4391\n",
      "Epoch [790/1000], Loss: 18.9597\n",
      "Epoch [800/1000], Loss: 18.5224\n",
      "Epoch [810/1000], Loss: 18.1238\n",
      "Epoch [820/1000], Loss: 17.7610\n",
      "Epoch [830/1000], Loss: 17.4311\n",
      "Epoch [840/1000], Loss: 17.1313\n",
      "Epoch [850/1000], Loss: 16.8592\n",
      "Epoch [860/1000], Loss: 16.6125\n",
      "Epoch [870/1000], Loss: 16.3891\n",
      "Epoch [880/1000], Loss: 16.1869\n",
      "Epoch [890/1000], Loss: 16.0042\n",
      "Epoch [900/1000], Loss: 15.8392\n",
      "Epoch [910/1000], Loss: 15.6903\n",
      "Epoch [920/1000], Loss: 15.5562\n",
      "Epoch [930/1000], Loss: 15.4355\n",
      "Epoch [940/1000], Loss: 15.3269\n",
      "Epoch [950/1000], Loss: 15.2294\n",
      "Epoch [960/1000], Loss: 15.1419\n",
      "Epoch [970/1000], Loss: 15.0635\n",
      "Epoch [980/1000], Loss: 14.9933\n",
      "Epoch [990/1000], Loss: 14.9304\n",
      "Epoch [1000/1000], Loss: 14.8743\n",
      "Predicted days_remaining for parent_id 234: [34.96106719970703, 35.11427307128906, 35.113563537597656, 35.11499786376953, 35.115142822265625, 35.114356994628906, 35.113807678222656, 35.113182067871094]\n",
      "Training for parent_id 238...\n",
      "Epoch [10/1000], Loss: 720.4537\n",
      "Epoch [20/1000], Loss: 644.3578\n",
      "Epoch [30/1000], Loss: 594.7697\n",
      "Epoch [40/1000], Loss: 556.1375\n",
      "Epoch [50/1000], Loss: 521.3575\n",
      "Epoch [60/1000], Loss: 488.7211\n",
      "Epoch [70/1000], Loss: 458.1344\n",
      "Epoch [80/1000], Loss: 429.4464\n",
      "Epoch [90/1000], Loss: 402.5109\n",
      "Epoch [100/1000], Loss: 377.2001\n",
      "Epoch [110/1000], Loss: 353.3945\n",
      "Epoch [120/1000], Loss: 330.9873\n",
      "Epoch [130/1000], Loss: 309.8850\n",
      "Epoch [140/1000], Loss: 290.0056\n",
      "Epoch [150/1000], Loss: 271.2773\n",
      "Epoch [160/1000], Loss: 253.6359\n",
      "Epoch [170/1000], Loss: 237.0231\n",
      "Epoch [180/1000], Loss: 221.3854\n",
      "Epoch [190/1000], Loss: 206.6736\n",
      "Epoch [200/1000], Loss: 192.8415\n",
      "Epoch [210/1000], Loss: 179.8456\n",
      "Epoch [220/1000], Loss: 167.6450\n",
      "Epoch [230/1000], Loss: 156.2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/1000], Loss: 145.4749\n",
      "Epoch [250/1000], Loss: 135.4324\n",
      "Epoch [260/1000], Loss: 126.0388\n",
      "Epoch [270/1000], Loss: 117.2611\n",
      "Epoch [280/1000], Loss: 109.0677\n",
      "Epoch [290/1000], Loss: 101.4281\n",
      "Epoch [300/1000], Loss: 94.3128\n",
      "Epoch [310/1000], Loss: 87.6935\n",
      "Epoch [320/1000], Loss: 81.5429\n",
      "Epoch [330/1000], Loss: 75.8347\n",
      "Epoch [340/1000], Loss: 70.5436\n",
      "Epoch [350/1000], Loss: 65.6454\n",
      "Epoch [360/1000], Loss: 61.1165\n",
      "Epoch [370/1000], Loss: 56.9346\n",
      "Epoch [380/1000], Loss: 53.0782\n",
      "Epoch [390/1000], Loss: 49.5266\n",
      "Epoch [400/1000], Loss: 46.2603\n",
      "Epoch [410/1000], Loss: 43.2604\n",
      "Epoch [420/1000], Loss: 40.5089\n",
      "Epoch [430/1000], Loss: 37.9888\n",
      "Epoch [440/1000], Loss: 35.6840\n",
      "Epoch [450/1000], Loss: 33.5790\n",
      "Epoch [460/1000], Loss: 31.6592\n",
      "Epoch [470/1000], Loss: 29.9110\n",
      "Epoch [480/1000], Loss: 28.3212\n",
      "Epoch [490/1000], Loss: 26.8776\n",
      "Epoch [500/1000], Loss: 25.5688\n",
      "Epoch [510/1000], Loss: 24.3838\n",
      "Epoch [520/1000], Loss: 23.3127\n",
      "Epoch [530/1000], Loss: 22.3458\n",
      "Epoch [540/1000], Loss: 21.4744\n",
      "Epoch [550/1000], Loss: 20.6902\n",
      "Epoch [560/1000], Loss: 19.9856\n",
      "Epoch [570/1000], Loss: 19.3535\n",
      "Epoch [580/1000], Loss: 18.7872\n",
      "Epoch [590/1000], Loss: 18.2807\n",
      "Epoch [600/1000], Loss: 17.8283\n",
      "Epoch [610/1000], Loss: 17.4250\n",
      "Epoch [620/1000], Loss: 17.0659\n",
      "Epoch [630/1000], Loss: 16.7467\n",
      "Epoch [640/1000], Loss: 16.4634\n",
      "Epoch [650/1000], Loss: 16.2124\n",
      "Epoch [660/1000], Loss: 15.9903\n",
      "Epoch [670/1000], Loss: 15.7941\n",
      "Epoch [680/1000], Loss: 15.6210\n",
      "Epoch [690/1000], Loss: 15.4686\n",
      "Epoch [700/1000], Loss: 15.3346\n",
      "Epoch [710/1000], Loss: 15.2170\n",
      "Epoch [720/1000], Loss: 15.1139\n",
      "Epoch [730/1000], Loss: 15.0237\n",
      "Epoch [740/1000], Loss: 14.9448\n",
      "Epoch [750/1000], Loss: 14.8761\n",
      "Epoch [760/1000], Loss: 14.8162\n",
      "Epoch [770/1000], Loss: 14.7641\n",
      "Epoch [780/1000], Loss: 14.7189\n",
      "Epoch [790/1000], Loss: 14.6797\n",
      "Epoch [800/1000], Loss: 14.6458\n",
      "Epoch [810/1000], Loss: 14.6165\n",
      "Epoch [820/1000], Loss: 14.5912\n",
      "Epoch [830/1000], Loss: 14.5694\n",
      "Epoch [840/1000], Loss: 14.5507\n",
      "Epoch [850/1000], Loss: 14.5346\n",
      "Epoch [860/1000], Loss: 14.5209\n",
      "Epoch [870/1000], Loss: 14.5091\n",
      "Epoch [880/1000], Loss: 14.4990\n",
      "Epoch [890/1000], Loss: 14.4904\n",
      "Epoch [900/1000], Loss: 14.4831\n",
      "Epoch [910/1000], Loss: 14.4768\n",
      "Epoch [920/1000], Loss: 14.4715\n",
      "Epoch [930/1000], Loss: 14.4671\n",
      "Epoch [940/1000], Loss: 14.4633\n",
      "Epoch [950/1000], Loss: 14.4601\n",
      "Epoch [960/1000], Loss: 14.4574\n",
      "Epoch [970/1000], Loss: 14.4551\n",
      "Epoch [980/1000], Loss: 14.4532\n",
      "Epoch [990/1000], Loss: 14.4515\n",
      "Epoch [1000/1000], Loss: 14.4502\n",
      "Predicted days_remaining for parent_id 238: [28.461612701416016, 28.69475746154785, 28.701766967773438, 28.70139503479004, 28.701684951782227, 28.701383590698242, 28.70164680480957, 28.698402404785156]\n",
      "Training for parent_id 256...\n",
      "Epoch [10/1000], Loss: 77.4562\n",
      "Epoch [20/1000], Loss: 53.3956\n",
      "Epoch [30/1000], Loss: 39.7614\n",
      "Epoch [40/1000], Loss: 31.7944\n",
      "Epoch [50/1000], Loss: 26.4403\n",
      "Epoch [60/1000], Loss: 22.6476\n",
      "Epoch [70/1000], Loss: 19.9469\n",
      "Epoch [80/1000], Loss: 18.0627\n",
      "Epoch [90/1000], Loss: 16.7869\n",
      "Epoch [100/1000], Loss: 15.9511\n",
      "Epoch [110/1000], Loss: 15.4220\n",
      "Epoch [120/1000], Loss: 15.0975\n",
      "Epoch [130/1000], Loss: 14.9041\n",
      "Epoch [140/1000], Loss: 14.7908\n",
      "Epoch [150/1000], Loss: 14.7244\n",
      "Epoch [160/1000], Loss: 14.6845\n",
      "Epoch [170/1000], Loss: 14.6588\n",
      "Epoch [180/1000], Loss: 14.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000], Loss: 14.6269\n",
      "Epoch [200/1000], Loss: 14.6152\n",
      "Epoch [210/1000], Loss: 14.6050\n",
      "Epoch [220/1000], Loss: 14.5958\n",
      "Epoch [230/1000], Loss: 14.5873\n",
      "Epoch [240/1000], Loss: 14.5796\n",
      "Epoch [250/1000], Loss: 14.5725\n",
      "Epoch [260/1000], Loss: 14.5659\n",
      "Epoch [270/1000], Loss: 14.5598\n",
      "Epoch [280/1000], Loss: 14.5542\n",
      "Epoch [290/1000], Loss: 14.5489\n",
      "Epoch [300/1000], Loss: 14.5441\n",
      "Epoch [310/1000], Loss: 14.5395\n",
      "Epoch [320/1000], Loss: 14.5352\n",
      "Epoch [330/1000], Loss: 14.5313\n",
      "Epoch [340/1000], Loss: 14.5275\n",
      "Epoch [350/1000], Loss: 14.5240\n",
      "Epoch [360/1000], Loss: 14.5207\n",
      "Epoch [370/1000], Loss: 14.5176\n",
      "Epoch [380/1000], Loss: 14.5147\n",
      "Epoch [390/1000], Loss: 14.5119\n",
      "Epoch [400/1000], Loss: 14.5093\n",
      "Epoch [410/1000], Loss: 14.5069\n",
      "Epoch [420/1000], Loss: 14.5045\n",
      "Epoch [430/1000], Loss: 14.5023\n",
      "Epoch [440/1000], Loss: 14.5002\n",
      "Epoch [450/1000], Loss: 14.4982\n",
      "Epoch [460/1000], Loss: 14.4963\n",
      "Epoch [470/1000], Loss: 14.4945\n",
      "Epoch [480/1000], Loss: 14.4927\n",
      "Epoch [490/1000], Loss: 14.4911\n",
      "Epoch [500/1000], Loss: 14.4895\n",
      "Epoch [510/1000], Loss: 14.4880\n",
      "Epoch [520/1000], Loss: 14.4866\n",
      "Epoch [530/1000], Loss: 14.4852\n",
      "Epoch [540/1000], Loss: 14.4839\n",
      "Epoch [550/1000], Loss: 14.4826\n",
      "Epoch [560/1000], Loss: 14.4814\n",
      "Epoch [570/1000], Loss: 14.4802\n",
      "Epoch [580/1000], Loss: 14.4791\n",
      "Epoch [590/1000], Loss: 14.4781\n",
      "Epoch [600/1000], Loss: 14.4770\n",
      "Epoch [610/1000], Loss: 14.4760\n",
      "Epoch [620/1000], Loss: 14.4751\n",
      "Epoch [630/1000], Loss: 14.4742\n",
      "Epoch [640/1000], Loss: 14.4733\n",
      "Epoch [650/1000], Loss: 14.4724\n",
      "Epoch [660/1000], Loss: 14.4716\n",
      "Epoch [670/1000], Loss: 14.4708\n",
      "Epoch [680/1000], Loss: 14.4701\n",
      "Epoch [690/1000], Loss: 14.4693\n",
      "Epoch [700/1000], Loss: 14.4686\n",
      "Epoch [710/1000], Loss: 14.4679\n",
      "Epoch [720/1000], Loss: 14.4673\n",
      "Epoch [730/1000], Loss: 14.4666\n",
      "Epoch [740/1000], Loss: 14.4660\n",
      "Epoch [750/1000], Loss: 14.4654\n",
      "Epoch [760/1000], Loss: 14.4648\n",
      "Epoch [770/1000], Loss: 14.4642\n",
      "Epoch [780/1000], Loss: 14.4637\n",
      "Epoch [790/1000], Loss: 14.4632\n",
      "Epoch [800/1000], Loss: 14.4626\n",
      "Epoch [810/1000], Loss: 14.4621\n",
      "Epoch [820/1000], Loss: 14.4616\n",
      "Epoch [830/1000], Loss: 14.4612\n",
      "Epoch [840/1000], Loss: 14.4607\n",
      "Epoch [850/1000], Loss: 14.4603\n",
      "Epoch [860/1000], Loss: 14.4598\n",
      "Epoch [870/1000], Loss: 14.4594\n",
      "Epoch [880/1000], Loss: 14.4590\n",
      "Epoch [890/1000], Loss: 14.4586\n",
      "Epoch [900/1000], Loss: 14.4582\n",
      "Epoch [910/1000], Loss: 14.4578\n",
      "Epoch [920/1000], Loss: 14.4575\n",
      "Epoch [930/1000], Loss: 14.4571\n",
      "Epoch [940/1000], Loss: 14.4568\n",
      "Epoch [950/1000], Loss: 14.4564\n",
      "Epoch [960/1000], Loss: 14.4561\n",
      "Epoch [970/1000], Loss: 14.4558\n",
      "Epoch [980/1000], Loss: 14.4555\n",
      "Epoch [990/1000], Loss: 14.4552\n",
      "Epoch [1000/1000], Loss: 14.4549\n",
      "Predicted days_remaining for parent_id 256: [9.40051555633545, 9.79945182800293, 9.79515266418457, 9.802285194396973, 9.795781135559082, 9.801896095275879, 9.799210548400879, 9.79514217376709]\n",
      "Training for parent_id 265...\n",
      "Epoch [10/1000], Loss: 85.5646\n",
      "Epoch [20/1000], Loss: 61.2274\n",
      "Epoch [30/1000], Loss: 44.8203\n",
      "Epoch [40/1000], Loss: 35.2953\n",
      "Epoch [50/1000], Loss: 29.0134\n",
      "Epoch [60/1000], Loss: 24.5312\n",
      "Epoch [70/1000], Loss: 21.2994\n",
      "Epoch [80/1000], Loss: 18.9994\n",
      "Epoch [90/1000], Loss: 17.3979\n",
      "Epoch [100/1000], Loss: 16.3110\n",
      "Epoch [110/1000], Loss: 15.5940\n",
      "Epoch [120/1000], Loss: 15.1350\n",
      "Epoch [130/1000], Loss: 14.8506\n",
      "Epoch [140/1000], Loss: 14.6800\n",
      "Epoch [150/1000], Loss: 14.5811\n",
      "Epoch [160/1000], Loss: 14.5258\n",
      "Epoch [170/1000], Loss: 14.4958\n",
      "Epoch [180/1000], Loss: 14.4800\n",
      "Epoch [190/1000], Loss: 14.4718\n",
      "Epoch [200/1000], Loss: 14.4676\n",
      "Epoch [210/1000], Loss: 14.4652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 14.4638\n",
      "Epoch [230/1000], Loss: 14.4628\n",
      "Epoch [240/1000], Loss: 14.4620\n",
      "Epoch [250/1000], Loss: 14.4613\n",
      "Epoch [260/1000], Loss: 14.4606\n",
      "Epoch [270/1000], Loss: 14.4600\n",
      "Epoch [280/1000], Loss: 14.4594\n",
      "Epoch [290/1000], Loss: 14.4588\n",
      "Epoch [300/1000], Loss: 14.4582\n",
      "Epoch [310/1000], Loss: 14.4577\n",
      "Epoch [320/1000], Loss: 14.4571\n",
      "Epoch [330/1000], Loss: 14.4566\n",
      "Epoch [340/1000], Loss: 14.4561\n",
      "Epoch [350/1000], Loss: 14.4556\n",
      "Epoch [360/1000], Loss: 14.4552\n",
      "Epoch [370/1000], Loss: 14.4547\n",
      "Epoch [380/1000], Loss: 14.4543\n",
      "Epoch [390/1000], Loss: 14.4539\n",
      "Epoch [400/1000], Loss: 14.4535\n",
      "Epoch [410/1000], Loss: 14.4531\n",
      "Epoch [420/1000], Loss: 14.4527\n",
      "Epoch [430/1000], Loss: 14.4524\n",
      "Epoch [440/1000], Loss: 14.4520\n",
      "Epoch [450/1000], Loss: 14.4517\n",
      "Epoch [460/1000], Loss: 14.4513\n",
      "Epoch [470/1000], Loss: 14.4510\n",
      "Epoch [480/1000], Loss: 14.4507\n",
      "Epoch [490/1000], Loss: 14.4504\n",
      "Epoch [500/1000], Loss: 14.4501\n",
      "Epoch [510/1000], Loss: 14.4499\n",
      "Epoch [520/1000], Loss: 14.4496\n",
      "Epoch [530/1000], Loss: 14.4493\n",
      "Epoch [540/1000], Loss: 14.4491\n",
      "Epoch [550/1000], Loss: 14.4488\n",
      "Epoch [560/1000], Loss: 14.4486\n",
      "Epoch [570/1000], Loss: 14.4484\n",
      "Epoch [580/1000], Loss: 14.4481\n",
      "Epoch [590/1000], Loss: 14.4479\n",
      "Epoch [600/1000], Loss: 14.4477\n",
      "Epoch [610/1000], Loss: 14.4475\n",
      "Epoch [620/1000], Loss: 14.4473\n",
      "Epoch [630/1000], Loss: 14.4471\n",
      "Epoch [640/1000], Loss: 14.4469\n",
      "Epoch [650/1000], Loss: 14.4468\n",
      "Epoch [660/1000], Loss: 14.4466\n",
      "Epoch [670/1000], Loss: 14.4464\n",
      "Epoch [680/1000], Loss: 14.4462\n",
      "Epoch [690/1000], Loss: 14.4461\n",
      "Epoch [700/1000], Loss: 14.4459\n",
      "Epoch [710/1000], Loss: 14.4458\n",
      "Epoch [720/1000], Loss: 14.4456\n",
      "Epoch [730/1000], Loss: 14.4455\n",
      "Epoch [740/1000], Loss: 14.4453\n",
      "Epoch [750/1000], Loss: 14.4452\n",
      "Epoch [760/1000], Loss: 14.4451\n",
      "Epoch [770/1000], Loss: 14.4449\n",
      "Epoch [780/1000], Loss: 14.4448\n",
      "Epoch [790/1000], Loss: 14.4447\n",
      "Epoch [800/1000], Loss: 14.4446\n",
      "Epoch [810/1000], Loss: 14.4444\n",
      "Epoch [820/1000], Loss: 14.4443\n",
      "Epoch [830/1000], Loss: 14.4442\n",
      "Epoch [840/1000], Loss: 14.4441\n",
      "Epoch [850/1000], Loss: 14.4440\n",
      "Epoch [860/1000], Loss: 14.4439\n",
      "Epoch [870/1000], Loss: 14.4438\n",
      "Epoch [880/1000], Loss: 14.4437\n",
      "Epoch [890/1000], Loss: 14.4436\n",
      "Epoch [900/1000], Loss: 14.4435\n",
      "Epoch [910/1000], Loss: 14.4434\n",
      "Epoch [920/1000], Loss: 14.4433\n",
      "Epoch [930/1000], Loss: 14.4432\n",
      "Epoch [940/1000], Loss: 14.4431\n",
      "Epoch [950/1000], Loss: 14.4430\n",
      "Epoch [960/1000], Loss: 14.4430\n",
      "Epoch [970/1000], Loss: 14.4429\n",
      "Epoch [980/1000], Loss: 14.4428\n",
      "Epoch [990/1000], Loss: 14.4427\n",
      "Epoch [1000/1000], Loss: 14.4426\n",
      "Predicted days_remaining for parent_id 265: [9.563738822937012, 9.791886329650879, 9.790078163146973, 9.774490356445312, 9.770207405090332, 9.780821800231934, 9.782639503479004, 9.744290351867676]\n",
      "Training for parent_id 277...\n",
      "Epoch [10/1000], Loss: 490.8363\n",
      "Epoch [20/1000], Loss: 425.9020\n",
      "Epoch [30/1000], Loss: 381.8972\n",
      "Epoch [40/1000], Loss: 350.0114\n",
      "Epoch [50/1000], Loss: 322.6328\n",
      "Epoch [60/1000], Loss: 297.7125\n",
      "Epoch [70/1000], Loss: 274.7816\n",
      "Epoch [80/1000], Loss: 253.6121\n",
      "Epoch [90/1000], Loss: 234.0264\n",
      "Epoch [100/1000], Loss: 215.8824\n",
      "Epoch [110/1000], Loss: 199.0620\n",
      "Epoch [120/1000], Loss: 183.4646\n",
      "Epoch [130/1000], Loss: 169.0028\n",
      "Epoch [140/1000], Loss: 155.5996\n",
      "Epoch [150/1000], Loss: 143.1855\n",
      "Epoch [160/1000], Loss: 131.6976\n",
      "Epoch [170/1000], Loss: 121.0777\n",
      "Epoch [180/1000], Loss: 111.2718\n",
      "Epoch [190/1000], Loss: 102.2293\n",
      "Epoch [200/1000], Loss: 93.9024\n",
      "Epoch [210/1000], Loss: 86.2459\n",
      "Epoch [220/1000], Loss: 79.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000], Loss: 72.7748\n",
      "Epoch [240/1000], Loss: 66.8805\n",
      "Epoch [250/1000], Loss: 61.4970\n",
      "Epoch [260/1000], Loss: 56.5890\n",
      "Epoch [270/1000], Loss: 52.1229\n",
      "Epoch [280/1000], Loss: 48.0667\n",
      "Epoch [290/1000], Loss: 44.3899\n",
      "Epoch [300/1000], Loss: 41.0637\n",
      "Epoch [310/1000], Loss: 38.0609\n",
      "Epoch [320/1000], Loss: 35.3555\n",
      "Epoch [330/1000], Loss: 32.9231\n",
      "Epoch [340/1000], Loss: 30.7409\n",
      "Epoch [350/1000], Loss: 28.7873\n",
      "Epoch [360/1000], Loss: 27.0422\n",
      "Epoch [370/1000], Loss: 25.4866\n",
      "Epoch [380/1000], Loss: 24.1031\n",
      "Epoch [390/1000], Loss: 22.8754\n",
      "Epoch [400/1000], Loss: 21.7884\n",
      "Epoch [410/1000], Loss: 20.8280\n",
      "Epoch [420/1000], Loss: 19.9815\n",
      "Epoch [430/1000], Loss: 19.2371\n",
      "Epoch [440/1000], Loss: 18.5839\n",
      "Epoch [450/1000], Loss: 18.0120\n",
      "Epoch [460/1000], Loss: 17.5126\n",
      "Epoch [470/1000], Loss: 17.0774\n",
      "Epoch [480/1000], Loss: 16.6990\n",
      "Epoch [490/1000], Loss: 16.3708\n",
      "Epoch [500/1000], Loss: 16.0868\n",
      "Epoch [510/1000], Loss: 15.8416\n",
      "Epoch [520/1000], Loss: 15.6303\n",
      "Epoch [530/1000], Loss: 15.4488\n",
      "Epoch [540/1000], Loss: 15.2932\n",
      "Epoch [550/1000], Loss: 15.1601\n",
      "Epoch [560/1000], Loss: 15.0465\n",
      "Epoch [570/1000], Loss: 14.9498\n",
      "Epoch [580/1000], Loss: 14.8676\n",
      "Epoch [590/1000], Loss: 14.7980\n",
      "Epoch [600/1000], Loss: 14.7392\n",
      "Epoch [610/1000], Loss: 14.6895\n",
      "Epoch [620/1000], Loss: 14.6478\n",
      "Epoch [630/1000], Loss: 14.6127\n",
      "Epoch [640/1000], Loss: 14.5833\n",
      "Epoch [650/1000], Loss: 14.5588\n",
      "Epoch [660/1000], Loss: 14.5383\n",
      "Epoch [670/1000], Loss: 14.5213\n",
      "Epoch [680/1000], Loss: 14.5072\n",
      "Epoch [690/1000], Loss: 14.4955\n",
      "Epoch [700/1000], Loss: 14.4858\n",
      "Epoch [710/1000], Loss: 14.4779\n",
      "Epoch [720/1000], Loss: 14.4713\n",
      "Epoch [730/1000], Loss: 14.4659\n",
      "Epoch [740/1000], Loss: 14.4615\n",
      "Epoch [750/1000], Loss: 14.4580\n",
      "Epoch [760/1000], Loss: 14.4550\n",
      "Epoch [770/1000], Loss: 14.4527\n",
      "Epoch [780/1000], Loss: 14.4507\n",
      "Epoch [790/1000], Loss: 14.4492\n",
      "Epoch [800/1000], Loss: 14.4479\n",
      "Epoch [810/1000], Loss: 14.4469\n",
      "Epoch [820/1000], Loss: 14.4461\n",
      "Epoch [830/1000], Loss: 14.4454\n",
      "Epoch [840/1000], Loss: 14.4449\n",
      "Epoch [850/1000], Loss: 14.4444\n",
      "Epoch [860/1000], Loss: 14.4441\n",
      "Epoch [870/1000], Loss: 14.4438\n",
      "Epoch [880/1000], Loss: 14.4436\n",
      "Epoch [890/1000], Loss: 14.4434\n",
      "Epoch [900/1000], Loss: 14.4433\n",
      "Epoch [910/1000], Loss: 14.4432\n",
      "Epoch [920/1000], Loss: 14.4431\n",
      "Epoch [930/1000], Loss: 14.4430\n",
      "Epoch [940/1000], Loss: 14.4429\n",
      "Epoch [950/1000], Loss: 14.4429\n",
      "Epoch [960/1000], Loss: 14.4428\n",
      "Epoch [970/1000], Loss: 14.4428\n",
      "Epoch [980/1000], Loss: 14.4428\n",
      "Epoch [990/1000], Loss: 14.4427\n",
      "Epoch [1000/1000], Loss: 14.4427\n",
      "Predicted days_remaining for parent_id 277: [23.554183959960938, 23.771177291870117, 23.772354125976562, 23.770526885986328, 23.772315979003906, 23.771530151367188, 23.77292251586914, 23.772308349609375]\n",
      "Training for parent_id 281...\n",
      "Epoch [10/1000], Loss: 219.4698\n",
      "Epoch [20/1000], Loss: 173.1667\n",
      "Epoch [30/1000], Loss: 140.8031\n",
      "Epoch [40/1000], Loss: 119.1867\n",
      "Epoch [50/1000], Loss: 102.9362\n",
      "Epoch [60/1000], Loss: 89.4371\n",
      "Epoch [70/1000], Loss: 77.8280\n",
      "Epoch [80/1000], Loss: 67.8527\n",
      "Epoch [90/1000], Loss: 59.2863\n",
      "Epoch [100/1000], Loss: 51.9412\n",
      "Epoch [110/1000], Loss: 45.6592\n",
      "Epoch [120/1000], Loss: 40.3046\n",
      "Epoch [130/1000], Loss: 35.7588\n",
      "Epoch [140/1000], Loss: 31.9179\n",
      "Epoch [150/1000], Loss: 28.6892\n",
      "Epoch [160/1000], Loss: 25.9904\n",
      "Epoch [170/1000], Loss: 23.7481\n",
      "Epoch [180/1000], Loss: 21.8966\n",
      "Epoch [190/1000], Loss: 20.3778\n",
      "Epoch [200/1000], Loss: 19.1404\n",
      "Epoch [210/1000], Loss: 18.1391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 17.3345\n",
      "Epoch [230/1000], Loss: 16.6927\n",
      "Epoch [240/1000], Loss: 16.1844\n",
      "Epoch [250/1000], Loss: 15.7847\n",
      "Epoch [260/1000], Loss: 15.4727\n",
      "Epoch [270/1000], Loss: 15.2309\n",
      "Epoch [280/1000], Loss: 15.0449\n",
      "Epoch [290/1000], Loss: 14.9028\n",
      "Epoch [300/1000], Loss: 14.7950\n",
      "Epoch [310/1000], Loss: 14.7138\n",
      "Epoch [320/1000], Loss: 14.6531\n",
      "Epoch [330/1000], Loss: 14.6079\n",
      "Epoch [340/1000], Loss: 14.5745\n",
      "Epoch [350/1000], Loss: 14.5499\n",
      "Epoch [360/1000], Loss: 14.5319\n",
      "Epoch [370/1000], Loss: 14.5188\n",
      "Epoch [380/1000], Loss: 14.5092\n",
      "Epoch [390/1000], Loss: 14.5022\n",
      "Epoch [400/1000], Loss: 14.4970\n",
      "Epoch [410/1000], Loss: 14.4932\n",
      "Epoch [420/1000], Loss: 14.4904\n",
      "Epoch [430/1000], Loss: 14.4882\n",
      "Epoch [440/1000], Loss: 14.4864\n",
      "Epoch [450/1000], Loss: 14.4851\n",
      "Epoch [460/1000], Loss: 14.4839\n",
      "Epoch [470/1000], Loss: 14.4829\n",
      "Epoch [480/1000], Loss: 14.4820\n",
      "Epoch [490/1000], Loss: 14.4812\n",
      "Epoch [500/1000], Loss: 14.4804\n",
      "Epoch [510/1000], Loss: 14.4797\n",
      "Epoch [520/1000], Loss: 14.4791\n",
      "Epoch [530/1000], Loss: 14.4784\n",
      "Epoch [540/1000], Loss: 14.4778\n",
      "Epoch [550/1000], Loss: 14.4771\n",
      "Epoch [560/1000], Loss: 14.4765\n",
      "Epoch [570/1000], Loss: 14.4759\n",
      "Epoch [580/1000], Loss: 14.4754\n",
      "Epoch [590/1000], Loss: 14.4748\n",
      "Epoch [600/1000], Loss: 14.4742\n",
      "Epoch [610/1000], Loss: 14.4737\n",
      "Epoch [620/1000], Loss: 14.4732\n",
      "Epoch [630/1000], Loss: 14.4726\n",
      "Epoch [640/1000], Loss: 14.4721\n",
      "Epoch [650/1000], Loss: 14.4716\n",
      "Epoch [660/1000], Loss: 14.4711\n",
      "Epoch [670/1000], Loss: 14.4706\n",
      "Epoch [680/1000], Loss: 14.4701\n",
      "Epoch [690/1000], Loss: 14.4697\n",
      "Epoch [700/1000], Loss: 14.4692\n",
      "Epoch [710/1000], Loss: 14.4688\n",
      "Epoch [720/1000], Loss: 14.4683\n",
      "Epoch [730/1000], Loss: 14.4679\n",
      "Epoch [740/1000], Loss: 14.4675\n",
      "Epoch [750/1000], Loss: 14.4670\n",
      "Epoch [760/1000], Loss: 14.4666\n",
      "Epoch [770/1000], Loss: 14.4662\n",
      "Epoch [780/1000], Loss: 14.4658\n",
      "Epoch [790/1000], Loss: 14.4654\n",
      "Epoch [800/1000], Loss: 14.4651\n",
      "Epoch [810/1000], Loss: 14.4647\n",
      "Epoch [820/1000], Loss: 14.4643\n",
      "Epoch [830/1000], Loss: 14.4640\n",
      "Epoch [840/1000], Loss: 14.4636\n",
      "Epoch [850/1000], Loss: 14.4633\n",
      "Epoch [860/1000], Loss: 14.4629\n",
      "Epoch [870/1000], Loss: 14.4626\n",
      "Epoch [880/1000], Loss: 14.4622\n",
      "Epoch [890/1000], Loss: 14.4619\n",
      "Epoch [900/1000], Loss: 14.4616\n",
      "Epoch [910/1000], Loss: 14.4613\n",
      "Epoch [920/1000], Loss: 14.4610\n",
      "Epoch [930/1000], Loss: 14.4607\n",
      "Epoch [940/1000], Loss: 14.4604\n",
      "Epoch [950/1000], Loss: 14.4601\n",
      "Epoch [960/1000], Loss: 14.4598\n",
      "Epoch [970/1000], Loss: 14.4595\n",
      "Epoch [980/1000], Loss: 14.4592\n",
      "Epoch [990/1000], Loss: 14.4590\n",
      "Epoch [1000/1000], Loss: 14.4587\n",
      "Predicted days_remaining for parent_id 281: [15.364958763122559, 15.804984092712402, 15.803414344787598, 15.807806968688965, 15.808951377868652, 15.803326606750488, 15.80173397064209, 15.804696083068848]\n",
      "Training for parent_id 289...\n",
      "Epoch [10/1000], Loss: 130.5774\n",
      "Epoch [20/1000], Loss: 94.5935\n",
      "Epoch [30/1000], Loss: 74.2642\n",
      "Epoch [40/1000], Loss: 61.6939\n",
      "Epoch [50/1000], Loss: 52.4141\n",
      "Epoch [60/1000], Loss: 44.9099\n",
      "Epoch [70/1000], Loss: 38.7253\n",
      "Epoch [80/1000], Loss: 33.5783\n",
      "Epoch [90/1000], Loss: 29.3369\n",
      "Epoch [100/1000], Loss: 25.9109\n",
      "Epoch [110/1000], Loss: 23.1792\n",
      "Epoch [120/1000], Loss: 21.0260\n",
      "Epoch [130/1000], Loss: 19.3483\n",
      "Epoch [140/1000], Loss: 18.0563\n",
      "Epoch [150/1000], Loss: 17.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000], Loss: 16.3358\n",
      "Epoch [170/1000], Loss: 15.7890\n",
      "Epoch [180/1000], Loss: 15.3891\n",
      "Epoch [190/1000], Loss: 15.1006\n",
      "Epoch [200/1000], Loss: 14.8955\n",
      "Epoch [210/1000], Loss: 14.7516\n",
      "Epoch [220/1000], Loss: 14.6520\n",
      "Epoch [230/1000], Loss: 14.5842\n",
      "Epoch [240/1000], Loss: 14.5385\n",
      "Epoch [250/1000], Loss: 14.5082\n",
      "Epoch [260/1000], Loss: 14.4884\n",
      "Epoch [270/1000], Loss: 14.4755\n",
      "Epoch [280/1000], Loss: 14.4673\n",
      "Epoch [290/1000], Loss: 14.4621\n",
      "Epoch [300/1000], Loss: 14.4588\n",
      "Epoch [310/1000], Loss: 14.4567\n",
      "Epoch [320/1000], Loss: 14.4554\n",
      "Epoch [330/1000], Loss: 14.4545\n",
      "Epoch [340/1000], Loss: 14.4539\n",
      "Epoch [350/1000], Loss: 14.4535\n",
      "Epoch [360/1000], Loss: 14.4532\n",
      "Epoch [370/1000], Loss: 14.4529\n",
      "Epoch [380/1000], Loss: 14.4526\n",
      "Epoch [390/1000], Loss: 14.4524\n",
      "Epoch [400/1000], Loss: 14.4522\n",
      "Epoch [410/1000], Loss: 14.4519\n",
      "Epoch [420/1000], Loss: 14.4517\n",
      "Epoch [430/1000], Loss: 14.4515\n",
      "Epoch [440/1000], Loss: 14.4513\n",
      "Epoch [450/1000], Loss: 14.4511\n",
      "Epoch [460/1000], Loss: 14.4509\n",
      "Epoch [470/1000], Loss: 14.4508\n",
      "Epoch [480/1000], Loss: 14.4506\n",
      "Epoch [490/1000], Loss: 14.4504\n",
      "Epoch [500/1000], Loss: 14.4502\n",
      "Epoch [510/1000], Loss: 14.4500\n",
      "Epoch [520/1000], Loss: 14.4498\n",
      "Epoch [530/1000], Loss: 14.4497\n",
      "Epoch [540/1000], Loss: 14.4495\n",
      "Epoch [550/1000], Loss: 14.4493\n",
      "Epoch [560/1000], Loss: 14.4492\n",
      "Epoch [570/1000], Loss: 14.4490\n",
      "Epoch [580/1000], Loss: 14.4488\n",
      "Epoch [590/1000], Loss: 14.4487\n",
      "Epoch [600/1000], Loss: 14.4485\n",
      "Epoch [610/1000], Loss: 14.4484\n",
      "Epoch [620/1000], Loss: 14.4482\n",
      "Epoch [630/1000], Loss: 14.4481\n",
      "Epoch [640/1000], Loss: 14.4479\n",
      "Epoch [650/1000], Loss: 14.4478\n",
      "Epoch [660/1000], Loss: 14.4476\n",
      "Epoch [670/1000], Loss: 14.4475\n",
      "Epoch [680/1000], Loss: 14.4474\n",
      "Epoch [690/1000], Loss: 14.4472\n",
      "Epoch [700/1000], Loss: 14.4471\n",
      "Epoch [710/1000], Loss: 14.4470\n",
      "Epoch [720/1000], Loss: 14.4469\n",
      "Epoch [730/1000], Loss: 14.4467\n",
      "Epoch [740/1000], Loss: 14.4466\n",
      "Epoch [750/1000], Loss: 14.4465\n",
      "Epoch [760/1000], Loss: 14.4464\n",
      "Epoch [770/1000], Loss: 14.4463\n",
      "Epoch [780/1000], Loss: 14.4461\n",
      "Epoch [790/1000], Loss: 14.4460\n",
      "Epoch [800/1000], Loss: 14.4459\n",
      "Epoch [810/1000], Loss: 14.4458\n",
      "Epoch [820/1000], Loss: 14.4457\n",
      "Epoch [830/1000], Loss: 14.4456\n",
      "Epoch [840/1000], Loss: 14.4455\n",
      "Epoch [850/1000], Loss: 14.4454\n",
      "Epoch [860/1000], Loss: 14.4453\n",
      "Epoch [870/1000], Loss: 14.4452\n",
      "Epoch [880/1000], Loss: 14.4451\n",
      "Epoch [890/1000], Loss: 14.4450\n",
      "Epoch [900/1000], Loss: 14.4449\n",
      "Epoch [910/1000], Loss: 14.4448\n",
      "Epoch [920/1000], Loss: 14.4447\n",
      "Epoch [930/1000], Loss: 14.4446\n",
      "Epoch [940/1000], Loss: 14.4446\n",
      "Epoch [950/1000], Loss: 14.4445\n",
      "Epoch [960/1000], Loss: 14.4444\n",
      "Epoch [970/1000], Loss: 14.4443\n",
      "Epoch [980/1000], Loss: 14.4442\n",
      "Epoch [990/1000], Loss: 14.4441\n",
      "Epoch [1000/1000], Loss: 14.4441\n",
      "Predicted days_remaining for parent_id 289: [12.536373138427734, 12.78130054473877, 12.780755996704102, 12.769969940185547, 12.783236503601074, 12.785320281982422, 12.789482116699219, 12.773456573486328]\n",
      "Training for parent_id 302...\n",
      "Epoch [10/1000], Loss: 97.2242\n",
      "Epoch [20/1000], Loss: 67.3225\n",
      "Epoch [30/1000], Loss: 49.7506\n",
      "Epoch [40/1000], Loss: 39.4574\n",
      "Epoch [50/1000], Loss: 32.5319\n",
      "Epoch [60/1000], Loss: 27.4519\n",
      "Epoch [70/1000], Loss: 23.6655\n",
      "Epoch [80/1000], Loss: 20.8685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 18.8382\n",
      "Epoch [100/1000], Loss: 17.3958\n",
      "Epoch [110/1000], Loss: 16.3953\n",
      "Epoch [120/1000], Loss: 15.7188\n",
      "Epoch [130/1000], Loss: 15.2732\n",
      "Epoch [140/1000], Loss: 14.9874\n",
      "Epoch [150/1000], Loss: 14.8087\n",
      "Epoch [160/1000], Loss: 14.6995\n",
      "Epoch [170/1000], Loss: 14.6340\n",
      "Epoch [180/1000], Loss: 14.5951\n",
      "Epoch [190/1000], Loss: 14.5718\n",
      "Epoch [200/1000], Loss: 14.5574\n",
      "Epoch [210/1000], Loss: 14.5481\n",
      "Epoch [220/1000], Loss: 14.5414\n",
      "Epoch [230/1000], Loss: 14.5362\n",
      "Epoch [240/1000], Loss: 14.5318\n",
      "Epoch [250/1000], Loss: 14.5280\n",
      "Epoch [260/1000], Loss: 14.5244\n",
      "Epoch [270/1000], Loss: 14.5211\n",
      "Epoch [280/1000], Loss: 14.5180\n",
      "Epoch [290/1000], Loss: 14.5151\n",
      "Epoch [300/1000], Loss: 14.5123\n",
      "Epoch [310/1000], Loss: 14.5097\n",
      "Epoch [320/1000], Loss: 14.5072\n",
      "Epoch [330/1000], Loss: 14.5049\n",
      "Epoch [340/1000], Loss: 14.5026\n",
      "Epoch [350/1000], Loss: 14.5005\n",
      "Epoch [360/1000], Loss: 14.4985\n",
      "Epoch [370/1000], Loss: 14.4966\n",
      "Epoch [380/1000], Loss: 14.4947\n",
      "Epoch [390/1000], Loss: 14.4930\n",
      "Epoch [400/1000], Loss: 14.4913\n",
      "Epoch [410/1000], Loss: 14.4897\n",
      "Epoch [420/1000], Loss: 14.4882\n",
      "Epoch [430/1000], Loss: 14.4867\n",
      "Epoch [440/1000], Loss: 14.4853\n",
      "Epoch [450/1000], Loss: 14.4840\n",
      "Epoch [460/1000], Loss: 14.4827\n",
      "Epoch [470/1000], Loss: 14.4815\n",
      "Epoch [480/1000], Loss: 14.4803\n",
      "Epoch [490/1000], Loss: 14.4792\n",
      "Epoch [500/1000], Loss: 14.4781\n",
      "Epoch [510/1000], Loss: 14.4770\n",
      "Epoch [520/1000], Loss: 14.4760\n",
      "Epoch [530/1000], Loss: 14.4751\n",
      "Epoch [540/1000], Loss: 14.4741\n",
      "Epoch [550/1000], Loss: 14.4732\n",
      "Epoch [560/1000], Loss: 14.4724\n",
      "Epoch [570/1000], Loss: 14.4715\n",
      "Epoch [580/1000], Loss: 14.4707\n",
      "Epoch [590/1000], Loss: 14.4700\n",
      "Epoch [600/1000], Loss: 14.4692\n",
      "Epoch [610/1000], Loss: 14.4685\n",
      "Epoch [620/1000], Loss: 14.4678\n",
      "Epoch [630/1000], Loss: 14.4671\n",
      "Epoch [640/1000], Loss: 14.4665\n",
      "Epoch [650/1000], Loss: 14.4659\n",
      "Epoch [660/1000], Loss: 14.4653\n",
      "Epoch [670/1000], Loss: 14.4647\n",
      "Epoch [680/1000], Loss: 14.4641\n",
      "Epoch [690/1000], Loss: 14.4635\n",
      "Epoch [700/1000], Loss: 14.4630\n",
      "Epoch [710/1000], Loss: 14.4625\n",
      "Epoch [720/1000], Loss: 14.4620\n",
      "Epoch [730/1000], Loss: 14.4615\n",
      "Epoch [740/1000], Loss: 14.4610\n",
      "Epoch [750/1000], Loss: 14.4606\n",
      "Epoch [760/1000], Loss: 14.4601\n",
      "Epoch [770/1000], Loss: 14.4597\n",
      "Epoch [780/1000], Loss: 14.4593\n",
      "Epoch [790/1000], Loss: 14.4589\n",
      "Epoch [800/1000], Loss: 14.4585\n",
      "Epoch [810/1000], Loss: 14.4581\n",
      "Epoch [820/1000], Loss: 14.4577\n",
      "Epoch [830/1000], Loss: 14.4573\n",
      "Epoch [840/1000], Loss: 14.4570\n",
      "Epoch [850/1000], Loss: 14.4566\n",
      "Epoch [860/1000], Loss: 14.4563\n",
      "Epoch [870/1000], Loss: 14.4560\n",
      "Epoch [880/1000], Loss: 14.4556\n",
      "Epoch [890/1000], Loss: 14.4553\n",
      "Epoch [900/1000], Loss: 14.4550\n",
      "Epoch [910/1000], Loss: 14.4547\n",
      "Epoch [920/1000], Loss: 14.4544\n",
      "Epoch [930/1000], Loss: 14.4541\n",
      "Epoch [940/1000], Loss: 14.4539\n",
      "Epoch [950/1000], Loss: 14.4536\n",
      "Epoch [960/1000], Loss: 14.4533\n",
      "Epoch [970/1000], Loss: 14.4531\n",
      "Epoch [980/1000], Loss: 14.4528\n",
      "Epoch [990/1000], Loss: 14.4526\n",
      "Epoch [1000/1000], Loss: 14.4524\n",
      "Predicted days_remaining for parent_id 302: [10.427287101745605, 10.790772438049316, 10.801436424255371, 10.793784141540527, 10.792823791503906, 10.799803733825684, 10.801898956298828, 10.785296440124512]\n",
      "Training for parent_id 310...\n",
      "Epoch [10/1000], Loss: 2787.9822\n",
      "Epoch [20/1000], Loss: 2616.9749\n",
      "Epoch [30/1000], Loss: 2509.0771\n",
      "Epoch [40/1000], Loss: 2426.4441\n",
      "Epoch [50/1000], Loss: 2351.1667\n",
      "Epoch [60/1000], Loss: 2279.7859\n",
      "Epoch [70/1000], Loss: 2211.4326\n",
      "Epoch [80/1000], Loss: 2145.6536\n",
      "Epoch [90/1000], Loss: 2082.1492\n",
      "Epoch [100/1000], Loss: 2020.7020\n",
      "Epoch [110/1000], Loss: 1961.1440\n",
      "Epoch [120/1000], Loss: 1903.3416\n",
      "Epoch [130/1000], Loss: 1847.1859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 1792.5873\n",
      "Epoch [150/1000], Loss: 1739.4694\n",
      "Epoch [160/1000], Loss: 1687.7676\n",
      "Epoch [170/1000], Loss: 1637.4259\n",
      "Epoch [180/1000], Loss: 1588.3943\n",
      "Epoch [190/1000], Loss: 1540.6292\n",
      "Epoch [200/1000], Loss: 1494.0909\n",
      "Epoch [210/1000], Loss: 1448.7437\n",
      "Epoch [220/1000], Loss: 1404.5538\n",
      "Epoch [230/1000], Loss: 1361.4917\n",
      "Epoch [240/1000], Loss: 1319.5282\n",
      "Epoch [250/1000], Loss: 1278.6367\n",
      "Epoch [260/1000], Loss: 1238.7922\n",
      "Epoch [270/1000], Loss: 1199.9705\n",
      "Epoch [280/1000], Loss: 1162.1487\n",
      "Epoch [290/1000], Loss: 1125.3051\n",
      "Epoch [300/1000], Loss: 1089.4186\n",
      "Epoch [310/1000], Loss: 1054.4688\n",
      "Epoch [320/1000], Loss: 1020.4360\n",
      "Epoch [330/1000], Loss: 987.3012\n",
      "Epoch [340/1000], Loss: 955.0460\n",
      "Epoch [350/1000], Loss: 923.6520\n",
      "Epoch [360/1000], Loss: 893.1019\n",
      "Epoch [370/1000], Loss: 863.3784\n",
      "Epoch [380/1000], Loss: 834.4645\n",
      "Epoch [390/1000], Loss: 806.3438\n",
      "Epoch [400/1000], Loss: 778.9999\n",
      "Epoch [410/1000], Loss: 752.4171\n",
      "Epoch [420/1000], Loss: 726.5796\n",
      "Epoch [430/1000], Loss: 701.4719\n",
      "Epoch [440/1000], Loss: 677.0790\n",
      "Epoch [450/1000], Loss: 653.3859\n",
      "Epoch [460/1000], Loss: 630.3781\n",
      "Epoch [470/1000], Loss: 608.0406\n",
      "Epoch [480/1000], Loss: 586.3596\n",
      "Epoch [490/1000], Loss: 565.3206\n",
      "Epoch [500/1000], Loss: 544.9102\n",
      "Epoch [510/1000], Loss: 525.1141\n",
      "Epoch [520/1000], Loss: 505.9190\n",
      "Epoch [530/1000], Loss: 487.3118\n",
      "Epoch [540/1000], Loss: 469.2792\n",
      "Epoch [550/1000], Loss: 451.8077\n",
      "Epoch [560/1000], Loss: 434.8851\n",
      "Epoch [570/1000], Loss: 418.4986\n",
      "Epoch [580/1000], Loss: 402.6353\n",
      "Epoch [590/1000], Loss: 387.2833\n",
      "Epoch [600/1000], Loss: 372.4301\n",
      "Epoch [610/1000], Loss: 358.0639\n",
      "Epoch [620/1000], Loss: 344.1728\n",
      "Epoch [630/1000], Loss: 330.7451\n",
      "Epoch [640/1000], Loss: 317.7692\n",
      "Epoch [650/1000], Loss: 305.2338\n",
      "Epoch [660/1000], Loss: 293.1279\n",
      "Epoch [670/1000], Loss: 281.4401\n",
      "Epoch [680/1000], Loss: 270.1597\n",
      "Epoch [690/1000], Loss: 259.2760\n",
      "Epoch [700/1000], Loss: 248.7783\n",
      "Epoch [710/1000], Loss: 238.6565\n",
      "Epoch [720/1000], Loss: 228.9000\n",
      "Epoch [730/1000], Loss: 219.4991\n",
      "Epoch [740/1000], Loss: 210.4437\n",
      "Epoch [750/1000], Loss: 201.7241\n",
      "Epoch [760/1000], Loss: 193.3306\n",
      "Epoch [770/1000], Loss: 185.2540\n",
      "Epoch [780/1000], Loss: 177.4850\n",
      "Epoch [790/1000], Loss: 170.0145\n",
      "Epoch [800/1000], Loss: 162.8335\n",
      "Epoch [810/1000], Loss: 155.9335\n",
      "Epoch [820/1000], Loss: 149.3056\n",
      "Epoch [830/1000], Loss: 142.9416\n",
      "Epoch [840/1000], Loss: 136.8331\n",
      "Epoch [850/1000], Loss: 130.9721\n",
      "Epoch [860/1000], Loss: 125.3508\n",
      "Epoch [870/1000], Loss: 119.9611\n",
      "Epoch [880/1000], Loss: 114.7958\n",
      "Epoch [890/1000], Loss: 109.8472\n",
      "Epoch [900/1000], Loss: 105.1081\n",
      "Epoch [910/1000], Loss: 100.5714\n",
      "Epoch [920/1000], Loss: 96.2302\n",
      "Epoch [930/1000], Loss: 92.0776\n",
      "Epoch [940/1000], Loss: 88.1071\n",
      "Epoch [950/1000], Loss: 84.3123\n",
      "Epoch [960/1000], Loss: 80.6867\n",
      "Epoch [970/1000], Loss: 77.2243\n",
      "Epoch [980/1000], Loss: 73.9191\n",
      "Epoch [990/1000], Loss: 70.7651\n",
      "Epoch [1000/1000], Loss: 67.7569\n",
      "Predicted days_remaining for parent_id 310: [47.397438049316406, 47.476619720458984, 47.47591781616211, 47.47920227050781, 47.47917938232422, 47.47908020019531, 47.479339599609375, 47.478248596191406]\n",
      "Training for parent_id 312...\n",
      "Epoch [10/1000], Loss: 1201.3762\n",
      "Epoch [20/1000], Loss: 1092.6099\n",
      "Epoch [30/1000], Loss: 1011.8436\n",
      "Epoch [40/1000], Loss: 954.1277\n",
      "Epoch [50/1000], Loss: 905.4683\n",
      "Epoch [60/1000], Loss: 860.4567\n",
      "Epoch [70/1000], Loss: 817.8308\n",
      "Epoch [80/1000], Loss: 777.6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 739.6259\n",
      "Epoch [100/1000], Loss: 703.5916\n",
      "Epoch [110/1000], Loss: 669.3360\n",
      "Epoch [120/1000], Loss: 636.7099\n",
      "Epoch [130/1000], Loss: 605.5922\n",
      "Epoch [140/1000], Loss: 575.8834\n",
      "Epoch [150/1000], Loss: 547.4999\n",
      "Epoch [160/1000], Loss: 520.3701\n",
      "Epoch [170/1000], Loss: 494.4317\n",
      "Epoch [180/1000], Loss: 469.6295\n",
      "Epoch [190/1000], Loss: 445.9137\n",
      "Epoch [200/1000], Loss: 423.2390\n",
      "Epoch [210/1000], Loss: 401.5633\n",
      "Epoch [220/1000], Loss: 380.8481\n",
      "Epoch [230/1000], Loss: 361.0563\n",
      "Epoch [240/1000], Loss: 342.1537\n",
      "Epoch [250/1000], Loss: 324.1072\n",
      "Epoch [260/1000], Loss: 306.8855\n",
      "Epoch [270/1000], Loss: 290.4585\n",
      "Epoch [280/1000], Loss: 274.7971\n",
      "Epoch [290/1000], Loss: 259.8733\n",
      "Epoch [300/1000], Loss: 245.6602\n",
      "Epoch [310/1000], Loss: 232.1314\n",
      "Epoch [320/1000], Loss: 219.2616\n",
      "Epoch [330/1000], Loss: 207.0260\n",
      "Epoch [340/1000], Loss: 195.4005\n",
      "Epoch [350/1000], Loss: 184.3619\n",
      "Epoch [360/1000], Loss: 173.8871\n",
      "Epoch [370/1000], Loss: 163.9541\n",
      "Epoch [380/1000], Loss: 154.5414\n",
      "Epoch [390/1000], Loss: 145.6278\n",
      "Epoch [400/1000], Loss: 137.1928\n",
      "Epoch [410/1000], Loss: 129.2166\n",
      "Epoch [420/1000], Loss: 121.6796\n",
      "Epoch [430/1000], Loss: 114.5630\n",
      "Epoch [440/1000], Loss: 107.8485\n",
      "Epoch [450/1000], Loss: 101.5181\n",
      "Epoch [460/1000], Loss: 95.5545\n",
      "Epoch [470/1000], Loss: 89.9409\n",
      "Epoch [480/1000], Loss: 84.6610\n",
      "Epoch [490/1000], Loss: 79.6990\n",
      "Epoch [500/1000], Loss: 75.0395\n",
      "Epoch [510/1000], Loss: 70.6676\n",
      "Epoch [520/1000], Loss: 66.5691\n",
      "Epoch [530/1000], Loss: 62.7300\n",
      "Epoch [540/1000], Loss: 59.1371\n",
      "Epoch [550/1000], Loss: 55.7774\n",
      "Epoch [560/1000], Loss: 52.6384\n",
      "Epoch [570/1000], Loss: 49.7083\n",
      "Epoch [580/1000], Loss: 46.9756\n",
      "Epoch [590/1000], Loss: 44.4292\n",
      "Epoch [600/1000], Loss: 42.0585\n",
      "Epoch [610/1000], Loss: 39.8535\n",
      "Epoch [620/1000], Loss: 37.8043\n",
      "Epoch [630/1000], Loss: 35.9017\n",
      "Epoch [640/1000], Loss: 34.1368\n",
      "Epoch [650/1000], Loss: 32.5013\n",
      "Epoch [660/1000], Loss: 30.9869\n",
      "Epoch [670/1000], Loss: 29.5861\n",
      "Epoch [680/1000], Loss: 28.2915\n",
      "Epoch [690/1000], Loss: 27.0963\n",
      "Epoch [700/1000], Loss: 25.9937\n",
      "Epoch [710/1000], Loss: 24.9777\n",
      "Epoch [720/1000], Loss: 24.0423\n",
      "Epoch [730/1000], Loss: 23.1820\n",
      "Epoch [740/1000], Loss: 22.3915\n",
      "Epoch [750/1000], Loss: 21.6658\n",
      "Epoch [760/1000], Loss: 21.0002\n",
      "Epoch [770/1000], Loss: 20.3905\n",
      "Epoch [780/1000], Loss: 19.8324\n",
      "Epoch [790/1000], Loss: 19.3220\n",
      "Epoch [800/1000], Loss: 18.8559\n",
      "Epoch [810/1000], Loss: 18.4304\n",
      "Epoch [820/1000], Loss: 18.0426\n",
      "Epoch [830/1000], Loss: 17.6893\n",
      "Epoch [840/1000], Loss: 17.3679\n",
      "Epoch [850/1000], Loss: 17.0758\n",
      "Epoch [860/1000], Loss: 16.8105\n",
      "Epoch [870/1000], Loss: 16.5698\n",
      "Epoch [880/1000], Loss: 16.3518\n",
      "Epoch [890/1000], Loss: 16.1543\n",
      "Epoch [900/1000], Loss: 15.9758\n",
      "Epoch [910/1000], Loss: 15.8145\n",
      "Epoch [920/1000], Loss: 15.6690\n",
      "Epoch [930/1000], Loss: 15.5377\n",
      "Epoch [940/1000], Loss: 15.4195\n",
      "Epoch [950/1000], Loss: 15.3131\n",
      "Epoch [960/1000], Loss: 15.2175\n",
      "Epoch [970/1000], Loss: 15.1317\n",
      "Epoch [980/1000], Loss: 15.0547\n",
      "Epoch [990/1000], Loss: 14.9858\n",
      "Epoch [1000/1000], Loss: 14.9240\n",
      "Predicted days_remaining for parent_id 312: [34.955291748046875, 35.072906494140625, 35.07299041748047, 35.07085037231445, 35.07389831542969, 35.07193374633789, 35.07235336303711, 35.07176971435547]\n",
      "Training for parent_id 315...\n",
      "Epoch [10/1000], Loss: 1396.5387\n",
      "Epoch [20/1000], Loss: 1285.8876\n",
      "Epoch [30/1000], Loss: 1200.3958\n",
      "Epoch [40/1000], Loss: 1134.1683\n",
      "Epoch [50/1000], Loss: 1078.1154\n",
      "Epoch [60/1000], Loss: 1026.9171\n",
      "Epoch [70/1000], Loss: 979.0443\n",
      "Epoch [80/1000], Loss: 933.9065\n",
      "Epoch [90/1000], Loss: 891.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 850.6321\n",
      "Epoch [110/1000], Loss: 812.0303\n",
      "Epoch [120/1000], Loss: 775.1995\n",
      "Epoch [130/1000], Loss: 739.9974\n",
      "Epoch [140/1000], Loss: 706.3100\n",
      "Epoch [150/1000], Loss: 674.0431\n",
      "Epoch [160/1000], Loss: 643.1169\n",
      "Epoch [170/1000], Loss: 613.4625\n",
      "Epoch [180/1000], Loss: 585.0200\n",
      "Epoch [190/1000], Loss: 557.7357\n",
      "Epoch [200/1000], Loss: 531.5614\n",
      "Epoch [210/1000], Loss: 506.4527\n",
      "Epoch [220/1000], Loss: 482.3689\n",
      "Epoch [230/1000], Loss: 459.2721\n",
      "Epoch [240/1000], Loss: 437.1266\n",
      "Epoch [250/1000], Loss: 415.8989\n",
      "Epoch [260/1000], Loss: 395.5568\n",
      "Epoch [270/1000], Loss: 376.0702\n",
      "Epoch [280/1000], Loss: 357.4096\n",
      "Epoch [290/1000], Loss: 339.5471\n",
      "Epoch [300/1000], Loss: 322.4557\n",
      "Epoch [310/1000], Loss: 306.1089\n",
      "Epoch [320/1000], Loss: 290.4816\n",
      "Epoch [330/1000], Loss: 275.5491\n",
      "Epoch [340/1000], Loss: 261.2876\n",
      "Epoch [350/1000], Loss: 247.6738\n",
      "Epoch [360/1000], Loss: 234.6850\n",
      "Epoch [370/1000], Loss: 222.2993\n",
      "Epoch [380/1000], Loss: 210.4950\n",
      "Epoch [390/1000], Loss: 199.2512\n",
      "Epoch [400/1000], Loss: 188.5475\n",
      "Epoch [410/1000], Loss: 178.3639\n",
      "Epoch [420/1000], Loss: 168.6808\n",
      "Epoch [430/1000], Loss: 159.4794\n",
      "Epoch [440/1000], Loss: 150.7410\n",
      "Epoch [450/1000], Loss: 142.4475\n",
      "Epoch [460/1000], Loss: 134.5814\n",
      "Epoch [470/1000], Loss: 127.1254\n",
      "Epoch [480/1000], Loss: 120.0628\n",
      "Epoch [490/1000], Loss: 113.3774\n",
      "Epoch [500/1000], Loss: 107.0532\n",
      "Epoch [510/1000], Loss: 101.0747\n",
      "Epoch [520/1000], Loss: 95.4272\n",
      "Epoch [530/1000], Loss: 90.0958\n",
      "Epoch [540/1000], Loss: 85.0665\n",
      "Epoch [550/1000], Loss: 80.3257\n",
      "Epoch [560/1000], Loss: 75.8598\n",
      "Epoch [570/1000], Loss: 71.6562\n",
      "Epoch [580/1000], Loss: 67.7022\n",
      "Epoch [590/1000], Loss: 63.9859\n",
      "Epoch [600/1000], Loss: 60.4955\n",
      "Epoch [610/1000], Loss: 57.2198\n",
      "Epoch [620/1000], Loss: 54.1480\n",
      "Epoch [630/1000], Loss: 51.2695\n",
      "Epoch [640/1000], Loss: 48.5744\n",
      "Epoch [650/1000], Loss: 46.0528\n",
      "Epoch [660/1000], Loss: 43.6955\n",
      "Epoch [670/1000], Loss: 41.4935\n",
      "Epoch [680/1000], Loss: 39.4381\n",
      "Epoch [690/1000], Loss: 37.5213\n",
      "Epoch [700/1000], Loss: 35.7351\n",
      "Epoch [710/1000], Loss: 34.0719\n",
      "Epoch [720/1000], Loss: 32.5244\n",
      "Epoch [730/1000], Loss: 31.0860\n",
      "Epoch [740/1000], Loss: 29.7500\n",
      "Epoch [750/1000], Loss: 28.5100\n",
      "Epoch [760/1000], Loss: 27.3602\n",
      "Epoch [770/1000], Loss: 26.2949\n",
      "Epoch [780/1000], Loss: 25.3088\n",
      "Epoch [790/1000], Loss: 24.3967\n",
      "Epoch [800/1000], Loss: 23.5537\n",
      "Epoch [810/1000], Loss: 22.7753\n",
      "Epoch [820/1000], Loss: 22.0572\n",
      "Epoch [830/1000], Loss: 21.3953\n",
      "Epoch [840/1000], Loss: 20.7856\n",
      "Epoch [850/1000], Loss: 20.2246\n",
      "Epoch [860/1000], Loss: 19.7088\n",
      "Epoch [870/1000], Loss: 19.2350\n",
      "Epoch [880/1000], Loss: 18.8002\n",
      "Epoch [890/1000], Loss: 18.4015\n",
      "Epoch [900/1000], Loss: 18.0362\n",
      "Epoch [910/1000], Loss: 17.7018\n",
      "Epoch [920/1000], Loss: 17.3960\n",
      "Epoch [930/1000], Loss: 17.1165\n",
      "Epoch [940/1000], Loss: 16.8614\n",
      "Epoch [950/1000], Loss: 16.6287\n",
      "Epoch [960/1000], Loss: 16.4167\n",
      "Epoch [970/1000], Loss: 16.2237\n",
      "Epoch [980/1000], Loss: 16.0481\n",
      "Epoch [990/1000], Loss: 15.8885\n",
      "Epoch [1000/1000], Loss: 15.7436\n",
      "Predicted days_remaining for parent_id 315: [37.51995849609375, 37.629676818847656, 37.62976837158203, 37.628440856933594, 37.629268646240234, 37.629268646240234, 37.6159553527832, 37.62767791748047]\n",
      "Training for parent_id 316...\n",
      "Epoch [10/1000], Loss: 231.0284\n",
      "Epoch [20/1000], Loss: 184.5919\n",
      "Epoch [30/1000], Loss: 156.7529\n",
      "Epoch [40/1000], Loss: 137.3976\n",
      "Epoch [50/1000], Loss: 121.3098\n",
      "Epoch [60/1000], Loss: 107.2888\n",
      "Epoch [70/1000], Loss: 94.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000], Loss: 84.0312\n",
      "Epoch [90/1000], Loss: 74.4077\n",
      "Epoch [100/1000], Loss: 65.9322\n",
      "Epoch [110/1000], Loss: 58.4881\n",
      "Epoch [120/1000], Loss: 51.9709\n",
      "Epoch [130/1000], Loss: 46.2857\n",
      "Epoch [140/1000], Loss: 41.3460\n",
      "Epoch [150/1000], Loss: 37.0719\n",
      "Epoch [160/1000], Loss: 33.3904\n",
      "Epoch [170/1000], Loss: 30.2340\n",
      "Epoch [180/1000], Loss: 27.5410\n",
      "Epoch [190/1000], Loss: 25.2550\n",
      "Epoch [200/1000], Loss: 23.3246\n",
      "Epoch [210/1000], Loss: 21.7031\n",
      "Epoch [220/1000], Loss: 20.3485\n",
      "Epoch [230/1000], Loss: 19.2232\n",
      "Epoch [240/1000], Loss: 18.2935\n",
      "Epoch [250/1000], Loss: 17.5298\n",
      "Epoch [260/1000], Loss: 16.9061\n",
      "Epoch [270/1000], Loss: 16.3996\n",
      "Epoch [280/1000], Loss: 15.9907\n",
      "Epoch [290/1000], Loss: 15.6625\n",
      "Epoch [300/1000], Loss: 15.4006\n",
      "Epoch [310/1000], Loss: 15.1928\n",
      "Epoch [320/1000], Loss: 15.0289\n",
      "Epoch [330/1000], Loss: 14.9003\n",
      "Epoch [340/1000], Loss: 14.8001\n",
      "Epoch [350/1000], Loss: 14.7223\n",
      "Epoch [360/1000], Loss: 14.6623\n",
      "Epoch [370/1000], Loss: 14.6162\n",
      "Epoch [380/1000], Loss: 14.5811\n",
      "Epoch [390/1000], Loss: 14.5543\n",
      "Epoch [400/1000], Loss: 14.5341\n",
      "Epoch [410/1000], Loss: 14.5189\n",
      "Epoch [420/1000], Loss: 14.5074\n",
      "Epoch [430/1000], Loss: 14.4988\n",
      "Epoch [440/1000], Loss: 14.4923\n",
      "Epoch [450/1000], Loss: 14.4875\n",
      "Epoch [460/1000], Loss: 14.4838\n",
      "Epoch [470/1000], Loss: 14.4811\n",
      "Epoch [480/1000], Loss: 14.4789\n",
      "Epoch [490/1000], Loss: 14.4773\n",
      "Epoch [500/1000], Loss: 14.4760\n",
      "Epoch [510/1000], Loss: 14.4749\n",
      "Epoch [520/1000], Loss: 14.4740\n",
      "Epoch [530/1000], Loss: 14.4733\n",
      "Epoch [540/1000], Loss: 14.4726\n",
      "Epoch [550/1000], Loss: 14.4720\n",
      "Epoch [560/1000], Loss: 14.4715\n",
      "Epoch [570/1000], Loss: 14.4710\n",
      "Epoch [580/1000], Loss: 14.4705\n",
      "Epoch [590/1000], Loss: 14.4700\n",
      "Epoch [600/1000], Loss: 14.4696\n",
      "Epoch [610/1000], Loss: 14.4691\n",
      "Epoch [620/1000], Loss: 14.4687\n",
      "Epoch [630/1000], Loss: 14.4683\n",
      "Epoch [640/1000], Loss: 14.4679\n",
      "Epoch [650/1000], Loss: 14.4675\n",
      "Epoch [660/1000], Loss: 14.4671\n",
      "Epoch [670/1000], Loss: 14.4667\n",
      "Epoch [680/1000], Loss: 14.4663\n",
      "Epoch [690/1000], Loss: 14.4660\n",
      "Epoch [700/1000], Loss: 14.4656\n",
      "Epoch [710/1000], Loss: 14.4652\n",
      "Epoch [720/1000], Loss: 14.4649\n",
      "Epoch [730/1000], Loss: 14.4645\n",
      "Epoch [740/1000], Loss: 14.4642\n",
      "Epoch [750/1000], Loss: 14.4638\n",
      "Epoch [760/1000], Loss: 14.4635\n",
      "Epoch [770/1000], Loss: 14.4632\n",
      "Epoch [780/1000], Loss: 14.4629\n",
      "Epoch [790/1000], Loss: 14.4625\n",
      "Epoch [800/1000], Loss: 14.4622\n",
      "Epoch [810/1000], Loss: 14.4619\n",
      "Epoch [820/1000], Loss: 14.4616\n",
      "Epoch [830/1000], Loss: 14.4613\n",
      "Epoch [840/1000], Loss: 14.4610\n",
      "Epoch [850/1000], Loss: 14.4607\n",
      "Epoch [860/1000], Loss: 14.4605\n",
      "Epoch [870/1000], Loss: 14.4602\n",
      "Epoch [880/1000], Loss: 14.4599\n",
      "Epoch [890/1000], Loss: 14.4596\n",
      "Epoch [900/1000], Loss: 14.4594\n",
      "Epoch [910/1000], Loss: 14.4591\n",
      "Epoch [920/1000], Loss: 14.4588\n",
      "Epoch [930/1000], Loss: 14.4586\n",
      "Epoch [940/1000], Loss: 14.4583\n",
      "Epoch [950/1000], Loss: 14.4581\n",
      "Epoch [960/1000], Loss: 14.4578\n",
      "Epoch [970/1000], Loss: 14.4576\n",
      "Epoch [980/1000], Loss: 14.4574\n",
      "Epoch [990/1000], Loss: 14.4571\n",
      "Epoch [1000/1000], Loss: 14.4569\n",
      "Predicted days_remaining for parent_id 316: [16.3820743560791, 16.809419631958008, 16.80472755432129, 16.806631088256836, 16.795969009399414, 16.796342849731445, 16.803455352783203, 16.803003311157227]\n",
      "Training for parent_id 317...\n",
      "Epoch [10/1000], Loss: 1307.5977\n",
      "Epoch [20/1000], Loss: 1195.5428\n",
      "Epoch [30/1000], Loss: 1114.5201\n",
      "Epoch [40/1000], Loss: 1053.7346\n",
      "Epoch [50/1000], Loss: 1001.0872\n",
      "Epoch [60/1000], Loss: 952.8340\n",
      "Epoch [70/1000], Loss: 907.6739\n",
      "Epoch [80/1000], Loss: 865.0369\n",
      "Epoch [90/1000], Loss: 824.5967\n",
      "Epoch [100/1000], Loss: 786.1256\n",
      "Epoch [110/1000], Loss: 749.4511\n",
      "Epoch [120/1000], Loss: 714.4353\n",
      "Epoch [130/1000], Loss: 680.9657\n",
      "Epoch [140/1000], Loss: 648.9474\n",
      "Epoch [150/1000], Loss: 618.2994\n",
      "Epoch [160/1000], Loss: 588.9509\n",
      "Epoch [170/1000], Loss: 560.8397\n",
      "Epoch [180/1000], Loss: 533.9099\n",
      "Epoch [190/1000], Loss: 508.1107\n",
      "Epoch [200/1000], Loss: 483.3961\n",
      "Epoch [210/1000], Loss: 459.7230\n",
      "Epoch [220/1000], Loss: 437.0518\n",
      "Epoch [230/1000], Loss: 415.3453\n",
      "Epoch [240/1000], Loss: 394.5681\n",
      "Epoch [250/1000], Loss: 374.6866\n",
      "Epoch [260/1000], Loss: 355.6692\n",
      "Epoch [270/1000], Loss: 337.4852\n",
      "Epoch [280/1000], Loss: 320.1053\n",
      "Epoch [290/1000], Loss: 303.5014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 287.6461\n",
      "Epoch [310/1000], Loss: 272.5130\n",
      "Epoch [320/1000], Loss: 258.0764\n",
      "Epoch [330/1000], Loss: 244.3117\n",
      "Epoch [340/1000], Loss: 231.1945\n",
      "Epoch [350/1000], Loss: 218.7016\n",
      "Epoch [360/1000], Loss: 206.8098\n",
      "Epoch [370/1000], Loss: 195.4970\n",
      "Epoch [380/1000], Loss: 184.7414\n",
      "Epoch [390/1000], Loss: 174.5220\n",
      "Epoch [400/1000], Loss: 164.8180\n",
      "Epoch [410/1000], Loss: 155.6094\n",
      "Epoch [420/1000], Loss: 146.8766\n",
      "Epoch [430/1000], Loss: 138.6005\n",
      "Epoch [440/1000], Loss: 130.7625\n",
      "Epoch [450/1000], Loss: 123.3444\n",
      "Epoch [460/1000], Loss: 116.3288\n",
      "Epoch [470/1000], Loss: 109.6983\n",
      "Epoch [480/1000], Loss: 103.4364\n",
      "Epoch [490/1000], Loss: 97.5267\n",
      "Epoch [500/1000], Loss: 91.9538\n",
      "Epoch [510/1000], Loss: 86.7020\n",
      "Epoch [520/1000], Loss: 81.7568\n",
      "Epoch [530/1000], Loss: 77.1038\n",
      "Epoch [540/1000], Loss: 72.7290\n",
      "Epoch [550/1000], Loss: 68.6189\n",
      "Epoch [560/1000], Loss: 64.7607\n",
      "Epoch [570/1000], Loss: 61.1416\n",
      "Epoch [580/1000], Loss: 57.7496\n",
      "Epoch [590/1000], Loss: 54.5731\n",
      "Epoch [600/1000], Loss: 51.6005\n",
      "Epoch [610/1000], Loss: 48.8213\n",
      "Epoch [620/1000], Loss: 46.2249\n",
      "Epoch [630/1000], Loss: 43.8014\n",
      "Epoch [640/1000], Loss: 41.5410\n",
      "Epoch [650/1000], Loss: 39.4346\n",
      "Epoch [660/1000], Loss: 37.4734\n",
      "Epoch [670/1000], Loss: 35.6488\n",
      "Epoch [680/1000], Loss: 33.9529\n",
      "Epoch [690/1000], Loss: 32.3779\n",
      "Epoch [700/1000], Loss: 30.9165\n",
      "Epoch [710/1000], Loss: 29.5616\n",
      "Epoch [720/1000], Loss: 28.3066\n",
      "Epoch [730/1000], Loss: 27.1452\n",
      "Epoch [740/1000], Loss: 26.0712\n",
      "Epoch [750/1000], Loss: 25.0791\n",
      "Epoch [760/1000], Loss: 24.1634\n",
      "Epoch [770/1000], Loss: 23.3189\n",
      "Epoch [780/1000], Loss: 22.5409\n",
      "Epoch [790/1000], Loss: 21.8246\n",
      "Epoch [800/1000], Loss: 21.1660\n",
      "Epoch [810/1000], Loss: 20.5608\n",
      "Epoch [820/1000], Loss: 20.0052\n",
      "Epoch [830/1000], Loss: 19.4956\n",
      "Epoch [840/1000], Loss: 19.0287\n",
      "Epoch [850/1000], Loss: 18.6013\n",
      "Epoch [860/1000], Loss: 18.2103\n",
      "Epoch [870/1000], Loss: 17.8531\n",
      "Epoch [880/1000], Loss: 17.5270\n",
      "Epoch [890/1000], Loss: 17.2295\n",
      "Epoch [900/1000], Loss: 16.9585\n",
      "Epoch [910/1000], Loss: 16.7118\n",
      "Epoch [920/1000], Loss: 16.4874\n",
      "Epoch [930/1000], Loss: 16.2835\n",
      "Epoch [940/1000], Loss: 16.0984\n",
      "Epoch [950/1000], Loss: 15.9305\n",
      "Epoch [960/1000], Loss: 15.7784\n",
      "Epoch [970/1000], Loss: 15.6408\n",
      "Epoch [980/1000], Loss: 15.5163\n",
      "Epoch [990/1000], Loss: 15.4038\n",
      "Epoch [1000/1000], Loss: 15.3024\n",
      "Predicted days_remaining for parent_id 317: [36.66785430908203, 36.85109329223633, 36.851966857910156, 36.85195541381836, 36.85102462768555, 36.84939193725586, 36.84908676147461, 36.84490966796875]\n",
      "Training for parent_id 320...\n",
      "Epoch [10/1000], Loss: 1810.7284\n",
      "Epoch [20/1000], Loss: 1676.5361\n",
      "Epoch [30/1000], Loss: 1571.5380\n",
      "Epoch [40/1000], Loss: 1497.9714\n",
      "Epoch [50/1000], Loss: 1435.3342\n",
      "Epoch [60/1000], Loss: 1376.9175\n",
      "Epoch [70/1000], Loss: 1321.7495\n",
      "Epoch [80/1000], Loss: 1269.3118\n",
      "Epoch [90/1000], Loss: 1219.2959\n",
      "Epoch [100/1000], Loss: 1171.4811\n",
      "Epoch [110/1000], Loss: 1125.6708\n",
      "Epoch [120/1000], Loss: 1081.6948\n",
      "Epoch [130/1000], Loss: 1039.4127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 998.7109\n",
      "Epoch [150/1000], Loss: 959.4951\n",
      "Epoch [160/1000], Loss: 921.6860\n",
      "Epoch [170/1000], Loss: 885.2155\n",
      "Epoch [180/1000], Loss: 850.0245\n",
      "Epoch [190/1000], Loss: 816.0601\n",
      "Epoch [200/1000], Loss: 783.2748\n",
      "Epoch [210/1000], Loss: 751.6257\n",
      "Epoch [220/1000], Loss: 721.0735\n",
      "Epoch [230/1000], Loss: 691.5814\n",
      "Epoch [240/1000], Loss: 663.1152\n",
      "Epoch [250/1000], Loss: 635.6429\n",
      "Epoch [260/1000], Loss: 609.1340\n",
      "Epoch [270/1000], Loss: 583.5598\n",
      "Epoch [280/1000], Loss: 558.8924\n",
      "Epoch [290/1000], Loss: 535.1055\n",
      "Epoch [300/1000], Loss: 512.1736\n",
      "Epoch [310/1000], Loss: 490.0721\n",
      "Epoch [320/1000], Loss: 468.7773\n",
      "Epoch [330/1000], Loss: 448.2660\n",
      "Epoch [340/1000], Loss: 428.5159\n",
      "Epoch [350/1000], Loss: 409.5051\n",
      "Epoch [360/1000], Loss: 391.2124\n",
      "Epoch [370/1000], Loss: 373.6169\n",
      "Epoch [380/1000], Loss: 356.6985\n",
      "Epoch [390/1000], Loss: 340.4374\n",
      "Epoch [400/1000], Loss: 324.8140\n",
      "Epoch [410/1000], Loss: 309.8095\n",
      "Epoch [420/1000], Loss: 295.4053\n",
      "Epoch [430/1000], Loss: 281.5831\n",
      "Epoch [440/1000], Loss: 268.3253\n",
      "Epoch [450/1000], Loss: 255.6142\n",
      "Epoch [460/1000], Loss: 243.4327\n",
      "Epoch [470/1000], Loss: 231.7640\n",
      "Epoch [480/1000], Loss: 220.5918\n",
      "Epoch [490/1000], Loss: 209.8999\n",
      "Epoch [500/1000], Loss: 199.6725\n",
      "Epoch [510/1000], Loss: 189.8941\n",
      "Epoch [520/1000], Loss: 180.5495\n",
      "Epoch [530/1000], Loss: 171.6240\n",
      "Epoch [540/1000], Loss: 163.1031\n",
      "Epoch [550/1000], Loss: 154.9724\n",
      "Epoch [560/1000], Loss: 147.2181\n",
      "Epoch [570/1000], Loss: 139.8268\n",
      "Epoch [580/1000], Loss: 132.7849\n",
      "Epoch [590/1000], Loss: 126.0796\n",
      "Epoch [600/1000], Loss: 119.6983\n",
      "Epoch [610/1000], Loss: 113.6286\n",
      "Epoch [620/1000], Loss: 107.8584\n",
      "Epoch [630/1000], Loss: 102.3761\n",
      "Epoch [640/1000], Loss: 97.1700\n",
      "Epoch [650/1000], Loss: 92.2292\n",
      "Epoch [660/1000], Loss: 87.5427\n",
      "Epoch [670/1000], Loss: 83.1002\n",
      "Epoch [680/1000], Loss: 78.8911\n",
      "Epoch [690/1000], Loss: 74.9058\n",
      "Epoch [700/1000], Loss: 71.1344\n",
      "Epoch [710/1000], Loss: 67.5676\n",
      "Epoch [720/1000], Loss: 64.1964\n",
      "Epoch [730/1000], Loss: 61.0119\n",
      "Epoch [740/1000], Loss: 58.0056\n",
      "Epoch [750/1000], Loss: 55.1694\n",
      "Epoch [760/1000], Loss: 52.4951\n",
      "Epoch [770/1000], Loss: 49.9752\n",
      "Epoch [780/1000], Loss: 47.6022\n",
      "Epoch [790/1000], Loss: 45.3689\n",
      "Epoch [800/1000], Loss: 43.2685\n",
      "Epoch [810/1000], Loss: 41.2943\n",
      "Epoch [820/1000], Loss: 39.4399\n",
      "Epoch [830/1000], Loss: 37.6992\n",
      "Epoch [840/1000], Loss: 36.0662\n",
      "Epoch [850/1000], Loss: 34.5353\n",
      "Epoch [860/1000], Loss: 33.1010\n",
      "Epoch [870/1000], Loss: 31.7582\n",
      "Epoch [880/1000], Loss: 30.5018\n",
      "Epoch [890/1000], Loss: 29.3271\n",
      "Epoch [900/1000], Loss: 28.2294\n",
      "Epoch [910/1000], Loss: 27.2045\n",
      "Epoch [920/1000], Loss: 26.2481\n",
      "Epoch [930/1000], Loss: 25.3562\n",
      "Epoch [940/1000], Loss: 24.5251\n",
      "Epoch [950/1000], Loss: 23.7512\n",
      "Epoch [960/1000], Loss: 23.0311\n",
      "Epoch [970/1000], Loss: 22.3613\n",
      "Epoch [980/1000], Loss: 21.7389\n",
      "Epoch [990/1000], Loss: 21.1610\n",
      "Epoch [1000/1000], Loss: 20.6246\n",
      "Predicted days_remaining for parent_id 320: [41.16566848754883, 41.2886848449707, 41.28848648071289, 41.287879943847656, 41.28925704956055, 41.28901672363281, 41.288597106933594, 41.28887176513672]\n",
      "Training for parent_id 321...\n",
      "Epoch [10/1000], Loss: 708.7182\n",
      "Epoch [20/1000], Loss: 629.2814\n",
      "Epoch [30/1000], Loss: 574.3571\n",
      "Epoch [40/1000], Loss: 532.3977\n",
      "Epoch [50/1000], Loss: 495.9641\n",
      "Epoch [60/1000], Loss: 462.7245\n",
      "Epoch [70/1000], Loss: 432.0199\n",
      "Epoch [80/1000], Loss: 403.4921\n",
      "Epoch [90/1000], Loss: 376.8975\n",
      "Epoch [100/1000], Loss: 352.0476\n",
      "Epoch [110/1000], Loss: 328.7884\n",
      "Epoch [120/1000], Loss: 306.9911\n",
      "Epoch [130/1000], Loss: 286.5465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 267.3613\n",
      "Epoch [150/1000], Loss: 249.3541\n",
      "Epoch [160/1000], Loss: 232.4529\n",
      "Epoch [170/1000], Loss: 216.5935\n",
      "Epoch [180/1000], Loss: 201.7172\n",
      "Epoch [190/1000], Loss: 187.7705\n",
      "Epoch [200/1000], Loss: 174.7035\n",
      "Epoch [210/1000], Loss: 162.4696\n",
      "Epoch [220/1000], Loss: 151.0250\n",
      "Epoch [230/1000], Loss: 140.3283\n",
      "Epoch [240/1000], Loss: 130.3400\n",
      "Epoch [250/1000], Loss: 121.0226\n",
      "Epoch [260/1000], Loss: 112.3400\n",
      "Epoch [270/1000], Loss: 104.2582\n",
      "Epoch [280/1000], Loss: 96.7440\n",
      "Epoch [290/1000], Loss: 89.7658\n",
      "Epoch [300/1000], Loss: 83.2932\n",
      "Epoch [310/1000], Loss: 77.2972\n",
      "Epoch [320/1000], Loss: 71.7496\n",
      "Epoch [330/1000], Loss: 66.6237\n",
      "Epoch [340/1000], Loss: 61.8938\n",
      "Epoch [350/1000], Loss: 57.5351\n",
      "Epoch [360/1000], Loss: 53.5242\n",
      "Epoch [370/1000], Loss: 49.8383\n",
      "Epoch [380/1000], Loss: 46.4560\n",
      "Epoch [390/1000], Loss: 43.3568\n",
      "Epoch [400/1000], Loss: 40.5212\n",
      "Epoch [410/1000], Loss: 37.9304\n",
      "Epoch [420/1000], Loss: 35.5670\n",
      "Epoch [430/1000], Loss: 33.4142\n",
      "Epoch [440/1000], Loss: 31.4563\n",
      "Epoch [450/1000], Loss: 29.6783\n",
      "Epoch [460/1000], Loss: 28.0662\n",
      "Epoch [470/1000], Loss: 26.6068\n",
      "Epoch [480/1000], Loss: 25.2877\n",
      "Epoch [490/1000], Loss: 24.0973\n",
      "Epoch [500/1000], Loss: 23.0248\n",
      "Epoch [510/1000], Loss: 22.0600\n",
      "Epoch [520/1000], Loss: 21.1934\n",
      "Epoch [530/1000], Loss: 20.4165\n",
      "Epoch [540/1000], Loss: 19.7209\n",
      "Epoch [550/1000], Loss: 19.0992\n",
      "Epoch [560/1000], Loss: 18.5445\n",
      "Epoch [570/1000], Loss: 18.0503\n",
      "Epoch [580/1000], Loss: 17.6107\n",
      "Epoch [590/1000], Loss: 17.2205\n",
      "Epoch [600/1000], Loss: 16.8745\n",
      "Epoch [610/1000], Loss: 16.5683\n",
      "Epoch [620/1000], Loss: 16.2978\n",
      "Epoch [630/1000], Loss: 16.0592\n",
      "Epoch [640/1000], Loss: 15.8491\n",
      "Epoch [650/1000], Loss: 15.6643\n",
      "Epoch [660/1000], Loss: 15.5022\n",
      "Epoch [670/1000], Loss: 15.3601\n",
      "Epoch [680/1000], Loss: 15.2358\n",
      "Epoch [690/1000], Loss: 15.1273\n",
      "Epoch [700/1000], Loss: 15.0327\n",
      "Epoch [710/1000], Loss: 14.9503\n",
      "Epoch [720/1000], Loss: 14.8788\n",
      "Epoch [730/1000], Loss: 14.8167\n",
      "Epoch [740/1000], Loss: 14.7629\n",
      "Epoch [750/1000], Loss: 14.7165\n",
      "Epoch [760/1000], Loss: 14.6764\n",
      "Epoch [770/1000], Loss: 14.6418\n",
      "Epoch [780/1000], Loss: 14.6121\n",
      "Epoch [790/1000], Loss: 14.5866\n",
      "Epoch [800/1000], Loss: 14.5647\n",
      "Epoch [810/1000], Loss: 14.5460\n",
      "Epoch [820/1000], Loss: 14.5300\n",
      "Epoch [830/1000], Loss: 14.5163\n",
      "Epoch [840/1000], Loss: 14.5047\n",
      "Epoch [850/1000], Loss: 14.4948\n",
      "Epoch [860/1000], Loss: 14.4865\n",
      "Epoch [870/1000], Loss: 14.4794\n",
      "Epoch [880/1000], Loss: 14.4734\n",
      "Epoch [890/1000], Loss: 14.4683\n",
      "Epoch [900/1000], Loss: 14.4640\n",
      "Epoch [910/1000], Loss: 14.4604\n",
      "Epoch [920/1000], Loss: 14.4574\n",
      "Epoch [930/1000], Loss: 14.4549\n",
      "Epoch [940/1000], Loss: 14.4527\n",
      "Epoch [950/1000], Loss: 14.4510\n",
      "Epoch [960/1000], Loss: 14.4495\n",
      "Epoch [970/1000], Loss: 14.4483\n",
      "Epoch [980/1000], Loss: 14.4472\n",
      "Epoch [990/1000], Loss: 14.4464\n",
      "Epoch [1000/1000], Loss: 14.4457\n",
      "Predicted days_remaining for parent_id 321: [27.50700569152832, 27.721498489379883, 27.72415542602539, 27.72378921508789, 27.721891403198242, 27.718231201171875, 27.71772575378418, 27.72150993347168]\n",
      "Training for parent_id 326...\n",
      "Epoch [10/1000], Loss: 193.9193\n",
      "Epoch [20/1000], Loss: 150.5133\n",
      "Epoch [30/1000], Loss: 120.1224\n",
      "Epoch [40/1000], Loss: 99.3059\n",
      "Epoch [50/1000], Loss: 84.1787\n",
      "Epoch [60/1000], Loss: 72.2655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000], Loss: 62.3793\n",
      "Epoch [80/1000], Loss: 54.0442\n",
      "Epoch [90/1000], Loss: 47.0027\n",
      "Epoch [100/1000], Loss: 41.0647\n",
      "Epoch [110/1000], Loss: 36.0763\n",
      "Epoch [120/1000], Loss: 31.9091\n",
      "Epoch [130/1000], Loss: 28.4514\n",
      "Epoch [140/1000], Loss: 25.6028\n",
      "Epoch [150/1000], Loss: 23.2728\n",
      "Epoch [160/1000], Loss: 21.3810\n",
      "Epoch [170/1000], Loss: 19.8570\n",
      "Epoch [180/1000], Loss: 18.6392\n",
      "Epoch [190/1000], Loss: 17.6740\n",
      "Epoch [200/1000], Loss: 16.9156\n",
      "Epoch [210/1000], Loss: 16.3249\n",
      "Epoch [220/1000], Loss: 15.8686\n",
      "Epoch [230/1000], Loss: 15.5194\n",
      "Epoch [240/1000], Loss: 15.2543\n",
      "Epoch [250/1000], Loss: 15.0549\n",
      "Epoch [260/1000], Loss: 14.9061\n",
      "Epoch [270/1000], Loss: 14.7960\n",
      "Epoch [280/1000], Loss: 14.7151\n",
      "Epoch [290/1000], Loss: 14.6562\n",
      "Epoch [300/1000], Loss: 14.6135\n",
      "Epoch [310/1000], Loss: 14.5826\n",
      "Epoch [320/1000], Loss: 14.5605\n",
      "Epoch [330/1000], Loss: 14.5446\n",
      "Epoch [340/1000], Loss: 14.5331\n",
      "Epoch [350/1000], Loss: 14.5248\n",
      "Epoch [360/1000], Loss: 14.5187\n",
      "Epoch [370/1000], Loss: 14.5141\n",
      "Epoch [380/1000], Loss: 14.5106\n",
      "Epoch [390/1000], Loss: 14.5078\n",
      "Epoch [400/1000], Loss: 14.5055\n",
      "Epoch [410/1000], Loss: 14.5036\n",
      "Epoch [420/1000], Loss: 14.5019\n",
      "Epoch [430/1000], Loss: 14.5004\n",
      "Epoch [440/1000], Loss: 14.4989\n",
      "Epoch [450/1000], Loss: 14.4976\n",
      "Epoch [460/1000], Loss: 14.4964\n",
      "Epoch [470/1000], Loss: 14.4951\n",
      "Epoch [480/1000], Loss: 14.4940\n",
      "Epoch [490/1000], Loss: 14.4928\n",
      "Epoch [500/1000], Loss: 14.4918\n",
      "Epoch [510/1000], Loss: 14.4907\n",
      "Epoch [520/1000], Loss: 14.4897\n",
      "Epoch [530/1000], Loss: 14.4886\n",
      "Epoch [540/1000], Loss: 14.4877\n",
      "Epoch [550/1000], Loss: 14.4867\n",
      "Epoch [560/1000], Loss: 14.4858\n",
      "Epoch [570/1000], Loss: 14.4849\n",
      "Epoch [580/1000], Loss: 14.4840\n",
      "Epoch [590/1000], Loss: 14.4831\n",
      "Epoch [600/1000], Loss: 14.4823\n",
      "Epoch [610/1000], Loss: 14.4815\n",
      "Epoch [620/1000], Loss: 14.4807\n",
      "Epoch [630/1000], Loss: 14.4799\n",
      "Epoch [640/1000], Loss: 14.4791\n",
      "Epoch [650/1000], Loss: 14.4784\n",
      "Epoch [660/1000], Loss: 14.4777\n",
      "Epoch [670/1000], Loss: 14.4770\n",
      "Epoch [680/1000], Loss: 14.4763\n",
      "Epoch [690/1000], Loss: 14.4756\n",
      "Epoch [700/1000], Loss: 14.4750\n",
      "Epoch [710/1000], Loss: 14.4743\n",
      "Epoch [720/1000], Loss: 14.4737\n",
      "Epoch [730/1000], Loss: 14.4731\n",
      "Epoch [740/1000], Loss: 14.4725\n",
      "Epoch [750/1000], Loss: 14.4719\n",
      "Epoch [760/1000], Loss: 14.4714\n",
      "Epoch [770/1000], Loss: 14.4708\n",
      "Epoch [780/1000], Loss: 14.4703\n",
      "Epoch [790/1000], Loss: 14.4697\n",
      "Epoch [800/1000], Loss: 14.4692\n",
      "Epoch [810/1000], Loss: 14.4687\n",
      "Epoch [820/1000], Loss: 14.4682\n",
      "Epoch [830/1000], Loss: 14.4677\n",
      "Epoch [840/1000], Loss: 14.4673\n",
      "Epoch [850/1000], Loss: 14.4668\n",
      "Epoch [860/1000], Loss: 14.4664\n",
      "Epoch [870/1000], Loss: 14.4659\n",
      "Epoch [880/1000], Loss: 14.4655\n",
      "Epoch [890/1000], Loss: 14.4651\n",
      "Epoch [900/1000], Loss: 14.4647\n",
      "Epoch [910/1000], Loss: 14.4642\n",
      "Epoch [920/1000], Loss: 14.4638\n",
      "Epoch [930/1000], Loss: 14.4635\n",
      "Epoch [940/1000], Loss: 14.4631\n",
      "Epoch [950/1000], Loss: 14.4627\n",
      "Epoch [960/1000], Loss: 14.4623\n",
      "Epoch [970/1000], Loss: 14.4620\n",
      "Epoch [980/1000], Loss: 14.4616\n",
      "Epoch [990/1000], Loss: 14.4613\n",
      "Epoch [1000/1000], Loss: 14.4610\n",
      "Predicted days_remaining for parent_id 326: [14.345006942749023, 14.815618515014648, 14.8035249710083, 14.807514190673828, 14.811755180358887, 14.809528350830078, 14.804247856140137, 14.800748825073242]\n",
      "Training for parent_id 332...\n",
      "Epoch [10/1000], Loss: 123.2710\n",
      "Epoch [20/1000], Loss: 92.3915\n",
      "Epoch [30/1000], Loss: 72.7558\n",
      "Epoch [40/1000], Loss: 60.0838\n",
      "Epoch [50/1000], Loss: 50.4513\n",
      "Epoch [60/1000], Loss: 42.6363\n",
      "Epoch [70/1000], Loss: 36.2779\n",
      "Epoch [80/1000], Loss: 31.1652\n",
      "Epoch [90/1000], Loss: 27.1013\n",
      "Epoch [100/1000], Loss: 23.9104\n",
      "Epoch [110/1000], Loss: 21.4379\n",
      "Epoch [120/1000], Loss: 19.5487\n",
      "Epoch [130/1000], Loss: 18.1259\n",
      "Epoch [140/1000], Loss: 17.0701\n",
      "Epoch [150/1000], Loss: 16.2986\n",
      "Epoch [160/1000], Loss: 15.7435\n",
      "Epoch [170/1000], Loss: 15.3503\n",
      "Epoch [180/1000], Loss: 15.0761\n",
      "Epoch [190/1000], Loss: 14.8878\n",
      "Epoch [200/1000], Loss: 14.7604\n",
      "Epoch [210/1000], Loss: 14.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 14.6193\n",
      "Epoch [230/1000], Loss: 14.5825\n",
      "Epoch [240/1000], Loss: 14.5585\n",
      "Epoch [250/1000], Loss: 14.5428\n",
      "Epoch [260/1000], Loss: 14.5324\n",
      "Epoch [270/1000], Loss: 14.5252\n",
      "Epoch [280/1000], Loss: 14.5201\n",
      "Epoch [290/1000], Loss: 14.5162\n",
      "Epoch [300/1000], Loss: 14.5131\n",
      "Epoch [310/1000], Loss: 14.5104\n",
      "Epoch [320/1000], Loss: 14.5081\n",
      "Epoch [330/1000], Loss: 14.5059\n",
      "Epoch [340/1000], Loss: 14.5039\n",
      "Epoch [350/1000], Loss: 14.5020\n",
      "Epoch [360/1000], Loss: 14.5002\n",
      "Epoch [370/1000], Loss: 14.4985\n",
      "Epoch [380/1000], Loss: 14.4968\n",
      "Epoch [390/1000], Loss: 14.4952\n",
      "Epoch [400/1000], Loss: 14.4937\n",
      "Epoch [410/1000], Loss: 14.4922\n",
      "Epoch [420/1000], Loss: 14.4908\n",
      "Epoch [430/1000], Loss: 14.4895\n",
      "Epoch [440/1000], Loss: 14.4881\n",
      "Epoch [450/1000], Loss: 14.4869\n",
      "Epoch [460/1000], Loss: 14.4857\n",
      "Epoch [470/1000], Loss: 14.4845\n",
      "Epoch [480/1000], Loss: 14.4834\n",
      "Epoch [490/1000], Loss: 14.4823\n",
      "Epoch [500/1000], Loss: 14.4812\n",
      "Epoch [510/1000], Loss: 14.4802\n",
      "Epoch [520/1000], Loss: 14.4792\n",
      "Epoch [530/1000], Loss: 14.4782\n",
      "Epoch [540/1000], Loss: 14.4773\n",
      "Epoch [550/1000], Loss: 14.4764\n",
      "Epoch [560/1000], Loss: 14.4756\n",
      "Epoch [570/1000], Loss: 14.4747\n",
      "Epoch [580/1000], Loss: 14.4739\n",
      "Epoch [590/1000], Loss: 14.4731\n",
      "Epoch [600/1000], Loss: 14.4724\n",
      "Epoch [610/1000], Loss: 14.4717\n",
      "Epoch [620/1000], Loss: 14.4709\n",
      "Epoch [630/1000], Loss: 14.4703\n",
      "Epoch [640/1000], Loss: 14.4696\n",
      "Epoch [650/1000], Loss: 14.4689\n",
      "Epoch [660/1000], Loss: 14.4683\n",
      "Epoch [670/1000], Loss: 14.4677\n",
      "Epoch [680/1000], Loss: 14.4671\n",
      "Epoch [690/1000], Loss: 14.4665\n",
      "Epoch [700/1000], Loss: 14.4660\n",
      "Epoch [710/1000], Loss: 14.4654\n",
      "Epoch [720/1000], Loss: 14.4649\n",
      "Epoch [730/1000], Loss: 14.4644\n",
      "Epoch [740/1000], Loss: 14.4639\n",
      "Epoch [750/1000], Loss: 14.4634\n",
      "Epoch [760/1000], Loss: 14.4629\n",
      "Epoch [770/1000], Loss: 14.4625\n",
      "Epoch [780/1000], Loss: 14.4620\n",
      "Epoch [790/1000], Loss: 14.4616\n",
      "Epoch [800/1000], Loss: 14.4612\n",
      "Epoch [810/1000], Loss: 14.4607\n",
      "Epoch [820/1000], Loss: 14.4603\n",
      "Epoch [830/1000], Loss: 14.4599\n",
      "Epoch [840/1000], Loss: 14.4596\n",
      "Epoch [850/1000], Loss: 14.4592\n",
      "Epoch [860/1000], Loss: 14.4588\n",
      "Epoch [870/1000], Loss: 14.4585\n",
      "Epoch [880/1000], Loss: 14.4581\n",
      "Epoch [890/1000], Loss: 14.4578\n",
      "Epoch [900/1000], Loss: 14.4575\n",
      "Epoch [910/1000], Loss: 14.4571\n",
      "Epoch [920/1000], Loss: 14.4568\n",
      "Epoch [930/1000], Loss: 14.4565\n",
      "Epoch [940/1000], Loss: 14.4562\n",
      "Epoch [950/1000], Loss: 14.4559\n",
      "Epoch [960/1000], Loss: 14.4556\n",
      "Epoch [970/1000], Loss: 14.4553\n",
      "Epoch [980/1000], Loss: 14.4551\n",
      "Epoch [990/1000], Loss: 14.4548\n",
      "Epoch [1000/1000], Loss: 14.4545\n",
      "Predicted days_remaining for parent_id 332: [11.404428482055664, 11.798810958862305, 11.804237365722656, 11.802587509155273, 11.794461250305176, 11.792057037353516, 11.802392959594727, 11.79643726348877]\n",
      "Training for parent_id 340...\n",
      "Epoch [10/1000], Loss: 208.5470\n",
      "Epoch [20/1000], Loss: 162.7670\n",
      "Epoch [30/1000], Loss: 133.3699\n",
      "Epoch [40/1000], Loss: 113.6628\n",
      "Epoch [50/1000], Loss: 98.4874\n",
      "Epoch [60/1000], Loss: 85.8738\n",
      "Epoch [70/1000], Loss: 75.1043\n",
      "Epoch [80/1000], Loss: 65.8292\n",
      "Epoch [90/1000], Loss: 57.8249\n",
      "Epoch [100/1000], Loss: 50.9215\n",
      "Epoch [110/1000], Loss: 44.9804\n",
      "Epoch [120/1000], Loss: 39.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000], Loss: 35.5297\n",
      "Epoch [140/1000], Loss: 31.8271\n",
      "Epoch [150/1000], Loss: 28.6946\n",
      "Epoch [160/1000], Loss: 26.0593\n",
      "Epoch [170/1000], Loss: 23.8553\n",
      "Epoch [180/1000], Loss: 22.0233\n",
      "Epoch [190/1000], Loss: 20.5102\n",
      "Epoch [200/1000], Loss: 19.2686\n",
      "Epoch [210/1000], Loss: 18.2568\n",
      "Epoch [220/1000], Loss: 17.4377\n",
      "Epoch [230/1000], Loss: 16.7794\n",
      "Epoch [240/1000], Loss: 16.2538\n",
      "Epoch [250/1000], Loss: 15.8373\n",
      "Epoch [260/1000], Loss: 15.5095\n",
      "Epoch [270/1000], Loss: 15.2534\n",
      "Epoch [280/1000], Loss: 15.0548\n",
      "Epoch [290/1000], Loss: 14.9018\n",
      "Epoch [300/1000], Loss: 14.7848\n",
      "Epoch [310/1000], Loss: 14.6960\n",
      "Epoch [320/1000], Loss: 14.6291\n",
      "Epoch [330/1000], Loss: 14.5790\n",
      "Epoch [340/1000], Loss: 14.5418\n",
      "Epoch [350/1000], Loss: 14.5144\n",
      "Epoch [360/1000], Loss: 14.4943\n",
      "Epoch [370/1000], Loss: 14.4797\n",
      "Epoch [380/1000], Loss: 14.4691\n",
      "Epoch [390/1000], Loss: 14.4615\n",
      "Epoch [400/1000], Loss: 14.4561\n",
      "Epoch [410/1000], Loss: 14.4523\n",
      "Epoch [420/1000], Loss: 14.4496\n",
      "Epoch [430/1000], Loss: 14.4477\n",
      "Epoch [440/1000], Loss: 14.4464\n",
      "Epoch [450/1000], Loss: 14.4455\n",
      "Epoch [460/1000], Loss: 14.4448\n",
      "Epoch [470/1000], Loss: 14.4444\n",
      "Epoch [480/1000], Loss: 14.4441\n",
      "Epoch [490/1000], Loss: 14.4439\n",
      "Epoch [500/1000], Loss: 14.4437\n",
      "Epoch [510/1000], Loss: 14.4436\n",
      "Epoch [520/1000], Loss: 14.4435\n",
      "Epoch [530/1000], Loss: 14.4434\n",
      "Epoch [540/1000], Loss: 14.4433\n",
      "Epoch [550/1000], Loss: 14.4433\n",
      "Epoch [560/1000], Loss: 14.4432\n",
      "Epoch [570/1000], Loss: 14.4432\n",
      "Epoch [580/1000], Loss: 14.4431\n",
      "Epoch [590/1000], Loss: 14.4431\n",
      "Epoch [600/1000], Loss: 14.4430\n",
      "Epoch [610/1000], Loss: 14.4430\n",
      "Epoch [620/1000], Loss: 14.4429\n",
      "Epoch [630/1000], Loss: 14.4429\n",
      "Epoch [640/1000], Loss: 14.4429\n",
      "Epoch [650/1000], Loss: 14.4428\n",
      "Epoch [660/1000], Loss: 14.4428\n",
      "Epoch [670/1000], Loss: 14.4427\n",
      "Epoch [680/1000], Loss: 14.4427\n",
      "Epoch [690/1000], Loss: 14.4426\n",
      "Epoch [700/1000], Loss: 14.4426\n",
      "Epoch [710/1000], Loss: 14.4426\n",
      "Epoch [720/1000], Loss: 14.4425\n",
      "Epoch [730/1000], Loss: 14.4425\n",
      "Epoch [740/1000], Loss: 14.4424\n",
      "Epoch [750/1000], Loss: 14.4424\n",
      "Epoch [760/1000], Loss: 14.4424\n",
      "Epoch [770/1000], Loss: 14.4423\n",
      "Epoch [780/1000], Loss: 14.4423\n",
      "Epoch [790/1000], Loss: 14.4422\n",
      "Epoch [800/1000], Loss: 14.4422\n",
      "Epoch [810/1000], Loss: 14.4422\n",
      "Epoch [820/1000], Loss: 14.4421\n",
      "Epoch [830/1000], Loss: 14.4421\n",
      "Epoch [840/1000], Loss: 14.4420\n",
      "Epoch [850/1000], Loss: 14.4420\n",
      "Epoch [860/1000], Loss: 14.4420\n",
      "Epoch [870/1000], Loss: 14.4419\n",
      "Epoch [880/1000], Loss: 14.4419\n",
      "Epoch [890/1000], Loss: 14.4419\n",
      "Epoch [900/1000], Loss: 14.4418\n",
      "Epoch [910/1000], Loss: 14.4418\n",
      "Epoch [920/1000], Loss: 14.4417\n",
      "Epoch [930/1000], Loss: 14.4417\n",
      "Epoch [940/1000], Loss: 14.4417\n",
      "Epoch [950/1000], Loss: 14.4416\n",
      "Epoch [960/1000], Loss: 14.4416\n",
      "Epoch [970/1000], Loss: 14.4416\n",
      "Epoch [980/1000], Loss: 14.4415\n",
      "Epoch [990/1000], Loss: 14.4415\n",
      "Epoch [1000/1000], Loss: 14.4415\n",
      "Predicted days_remaining for parent_id 340: [15.583932876586914, 15.775397300720215, 15.7762451171875, 15.781399726867676, 15.775897026062012, 15.770301818847656, 15.771978378295898, 15.765899658203125]\n",
      "Training for parent_id 346...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 322.0196\n",
      "Epoch [20/1000], Loss: 268.6207\n",
      "Epoch [30/1000], Loss: 230.8758\n",
      "Epoch [40/1000], Loss: 204.3061\n",
      "Epoch [50/1000], Loss: 182.8070\n",
      "Epoch [60/1000], Loss: 164.0984\n",
      "Epoch [70/1000], Loss: 147.4290\n",
      "Epoch [80/1000], Loss: 132.4745\n",
      "Epoch [90/1000], Loss: 119.0290\n",
      "Epoch [100/1000], Loss: 106.9362\n",
      "Epoch [110/1000], Loss: 96.0667\n",
      "Epoch [120/1000], Loss: 86.3080\n",
      "Epoch [130/1000], Loss: 77.5601\n",
      "Epoch [140/1000], Loss: 69.7327\n",
      "Epoch [150/1000], Loss: 62.7438\n",
      "Epoch [160/1000], Loss: 56.5184\n",
      "Epoch [170/1000], Loss: 50.9874\n",
      "Epoch [180/1000], Loss: 46.0871\n",
      "Epoch [190/1000], Loss: 41.7584\n",
      "Epoch [200/1000], Loss: 37.9466\n",
      "Epoch [210/1000], Loss: 34.6008\n",
      "Epoch [220/1000], Loss: 31.6739\n",
      "Epoch [230/1000], Loss: 29.1224\n",
      "Epoch [240/1000], Loss: 26.9060\n",
      "Epoch [250/1000], Loss: 24.9879\n",
      "Epoch [260/1000], Loss: 23.3339\n",
      "Epoch [270/1000], Loss: 21.9131\n",
      "Epoch [280/1000], Loss: 20.6974\n",
      "Epoch [290/1000], Loss: 19.6611\n",
      "Epoch [300/1000], Loss: 18.7812\n",
      "Epoch [310/1000], Loss: 18.0371\n",
      "Epoch [320/1000], Loss: 17.4103\n",
      "Epoch [330/1000], Loss: 16.8845\n",
      "Epoch [340/1000], Loss: 16.4450\n",
      "Epoch [350/1000], Loss: 16.0793\n",
      "Epoch [360/1000], Loss: 15.7762\n",
      "Epoch [370/1000], Loss: 15.5259\n",
      "Epoch [380/1000], Loss: 15.3202\n",
      "Epoch [390/1000], Loss: 15.1517\n",
      "Epoch [400/1000], Loss: 15.0142\n",
      "Epoch [410/1000], Loss: 14.9025\n",
      "Epoch [420/1000], Loss: 14.8122\n",
      "Epoch [430/1000], Loss: 14.7394\n",
      "Epoch [440/1000], Loss: 14.6809\n",
      "Epoch [450/1000], Loss: 14.6342\n",
      "Epoch [460/1000], Loss: 14.5969\n",
      "Epoch [470/1000], Loss: 14.5674\n",
      "Epoch [480/1000], Loss: 14.5440\n",
      "Epoch [490/1000], Loss: 14.5256\n",
      "Epoch [500/1000], Loss: 14.5111\n",
      "Epoch [510/1000], Loss: 14.4998\n",
      "Epoch [520/1000], Loss: 14.4909\n",
      "Epoch [530/1000], Loss: 14.4840\n",
      "Epoch [540/1000], Loss: 14.4787\n",
      "Epoch [550/1000], Loss: 14.4745\n",
      "Epoch [560/1000], Loss: 14.4713\n",
      "Epoch [570/1000], Loss: 14.4688\n",
      "Epoch [580/1000], Loss: 14.4669\n",
      "Epoch [590/1000], Loss: 14.4653\n",
      "Epoch [600/1000], Loss: 14.4641\n",
      "Epoch [610/1000], Loss: 14.4632\n",
      "Epoch [620/1000], Loss: 14.4624\n",
      "Epoch [630/1000], Loss: 14.4618\n",
      "Epoch [640/1000], Loss: 14.4613\n",
      "Epoch [650/1000], Loss: 14.4609\n",
      "Epoch [660/1000], Loss: 14.4605\n",
      "Epoch [670/1000], Loss: 14.4602\n",
      "Epoch [680/1000], Loss: 14.4599\n",
      "Epoch [690/1000], Loss: 14.4597\n",
      "Epoch [700/1000], Loss: 14.4594\n",
      "Epoch [710/1000], Loss: 14.4592\n",
      "Epoch [720/1000], Loss: 14.4590\n",
      "Epoch [730/1000], Loss: 14.4588\n",
      "Epoch [740/1000], Loss: 14.4585\n",
      "Epoch [750/1000], Loss: 14.4583\n",
      "Epoch [760/1000], Loss: 14.4582\n",
      "Epoch [770/1000], Loss: 14.4580\n",
      "Epoch [780/1000], Loss: 14.4578\n",
      "Epoch [790/1000], Loss: 14.4576\n",
      "Epoch [800/1000], Loss: 14.4574\n",
      "Epoch [810/1000], Loss: 14.4572\n",
      "Epoch [820/1000], Loss: 14.4570\n",
      "Epoch [830/1000], Loss: 14.4569\n",
      "Epoch [840/1000], Loss: 14.4567\n",
      "Epoch [850/1000], Loss: 14.4565\n",
      "Epoch [860/1000], Loss: 14.4563\n",
      "Epoch [870/1000], Loss: 14.4562\n",
      "Epoch [880/1000], Loss: 14.4560\n",
      "Epoch [890/1000], Loss: 14.4558\n",
      "Epoch [900/1000], Loss: 14.4557\n",
      "Epoch [910/1000], Loss: 14.4555\n",
      "Epoch [920/1000], Loss: 14.4553\n",
      "Epoch [930/1000], Loss: 14.4552\n",
      "Epoch [940/1000], Loss: 14.4550\n",
      "Epoch [950/1000], Loss: 14.4549\n",
      "Epoch [960/1000], Loss: 14.4547\n",
      "Epoch [970/1000], Loss: 14.4546\n",
      "Epoch [980/1000], Loss: 14.4544\n",
      "Epoch [990/1000], Loss: 14.4543\n",
      "Epoch [1000/1000], Loss: 14.4541\n",
      "Predicted days_remaining for parent_id 346: [18.40973472595215, 18.80124282836914, 18.798751831054688, 18.802064895629883, 18.80071449279785, 18.79460334777832, 18.796558380126953, 18.7995662689209]\n",
      "Training for parent_id 348...\n",
      "Epoch [10/1000], Loss: 934.2257\n",
      "Epoch [20/1000], Loss: 846.8910\n",
      "Epoch [30/1000], Loss: 780.4501\n",
      "Epoch [40/1000], Loss: 731.4355\n",
      "Epoch [50/1000], Loss: 689.1746\n",
      "Epoch [60/1000], Loss: 650.1408\n",
      "Epoch [70/1000], Loss: 613.6456\n",
      "Epoch [80/1000], Loss: 579.3506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 547.0277\n",
      "Epoch [100/1000], Loss: 516.5005\n",
      "Epoch [110/1000], Loss: 487.6246\n",
      "Epoch [120/1000], Loss: 460.2787\n",
      "Epoch [130/1000], Loss: 434.3593\n",
      "Epoch [140/1000], Loss: 409.7772\n",
      "Epoch [150/1000], Loss: 386.4542\n",
      "Epoch [160/1000], Loss: 364.3218\n",
      "Epoch [170/1000], Loss: 343.3186\n",
      "Epoch [180/1000], Loss: 323.3889\n",
      "Epoch [190/1000], Loss: 304.4819\n",
      "Epoch [200/1000], Loss: 286.5508\n",
      "Epoch [210/1000], Loss: 269.5515\n",
      "Epoch [220/1000], Loss: 253.4433\n",
      "Epoch [230/1000], Loss: 238.1871\n",
      "Epoch [240/1000], Loss: 223.7462\n",
      "Epoch [250/1000], Loss: 210.0854\n",
      "Epoch [260/1000], Loss: 197.1711\n",
      "Epoch [270/1000], Loss: 184.9709\n",
      "Epoch [280/1000], Loss: 173.4537\n",
      "Epoch [290/1000], Loss: 162.5895\n",
      "Epoch [300/1000], Loss: 152.3494\n",
      "Epoch [310/1000], Loss: 142.7054\n",
      "Epoch [320/1000], Loss: 133.6303\n",
      "Epoch [330/1000], Loss: 125.0980\n",
      "Epoch [340/1000], Loss: 117.0831\n",
      "Epoch [350/1000], Loss: 109.5610\n",
      "Epoch [360/1000], Loss: 102.5079\n",
      "Epoch [370/1000], Loss: 95.9007\n",
      "Epoch [380/1000], Loss: 89.7173\n",
      "Epoch [390/1000], Loss: 83.9361\n",
      "Epoch [400/1000], Loss: 78.5362\n",
      "Epoch [410/1000], Loss: 73.4976\n",
      "Epoch [420/1000], Loss: 68.8009\n",
      "Epoch [430/1000], Loss: 64.4275\n",
      "Epoch [440/1000], Loss: 60.3593\n",
      "Epoch [450/1000], Loss: 56.5790\n",
      "Epoch [460/1000], Loss: 53.0701\n",
      "Epoch [470/1000], Loss: 49.8166\n",
      "Epoch [480/1000], Loss: 46.8032\n",
      "Epoch [490/1000], Loss: 44.0152\n",
      "Epoch [500/1000], Loss: 41.4387\n",
      "Epoch [510/1000], Loss: 39.0603\n",
      "Epoch [520/1000], Loss: 36.8673\n",
      "Epoch [530/1000], Loss: 34.8474\n",
      "Epoch [540/1000], Loss: 32.9893\n",
      "Epoch [550/1000], Loss: 31.2819\n",
      "Epoch [560/1000], Loss: 29.7148\n",
      "Epoch [570/1000], Loss: 28.2782\n",
      "Epoch [580/1000], Loss: 26.9627\n",
      "Epoch [590/1000], Loss: 25.7597\n",
      "Epoch [600/1000], Loss: 24.6607\n",
      "Epoch [610/1000], Loss: 23.6580\n",
      "Epoch [620/1000], Loss: 22.7442\n",
      "Epoch [630/1000], Loss: 21.9126\n",
      "Epoch [640/1000], Loss: 21.1565\n",
      "Epoch [650/1000], Loss: 20.4700\n",
      "Epoch [660/1000], Loss: 19.8475\n",
      "Epoch [670/1000], Loss: 19.2836\n",
      "Epoch [680/1000], Loss: 18.7735\n",
      "Epoch [690/1000], Loss: 18.3126\n",
      "Epoch [700/1000], Loss: 17.8967\n",
      "Epoch [710/1000], Loss: 17.5219\n",
      "Epoch [720/1000], Loss: 17.1844\n",
      "Epoch [730/1000], Loss: 16.8811\n",
      "Epoch [740/1000], Loss: 16.6087\n",
      "Epoch [750/1000], Loss: 16.3644\n",
      "Epoch [760/1000], Loss: 16.1457\n",
      "Epoch [770/1000], Loss: 15.9500\n",
      "Epoch [780/1000], Loss: 15.7751\n",
      "Epoch [790/1000], Loss: 15.6192\n",
      "Epoch [800/1000], Loss: 15.4802\n",
      "Epoch [810/1000], Loss: 15.3564\n",
      "Epoch [820/1000], Loss: 15.2465\n",
      "Epoch [830/1000], Loss: 15.1489\n",
      "Epoch [840/1000], Loss: 15.0623\n",
      "Epoch [850/1000], Loss: 14.9857\n",
      "Epoch [860/1000], Loss: 14.9179\n",
      "Epoch [870/1000], Loss: 14.8581\n",
      "Epoch [880/1000], Loss: 14.8053\n",
      "Epoch [890/1000], Loss: 14.7588\n",
      "Epoch [900/1000], Loss: 14.7179\n",
      "Epoch [910/1000], Loss: 14.6820\n",
      "Epoch [920/1000], Loss: 14.6504\n",
      "Epoch [930/1000], Loss: 14.6228\n",
      "Epoch [940/1000], Loss: 14.5986\n",
      "Epoch [950/1000], Loss: 14.5775\n",
      "Epoch [960/1000], Loss: 14.5590\n",
      "Epoch [970/1000], Loss: 14.5429\n",
      "Epoch [980/1000], Loss: 14.5289\n",
      "Epoch [990/1000], Loss: 14.5168\n",
      "Epoch [1000/1000], Loss: 14.5062\n",
      "Predicted days_remaining for parent_id 348: [31.335132598876953, 31.517976760864258, 31.51971435546875, 31.519845962524414, 31.519847869873047, 31.522563934326172, 31.520858764648438, 31.52081298828125]\n",
      "Training for parent_id 360...\n",
      "Epoch [10/1000], Loss: 392.1599\n",
      "Epoch [20/1000], Loss: 331.9096\n",
      "Epoch [30/1000], Loss: 293.8123\n",
      "Epoch [40/1000], Loss: 266.6931\n",
      "Epoch [50/1000], Loss: 243.7714\n",
      "Epoch [60/1000], Loss: 223.0805\n",
      "Epoch [70/1000], Loss: 204.1821\n",
      "Epoch [80/1000], Loss: 186.8628\n",
      "Epoch [90/1000], Loss: 170.9597\n",
      "Epoch [100/1000], Loss: 156.3423\n",
      "Epoch [110/1000], Loss: 142.9035\n",
      "Epoch [120/1000], Loss: 130.5528\n",
      "Epoch [130/1000], Loss: 119.2112\n",
      "Epoch [140/1000], Loss: 108.8076\n",
      "Epoch [150/1000], Loss: 99.2775\n",
      "Epoch [160/1000], Loss: 90.5610\n",
      "Epoch [170/1000], Loss: 82.6023\n",
      "Epoch [180/1000], Loss: 75.3490\n",
      "Epoch [190/1000], Loss: 68.7516\n",
      "Epoch [200/1000], Loss: 62.7632\n",
      "Epoch [210/1000], Loss: 57.3393\n",
      "Epoch [220/1000], Loss: 52.4378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000], Loss: 48.0185\n",
      "Epoch [240/1000], Loss: 44.0436\n",
      "Epoch [250/1000], Loss: 40.4770\n",
      "Epoch [260/1000], Loss: 37.2849\n",
      "Epoch [270/1000], Loss: 34.4352\n",
      "Epoch [280/1000], Loss: 31.8977\n",
      "Epoch [290/1000], Loss: 29.6441\n",
      "Epoch [300/1000], Loss: 27.6482\n",
      "Epoch [310/1000], Loss: 25.8850\n",
      "Epoch [320/1000], Loss: 24.3319\n",
      "Epoch [330/1000], Loss: 22.9675\n",
      "Epoch [340/1000], Loss: 21.7722\n",
      "Epoch [350/1000], Loss: 20.7280\n",
      "Epoch [360/1000], Loss: 19.8183\n",
      "Epoch [370/1000], Loss: 19.0282\n",
      "Epoch [380/1000], Loss: 18.3437\n",
      "Epoch [390/1000], Loss: 17.7526\n",
      "Epoch [400/1000], Loss: 17.2435\n",
      "Epoch [410/1000], Loss: 16.8063\n",
      "Epoch [420/1000], Loss: 16.4319\n",
      "Epoch [430/1000], Loss: 16.1123\n",
      "Epoch [440/1000], Loss: 15.8402\n",
      "Epoch [450/1000], Loss: 15.6093\n",
      "Epoch [460/1000], Loss: 15.4138\n",
      "Epoch [470/1000], Loss: 15.2488\n",
      "Epoch [480/1000], Loss: 15.1100\n",
      "Epoch [490/1000], Loss: 14.9935\n",
      "Epoch [500/1000], Loss: 14.8961\n",
      "Epoch [510/1000], Loss: 14.8148\n",
      "Epoch [520/1000], Loss: 14.7472\n",
      "Epoch [530/1000], Loss: 14.6911\n",
      "Epoch [540/1000], Loss: 14.6447\n",
      "Epoch [550/1000], Loss: 14.6065\n",
      "Epoch [560/1000], Loss: 14.5751\n",
      "Epoch [570/1000], Loss: 14.5493\n",
      "Epoch [580/1000], Loss: 14.5283\n",
      "Epoch [590/1000], Loss: 14.5111\n",
      "Epoch [600/1000], Loss: 14.4971\n",
      "Epoch [610/1000], Loss: 14.4858\n",
      "Epoch [620/1000], Loss: 14.4767\n",
      "Epoch [630/1000], Loss: 14.4694\n",
      "Epoch [640/1000], Loss: 14.4635\n",
      "Epoch [650/1000], Loss: 14.4587\n",
      "Epoch [660/1000], Loss: 14.4549\n",
      "Epoch [670/1000], Loss: 14.4519\n",
      "Epoch [680/1000], Loss: 14.4495\n",
      "Epoch [690/1000], Loss: 14.4476\n",
      "Epoch [700/1000], Loss: 14.4461\n",
      "Epoch [710/1000], Loss: 14.4450\n",
      "Epoch [720/1000], Loss: 14.4440\n",
      "Epoch [730/1000], Loss: 14.4433\n",
      "Epoch [740/1000], Loss: 14.4427\n",
      "Epoch [750/1000], Loss: 14.4423\n",
      "Epoch [760/1000], Loss: 14.4420\n",
      "Epoch [770/1000], Loss: 14.4417\n",
      "Epoch [780/1000], Loss: 14.4415\n",
      "Epoch [790/1000], Loss: 14.4413\n",
      "Epoch [800/1000], Loss: 14.4412\n",
      "Epoch [810/1000], Loss: 14.4411\n",
      "Epoch [820/1000], Loss: 14.4410\n",
      "Epoch [830/1000], Loss: 14.4409\n",
      "Epoch [840/1000], Loss: 14.4409\n",
      "Epoch [850/1000], Loss: 14.4409\n",
      "Epoch [860/1000], Loss: 14.4408\n",
      "Epoch [870/1000], Loss: 14.4408\n",
      "Epoch [880/1000], Loss: 14.4408\n",
      "Epoch [890/1000], Loss: 14.4408\n",
      "Epoch [900/1000], Loss: 14.4407\n",
      "Epoch [910/1000], Loss: 14.4407\n",
      "Epoch [920/1000], Loss: 14.4407\n",
      "Epoch [930/1000], Loss: 14.4407\n",
      "Epoch [940/1000], Loss: 14.4407\n",
      "Epoch [950/1000], Loss: 14.4407\n",
      "Epoch [960/1000], Loss: 14.4407\n",
      "Epoch [970/1000], Loss: 14.4407\n",
      "Epoch [980/1000], Loss: 14.4406\n",
      "Epoch [990/1000], Loss: 14.4406\n",
      "Epoch [1000/1000], Loss: 14.4406\n",
      "Predicted days_remaining for parent_id 360: [21.60152244567871, 21.76898765563965, 21.770915985107422, 21.76935386657715, 21.7711181640625, 21.771303176879883, 21.7718505859375, 21.76934814453125]\n",
      "Training for parent_id 362...\n",
      "Epoch [10/1000], Loss: 1078.7172\n",
      "Epoch [20/1000], Loss: 968.9285\n",
      "Epoch [30/1000], Loss: 887.1550\n",
      "Epoch [40/1000], Loss: 830.0615\n",
      "Epoch [50/1000], Loss: 781.9799\n",
      "Epoch [60/1000], Loss: 738.0941\n",
      "Epoch [70/1000], Loss: 697.0712\n",
      "Epoch [80/1000], Loss: 658.5606\n",
      "Epoch [90/1000], Loss: 622.3116\n",
      "Epoch [100/1000], Loss: 588.1454\n",
      "Epoch [110/1000], Loss: 555.9141\n",
      "Epoch [120/1000], Loss: 525.4637\n",
      "Epoch [130/1000], Loss: 496.6508\n",
      "Epoch [140/1000], Loss: 469.3508\n",
      "Epoch [150/1000], Loss: 443.4586\n",
      "Epoch [160/1000], Loss: 418.8846\n",
      "Epoch [170/1000], Loss: 395.5515\n",
      "Epoch [180/1000], Loss: 373.3914\n",
      "Epoch [190/1000], Loss: 352.3438\n",
      "Epoch [200/1000], Loss: 332.3542\n",
      "Epoch [210/1000], Loss: 313.3724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 295.3524\n",
      "Epoch [230/1000], Loss: 278.2511\n",
      "Epoch [240/1000], Loss: 262.0282\n",
      "Epoch [250/1000], Loss: 246.6457\n",
      "Epoch [260/1000], Loss: 232.0673\n",
      "Epoch [270/1000], Loss: 218.2586\n",
      "Epoch [280/1000], Loss: 205.1868\n",
      "Epoch [290/1000], Loss: 192.8201\n",
      "Epoch [300/1000], Loss: 181.1283\n",
      "Epoch [310/1000], Loss: 170.0821\n",
      "Epoch [320/1000], Loss: 159.6533\n",
      "Epoch [330/1000], Loss: 149.8148\n",
      "Epoch [340/1000], Loss: 140.5400\n",
      "Epoch [350/1000], Loss: 131.8037\n",
      "Epoch [360/1000], Loss: 123.5811\n",
      "Epoch [370/1000], Loss: 115.8484\n",
      "Epoch [380/1000], Loss: 108.5825\n",
      "Epoch [390/1000], Loss: 101.7612\n",
      "Epoch [400/1000], Loss: 95.3629\n",
      "Epoch [410/1000], Loss: 89.3665\n",
      "Epoch [420/1000], Loss: 83.7521\n",
      "Epoch [430/1000], Loss: 78.5001\n",
      "Epoch [440/1000], Loss: 73.5917\n",
      "Epoch [450/1000], Loss: 69.0088\n",
      "Epoch [460/1000], Loss: 64.7338\n",
      "Epoch [470/1000], Loss: 60.7501\n",
      "Epoch [480/1000], Loss: 57.0413\n",
      "Epoch [490/1000], Loss: 53.5920\n",
      "Epoch [500/1000], Loss: 50.3872\n",
      "Epoch [510/1000], Loss: 47.4125\n",
      "Epoch [520/1000], Loss: 44.6544\n",
      "Epoch [530/1000], Loss: 42.0997\n",
      "Epoch [540/1000], Loss: 39.7358\n",
      "Epoch [550/1000], Loss: 37.5508\n",
      "Epoch [560/1000], Loss: 35.5333\n",
      "Epoch [570/1000], Loss: 33.6725\n",
      "Epoch [580/1000], Loss: 31.9579\n",
      "Epoch [590/1000], Loss: 30.3799\n",
      "Epoch [600/1000], Loss: 28.9291\n",
      "Epoch [610/1000], Loss: 27.5968\n",
      "Epoch [620/1000], Loss: 26.3746\n",
      "Epoch [630/1000], Loss: 25.2546\n",
      "Epoch [640/1000], Loss: 24.2294\n",
      "Epoch [650/1000], Loss: 23.2922\n",
      "Epoch [660/1000], Loss: 22.4361\n",
      "Epoch [670/1000], Loss: 21.6553\n",
      "Epoch [680/1000], Loss: 20.9437\n",
      "Epoch [690/1000], Loss: 20.2961\n",
      "Epoch [700/1000], Loss: 19.7073\n",
      "Epoch [710/1000], Loss: 19.1726\n",
      "Epoch [720/1000], Loss: 18.6876\n",
      "Epoch [730/1000], Loss: 18.2482\n",
      "Epoch [740/1000], Loss: 17.8506\n",
      "Epoch [750/1000], Loss: 17.4911\n",
      "Epoch [760/1000], Loss: 17.1666\n",
      "Epoch [770/1000], Loss: 16.8739\n",
      "Epoch [780/1000], Loss: 16.6103\n",
      "Epoch [790/1000], Loss: 16.3731\n",
      "Epoch [800/1000], Loss: 16.1600\n",
      "Epoch [810/1000], Loss: 15.9686\n",
      "Epoch [820/1000], Loss: 15.7971\n",
      "Epoch [830/1000], Loss: 15.6435\n",
      "Epoch [840/1000], Loss: 15.5061\n",
      "Epoch [850/1000], Loss: 15.3834\n",
      "Epoch [860/1000], Loss: 15.2739\n",
      "Epoch [870/1000], Loss: 15.1763\n",
      "Epoch [880/1000], Loss: 15.0894\n",
      "Epoch [890/1000], Loss: 15.0122\n",
      "Epoch [900/1000], Loss: 14.9436\n",
      "Epoch [910/1000], Loss: 14.8828\n",
      "Epoch [920/1000], Loss: 14.8289\n",
      "Epoch [930/1000], Loss: 14.7812\n",
      "Epoch [940/1000], Loss: 14.7391\n",
      "Epoch [950/1000], Loss: 14.7019\n",
      "Epoch [960/1000], Loss: 14.6691\n",
      "Epoch [970/1000], Loss: 14.6402\n",
      "Epoch [980/1000], Loss: 14.6148\n",
      "Epoch [990/1000], Loss: 14.5925\n",
      "Epoch [1000/1000], Loss: 14.5730\n",
      "Predicted days_remaining for parent_id 362: [33.194671630859375, 33.422447204589844, 33.42144012451172, 33.4203987121582, 33.42106246948242, 33.42219924926758, 33.42001724243164, 33.41495895385742]\n",
      "Training for parent_id 366...\n",
      "Epoch [10/1000], Loss: 872.3558\n",
      "Epoch [20/1000], Loss: 777.1547\n",
      "Epoch [30/1000], Loss: 710.8950\n",
      "Epoch [40/1000], Loss: 663.7371\n",
      "Epoch [50/1000], Loss: 623.0426\n",
      "Epoch [60/1000], Loss: 585.3701\n",
      "Epoch [70/1000], Loss: 550.2831\n",
      "Epoch [80/1000], Loss: 517.5153\n",
      "Epoch [90/1000], Loss: 486.8185\n",
      "Epoch [100/1000], Loss: 457.9831\n",
      "Epoch [110/1000], Loss: 430.8364\n",
      "Epoch [120/1000], Loss: 405.2376\n",
      "Epoch [130/1000], Loss: 381.0700\n",
      "Epoch [140/1000], Loss: 358.2356\n",
      "Epoch [150/1000], Loss: 336.6506\n",
      "Epoch [160/1000], Loss: 316.2419\n",
      "Epoch [170/1000], Loss: 296.9450\n",
      "Epoch [180/1000], Loss: 278.7014\n",
      "Epoch [190/1000], Loss: 261.4584\n",
      "Epoch [200/1000], Loss: 245.1670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/1000], Loss: 229.7818\n",
      "Epoch [220/1000], Loss: 215.2601\n",
      "Epoch [230/1000], Loss: 201.5619\n",
      "Epoch [240/1000], Loss: 188.6490\n",
      "Epoch [250/1000], Loss: 176.4851\n",
      "Epoch [260/1000], Loss: 165.0353\n",
      "Epoch [270/1000], Loss: 154.2665\n",
      "Epoch [280/1000], Loss: 144.1466\n",
      "Epoch [290/1000], Loss: 134.6449\n",
      "Epoch [300/1000], Loss: 125.7315\n",
      "Epoch [310/1000], Loss: 117.3779\n",
      "Epoch [320/1000], Loss: 109.5564\n",
      "Epoch [330/1000], Loss: 102.2404\n",
      "Epoch [340/1000], Loss: 95.4040\n",
      "Epoch [350/1000], Loss: 89.0224\n",
      "Epoch [360/1000], Loss: 83.0716\n",
      "Epoch [370/1000], Loss: 77.5285\n",
      "Epoch [380/1000], Loss: 72.3707\n",
      "Epoch [390/1000], Loss: 67.5768\n",
      "Epoch [400/1000], Loss: 63.1262\n",
      "Epoch [410/1000], Loss: 58.9989\n",
      "Epoch [420/1000], Loss: 55.1759\n",
      "Epoch [430/1000], Loss: 51.6389\n",
      "Epoch [440/1000], Loss: 48.3704\n",
      "Epoch [450/1000], Loss: 45.3536\n",
      "Epoch [460/1000], Loss: 42.5725\n",
      "Epoch [470/1000], Loss: 40.0118\n",
      "Epoch [480/1000], Loss: 37.6570\n",
      "Epoch [490/1000], Loss: 35.4941\n",
      "Epoch [500/1000], Loss: 33.5101\n",
      "Epoch [510/1000], Loss: 31.6925\n",
      "Epoch [520/1000], Loss: 30.0293\n",
      "Epoch [530/1000], Loss: 28.5095\n",
      "Epoch [540/1000], Loss: 27.1224\n",
      "Epoch [550/1000], Loss: 25.8581\n",
      "Epoch [560/1000], Loss: 24.7073\n",
      "Epoch [570/1000], Loss: 23.6611\n",
      "Epoch [580/1000], Loss: 22.7113\n",
      "Epoch [590/1000], Loss: 21.8501\n",
      "Epoch [600/1000], Loss: 21.0703\n",
      "Epoch [610/1000], Loss: 20.3652\n",
      "Epoch [620/1000], Loss: 19.7283\n",
      "Epoch [630/1000], Loss: 19.1540\n",
      "Epoch [640/1000], Loss: 18.6368\n",
      "Epoch [650/1000], Loss: 18.1716\n",
      "Epoch [660/1000], Loss: 17.7537\n",
      "Epoch [670/1000], Loss: 17.3789\n",
      "Epoch [680/1000], Loss: 17.0432\n",
      "Epoch [690/1000], Loss: 16.7429\n",
      "Epoch [700/1000], Loss: 16.4746\n",
      "Epoch [710/1000], Loss: 16.2352\n",
      "Epoch [720/1000], Loss: 16.0220\n",
      "Epoch [730/1000], Loss: 15.8323\n",
      "Epoch [740/1000], Loss: 15.6638\n",
      "Epoch [750/1000], Loss: 15.5143\n",
      "Epoch [760/1000], Loss: 15.3819\n",
      "Epoch [770/1000], Loss: 15.2647\n",
      "Epoch [780/1000], Loss: 15.1612\n",
      "Epoch [790/1000], Loss: 15.0699\n",
      "Epoch [800/1000], Loss: 14.9894\n",
      "Epoch [810/1000], Loss: 14.9186\n",
      "Epoch [820/1000], Loss: 14.8564\n",
      "Epoch [830/1000], Loss: 14.8018\n",
      "Epoch [840/1000], Loss: 14.7540\n",
      "Epoch [850/1000], Loss: 14.7122\n",
      "Epoch [860/1000], Loss: 14.6757\n",
      "Epoch [870/1000], Loss: 14.6438\n",
      "Epoch [880/1000], Loss: 14.6161\n",
      "Epoch [890/1000], Loss: 14.5919\n",
      "Epoch [900/1000], Loss: 14.5709\n",
      "Epoch [910/1000], Loss: 14.5527\n",
      "Epoch [920/1000], Loss: 14.5370\n",
      "Epoch [930/1000], Loss: 14.5233\n",
      "Epoch [940/1000], Loss: 14.5115\n",
      "Epoch [950/1000], Loss: 14.5014\n",
      "Epoch [960/1000], Loss: 14.4926\n",
      "Epoch [970/1000], Loss: 14.4851\n",
      "Epoch [980/1000], Loss: 14.4787\n",
      "Epoch [990/1000], Loss: 14.4731\n",
      "Epoch [1000/1000], Loss: 14.4684\n",
      "Predicted days_remaining for parent_id 366: [30.41202163696289, 30.616943359375, 30.611753463745117, 30.614452362060547, 30.614994049072266, 30.614036560058594, 30.615312576293945, 30.611371994018555]\n",
      "Training for parent_id 371...\n",
      "Epoch [10/1000], Loss: 245.5420\n",
      "Epoch [20/1000], Loss: 192.5564\n",
      "Epoch [30/1000], Loss: 156.6705\n",
      "Epoch [40/1000], Loss: 133.7985\n",
      "Epoch [50/1000], Loss: 116.9765\n",
      "Epoch [60/1000], Loss: 102.9640\n",
      "Epoch [70/1000], Loss: 90.8046\n",
      "Epoch [80/1000], Loss: 80.1237\n",
      "Epoch [90/1000], Loss: 70.7145\n",
      "Epoch [100/1000], Loss: 62.4538\n",
      "Epoch [110/1000], Loss: 55.2354\n",
      "Epoch [120/1000], Loss: 48.9541\n",
      "Epoch [130/1000], Loss: 43.5107\n",
      "Epoch [140/1000], Loss: 38.8135\n",
      "Epoch [150/1000], Loss: 34.7786\n",
      "Epoch [160/1000], Loss: 31.3293\n",
      "Epoch [170/1000], Loss: 28.3954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000], Loss: 25.9130\n",
      "Epoch [190/1000], Loss: 23.8239\n",
      "Epoch [200/1000], Loss: 22.0755\n",
      "Epoch [210/1000], Loss: 20.6205\n",
      "Epoch [220/1000], Loss: 19.4168\n",
      "Epoch [230/1000], Loss: 18.4268\n",
      "Epoch [240/1000], Loss: 17.6174\n",
      "Epoch [250/1000], Loss: 16.9597\n",
      "Epoch [260/1000], Loss: 16.4286\n",
      "Epoch [270/1000], Loss: 16.0022\n",
      "Epoch [280/1000], Loss: 15.6621\n",
      "Epoch [290/1000], Loss: 15.3924\n",
      "Epoch [300/1000], Loss: 15.1799\n",
      "Epoch [310/1000], Loss: 15.0135\n",
      "Epoch [320/1000], Loss: 14.8840\n",
      "Epoch [330/1000], Loss: 14.7838\n",
      "Epoch [340/1000], Loss: 14.7068\n",
      "Epoch [350/1000], Loss: 14.6478\n",
      "Epoch [360/1000], Loss: 14.6031\n",
      "Epoch [370/1000], Loss: 14.5692\n",
      "Epoch [380/1000], Loss: 14.5437\n",
      "Epoch [390/1000], Loss: 14.5247\n",
      "Epoch [400/1000], Loss: 14.5105\n",
      "Epoch [410/1000], Loss: 14.4999\n",
      "Epoch [420/1000], Loss: 14.4921\n",
      "Epoch [430/1000], Loss: 14.4863\n",
      "Epoch [440/1000], Loss: 14.4820\n",
      "Epoch [450/1000], Loss: 14.4788\n",
      "Epoch [460/1000], Loss: 14.4764\n",
      "Epoch [470/1000], Loss: 14.4746\n",
      "Epoch [480/1000], Loss: 14.4732\n",
      "Epoch [490/1000], Loss: 14.4721\n",
      "Epoch [500/1000], Loss: 14.4712\n",
      "Epoch [510/1000], Loss: 14.4705\n",
      "Epoch [520/1000], Loss: 14.4699\n",
      "Epoch [530/1000], Loss: 14.4693\n",
      "Epoch [540/1000], Loss: 14.4688\n",
      "Epoch [550/1000], Loss: 14.4684\n",
      "Epoch [560/1000], Loss: 14.4680\n",
      "Epoch [570/1000], Loss: 14.4676\n",
      "Epoch [580/1000], Loss: 14.4672\n",
      "Epoch [590/1000], Loss: 14.4668\n",
      "Epoch [600/1000], Loss: 14.4664\n",
      "Epoch [610/1000], Loss: 14.4661\n",
      "Epoch [620/1000], Loss: 14.4657\n",
      "Epoch [630/1000], Loss: 14.4654\n",
      "Epoch [640/1000], Loss: 14.4650\n",
      "Epoch [650/1000], Loss: 14.4647\n",
      "Epoch [660/1000], Loss: 14.4643\n",
      "Epoch [670/1000], Loss: 14.4640\n",
      "Epoch [680/1000], Loss: 14.4637\n",
      "Epoch [690/1000], Loss: 14.4634\n",
      "Epoch [700/1000], Loss: 14.4631\n",
      "Epoch [710/1000], Loss: 14.4628\n",
      "Epoch [720/1000], Loss: 14.4625\n",
      "Epoch [730/1000], Loss: 14.4622\n",
      "Epoch [740/1000], Loss: 14.4619\n",
      "Epoch [750/1000], Loss: 14.4616\n",
      "Epoch [760/1000], Loss: 14.4613\n",
      "Epoch [770/1000], Loss: 14.4610\n",
      "Epoch [780/1000], Loss: 14.4607\n",
      "Epoch [790/1000], Loss: 14.4605\n",
      "Epoch [800/1000], Loss: 14.4602\n",
      "Epoch [810/1000], Loss: 14.4599\n",
      "Epoch [820/1000], Loss: 14.4597\n",
      "Epoch [830/1000], Loss: 14.4594\n",
      "Epoch [840/1000], Loss: 14.4591\n",
      "Epoch [850/1000], Loss: 14.4589\n",
      "Epoch [860/1000], Loss: 14.4586\n",
      "Epoch [870/1000], Loss: 14.4584\n",
      "Epoch [880/1000], Loss: 14.4582\n",
      "Epoch [890/1000], Loss: 14.4579\n",
      "Epoch [900/1000], Loss: 14.4577\n",
      "Epoch [910/1000], Loss: 14.4575\n",
      "Epoch [920/1000], Loss: 14.4572\n",
      "Epoch [930/1000], Loss: 14.4570\n",
      "Epoch [940/1000], Loss: 14.4568\n",
      "Epoch [950/1000], Loss: 14.4566\n",
      "Epoch [960/1000], Loss: 14.4564\n",
      "Epoch [970/1000], Loss: 14.4562\n",
      "Epoch [980/1000], Loss: 14.4559\n",
      "Epoch [990/1000], Loss: 14.4557\n",
      "Epoch [1000/1000], Loss: 14.4555\n",
      "Predicted days_remaining for parent_id 371: [16.395048141479492, 16.802799224853516, 16.797367095947266, 16.798851013183594, 16.802621841430664, 16.802440643310547, 16.803268432617188, 16.79898452758789]\n",
      "Training for parent_id 376...\n",
      "Epoch [10/1000], Loss: 681.4644\n",
      "Epoch [20/1000], Loss: 601.3146\n",
      "Epoch [30/1000], Loss: 545.3434\n",
      "Epoch [40/1000], Loss: 504.6003\n",
      "Epoch [50/1000], Loss: 469.8876\n",
      "Epoch [60/1000], Loss: 438.1059\n",
      "Epoch [70/1000], Loss: 408.4972\n",
      "Epoch [80/1000], Loss: 380.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 355.1848\n",
      "Epoch [100/1000], Loss: 331.1645\n",
      "Epoch [110/1000], Loss: 308.7110\n",
      "Epoch [120/1000], Loss: 287.7050\n",
      "Epoch [130/1000], Loss: 268.0405\n",
      "Epoch [140/1000], Loss: 249.6238\n",
      "Epoch [150/1000], Loss: 232.3725\n",
      "Epoch [160/1000], Loss: 216.2136\n",
      "Epoch [170/1000], Loss: 201.0820\n",
      "Epoch [180/1000], Loss: 186.9183\n",
      "Epoch [190/1000], Loss: 173.6682\n",
      "Epoch [200/1000], Loss: 161.2813\n",
      "Epoch [210/1000], Loss: 149.7105\n",
      "Epoch [220/1000], Loss: 138.9117\n",
      "Epoch [230/1000], Loss: 128.8427\n",
      "Epoch [240/1000], Loss: 119.4638\n",
      "Epoch [250/1000], Loss: 110.7373\n",
      "Epoch [260/1000], Loss: 102.6269\n",
      "Epoch [270/1000], Loss: 95.0979\n",
      "Epoch [280/1000], Loss: 88.1172\n",
      "Epoch [290/1000], Loss: 81.6531\n",
      "Epoch [300/1000], Loss: 75.6751\n",
      "Epoch [310/1000], Loss: 70.1539\n",
      "Epoch [320/1000], Loss: 65.0617\n",
      "Epoch [330/1000], Loss: 60.3716\n",
      "Epoch [340/1000], Loss: 56.0580\n",
      "Epoch [350/1000], Loss: 52.0964\n",
      "Epoch [360/1000], Loss: 48.4634\n",
      "Epoch [370/1000], Loss: 45.1367\n",
      "Epoch [380/1000], Loss: 42.0951\n",
      "Epoch [390/1000], Loss: 39.3185\n",
      "Epoch [400/1000], Loss: 36.7876\n",
      "Epoch [410/1000], Loss: 34.4844\n",
      "Epoch [420/1000], Loss: 32.3916\n",
      "Epoch [430/1000], Loss: 30.4931\n",
      "Epoch [440/1000], Loss: 28.7737\n",
      "Epoch [450/1000], Loss: 27.2188\n",
      "Epoch [460/1000], Loss: 25.8152\n",
      "Epoch [470/1000], Loss: 24.5502\n",
      "Epoch [480/1000], Loss: 23.4119\n",
      "Epoch [490/1000], Loss: 22.3894\n",
      "Epoch [500/1000], Loss: 21.4724\n",
      "Epoch [510/1000], Loss: 20.6515\n",
      "Epoch [520/1000], Loss: 19.9178\n",
      "Epoch [530/1000], Loss: 19.2631\n",
      "Epoch [540/1000], Loss: 18.6800\n",
      "Epoch [550/1000], Loss: 18.1615\n",
      "Epoch [560/1000], Loss: 17.7012\n",
      "Epoch [570/1000], Loss: 17.2933\n",
      "Epoch [580/1000], Loss: 16.9325\n",
      "Epoch [590/1000], Loss: 16.6138\n",
      "Epoch [600/1000], Loss: 16.3329\n",
      "Epoch [610/1000], Loss: 16.0857\n",
      "Epoch [620/1000], Loss: 15.8685\n",
      "Epoch [630/1000], Loss: 15.6780\n",
      "Epoch [640/1000], Loss: 15.5113\n",
      "Epoch [650/1000], Loss: 15.3655\n",
      "Epoch [660/1000], Loss: 15.2384\n",
      "Epoch [670/1000], Loss: 15.1276\n",
      "Epoch [680/1000], Loss: 15.0314\n",
      "Epoch [690/1000], Loss: 14.9478\n",
      "Epoch [700/1000], Loss: 14.8754\n",
      "Epoch [710/1000], Loss: 14.8128\n",
      "Epoch [720/1000], Loss: 14.7588\n",
      "Epoch [730/1000], Loss: 14.7122\n",
      "Epoch [740/1000], Loss: 14.6722\n",
      "Epoch [750/1000], Loss: 14.6378\n",
      "Epoch [760/1000], Loss: 14.6083\n",
      "Epoch [770/1000], Loss: 14.5830\n",
      "Epoch [780/1000], Loss: 14.5615\n",
      "Epoch [790/1000], Loss: 14.5431\n",
      "Epoch [800/1000], Loss: 14.5274\n",
      "Epoch [810/1000], Loss: 14.5142\n",
      "Epoch [820/1000], Loss: 14.5029\n",
      "Epoch [830/1000], Loss: 14.4933\n",
      "Epoch [840/1000], Loss: 14.4853\n",
      "Epoch [850/1000], Loss: 14.4784\n",
      "Epoch [860/1000], Loss: 14.4727\n",
      "Epoch [870/1000], Loss: 14.4679\n",
      "Epoch [880/1000], Loss: 14.4638\n",
      "Epoch [890/1000], Loss: 14.4604\n",
      "Epoch [900/1000], Loss: 14.4576\n",
      "Epoch [910/1000], Loss: 14.4552\n",
      "Epoch [920/1000], Loss: 14.4532\n",
      "Epoch [930/1000], Loss: 14.4516\n",
      "Epoch [940/1000], Loss: 14.4502\n",
      "Epoch [950/1000], Loss: 14.4491\n",
      "Epoch [960/1000], Loss: 14.4481\n",
      "Epoch [970/1000], Loss: 14.4474\n",
      "Epoch [980/1000], Loss: 14.4467\n",
      "Epoch [990/1000], Loss: 14.4462\n",
      "Epoch [1000/1000], Loss: 14.4457\n",
      "Predicted days_remaining for parent_id 376: [27.4958438873291, 27.739063262939453, 27.73821449279785, 27.73753547668457, 27.735685348510742, 27.736709594726562, 27.7387752532959, 27.74066734313965]\n",
      "Training for parent_id 377...\n",
      "Epoch [10/1000], Loss: 88.4085\n",
      "Epoch [20/1000], Loss: 61.5880\n",
      "Epoch [30/1000], Loss: 44.7464\n",
      "Epoch [40/1000], Loss: 35.4889\n",
      "Epoch [50/1000], Loss: 29.2876\n",
      "Epoch [60/1000], Loss: 24.7985\n",
      "Epoch [70/1000], Loss: 21.5349\n",
      "Epoch [80/1000], Loss: 19.1995\n",
      "Epoch [90/1000], Loss: 17.5699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 16.4656\n",
      "Epoch [110/1000], Loss: 15.7404\n",
      "Epoch [120/1000], Loss: 15.2791\n",
      "Epoch [130/1000], Loss: 14.9947\n",
      "Epoch [140/1000], Loss: 14.8241\n",
      "Epoch [150/1000], Loss: 14.7240\n",
      "Epoch [160/1000], Loss: 14.6658\n",
      "Epoch [170/1000], Loss: 14.6314\n",
      "Epoch [180/1000], Loss: 14.6102\n",
      "Epoch [190/1000], Loss: 14.5960\n",
      "Epoch [200/1000], Loss: 14.5856\n",
      "Epoch [210/1000], Loss: 14.5772\n",
      "Epoch [220/1000], Loss: 14.5700\n",
      "Epoch [230/1000], Loss: 14.5635\n",
      "Epoch [240/1000], Loss: 14.5575\n",
      "Epoch [250/1000], Loss: 14.5520\n",
      "Epoch [260/1000], Loss: 14.5469\n",
      "Epoch [270/1000], Loss: 14.5421\n",
      "Epoch [280/1000], Loss: 14.5377\n",
      "Epoch [290/1000], Loss: 14.5335\n",
      "Epoch [300/1000], Loss: 14.5296\n",
      "Epoch [310/1000], Loss: 14.5260\n",
      "Epoch [320/1000], Loss: 14.5225\n",
      "Epoch [330/1000], Loss: 14.5193\n",
      "Epoch [340/1000], Loss: 14.5162\n",
      "Epoch [350/1000], Loss: 14.5134\n",
      "Epoch [360/1000], Loss: 14.5106\n",
      "Epoch [370/1000], Loss: 14.5081\n",
      "Epoch [380/1000], Loss: 14.5056\n",
      "Epoch [390/1000], Loss: 14.5033\n",
      "Epoch [400/1000], Loss: 14.5012\n",
      "Epoch [410/1000], Loss: 14.4991\n",
      "Epoch [420/1000], Loss: 14.4971\n",
      "Epoch [430/1000], Loss: 14.4952\n",
      "Epoch [440/1000], Loss: 14.4934\n",
      "Epoch [450/1000], Loss: 14.4917\n",
      "Epoch [460/1000], Loss: 14.4901\n",
      "Epoch [470/1000], Loss: 14.4886\n",
      "Epoch [480/1000], Loss: 14.4871\n",
      "Epoch [490/1000], Loss: 14.4857\n",
      "Epoch [500/1000], Loss: 14.4843\n",
      "Epoch [510/1000], Loss: 14.4830\n",
      "Epoch [520/1000], Loss: 14.4818\n",
      "Epoch [530/1000], Loss: 14.4806\n",
      "Epoch [540/1000], Loss: 14.4795\n",
      "Epoch [550/1000], Loss: 14.4784\n",
      "Epoch [560/1000], Loss: 14.4773\n",
      "Epoch [570/1000], Loss: 14.4763\n",
      "Epoch [580/1000], Loss: 14.4753\n",
      "Epoch [590/1000], Loss: 14.4744\n",
      "Epoch [600/1000], Loss: 14.4735\n",
      "Epoch [610/1000], Loss: 14.4726\n",
      "Epoch [620/1000], Loss: 14.4718\n",
      "Epoch [630/1000], Loss: 14.4710\n",
      "Epoch [640/1000], Loss: 14.4702\n",
      "Epoch [650/1000], Loss: 14.4695\n",
      "Epoch [660/1000], Loss: 14.4688\n",
      "Epoch [670/1000], Loss: 14.4681\n",
      "Epoch [680/1000], Loss: 14.4674\n",
      "Epoch [690/1000], Loss: 14.4667\n",
      "Epoch [700/1000], Loss: 14.4661\n",
      "Epoch [710/1000], Loss: 14.4655\n",
      "Epoch [720/1000], Loss: 14.4649\n",
      "Epoch [730/1000], Loss: 14.4643\n",
      "Epoch [740/1000], Loss: 14.4638\n",
      "Epoch [750/1000], Loss: 14.4632\n",
      "Epoch [760/1000], Loss: 14.4627\n",
      "Epoch [770/1000], Loss: 14.4622\n",
      "Epoch [780/1000], Loss: 14.4617\n",
      "Epoch [790/1000], Loss: 14.4613\n",
      "Epoch [800/1000], Loss: 14.4608\n",
      "Epoch [810/1000], Loss: 14.4603\n",
      "Epoch [820/1000], Loss: 14.4599\n",
      "Epoch [830/1000], Loss: 14.4595\n",
      "Epoch [840/1000], Loss: 14.4591\n",
      "Epoch [850/1000], Loss: 14.4587\n",
      "Epoch [860/1000], Loss: 14.4583\n",
      "Epoch [870/1000], Loss: 14.4579\n",
      "Epoch [880/1000], Loss: 14.4575\n",
      "Epoch [890/1000], Loss: 14.4572\n",
      "Epoch [900/1000], Loss: 14.4568\n",
      "Epoch [910/1000], Loss: 14.4565\n",
      "Epoch [920/1000], Loss: 14.4562\n",
      "Epoch [930/1000], Loss: 14.4558\n",
      "Epoch [940/1000], Loss: 14.4555\n",
      "Epoch [950/1000], Loss: 14.4552\n",
      "Epoch [960/1000], Loss: 14.4549\n",
      "Epoch [970/1000], Loss: 14.4546\n",
      "Epoch [980/1000], Loss: 14.4543\n",
      "Epoch [990/1000], Loss: 14.4541\n",
      "Epoch [1000/1000], Loss: 14.4538\n",
      "Predicted days_remaining for parent_id 377: [9.41201114654541, 9.785059928894043, 9.799863815307617, 9.800015449523926, 9.809934616088867, 9.80145263671875, 9.798516273498535, 9.783731460571289]\n",
      "Training for parent_id 387...\n",
      "Epoch [10/1000], Loss: 349.2003\n",
      "Epoch [20/1000], Loss: 292.6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 253.2170\n",
      "Epoch [40/1000], Loss: 225.2552\n",
      "Epoch [50/1000], Loss: 202.7190\n",
      "Epoch [60/1000], Loss: 182.9212\n",
      "Epoch [70/1000], Loss: 165.1162\n",
      "Epoch [80/1000], Loss: 149.1166\n",
      "Epoch [90/1000], Loss: 134.7363\n",
      "Epoch [100/1000], Loss: 121.7763\n",
      "Epoch [110/1000], Loss: 110.0699\n",
      "Epoch [120/1000], Loss: 99.4886\n",
      "Epoch [130/1000], Loss: 89.9271\n",
      "Epoch [140/1000], Loss: 81.2953\n",
      "Epoch [150/1000], Loss: 73.5137\n",
      "Epoch [160/1000], Loss: 66.5110\n",
      "Epoch [170/1000], Loss: 60.2221\n",
      "Epoch [180/1000], Loss: 54.5872\n",
      "Epoch [190/1000], Loss: 49.5507\n",
      "Epoch [200/1000], Loss: 45.0608\n",
      "Epoch [210/1000], Loss: 41.0693\n",
      "Epoch [220/1000], Loss: 37.5309\n",
      "Epoch [230/1000], Loss: 34.4037\n",
      "Epoch [240/1000], Loss: 31.6482\n",
      "Epoch [250/1000], Loss: 29.2280\n",
      "Epoch [260/1000], Loss: 27.1089\n",
      "Epoch [270/1000], Loss: 25.2597\n",
      "Epoch [280/1000], Loss: 23.6513\n",
      "Epoch [290/1000], Loss: 22.2571\n",
      "Epoch [300/1000], Loss: 21.0526\n",
      "Epoch [310/1000], Loss: 20.0156\n",
      "Epoch [320/1000], Loss: 19.1260\n",
      "Epoch [330/1000], Loss: 18.3654\n",
      "Epoch [340/1000], Loss: 17.7174\n",
      "Epoch [350/1000], Loss: 17.1673\n",
      "Epoch [360/1000], Loss: 16.7020\n",
      "Epoch [370/1000], Loss: 16.3098\n",
      "Epoch [380/1000], Loss: 15.9803\n",
      "Epoch [390/1000], Loss: 15.7047\n",
      "Epoch [400/1000], Loss: 15.4748\n",
      "Epoch [410/1000], Loss: 15.2838\n",
      "Epoch [420/1000], Loss: 15.1256\n",
      "Epoch [430/1000], Loss: 14.9952\n",
      "Epoch [440/1000], Loss: 14.8880\n",
      "Epoch [450/1000], Loss: 14.8001\n",
      "Epoch [460/1000], Loss: 14.7285\n",
      "Epoch [470/1000], Loss: 14.6702\n",
      "Epoch [480/1000], Loss: 14.6230\n",
      "Epoch [490/1000], Loss: 14.5849\n",
      "Epoch [500/1000], Loss: 14.5542\n",
      "Epoch [510/1000], Loss: 14.5296\n",
      "Epoch [520/1000], Loss: 14.5100\n",
      "Epoch [530/1000], Loss: 14.4944\n",
      "Epoch [540/1000], Loss: 14.4820\n",
      "Epoch [550/1000], Loss: 14.4722\n",
      "Epoch [560/1000], Loss: 14.4645\n",
      "Epoch [570/1000], Loss: 14.4585\n",
      "Epoch [580/1000], Loss: 14.4537\n",
      "Epoch [590/1000], Loss: 14.4501\n",
      "Epoch [600/1000], Loss: 14.4472\n",
      "Epoch [610/1000], Loss: 14.4450\n",
      "Epoch [620/1000], Loss: 14.4433\n",
      "Epoch [630/1000], Loss: 14.4420\n",
      "Epoch [640/1000], Loss: 14.4410\n",
      "Epoch [650/1000], Loss: 14.4402\n",
      "Epoch [660/1000], Loss: 14.4397\n",
      "Epoch [670/1000], Loss: 14.4392\n",
      "Epoch [680/1000], Loss: 14.4389\n",
      "Epoch [690/1000], Loss: 14.4387\n",
      "Epoch [700/1000], Loss: 14.4385\n",
      "Epoch [710/1000], Loss: 14.4383\n",
      "Epoch [720/1000], Loss: 14.4382\n",
      "Epoch [730/1000], Loss: 14.4382\n",
      "Epoch [740/1000], Loss: 14.4381\n",
      "Epoch [750/1000], Loss: 14.4381\n",
      "Epoch [760/1000], Loss: 14.4380\n",
      "Epoch [770/1000], Loss: 14.4380\n",
      "Epoch [780/1000], Loss: 14.4380\n",
      "Epoch [790/1000], Loss: 14.4380\n",
      "Epoch [800/1000], Loss: 14.4380\n",
      "Epoch [810/1000], Loss: 14.4380\n",
      "Epoch [820/1000], Loss: 14.4380\n",
      "Epoch [830/1000], Loss: 14.4380\n",
      "Epoch [840/1000], Loss: 14.4380\n",
      "Epoch [850/1000], Loss: 14.4379\n",
      "Epoch [860/1000], Loss: 14.4379\n",
      "Epoch [870/1000], Loss: 14.4379\n",
      "Epoch [880/1000], Loss: 14.4379\n",
      "Epoch [890/1000], Loss: 14.4379\n",
      "Epoch [900/1000], Loss: 14.4379\n",
      "Epoch [910/1000], Loss: 14.4379\n",
      "Epoch [920/1000], Loss: 14.4379\n",
      "Epoch [930/1000], Loss: 14.4379\n",
      "Epoch [940/1000], Loss: 14.4379\n",
      "Epoch [950/1000], Loss: 14.4379\n",
      "Epoch [960/1000], Loss: 14.4379\n",
      "Epoch [970/1000], Loss: 14.4379\n",
      "Epoch [980/1000], Loss: 14.4379\n",
      "Epoch [990/1000], Loss: 14.4379\n",
      "Epoch [1000/1000], Loss: 14.4379\n",
      "Predicted days_remaining for parent_id 387: [19.69965362548828, 19.7654972076416, 19.762588500976562, 19.75927734375, 19.763179779052734, 19.759380340576172, 19.754051208496094, 19.735794067382812]\n",
      "Training for parent_id 392...\n",
      "Epoch [10/1000], Loss: 107.5628\n",
      "Epoch [20/1000], Loss: 78.8704\n",
      "Epoch [30/1000], Loss: 59.3910\n",
      "Epoch [40/1000], Loss: 47.7118\n",
      "Epoch [50/1000], Loss: 39.5505\n",
      "Epoch [60/1000], Loss: 33.3323\n",
      "Epoch [70/1000], Loss: 28.4961\n",
      "Epoch [80/1000], Loss: 24.7427\n",
      "Epoch [90/1000], Loss: 21.8733\n",
      "Epoch [100/1000], Loss: 19.7226\n",
      "Epoch [110/1000], Loss: 18.1424\n",
      "Epoch [120/1000], Loss: 17.0045\n",
      "Epoch [130/1000], Loss: 16.2019\n",
      "Epoch [140/1000], Loss: 15.6475\n",
      "Epoch [150/1000], Loss: 15.2724\n",
      "Epoch [160/1000], Loss: 15.0235\n",
      "Epoch [170/1000], Loss: 14.8614\n",
      "Epoch [180/1000], Loss: 14.7573\n",
      "Epoch [190/1000], Loss: 14.6911\n",
      "Epoch [200/1000], Loss: 14.6490\n",
      "Epoch [210/1000], Loss: 14.6220\n",
      "Epoch [220/1000], Loss: 14.6041\n",
      "Epoch [230/1000], Loss: 14.5917\n",
      "Epoch [240/1000], Loss: 14.5824\n",
      "Epoch [250/1000], Loss: 14.5752\n",
      "Epoch [260/1000], Loss: 14.5690\n",
      "Epoch [270/1000], Loss: 14.5636\n",
      "Epoch [280/1000], Loss: 14.5587\n",
      "Epoch [290/1000], Loss: 14.5541\n",
      "Epoch [300/1000], Loss: 14.5498\n",
      "Epoch [310/1000], Loss: 14.5458\n",
      "Epoch [320/1000], Loss: 14.5420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [330/1000], Loss: 14.5384\n",
      "Epoch [340/1000], Loss: 14.5350\n",
      "Epoch [350/1000], Loss: 14.5317\n",
      "Epoch [360/1000], Loss: 14.5286\n",
      "Epoch [370/1000], Loss: 14.5257\n",
      "Epoch [380/1000], Loss: 14.5229\n",
      "Epoch [390/1000], Loss: 14.5202\n",
      "Epoch [400/1000], Loss: 14.5176\n",
      "Epoch [410/1000], Loss: 14.5152\n",
      "Epoch [420/1000], Loss: 14.5129\n",
      "Epoch [430/1000], Loss: 14.5107\n",
      "Epoch [440/1000], Loss: 14.5085\n",
      "Epoch [450/1000], Loss: 14.5065\n",
      "Epoch [460/1000], Loss: 14.5046\n",
      "Epoch [470/1000], Loss: 14.5027\n",
      "Epoch [480/1000], Loss: 14.5009\n",
      "Epoch [490/1000], Loss: 14.4992\n",
      "Epoch [500/1000], Loss: 14.4976\n",
      "Epoch [510/1000], Loss: 14.4960\n",
      "Epoch [520/1000], Loss: 14.4945\n",
      "Epoch [530/1000], Loss: 14.4930\n",
      "Epoch [540/1000], Loss: 14.4916\n",
      "Epoch [550/1000], Loss: 14.4903\n",
      "Epoch [560/1000], Loss: 14.4890\n",
      "Epoch [570/1000], Loss: 14.4877\n",
      "Epoch [580/1000], Loss: 14.4865\n",
      "Epoch [590/1000], Loss: 14.4854\n",
      "Epoch [600/1000], Loss: 14.4843\n",
      "Epoch [610/1000], Loss: 14.4832\n",
      "Epoch [620/1000], Loss: 14.4821\n",
      "Epoch [630/1000], Loss: 14.4811\n",
      "Epoch [640/1000], Loss: 14.4802\n",
      "Epoch [650/1000], Loss: 14.4792\n",
      "Epoch [660/1000], Loss: 14.4783\n",
      "Epoch [670/1000], Loss: 14.4774\n",
      "Epoch [680/1000], Loss: 14.4766\n",
      "Epoch [690/1000], Loss: 14.4758\n",
      "Epoch [700/1000], Loss: 14.4750\n",
      "Epoch [710/1000], Loss: 14.4742\n",
      "Epoch [720/1000], Loss: 14.4734\n",
      "Epoch [730/1000], Loss: 14.4727\n",
      "Epoch [740/1000], Loss: 14.4720\n",
      "Epoch [750/1000], Loss: 14.4713\n",
      "Epoch [760/1000], Loss: 14.4707\n",
      "Epoch [770/1000], Loss: 14.4700\n",
      "Epoch [780/1000], Loss: 14.4694\n",
      "Epoch [790/1000], Loss: 14.4688\n",
      "Epoch [800/1000], Loss: 14.4682\n",
      "Epoch [810/1000], Loss: 14.4676\n",
      "Epoch [820/1000], Loss: 14.4671\n",
      "Epoch [830/1000], Loss: 14.4665\n",
      "Epoch [840/1000], Loss: 14.4660\n",
      "Epoch [850/1000], Loss: 14.4655\n",
      "Epoch [860/1000], Loss: 14.4650\n",
      "Epoch [870/1000], Loss: 14.4645\n",
      "Epoch [880/1000], Loss: 14.4640\n",
      "Epoch [890/1000], Loss: 14.4636\n",
      "Epoch [900/1000], Loss: 14.4631\n",
      "Epoch [910/1000], Loss: 14.4627\n",
      "Epoch [920/1000], Loss: 14.4623\n",
      "Epoch [930/1000], Loss: 14.4618\n",
      "Epoch [940/1000], Loss: 14.4614\n",
      "Epoch [950/1000], Loss: 14.4610\n",
      "Epoch [960/1000], Loss: 14.4607\n",
      "Epoch [970/1000], Loss: 14.4603\n",
      "Epoch [980/1000], Loss: 14.4599\n",
      "Epoch [990/1000], Loss: 14.4596\n",
      "Epoch [1000/1000], Loss: 14.4592\n",
      "Predicted days_remaining for parent_id 392: [10.359692573547363, 10.811132431030273, 10.801995277404785, 10.800494194030762, 10.810301780700684, 10.807022094726562, 10.80993366241455, 10.78974437713623]\n",
      "Training for parent_id 393...\n",
      "Epoch [10/1000], Loss: 316.7774\n",
      "Epoch [20/1000], Loss: 257.0895\n",
      "Epoch [30/1000], Loss: 222.6834\n",
      "Epoch [40/1000], Loss: 198.5767\n",
      "Epoch [50/1000], Loss: 178.4493\n",
      "Epoch [60/1000], Loss: 160.6671\n",
      "Epoch [70/1000], Loss: 144.7050\n",
      "Epoch [80/1000], Loss: 130.3050\n",
      "Epoch [90/1000], Loss: 117.2971\n",
      "Epoch [100/1000], Loss: 105.5505\n",
      "Epoch [110/1000], Loss: 94.9556\n",
      "Epoch [120/1000], Loss: 85.4155\n",
      "Epoch [130/1000], Loss: 76.8424\n",
      "Epoch [140/1000], Loss: 69.1554\n",
      "Epoch [150/1000], Loss: 62.2799\n",
      "Epoch [160/1000], Loss: 56.1466\n",
      "Epoch [170/1000], Loss: 50.6906\n",
      "Epoch [180/1000], Loss: 45.8515\n",
      "Epoch [190/1000], Loss: 41.5730\n",
      "Epoch [200/1000], Loss: 37.8023\n",
      "Epoch [210/1000], Loss: 34.4903\n",
      "Epoch [220/1000], Loss: 31.5911\n",
      "Epoch [230/1000], Loss: 29.0623\n",
      "Epoch [240/1000], Loss: 26.8646\n",
      "Epoch [250/1000], Loss: 24.9615\n",
      "Epoch [260/1000], Loss: 23.3199\n",
      "Epoch [270/1000], Loss: 21.9091\n",
      "Epoch [280/1000], Loss: 20.7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/1000], Loss: 19.6715\n",
      "Epoch [300/1000], Loss: 18.7967\n",
      "Epoch [310/1000], Loss: 18.0566\n",
      "Epoch [320/1000], Loss: 17.4330\n",
      "Epoch [330/1000], Loss: 16.9095\n",
      "Epoch [340/1000], Loss: 16.4719\n",
      "Epoch [350/1000], Loss: 16.1075\n",
      "Epoch [360/1000], Loss: 15.8053\n",
      "Epoch [370/1000], Loss: 15.5556\n",
      "Epoch [380/1000], Loss: 15.3502\n",
      "Epoch [390/1000], Loss: 15.1819\n",
      "Epoch [400/1000], Loss: 15.0445\n",
      "Epoch [410/1000], Loss: 14.9328\n",
      "Epoch [420/1000], Loss: 14.8423\n",
      "Epoch [430/1000], Loss: 14.7693\n",
      "Epoch [440/1000], Loss: 14.7105\n",
      "Epoch [450/1000], Loss: 14.6635\n",
      "Epoch [460/1000], Loss: 14.6259\n",
      "Epoch [470/1000], Loss: 14.5960\n",
      "Epoch [480/1000], Loss: 14.5723\n",
      "Epoch [490/1000], Loss: 14.5535\n",
      "Epoch [500/1000], Loss: 14.5387\n",
      "Epoch [510/1000], Loss: 14.5270\n",
      "Epoch [520/1000], Loss: 14.5178\n",
      "Epoch [530/1000], Loss: 14.5105\n",
      "Epoch [540/1000], Loss: 14.5048\n",
      "Epoch [550/1000], Loss: 14.5003\n",
      "Epoch [560/1000], Loss: 14.4968\n",
      "Epoch [570/1000], Loss: 14.4939\n",
      "Epoch [580/1000], Loss: 14.4917\n",
      "Epoch [590/1000], Loss: 14.4898\n",
      "Epoch [600/1000], Loss: 14.4883\n",
      "Epoch [610/1000], Loss: 14.4870\n",
      "Epoch [620/1000], Loss: 14.4860\n",
      "Epoch [630/1000], Loss: 14.4850\n",
      "Epoch [640/1000], Loss: 14.4842\n",
      "Epoch [650/1000], Loss: 14.4835\n",
      "Epoch [660/1000], Loss: 14.4828\n",
      "Epoch [670/1000], Loss: 14.4822\n",
      "Epoch [680/1000], Loss: 14.4817\n",
      "Epoch [690/1000], Loss: 14.4811\n",
      "Epoch [700/1000], Loss: 14.4806\n",
      "Epoch [710/1000], Loss: 14.4801\n",
      "Epoch [720/1000], Loss: 14.4796\n",
      "Epoch [730/1000], Loss: 14.4791\n",
      "Epoch [740/1000], Loss: 14.4787\n",
      "Epoch [750/1000], Loss: 14.4782\n",
      "Epoch [760/1000], Loss: 14.4778\n",
      "Epoch [770/1000], Loss: 14.4773\n",
      "Epoch [780/1000], Loss: 14.4769\n",
      "Epoch [790/1000], Loss: 14.4765\n",
      "Epoch [800/1000], Loss: 14.4760\n",
      "Epoch [810/1000], Loss: 14.4756\n",
      "Epoch [820/1000], Loss: 14.4752\n",
      "Epoch [830/1000], Loss: 14.4748\n",
      "Epoch [840/1000], Loss: 14.4744\n",
      "Epoch [850/1000], Loss: 14.4740\n",
      "Epoch [860/1000], Loss: 14.4736\n",
      "Epoch [870/1000], Loss: 14.4732\n",
      "Epoch [880/1000], Loss: 14.4729\n",
      "Epoch [890/1000], Loss: 14.4725\n",
      "Epoch [900/1000], Loss: 14.4721\n",
      "Epoch [910/1000], Loss: 14.4717\n",
      "Epoch [920/1000], Loss: 14.4714\n",
      "Epoch [930/1000], Loss: 14.4710\n",
      "Epoch [940/1000], Loss: 14.4707\n",
      "Epoch [950/1000], Loss: 14.4703\n",
      "Epoch [960/1000], Loss: 14.4700\n",
      "Epoch [970/1000], Loss: 14.4696\n",
      "Epoch [980/1000], Loss: 14.4693\n",
      "Epoch [990/1000], Loss: 14.4690\n",
      "Epoch [1000/1000], Loss: 14.4687\n",
      "Predicted days_remaining for parent_id 393: [19.283742904663086, 19.81713104248047, 19.816387176513672, 19.81854248046875, 19.816726684570312, 19.81925392150879, 19.817611694335938, 19.814289093017578]\n",
      "Training for parent_id 395...\n",
      "Epoch [10/1000], Loss: 79.0874\n",
      "Epoch [20/1000], Loss: 55.1807\n",
      "Epoch [30/1000], Loss: 41.0292\n",
      "Epoch [40/1000], Loss: 32.7408\n",
      "Epoch [50/1000], Loss: 27.1575\n",
      "Epoch [60/1000], Loss: 23.1665\n",
      "Epoch [70/1000], Loss: 20.2977\n",
      "Epoch [80/1000], Loss: 18.2798\n",
      "Epoch [90/1000], Loss: 16.9057\n",
      "Epoch [100/1000], Loss: 16.0029\n",
      "Epoch [110/1000], Loss: 15.4310\n",
      "Epoch [120/1000], Loss: 15.0815\n",
      "Epoch [130/1000], Loss: 14.8749\n",
      "Epoch [140/1000], Loss: 14.7560\n",
      "Epoch [150/1000], Loss: 14.6885\n",
      "Epoch [160/1000], Loss: 14.6498\n",
      "Epoch [170/1000], Loss: 14.6266\n",
      "Epoch [180/1000], Loss: 14.6114\n",
      "Epoch [190/1000], Loss: 14.6002\n",
      "Epoch [200/1000], Loss: 14.5911\n",
      "Epoch [210/1000], Loss: 14.5833\n",
      "Epoch [220/1000], Loss: 14.5762\n",
      "Epoch [230/1000], Loss: 14.5697\n",
      "Epoch [240/1000], Loss: 14.5637\n",
      "Epoch [250/1000], Loss: 14.5581\n",
      "Epoch [260/1000], Loss: 14.5528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 14.5480\n",
      "Epoch [280/1000], Loss: 14.5434\n",
      "Epoch [290/1000], Loss: 14.5391\n",
      "Epoch [300/1000], Loss: 14.5351\n",
      "Epoch [310/1000], Loss: 14.5313\n",
      "Epoch [320/1000], Loss: 14.5277\n",
      "Epoch [330/1000], Loss: 14.5244\n",
      "Epoch [340/1000], Loss: 14.5212\n",
      "Epoch [350/1000], Loss: 14.5182\n",
      "Epoch [360/1000], Loss: 14.5154\n",
      "Epoch [370/1000], Loss: 14.5127\n",
      "Epoch [380/1000], Loss: 14.5102\n",
      "Epoch [390/1000], Loss: 14.5078\n",
      "Epoch [400/1000], Loss: 14.5055\n",
      "Epoch [410/1000], Loss: 14.5033\n",
      "Epoch [420/1000], Loss: 14.5013\n",
      "Epoch [430/1000], Loss: 14.4993\n",
      "Epoch [440/1000], Loss: 14.4974\n",
      "Epoch [450/1000], Loss: 14.4957\n",
      "Epoch [460/1000], Loss: 14.4940\n",
      "Epoch [470/1000], Loss: 14.4923\n",
      "Epoch [480/1000], Loss: 14.4908\n",
      "Epoch [490/1000], Loss: 14.4893\n",
      "Epoch [500/1000], Loss: 14.4879\n",
      "Epoch [510/1000], Loss: 14.4865\n",
      "Epoch [520/1000], Loss: 14.4852\n",
      "Epoch [530/1000], Loss: 14.4839\n",
      "Epoch [540/1000], Loss: 14.4827\n",
      "Epoch [550/1000], Loss: 14.4816\n",
      "Epoch [560/1000], Loss: 14.4804\n",
      "Epoch [570/1000], Loss: 14.4794\n",
      "Epoch [580/1000], Loss: 14.4783\n",
      "Epoch [590/1000], Loss: 14.4774\n",
      "Epoch [600/1000], Loss: 14.4764\n",
      "Epoch [610/1000], Loss: 14.4755\n",
      "Epoch [620/1000], Loss: 14.4746\n",
      "Epoch [630/1000], Loss: 14.4737\n",
      "Epoch [640/1000], Loss: 14.4729\n",
      "Epoch [650/1000], Loss: 14.4721\n",
      "Epoch [660/1000], Loss: 14.4714\n",
      "Epoch [670/1000], Loss: 14.4706\n",
      "Epoch [680/1000], Loss: 14.4699\n",
      "Epoch [690/1000], Loss: 14.4692\n",
      "Epoch [700/1000], Loss: 14.4685\n",
      "Epoch [710/1000], Loss: 14.4679\n",
      "Epoch [720/1000], Loss: 14.4672\n",
      "Epoch [730/1000], Loss: 14.4666\n",
      "Epoch [740/1000], Loss: 14.4660\n",
      "Epoch [750/1000], Loss: 14.4655\n",
      "Epoch [760/1000], Loss: 14.4649\n",
      "Epoch [770/1000], Loss: 14.4644\n",
      "Epoch [780/1000], Loss: 14.4639\n",
      "Epoch [790/1000], Loss: 14.4633\n",
      "Epoch [800/1000], Loss: 14.4629\n",
      "Epoch [810/1000], Loss: 14.4624\n",
      "Epoch [820/1000], Loss: 14.4619\n",
      "Epoch [830/1000], Loss: 14.4615\n",
      "Epoch [840/1000], Loss: 14.4610\n",
      "Epoch [850/1000], Loss: 14.4606\n",
      "Epoch [860/1000], Loss: 14.4602\n",
      "Epoch [870/1000], Loss: 14.4598\n",
      "Epoch [880/1000], Loss: 14.4594\n",
      "Epoch [890/1000], Loss: 14.4590\n",
      "Epoch [900/1000], Loss: 14.4586\n",
      "Epoch [910/1000], Loss: 14.4583\n",
      "Epoch [920/1000], Loss: 14.4579\n",
      "Epoch [930/1000], Loss: 14.4576\n",
      "Epoch [940/1000], Loss: 14.4572\n",
      "Epoch [950/1000], Loss: 14.4569\n",
      "Epoch [960/1000], Loss: 14.4566\n",
      "Epoch [970/1000], Loss: 14.4563\n",
      "Epoch [980/1000], Loss: 14.4559\n",
      "Epoch [990/1000], Loss: 14.4557\n",
      "Epoch [1000/1000], Loss: 14.4554\n",
      "Predicted days_remaining for parent_id 395: [9.395780563354492, 9.80733871459961, 9.809152603149414, 9.802452087402344, 9.789933204650879, 9.796916961669922, 9.795696258544922, 9.79217529296875]\n",
      "Training for parent_id 404...\n",
      "Epoch [10/1000], Loss: 111.9253\n",
      "Epoch [20/1000], Loss: 82.7450\n",
      "Epoch [30/1000], Loss: 66.9446\n",
      "Epoch [40/1000], Loss: 56.0782\n",
      "Epoch [50/1000], Loss: 47.8532\n",
      "Epoch [60/1000], Loss: 41.1233\n",
      "Epoch [70/1000], Loss: 35.5575\n",
      "Epoch [80/1000], Loss: 30.9871\n",
      "Epoch [90/1000], Loss: 27.2703\n",
      "Epoch [100/1000], Loss: 24.2809\n",
      "Epoch [110/1000], Loss: 21.9036\n",
      "Epoch [120/1000], Loss: 20.0352\n",
      "Epoch [130/1000], Loss: 18.5844\n",
      "Epoch [140/1000], Loss: 17.4722\n",
      "Epoch [150/1000], Loss: 16.6308\n",
      "Epoch [160/1000], Loss: 16.0028\n",
      "Epoch [170/1000], Loss: 15.5406\n",
      "Epoch [180/1000], Loss: 15.2053\n",
      "Epoch [190/1000], Loss: 14.9655\n",
      "Epoch [200/1000], Loss: 14.7964\n",
      "Epoch [210/1000], Loss: 14.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 14.5987\n",
      "Epoch [230/1000], Loss: 14.5445\n",
      "Epoch [240/1000], Loss: 14.5085\n",
      "Epoch [250/1000], Loss: 14.4850\n",
      "Epoch [260/1000], Loss: 14.4698\n",
      "Epoch [270/1000], Loss: 14.4601\n",
      "Epoch [280/1000], Loss: 14.4540\n",
      "Epoch [290/1000], Loss: 14.4503\n",
      "Epoch [300/1000], Loss: 14.4479\n",
      "Epoch [310/1000], Loss: 14.4465\n",
      "Epoch [320/1000], Loss: 14.4457\n",
      "Epoch [330/1000], Loss: 14.4451\n",
      "Epoch [340/1000], Loss: 14.4448\n",
      "Epoch [350/1000], Loss: 14.4446\n",
      "Epoch [360/1000], Loss: 14.4444\n",
      "Epoch [370/1000], Loss: 14.4443\n",
      "Epoch [380/1000], Loss: 14.4442\n",
      "Epoch [390/1000], Loss: 14.4441\n",
      "Epoch [400/1000], Loss: 14.4440\n",
      "Epoch [410/1000], Loss: 14.4439\n",
      "Epoch [420/1000], Loss: 14.4439\n",
      "Epoch [430/1000], Loss: 14.4438\n",
      "Epoch [440/1000], Loss: 14.4437\n",
      "Epoch [450/1000], Loss: 14.4436\n",
      "Epoch [460/1000], Loss: 14.4436\n",
      "Epoch [470/1000], Loss: 14.4435\n",
      "Epoch [480/1000], Loss: 14.4434\n",
      "Epoch [490/1000], Loss: 14.4433\n",
      "Epoch [500/1000], Loss: 14.4433\n",
      "Epoch [510/1000], Loss: 14.4432\n",
      "Epoch [520/1000], Loss: 14.4431\n",
      "Epoch [530/1000], Loss: 14.4431\n",
      "Epoch [540/1000], Loss: 14.4430\n",
      "Epoch [550/1000], Loss: 14.4429\n",
      "Epoch [560/1000], Loss: 14.4429\n",
      "Epoch [570/1000], Loss: 14.4428\n",
      "Epoch [580/1000], Loss: 14.4427\n",
      "Epoch [590/1000], Loss: 14.4427\n",
      "Epoch [600/1000], Loss: 14.4426\n",
      "Epoch [610/1000], Loss: 14.4425\n",
      "Epoch [620/1000], Loss: 14.4425\n",
      "Epoch [630/1000], Loss: 14.4424\n",
      "Epoch [640/1000], Loss: 14.4424\n",
      "Epoch [650/1000], Loss: 14.4423\n",
      "Epoch [660/1000], Loss: 14.4422\n",
      "Epoch [670/1000], Loss: 14.4422\n",
      "Epoch [680/1000], Loss: 14.4421\n",
      "Epoch [690/1000], Loss: 14.4421\n",
      "Epoch [700/1000], Loss: 14.4420\n",
      "Epoch [710/1000], Loss: 14.4420\n",
      "Epoch [720/1000], Loss: 14.4419\n",
      "Epoch [730/1000], Loss: 14.4419\n",
      "Epoch [740/1000], Loss: 14.4418\n",
      "Epoch [750/1000], Loss: 14.4418\n",
      "Epoch [760/1000], Loss: 14.4417\n",
      "Epoch [770/1000], Loss: 14.4417\n",
      "Epoch [780/1000], Loss: 14.4416\n",
      "Epoch [790/1000], Loss: 14.4416\n",
      "Epoch [800/1000], Loss: 14.4415\n",
      "Epoch [810/1000], Loss: 14.4415\n",
      "Epoch [820/1000], Loss: 14.4414\n",
      "Epoch [830/1000], Loss: 14.4414\n",
      "Epoch [840/1000], Loss: 14.4413\n",
      "Epoch [850/1000], Loss: 14.4413\n",
      "Epoch [860/1000], Loss: 14.4412\n",
      "Epoch [870/1000], Loss: 14.4412\n",
      "Epoch [880/1000], Loss: 14.4412\n",
      "Epoch [890/1000], Loss: 14.4411\n",
      "Epoch [900/1000], Loss: 14.4411\n",
      "Epoch [910/1000], Loss: 14.4410\n",
      "Epoch [920/1000], Loss: 14.4410\n",
      "Epoch [930/1000], Loss: 14.4410\n",
      "Epoch [940/1000], Loss: 14.4409\n",
      "Epoch [950/1000], Loss: 14.4409\n",
      "Epoch [960/1000], Loss: 14.4408\n",
      "Epoch [970/1000], Loss: 14.4408\n",
      "Epoch [980/1000], Loss: 14.4408\n",
      "Epoch [990/1000], Loss: 14.4407\n",
      "Epoch [1000/1000], Loss: 14.4407\n",
      "Predicted days_remaining for parent_id 404: [11.602673530578613, 11.782363891601562, 11.78221607208252, 11.765830039978027, 11.775897026062012, 11.768458366394043, 11.771259307861328, 11.751618385314941]\n",
      "Training for parent_id 406...\n",
      "Epoch [10/1000], Loss: 447.0535\n",
      "Epoch [20/1000], Loss: 379.9142\n",
      "Epoch [30/1000], Loss: 334.6008\n",
      "Epoch [40/1000], Loss: 302.6653\n",
      "Epoch [50/1000], Loss: 276.5397\n",
      "Epoch [60/1000], Loss: 253.3064\n",
      "Epoch [70/1000], Loss: 232.1827\n",
      "Epoch [80/1000], Loss: 212.7623\n",
      "Epoch [90/1000], Loss: 194.7677\n",
      "Epoch [100/1000], Loss: 178.1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 162.8708\n",
      "Epoch [120/1000], Loss: 148.8378\n",
      "Epoch [130/1000], Loss: 135.9588\n",
      "Epoch [140/1000], Loss: 124.1489\n",
      "Epoch [150/1000], Loss: 113.3296\n",
      "Epoch [160/1000], Loss: 103.4287\n",
      "Epoch [170/1000], Loss: 94.3793\n",
      "Epoch [180/1000], Loss: 86.1195\n",
      "Epoch [190/1000], Loss: 78.5920\n",
      "Epoch [200/1000], Loss: 71.7432\n",
      "Epoch [210/1000], Loss: 65.5232\n",
      "Epoch [220/1000], Loss: 59.8848\n",
      "Epoch [230/1000], Loss: 54.7839\n",
      "Epoch [240/1000], Loss: 50.1789\n",
      "Epoch [250/1000], Loss: 46.0305\n",
      "Epoch [260/1000], Loss: 42.3018\n",
      "Epoch [270/1000], Loss: 38.9579\n",
      "Epoch [280/1000], Loss: 35.9662\n",
      "Epoch [290/1000], Loss: 33.2959\n",
      "Epoch [300/1000], Loss: 30.9184\n",
      "Epoch [310/1000], Loss: 28.8068\n",
      "Epoch [320/1000], Loss: 26.9360\n",
      "Epoch [330/1000], Loss: 25.2828\n",
      "Epoch [340/1000], Loss: 23.8257\n",
      "Epoch [350/1000], Loss: 22.5447\n",
      "Epoch [360/1000], Loss: 21.4215\n",
      "Epoch [370/1000], Loss: 20.4392\n",
      "Epoch [380/1000], Loss: 19.5825\n",
      "Epoch [390/1000], Loss: 18.8373\n",
      "Epoch [400/1000], Loss: 18.1909\n",
      "Epoch [410/1000], Loss: 17.6316\n",
      "Epoch [420/1000], Loss: 17.1490\n",
      "Epoch [430/1000], Loss: 16.7338\n",
      "Epoch [440/1000], Loss: 16.3774\n",
      "Epoch [450/1000], Loss: 16.0725\n",
      "Epoch [460/1000], Loss: 15.8122\n",
      "Epoch [470/1000], Loss: 15.5907\n",
      "Epoch [480/1000], Loss: 15.4027\n",
      "Epoch [490/1000], Loss: 15.2436\n",
      "Epoch [500/1000], Loss: 15.1092\n",
      "Epoch [510/1000], Loss: 14.9962\n",
      "Epoch [520/1000], Loss: 14.9012\n",
      "Epoch [530/1000], Loss: 14.8217\n",
      "Epoch [540/1000], Loss: 14.7554\n",
      "Epoch [550/1000], Loss: 14.7001\n",
      "Epoch [560/1000], Loss: 14.6542\n",
      "Epoch [570/1000], Loss: 14.6162\n",
      "Epoch [580/1000], Loss: 14.5848\n",
      "Epoch [590/1000], Loss: 14.5590\n",
      "Epoch [600/1000], Loss: 14.5377\n",
      "Epoch [610/1000], Loss: 14.5204\n",
      "Epoch [620/1000], Loss: 14.5061\n",
      "Epoch [630/1000], Loss: 14.4946\n",
      "Epoch [640/1000], Loss: 14.4851\n",
      "Epoch [650/1000], Loss: 14.4775\n",
      "Epoch [660/1000], Loss: 14.4713\n",
      "Epoch [670/1000], Loss: 14.4664\n",
      "Epoch [680/1000], Loss: 14.4624\n",
      "Epoch [690/1000], Loss: 14.4592\n",
      "Epoch [700/1000], Loss: 14.4566\n",
      "Epoch [710/1000], Loss: 14.4545\n",
      "Epoch [720/1000], Loss: 14.4529\n",
      "Epoch [730/1000], Loss: 14.4516\n",
      "Epoch [740/1000], Loss: 14.4505\n",
      "Epoch [750/1000], Loss: 14.4497\n",
      "Epoch [760/1000], Loss: 14.4490\n",
      "Epoch [770/1000], Loss: 14.4485\n",
      "Epoch [780/1000], Loss: 14.4481\n",
      "Epoch [790/1000], Loss: 14.4477\n",
      "Epoch [800/1000], Loss: 14.4475\n",
      "Epoch [810/1000], Loss: 14.4472\n",
      "Epoch [820/1000], Loss: 14.4471\n",
      "Epoch [830/1000], Loss: 14.4469\n",
      "Epoch [840/1000], Loss: 14.4468\n",
      "Epoch [850/1000], Loss: 14.4467\n",
      "Epoch [860/1000], Loss: 14.4466\n",
      "Epoch [870/1000], Loss: 14.4465\n",
      "Epoch [880/1000], Loss: 14.4465\n",
      "Epoch [890/1000], Loss: 14.4464\n",
      "Epoch [900/1000], Loss: 14.4463\n",
      "Epoch [910/1000], Loss: 14.4463\n",
      "Epoch [920/1000], Loss: 14.4462\n",
      "Epoch [930/1000], Loss: 14.4462\n",
      "Epoch [940/1000], Loss: 14.4461\n",
      "Epoch [950/1000], Loss: 14.4461\n",
      "Epoch [960/1000], Loss: 14.4461\n",
      "Epoch [970/1000], Loss: 14.4460\n",
      "Epoch [980/1000], Loss: 14.4460\n",
      "Epoch [990/1000], Loss: 14.4459\n",
      "Epoch [1000/1000], Loss: 14.4459\n",
      "Predicted days_remaining for parent_id 406: [22.506927490234375, 22.785799026489258, 22.781089782714844, 22.776344299316406, 22.783763885498047, 22.785791397094727, 22.784704208374023, 22.78691864013672]\n",
      "Training for parent_id 412...\n",
      "Epoch [10/1000], Loss: 168.0441\n",
      "Epoch [20/1000], Loss: 128.0772\n",
      "Epoch [30/1000], Loss: 100.2025\n",
      "Epoch [40/1000], Loss: 82.8055\n",
      "Epoch [50/1000], Loss: 70.1480\n",
      "Epoch [60/1000], Loss: 60.0297\n",
      "Epoch [70/1000], Loss: 51.6560\n",
      "Epoch [80/1000], Loss: 44.6667\n",
      "Epoch [90/1000], Loss: 38.8228\n",
      "Epoch [100/1000], Loss: 33.9569\n",
      "Epoch [110/1000], Loss: 29.9379\n",
      "Epoch [120/1000], Loss: 26.6444\n",
      "Epoch [130/1000], Loss: 23.9672\n",
      "Epoch [140/1000], Loss: 21.8109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 20.0918\n",
      "Epoch [160/1000], Loss: 18.7358\n",
      "Epoch [170/1000], Loss: 17.6773\n",
      "Epoch [180/1000], Loss: 16.8596\n",
      "Epoch [190/1000], Loss: 16.2344\n",
      "Epoch [200/1000], Loss: 15.7615\n",
      "Epoch [210/1000], Loss: 15.4075\n",
      "Epoch [220/1000], Loss: 15.1454\n",
      "Epoch [230/1000], Loss: 14.9533\n",
      "Epoch [240/1000], Loss: 14.8140\n",
      "Epoch [250/1000], Loss: 14.7140\n",
      "Epoch [260/1000], Loss: 14.6430\n",
      "Epoch [270/1000], Loss: 14.5930\n",
      "Epoch [280/1000], Loss: 14.5582\n",
      "Epoch [290/1000], Loss: 14.5340\n",
      "Epoch [300/1000], Loss: 14.5174\n",
      "Epoch [310/1000], Loss: 14.5060\n",
      "Epoch [320/1000], Loss: 14.4982\n",
      "Epoch [330/1000], Loss: 14.4927\n",
      "Epoch [340/1000], Loss: 14.4889\n",
      "Epoch [350/1000], Loss: 14.4862\n",
      "Epoch [360/1000], Loss: 14.4842\n",
      "Epoch [370/1000], Loss: 14.4827\n",
      "Epoch [380/1000], Loss: 14.4814\n",
      "Epoch [390/1000], Loss: 14.4803\n",
      "Epoch [400/1000], Loss: 14.4794\n",
      "Epoch [410/1000], Loss: 14.4786\n",
      "Epoch [420/1000], Loss: 14.4778\n",
      "Epoch [430/1000], Loss: 14.4770\n",
      "Epoch [440/1000], Loss: 14.4763\n",
      "Epoch [450/1000], Loss: 14.4756\n",
      "Epoch [460/1000], Loss: 14.4749\n",
      "Epoch [470/1000], Loss: 14.4742\n",
      "Epoch [480/1000], Loss: 14.4736\n",
      "Epoch [490/1000], Loss: 14.4730\n",
      "Epoch [500/1000], Loss: 14.4723\n",
      "Epoch [510/1000], Loss: 14.4717\n",
      "Epoch [520/1000], Loss: 14.4711\n",
      "Epoch [530/1000], Loss: 14.4706\n",
      "Epoch [540/1000], Loss: 14.4700\n",
      "Epoch [550/1000], Loss: 14.4694\n",
      "Epoch [560/1000], Loss: 14.4689\n",
      "Epoch [570/1000], Loss: 14.4684\n",
      "Epoch [580/1000], Loss: 14.4679\n",
      "Epoch [590/1000], Loss: 14.4674\n",
      "Epoch [600/1000], Loss: 14.4669\n",
      "Epoch [610/1000], Loss: 14.4664\n",
      "Epoch [620/1000], Loss: 14.4659\n",
      "Epoch [630/1000], Loss: 14.4655\n",
      "Epoch [640/1000], Loss: 14.4650\n",
      "Epoch [650/1000], Loss: 14.4646\n",
      "Epoch [660/1000], Loss: 14.4641\n",
      "Epoch [670/1000], Loss: 14.4637\n",
      "Epoch [680/1000], Loss: 14.4633\n",
      "Epoch [690/1000], Loss: 14.4629\n",
      "Epoch [700/1000], Loss: 14.4625\n",
      "Epoch [710/1000], Loss: 14.4621\n",
      "Epoch [720/1000], Loss: 14.4617\n",
      "Epoch [730/1000], Loss: 14.4613\n",
      "Epoch [740/1000], Loss: 14.4610\n",
      "Epoch [750/1000], Loss: 14.4606\n",
      "Epoch [760/1000], Loss: 14.4603\n",
      "Epoch [770/1000], Loss: 14.4599\n",
      "Epoch [780/1000], Loss: 14.4596\n",
      "Epoch [790/1000], Loss: 14.4593\n",
      "Epoch [800/1000], Loss: 14.4590\n",
      "Epoch [810/1000], Loss: 14.4586\n",
      "Epoch [820/1000], Loss: 14.4583\n",
      "Epoch [830/1000], Loss: 14.4580\n",
      "Epoch [840/1000], Loss: 14.4577\n",
      "Epoch [850/1000], Loss: 14.4574\n",
      "Epoch [860/1000], Loss: 14.4572\n",
      "Epoch [870/1000], Loss: 14.4569\n",
      "Epoch [880/1000], Loss: 14.4566\n",
      "Epoch [890/1000], Loss: 14.4563\n",
      "Epoch [900/1000], Loss: 14.4561\n",
      "Epoch [910/1000], Loss: 14.4558\n",
      "Epoch [920/1000], Loss: 14.4556\n",
      "Epoch [930/1000], Loss: 14.4553\n",
      "Epoch [940/1000], Loss: 14.4551\n",
      "Epoch [950/1000], Loss: 14.4548\n",
      "Epoch [960/1000], Loss: 14.4546\n",
      "Epoch [970/1000], Loss: 14.4544\n",
      "Epoch [980/1000], Loss: 14.4541\n",
      "Epoch [990/1000], Loss: 14.4539\n",
      "Epoch [1000/1000], Loss: 14.4537\n",
      "Predicted days_remaining for parent_id 412: [13.413275718688965, 13.797243118286133, 13.796259880065918, 13.801619529724121, 13.79991626739502, 13.801506996154785, 13.796595573425293, 13.792014122009277]\n",
      "Training for parent_id 425...\n",
      "Epoch [10/1000], Loss: 1407.4669\n",
      "Epoch [20/1000], Loss: 1279.8086\n",
      "Epoch [30/1000], Loss: 1190.6880\n",
      "Epoch [40/1000], Loss: 1128.0383\n",
      "Epoch [50/1000], Loss: 1075.3660\n",
      "Epoch [60/1000], Loss: 1026.9276\n",
      "Epoch [70/1000], Loss: 981.3085\n",
      "Epoch [80/1000], Loss: 938.0615\n",
      "Epoch [90/1000], Loss: 896.9051\n",
      "Epoch [100/1000], Loss: 857.6238\n",
      "Epoch [110/1000], Loss: 820.0497\n",
      "Epoch [120/1000], Loss: 784.0516\n",
      "Epoch [130/1000], Loss: 749.5235\n",
      "Epoch [140/1000], Loss: 716.3789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 684.5447\n",
      "Epoch [160/1000], Loss: 653.9579\n",
      "Epoch [170/1000], Loss: 624.5638\n",
      "Epoch [180/1000], Loss: 596.3128\n",
      "Epoch [190/1000], Loss: 569.1606\n",
      "Epoch [200/1000], Loss: 543.0666\n",
      "Epoch [210/1000], Loss: 517.9929\n",
      "Epoch [220/1000], Loss: 493.9044\n",
      "Epoch [230/1000], Loss: 470.7678\n",
      "Epoch [240/1000], Loss: 448.5518\n",
      "Epoch [250/1000], Loss: 427.2263\n",
      "Epoch [260/1000], Loss: 406.7627\n",
      "Epoch [270/1000], Loss: 387.1331\n",
      "Epoch [280/1000], Loss: 368.3111\n",
      "Epoch [290/1000], Loss: 350.2708\n",
      "Epoch [300/1000], Loss: 332.9872\n",
      "Epoch [310/1000], Loss: 316.4359\n",
      "Epoch [320/1000], Loss: 300.5934\n",
      "Epoch [330/1000], Loss: 285.4366\n",
      "Epoch [340/1000], Loss: 270.9429\n",
      "Epoch [350/1000], Loss: 257.0905\n",
      "Epoch [360/1000], Loss: 243.8577\n",
      "Epoch [370/1000], Loss: 231.2237\n",
      "Epoch [380/1000], Loss: 219.1680\n",
      "Epoch [390/1000], Loss: 207.6704\n",
      "Epoch [400/1000], Loss: 196.7114\n",
      "Epoch [410/1000], Loss: 186.2717\n",
      "Epoch [420/1000], Loss: 176.3327\n",
      "Epoch [430/1000], Loss: 166.8758\n",
      "Epoch [440/1000], Loss: 157.8833\n",
      "Epoch [450/1000], Loss: 149.3376\n",
      "Epoch [460/1000], Loss: 141.2216\n",
      "Epoch [470/1000], Loss: 133.5185\n",
      "Epoch [480/1000], Loss: 126.2121\n",
      "Epoch [490/1000], Loss: 119.2863\n",
      "Epoch [500/1000], Loss: 112.7258\n",
      "Epoch [510/1000], Loss: 106.5154\n",
      "Epoch [520/1000], Loss: 100.6403\n",
      "Epoch [530/1000], Loss: 95.0863\n",
      "Epoch [540/1000], Loss: 89.8394\n",
      "Epoch [550/1000], Loss: 84.8861\n",
      "Epoch [560/1000], Loss: 80.2132\n",
      "Epoch [570/1000], Loss: 75.8081\n",
      "Epoch [580/1000], Loss: 71.6583\n",
      "Epoch [590/1000], Loss: 67.7518\n",
      "Epoch [600/1000], Loss: 64.0772\n",
      "Epoch [610/1000], Loss: 60.6231\n",
      "Epoch [620/1000], Loss: 57.3787\n",
      "Epoch [630/1000], Loss: 54.3336\n",
      "Epoch [640/1000], Loss: 51.4777\n",
      "Epoch [650/1000], Loss: 48.8012\n",
      "Epoch [660/1000], Loss: 46.2949\n",
      "Epoch [670/1000], Loss: 43.9496\n",
      "Epoch [680/1000], Loss: 41.7568\n",
      "Epoch [690/1000], Loss: 39.7081\n",
      "Epoch [700/1000], Loss: 37.7956\n",
      "Epoch [710/1000], Loss: 36.0116\n",
      "Epoch [720/1000], Loss: 34.3488\n",
      "Epoch [730/1000], Loss: 32.8002\n",
      "Epoch [740/1000], Loss: 31.3591\n",
      "Epoch [750/1000], Loss: 30.0192\n",
      "Epoch [760/1000], Loss: 28.7743\n",
      "Epoch [770/1000], Loss: 27.6187\n",
      "Epoch [780/1000], Loss: 26.5467\n",
      "Epoch [790/1000], Loss: 25.5533\n",
      "Epoch [800/1000], Loss: 24.6333\n",
      "Epoch [810/1000], Loss: 23.7821\n",
      "Epoch [820/1000], Loss: 22.9952\n",
      "Epoch [830/1000], Loss: 22.2683\n",
      "Epoch [840/1000], Loss: 21.5975\n",
      "Epoch [850/1000], Loss: 20.9788\n",
      "Epoch [860/1000], Loss: 20.4088\n",
      "Epoch [870/1000], Loss: 19.8841\n",
      "Epoch [880/1000], Loss: 19.4015\n",
      "Epoch [890/1000], Loss: 18.9579\n",
      "Epoch [900/1000], Loss: 18.5506\n",
      "Epoch [910/1000], Loss: 18.1769\n",
      "Epoch [920/1000], Loss: 17.8344\n",
      "Epoch [930/1000], Loss: 17.5207\n",
      "Epoch [940/1000], Loss: 17.2336\n",
      "Epoch [950/1000], Loss: 16.9712\n",
      "Epoch [960/1000], Loss: 16.7315\n",
      "Epoch [970/1000], Loss: 16.5127\n",
      "Epoch [980/1000], Loss: 16.3132\n",
      "Epoch [990/1000], Loss: 16.1314\n",
      "Epoch [1000/1000], Loss: 15.9660\n",
      "Predicted days_remaining for parent_id 425: [37.46385192871094, 37.53105163574219, 37.52935028076172, 37.52848815917969, 37.52081298828125, 37.52885055541992, 37.52971267700195, 37.529701232910156]\n",
      "Training for parent_id 430...\n",
      "Epoch [10/1000], Loss: 126.9022\n",
      "Epoch [20/1000], Loss: 95.6673\n",
      "Epoch [30/1000], Loss: 73.4062\n",
      "Epoch [40/1000], Loss: 59.1368\n",
      "Epoch [50/1000], Loss: 48.8557\n",
      "Epoch [60/1000], Loss: 40.7910\n",
      "Epoch [70/1000], Loss: 34.4209\n",
      "Epoch [80/1000], Loss: 29.4241\n",
      "Epoch [90/1000], Loss: 25.5455\n",
      "Epoch [100/1000], Loss: 22.5711\n",
      "Epoch [110/1000], Loss: 20.3201\n",
      "Epoch [120/1000], Loss: 18.6407\n",
      "Epoch [130/1000], Loss: 17.4064\n",
      "Epoch [140/1000], Loss: 16.5136\n",
      "Epoch [150/1000], Loss: 15.8783\n",
      "Epoch [160/1000], Loss: 15.4336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 15.1276\n",
      "Epoch [180/1000], Loss: 14.9204\n",
      "Epoch [190/1000], Loss: 14.7822\n",
      "Epoch [200/1000], Loss: 14.6914\n",
      "Epoch [210/1000], Loss: 14.6322\n",
      "Epoch [220/1000], Loss: 14.5940\n",
      "Epoch [230/1000], Loss: 14.5692\n",
      "Epoch [240/1000], Loss: 14.5530\n",
      "Epoch [250/1000], Loss: 14.5421\n",
      "Epoch [260/1000], Loss: 14.5345\n",
      "Epoch [270/1000], Loss: 14.5288\n",
      "Epoch [280/1000], Loss: 14.5244\n",
      "Epoch [290/1000], Loss: 14.5207\n",
      "Epoch [300/1000], Loss: 14.5174\n",
      "Epoch [310/1000], Loss: 14.5145\n",
      "Epoch [320/1000], Loss: 14.5118\n",
      "Epoch [330/1000], Loss: 14.5093\n",
      "Epoch [340/1000], Loss: 14.5069\n",
      "Epoch [350/1000], Loss: 14.5046\n",
      "Epoch [360/1000], Loss: 14.5025\n",
      "Epoch [370/1000], Loss: 14.5004\n",
      "Epoch [380/1000], Loss: 14.4985\n",
      "Epoch [390/1000], Loss: 14.4966\n",
      "Epoch [400/1000], Loss: 14.4948\n",
      "Epoch [410/1000], Loss: 14.4931\n",
      "Epoch [420/1000], Loss: 14.4915\n",
      "Epoch [430/1000], Loss: 14.4899\n",
      "Epoch [440/1000], Loss: 14.4884\n",
      "Epoch [450/1000], Loss: 14.4870\n",
      "Epoch [460/1000], Loss: 14.4856\n",
      "Epoch [470/1000], Loss: 14.4843\n",
      "Epoch [480/1000], Loss: 14.4830\n",
      "Epoch [490/1000], Loss: 14.4818\n",
      "Epoch [500/1000], Loss: 14.4806\n",
      "Epoch [510/1000], Loss: 14.4795\n",
      "Epoch [520/1000], Loss: 14.4784\n",
      "Epoch [530/1000], Loss: 14.4774\n",
      "Epoch [540/1000], Loss: 14.4764\n",
      "Epoch [550/1000], Loss: 14.4754\n",
      "Epoch [560/1000], Loss: 14.4745\n",
      "Epoch [570/1000], Loss: 14.4736\n",
      "Epoch [580/1000], Loss: 14.4727\n",
      "Epoch [590/1000], Loss: 14.4719\n",
      "Epoch [600/1000], Loss: 14.4711\n",
      "Epoch [610/1000], Loss: 14.4703\n",
      "Epoch [620/1000], Loss: 14.4696\n",
      "Epoch [630/1000], Loss: 14.4688\n",
      "Epoch [640/1000], Loss: 14.4681\n",
      "Epoch [650/1000], Loss: 14.4675\n",
      "Epoch [660/1000], Loss: 14.4668\n",
      "Epoch [670/1000], Loss: 14.4662\n",
      "Epoch [680/1000], Loss: 14.4656\n",
      "Epoch [690/1000], Loss: 14.4650\n",
      "Epoch [700/1000], Loss: 14.4644\n",
      "Epoch [710/1000], Loss: 14.4638\n",
      "Epoch [720/1000], Loss: 14.4633\n",
      "Epoch [730/1000], Loss: 14.4628\n",
      "Epoch [740/1000], Loss: 14.4623\n",
      "Epoch [750/1000], Loss: 14.4618\n",
      "Epoch [760/1000], Loss: 14.4613\n",
      "Epoch [770/1000], Loss: 14.4608\n",
      "Epoch [780/1000], Loss: 14.4604\n",
      "Epoch [790/1000], Loss: 14.4600\n",
      "Epoch [800/1000], Loss: 14.4595\n",
      "Epoch [810/1000], Loss: 14.4591\n",
      "Epoch [820/1000], Loss: 14.4587\n",
      "Epoch [830/1000], Loss: 14.4583\n",
      "Epoch [840/1000], Loss: 14.4579\n",
      "Epoch [850/1000], Loss: 14.4576\n",
      "Epoch [860/1000], Loss: 14.4572\n",
      "Epoch [870/1000], Loss: 14.4569\n",
      "Epoch [880/1000], Loss: 14.4565\n",
      "Epoch [890/1000], Loss: 14.4562\n",
      "Epoch [900/1000], Loss: 14.4559\n",
      "Epoch [910/1000], Loss: 14.4556\n",
      "Epoch [920/1000], Loss: 14.4553\n",
      "Epoch [930/1000], Loss: 14.4550\n",
      "Epoch [940/1000], Loss: 14.4547\n",
      "Epoch [950/1000], Loss: 14.4544\n",
      "Epoch [960/1000], Loss: 14.4541\n",
      "Epoch [970/1000], Loss: 14.4538\n",
      "Epoch [980/1000], Loss: 14.4536\n",
      "Epoch [990/1000], Loss: 14.4533\n",
      "Epoch [1000/1000], Loss: 14.4531\n",
      "Predicted days_remaining for parent_id 430: [11.423243522644043, 11.80572509765625, 11.80093002319336, 11.805368423461914, 11.748291969299316, 11.800521850585938, 11.808816909790039, 11.802206993103027]\n",
      "Training for parent_id 433...\n",
      "Epoch [10/1000], Loss: 455.4833\n",
      "Epoch [20/1000], Loss: 386.7267\n",
      "Epoch [30/1000], Loss: 339.8165\n",
      "Epoch [40/1000], Loss: 307.5832\n",
      "Epoch [50/1000], Loss: 280.8664\n",
      "Epoch [60/1000], Loss: 257.0460\n",
      "Epoch [70/1000], Loss: 235.4100\n",
      "Epoch [80/1000], Loss: 215.6255\n",
      "Epoch [90/1000], Loss: 197.4772\n",
      "Epoch [100/1000], Loss: 180.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 165.4821\n",
      "Epoch [120/1000], Loss: 151.3967\n",
      "Epoch [130/1000], Loss: 138.4530\n",
      "Epoch [140/1000], Loss: 126.5652\n",
      "Epoch [150/1000], Loss: 115.6562\n",
      "Epoch [160/1000], Loss: 105.6559\n",
      "Epoch [170/1000], Loss: 96.5001\n",
      "Epoch [180/1000], Loss: 88.1294\n",
      "Epoch [190/1000], Loss: 80.4886\n",
      "Epoch [200/1000], Loss: 73.5257\n",
      "Epoch [210/1000], Loss: 67.1921\n",
      "Epoch [220/1000], Loss: 61.4419\n",
      "Epoch [230/1000], Loss: 56.2317\n",
      "Epoch [240/1000], Loss: 51.5206\n",
      "Epoch [250/1000], Loss: 47.2700\n",
      "Epoch [260/1000], Loss: 43.4431\n",
      "Epoch [270/1000], Loss: 40.0056\n",
      "Epoch [280/1000], Loss: 36.9250\n",
      "Epoch [290/1000], Loss: 34.1707\n",
      "Epoch [300/1000], Loss: 31.7141\n",
      "Epoch [310/1000], Loss: 29.5283\n",
      "Epoch [320/1000], Loss: 27.5883\n",
      "Epoch [330/1000], Loss: 25.8707\n",
      "Epoch [340/1000], Loss: 24.3539\n",
      "Epoch [350/1000], Loss: 23.0178\n",
      "Epoch [360/1000], Loss: 21.8440\n",
      "Epoch [370/1000], Loss: 20.8154\n",
      "Epoch [380/1000], Loss: 19.9165\n",
      "Epoch [390/1000], Loss: 19.1328\n",
      "Epoch [400/1000], Loss: 18.4515\n",
      "Epoch [410/1000], Loss: 17.8607\n",
      "Epoch [420/1000], Loss: 17.3499\n",
      "Epoch [430/1000], Loss: 16.9093\n",
      "Epoch [440/1000], Loss: 16.5303\n",
      "Epoch [450/1000], Loss: 16.2051\n",
      "Epoch [460/1000], Loss: 15.9270\n",
      "Epoch [470/1000], Loss: 15.6897\n",
      "Epoch [480/1000], Loss: 15.4877\n",
      "Epoch [490/1000], Loss: 15.3163\n",
      "Epoch [500/1000], Loss: 15.1713\n",
      "Epoch [510/1000], Loss: 15.0488\n",
      "Epoch [520/1000], Loss: 14.9458\n",
      "Epoch [530/1000], Loss: 14.8592\n",
      "Epoch [540/1000], Loss: 14.7868\n",
      "Epoch [550/1000], Loss: 14.7263\n",
      "Epoch [560/1000], Loss: 14.6759\n",
      "Epoch [570/1000], Loss: 14.6341\n",
      "Epoch [580/1000], Loss: 14.5995\n",
      "Epoch [590/1000], Loss: 14.5708\n",
      "Epoch [600/1000], Loss: 14.5472\n",
      "Epoch [610/1000], Loss: 14.5279\n",
      "Epoch [620/1000], Loss: 14.5120\n",
      "Epoch [630/1000], Loss: 14.4990\n",
      "Epoch [640/1000], Loss: 14.4884\n",
      "Epoch [650/1000], Loss: 14.4798\n",
      "Epoch [660/1000], Loss: 14.4729\n",
      "Epoch [670/1000], Loss: 14.4672\n",
      "Epoch [680/1000], Loss: 14.4627\n",
      "Epoch [690/1000], Loss: 14.4590\n",
      "Epoch [700/1000], Loss: 14.4561\n",
      "Epoch [710/1000], Loss: 14.4537\n",
      "Epoch [720/1000], Loss: 14.4518\n",
      "Epoch [730/1000], Loss: 14.4503\n",
      "Epoch [740/1000], Loss: 14.4491\n",
      "Epoch [750/1000], Loss: 14.4482\n",
      "Epoch [760/1000], Loss: 14.4474\n",
      "Epoch [770/1000], Loss: 14.4468\n",
      "Epoch [780/1000], Loss: 14.4463\n",
      "Epoch [790/1000], Loss: 14.4460\n",
      "Epoch [800/1000], Loss: 14.4457\n",
      "Epoch [810/1000], Loss: 14.4454\n",
      "Epoch [820/1000], Loss: 14.4452\n",
      "Epoch [830/1000], Loss: 14.4451\n",
      "Epoch [840/1000], Loss: 14.4449\n",
      "Epoch [850/1000], Loss: 14.4448\n",
      "Epoch [860/1000], Loss: 14.4447\n",
      "Epoch [870/1000], Loss: 14.4447\n",
      "Epoch [880/1000], Loss: 14.4446\n",
      "Epoch [890/1000], Loss: 14.4445\n",
      "Epoch [900/1000], Loss: 14.4445\n",
      "Epoch [910/1000], Loss: 14.4445\n",
      "Epoch [920/1000], Loss: 14.4444\n",
      "Epoch [930/1000], Loss: 14.4444\n",
      "Epoch [940/1000], Loss: 14.4443\n",
      "Epoch [950/1000], Loss: 14.4443\n",
      "Epoch [960/1000], Loss: 14.4443\n",
      "Epoch [970/1000], Loss: 14.4442\n",
      "Epoch [980/1000], Loss: 14.4442\n",
      "Epoch [990/1000], Loss: 14.4442\n",
      "Epoch [1000/1000], Loss: 14.4441\n",
      "Predicted days_remaining for parent_id 433: [22.533252716064453, 22.783370971679688, 22.78125762939453, 22.783483505249023, 22.77227210998535, 22.777727127075195, 22.77610969543457, 22.781719207763672]\n",
      "Training for parent_id 434...\n",
      "Epoch [10/1000], Loss: 115.1576\n",
      "Epoch [20/1000], Loss: 81.6805\n",
      "Epoch [30/1000], Loss: 62.2746\n",
      "Epoch [40/1000], Loss: 50.4212\n",
      "Epoch [50/1000], Loss: 41.9567\n",
      "Epoch [60/1000], Loss: 35.4001\n",
      "Epoch [70/1000], Loss: 30.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000], Loss: 26.1953\n",
      "Epoch [90/1000], Loss: 23.0692\n",
      "Epoch [100/1000], Loss: 20.6868\n",
      "Epoch [110/1000], Loss: 18.9003\n",
      "Epoch [120/1000], Loss: 17.5833\n",
      "Epoch [130/1000], Loss: 16.6293\n",
      "Epoch [140/1000], Loss: 15.9509\n",
      "Epoch [150/1000], Loss: 15.4773\n",
      "Epoch [160/1000], Loss: 15.1527\n",
      "Epoch [170/1000], Loss: 14.9343\n",
      "Epoch [180/1000], Loss: 14.7898\n",
      "Epoch [190/1000], Loss: 14.6957\n",
      "Epoch [200/1000], Loss: 14.6352\n",
      "Epoch [210/1000], Loss: 14.5965\n",
      "Epoch [220/1000], Loss: 14.5718\n",
      "Epoch [230/1000], Loss: 14.5559\n",
      "Epoch [240/1000], Loss: 14.5453\n",
      "Epoch [250/1000], Loss: 14.5379\n",
      "Epoch [260/1000], Loss: 14.5324\n",
      "Epoch [270/1000], Loss: 14.5281\n",
      "Epoch [280/1000], Loss: 14.5245\n",
      "Epoch [290/1000], Loss: 14.5213\n",
      "Epoch [300/1000], Loss: 14.5184\n",
      "Epoch [310/1000], Loss: 14.5157\n",
      "Epoch [320/1000], Loss: 14.5132\n",
      "Epoch [330/1000], Loss: 14.5108\n",
      "Epoch [340/1000], Loss: 14.5085\n",
      "Epoch [350/1000], Loss: 14.5063\n",
      "Epoch [360/1000], Loss: 14.5042\n",
      "Epoch [370/1000], Loss: 14.5022\n",
      "Epoch [380/1000], Loss: 14.5003\n",
      "Epoch [390/1000], Loss: 14.4984\n",
      "Epoch [400/1000], Loss: 14.4967\n",
      "Epoch [410/1000], Loss: 14.4950\n",
      "Epoch [420/1000], Loss: 14.4934\n",
      "Epoch [430/1000], Loss: 14.4918\n",
      "Epoch [440/1000], Loss: 14.4904\n",
      "Epoch [450/1000], Loss: 14.4889\n",
      "Epoch [460/1000], Loss: 14.4876\n",
      "Epoch [470/1000], Loss: 14.4863\n",
      "Epoch [480/1000], Loss: 14.4850\n",
      "Epoch [490/1000], Loss: 14.4838\n",
      "Epoch [500/1000], Loss: 14.4826\n",
      "Epoch [510/1000], Loss: 14.4815\n",
      "Epoch [520/1000], Loss: 14.4804\n",
      "Epoch [530/1000], Loss: 14.4794\n",
      "Epoch [540/1000], Loss: 14.4784\n",
      "Epoch [550/1000], Loss: 14.4774\n",
      "Epoch [560/1000], Loss: 14.4764\n",
      "Epoch [570/1000], Loss: 14.4755\n",
      "Epoch [580/1000], Loss: 14.4747\n",
      "Epoch [590/1000], Loss: 14.4738\n",
      "Epoch [600/1000], Loss: 14.4730\n",
      "Epoch [610/1000], Loss: 14.4722\n",
      "Epoch [620/1000], Loss: 14.4715\n",
      "Epoch [630/1000], Loss: 14.4707\n",
      "Epoch [640/1000], Loss: 14.4700\n",
      "Epoch [650/1000], Loss: 14.4693\n",
      "Epoch [660/1000], Loss: 14.4686\n",
      "Epoch [670/1000], Loss: 14.4680\n",
      "Epoch [680/1000], Loss: 14.4674\n",
      "Epoch [690/1000], Loss: 14.4668\n",
      "Epoch [700/1000], Loss: 14.4662\n",
      "Epoch [710/1000], Loss: 14.4656\n",
      "Epoch [720/1000], Loss: 14.4650\n",
      "Epoch [730/1000], Loss: 14.4645\n",
      "Epoch [740/1000], Loss: 14.4640\n",
      "Epoch [750/1000], Loss: 14.4635\n",
      "Epoch [760/1000], Loss: 14.4630\n",
      "Epoch [770/1000], Loss: 14.4625\n",
      "Epoch [780/1000], Loss: 14.4620\n",
      "Epoch [790/1000], Loss: 14.4616\n",
      "Epoch [800/1000], Loss: 14.4611\n",
      "Epoch [810/1000], Loss: 14.4607\n",
      "Epoch [820/1000], Loss: 14.4603\n",
      "Epoch [830/1000], Loss: 14.4599\n",
      "Epoch [840/1000], Loss: 14.4595\n",
      "Epoch [850/1000], Loss: 14.4591\n",
      "Epoch [860/1000], Loss: 14.4587\n",
      "Epoch [870/1000], Loss: 14.4583\n",
      "Epoch [880/1000], Loss: 14.4580\n",
      "Epoch [890/1000], Loss: 14.4576\n",
      "Epoch [900/1000], Loss: 14.4573\n",
      "Epoch [910/1000], Loss: 14.4570\n",
      "Epoch [920/1000], Loss: 14.4566\n",
      "Epoch [930/1000], Loss: 14.4563\n",
      "Epoch [940/1000], Loss: 14.4560\n",
      "Epoch [950/1000], Loss: 14.4557\n",
      "Epoch [960/1000], Loss: 14.4554\n",
      "Epoch [970/1000], Loss: 14.4551\n",
      "Epoch [980/1000], Loss: 14.4548\n",
      "Epoch [990/1000], Loss: 14.4546\n",
      "Epoch [1000/1000], Loss: 14.4543\n",
      "Predicted days_remaining for parent_id 434: [11.407896995544434, 11.81216049194336, 11.810256004333496, 11.793901443481445, 11.8040132522583, 11.774189949035645, 11.791189193725586, 11.800890922546387]\n",
      "Training for parent_id 437...\n",
      "Epoch [10/1000], Loss: 540.4257\n",
      "Epoch [20/1000], Loss: 471.1724\n",
      "Epoch [30/1000], Loss: 426.9088\n",
      "Epoch [40/1000], Loss: 393.8060\n",
      "Epoch [50/1000], Loss: 365.0084\n",
      "Epoch [60/1000], Loss: 338.4875\n",
      "Epoch [70/1000], Loss: 313.8104\n",
      "Epoch [80/1000], Loss: 290.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 269.3724\n",
      "Epoch [100/1000], Loss: 249.4080\n",
      "Epoch [110/1000], Loss: 230.8220\n",
      "Epoch [120/1000], Loss: 213.5246\n",
      "Epoch [130/1000], Loss: 197.4313\n",
      "Epoch [140/1000], Loss: 182.4629\n",
      "Epoch [150/1000], Loss: 168.5462\n",
      "Epoch [160/1000], Loss: 155.6145\n",
      "Epoch [170/1000], Loss: 143.6062\n",
      "Epoch [180/1000], Loss: 132.4653\n",
      "Epoch [190/1000], Loss: 122.1391\n",
      "Epoch [200/1000], Loss: 112.5788\n",
      "Epoch [210/1000], Loss: 103.7383\n",
      "Epoch [220/1000], Loss: 95.5740\n",
      "Epoch [230/1000], Loss: 88.0445\n",
      "Epoch [240/1000], Loss: 81.1106\n",
      "Epoch [250/1000], Loss: 74.7347\n",
      "Epoch [260/1000], Loss: 68.8811\n",
      "Epoch [270/1000], Loss: 63.5157\n",
      "Epoch [280/1000], Loss: 58.6057\n",
      "Epoch [290/1000], Loss: 54.1203\n",
      "Epoch [300/1000], Loss: 50.0298\n",
      "Epoch [310/1000], Loss: 46.3060\n",
      "Epoch [320/1000], Loss: 42.9221\n",
      "Epoch [330/1000], Loss: 39.8528\n",
      "Epoch [340/1000], Loss: 37.0739\n",
      "Epoch [350/1000], Loss: 34.5628\n",
      "Epoch [360/1000], Loss: 32.2980\n",
      "Epoch [370/1000], Loss: 30.2592\n",
      "Epoch [380/1000], Loss: 28.4274\n",
      "Epoch [390/1000], Loss: 26.7850\n",
      "Epoch [400/1000], Loss: 25.3151\n",
      "Epoch [410/1000], Loss: 24.0024\n",
      "Epoch [420/1000], Loss: 22.8324\n",
      "Epoch [430/1000], Loss: 21.7917\n",
      "Epoch [440/1000], Loss: 20.8678\n",
      "Epoch [450/1000], Loss: 20.0494\n",
      "Epoch [460/1000], Loss: 19.3259\n",
      "Epoch [470/1000], Loss: 18.6876\n",
      "Epoch [480/1000], Loss: 18.1257\n",
      "Epoch [490/1000], Loss: 17.6320\n",
      "Epoch [500/1000], Loss: 17.1991\n",
      "Epoch [510/1000], Loss: 16.8204\n",
      "Epoch [520/1000], Loss: 16.4897\n",
      "Epoch [530/1000], Loss: 16.2017\n",
      "Epoch [540/1000], Loss: 15.9512\n",
      "Epoch [550/1000], Loss: 15.7340\n",
      "Epoch [560/1000], Loss: 15.5458\n",
      "Epoch [570/1000], Loss: 15.3833\n",
      "Epoch [580/1000], Loss: 15.2432\n",
      "Epoch [590/1000], Loss: 15.1227\n",
      "Epoch [600/1000], Loss: 15.0192\n",
      "Epoch [610/1000], Loss: 14.9306\n",
      "Epoch [620/1000], Loss: 14.8548\n",
      "Epoch [630/1000], Loss: 14.7902\n",
      "Epoch [640/1000], Loss: 14.7351\n",
      "Epoch [650/1000], Loss: 14.6884\n",
      "Epoch [660/1000], Loss: 14.6488\n",
      "Epoch [670/1000], Loss: 14.6153\n",
      "Epoch [680/1000], Loss: 14.5870\n",
      "Epoch [690/1000], Loss: 14.5632\n",
      "Epoch [700/1000], Loss: 14.5432\n",
      "Epoch [710/1000], Loss: 14.5264\n",
      "Epoch [720/1000], Loss: 14.5123\n",
      "Epoch [730/1000], Loss: 14.5006\n",
      "Epoch [740/1000], Loss: 14.4908\n",
      "Epoch [750/1000], Loss: 14.4826\n",
      "Epoch [760/1000], Loss: 14.4759\n",
      "Epoch [770/1000], Loss: 14.4703\n",
      "Epoch [780/1000], Loss: 14.4657\n",
      "Epoch [790/1000], Loss: 14.4618\n",
      "Epoch [800/1000], Loss: 14.4587\n",
      "Epoch [810/1000], Loss: 14.4561\n",
      "Epoch [820/1000], Loss: 14.4540\n",
      "Epoch [830/1000], Loss: 14.4523\n",
      "Epoch [840/1000], Loss: 14.4508\n",
      "Epoch [850/1000], Loss: 14.4497\n",
      "Epoch [860/1000], Loss: 14.4487\n",
      "Epoch [870/1000], Loss: 14.4480\n",
      "Epoch [880/1000], Loss: 14.4473\n",
      "Epoch [890/1000], Loss: 14.4468\n",
      "Epoch [900/1000], Loss: 14.4464\n",
      "Epoch [910/1000], Loss: 14.4461\n",
      "Epoch [920/1000], Loss: 14.4458\n",
      "Epoch [930/1000], Loss: 14.4456\n",
      "Epoch [940/1000], Loss: 14.4454\n",
      "Epoch [950/1000], Loss: 14.4452\n",
      "Epoch [960/1000], Loss: 14.4451\n",
      "Epoch [970/1000], Loss: 14.4450\n",
      "Epoch [980/1000], Loss: 14.4449\n",
      "Epoch [990/1000], Loss: 14.4448\n",
      "Epoch [1000/1000], Loss: 14.4448\n",
      "Predicted days_remaining for parent_id 437: [24.515193939208984, 24.767770767211914, 24.77096176147461, 24.771133422851562, 24.76891326904297, 24.771636962890625, 24.771066665649414, 24.773000717163086]\n",
      "Training for parent_id 445...\n",
      "Epoch [10/1000], Loss: 307.9262\n",
      "Epoch [20/1000], Loss: 255.1250\n",
      "Epoch [30/1000], Loss: 216.0469\n",
      "Epoch [40/1000], Loss: 188.2743\n",
      "Epoch [50/1000], Loss: 166.5116\n",
      "Epoch [60/1000], Loss: 148.0637\n",
      "Epoch [70/1000], Loss: 131.8145\n",
      "Epoch [80/1000], Loss: 117.3903\n",
      "Epoch [90/1000], Loss: 104.5833\n",
      "Epoch [100/1000], Loss: 93.2124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 83.1220\n",
      "Epoch [120/1000], Loss: 74.1769\n",
      "Epoch [130/1000], Loss: 66.2582\n",
      "Epoch [140/1000], Loss: 59.2606\n",
      "Epoch [150/1000], Loss: 53.0901\n",
      "Epoch [160/1000], Loss: 47.6625\n",
      "Epoch [170/1000], Loss: 42.9018\n",
      "Epoch [180/1000], Loss: 38.7390\n",
      "Epoch [190/1000], Loss: 35.1111\n",
      "Epoch [200/1000], Loss: 31.9606\n",
      "Epoch [210/1000], Loss: 29.2348\n",
      "Epoch [220/1000], Loss: 26.8857\n",
      "Epoch [230/1000], Loss: 24.8691\n",
      "Epoch [240/1000], Loss: 23.1453\n",
      "Epoch [250/1000], Loss: 21.6778\n",
      "Epoch [260/1000], Loss: 20.4340\n",
      "Epoch [270/1000], Loss: 19.3842\n",
      "Epoch [280/1000], Loss: 18.5022\n",
      "Epoch [290/1000], Loss: 17.7644\n",
      "Epoch [300/1000], Loss: 17.1500\n",
      "Epoch [310/1000], Loss: 16.6407\n",
      "Epoch [320/1000], Loss: 16.2205\n",
      "Epoch [330/1000], Loss: 15.8752\n",
      "Epoch [340/1000], Loss: 15.5929\n",
      "Epoch [350/1000], Loss: 15.3632\n",
      "Epoch [360/1000], Loss: 15.1770\n",
      "Epoch [370/1000], Loss: 15.0269\n",
      "Epoch [380/1000], Loss: 14.9063\n",
      "Epoch [390/1000], Loss: 14.8100\n",
      "Epoch [400/1000], Loss: 14.7333\n",
      "Epoch [410/1000], Loss: 14.6726\n",
      "Epoch [420/1000], Loss: 14.6247\n",
      "Epoch [430/1000], Loss: 14.5871\n",
      "Epoch [440/1000], Loss: 14.5577\n",
      "Epoch [450/1000], Loss: 14.5348\n",
      "Epoch [460/1000], Loss: 14.5171\n",
      "Epoch [470/1000], Loss: 14.5034\n",
      "Epoch [480/1000], Loss: 14.4928\n",
      "Epoch [490/1000], Loss: 14.4847\n",
      "Epoch [500/1000], Loss: 14.4785\n",
      "Epoch [510/1000], Loss: 14.4738\n",
      "Epoch [520/1000], Loss: 14.4702\n",
      "Epoch [530/1000], Loss: 14.4674\n",
      "Epoch [540/1000], Loss: 14.4653\n",
      "Epoch [550/1000], Loss: 14.4637\n",
      "Epoch [560/1000], Loss: 14.4625\n",
      "Epoch [570/1000], Loss: 14.4615\n",
      "Epoch [580/1000], Loss: 14.4607\n",
      "Epoch [590/1000], Loss: 14.4601\n",
      "Epoch [600/1000], Loss: 14.4596\n",
      "Epoch [610/1000], Loss: 14.4592\n",
      "Epoch [620/1000], Loss: 14.4589\n",
      "Epoch [630/1000], Loss: 14.4586\n",
      "Epoch [640/1000], Loss: 14.4583\n",
      "Epoch [650/1000], Loss: 14.4581\n",
      "Epoch [660/1000], Loss: 14.4578\n",
      "Epoch [670/1000], Loss: 14.4576\n",
      "Epoch [680/1000], Loss: 14.4574\n",
      "Epoch [690/1000], Loss: 14.4572\n",
      "Epoch [700/1000], Loss: 14.4570\n",
      "Epoch [710/1000], Loss: 14.4568\n",
      "Epoch [720/1000], Loss: 14.4566\n",
      "Epoch [730/1000], Loss: 14.4565\n",
      "Epoch [740/1000], Loss: 14.4563\n",
      "Epoch [750/1000], Loss: 14.4561\n",
      "Epoch [760/1000], Loss: 14.4559\n",
      "Epoch [770/1000], Loss: 14.4558\n",
      "Epoch [780/1000], Loss: 14.4556\n",
      "Epoch [790/1000], Loss: 14.4554\n",
      "Epoch [800/1000], Loss: 14.4552\n",
      "Epoch [810/1000], Loss: 14.4551\n",
      "Epoch [820/1000], Loss: 14.4549\n",
      "Epoch [830/1000], Loss: 14.4548\n",
      "Epoch [840/1000], Loss: 14.4546\n",
      "Epoch [850/1000], Loss: 14.4544\n",
      "Epoch [860/1000], Loss: 14.4543\n",
      "Epoch [870/1000], Loss: 14.4541\n",
      "Epoch [880/1000], Loss: 14.4540\n",
      "Epoch [890/1000], Loss: 14.4538\n",
      "Epoch [900/1000], Loss: 14.4537\n",
      "Epoch [910/1000], Loss: 14.4535\n",
      "Epoch [920/1000], Loss: 14.4534\n",
      "Epoch [930/1000], Loss: 14.4532\n",
      "Epoch [940/1000], Loss: 14.4531\n",
      "Epoch [950/1000], Loss: 14.4529\n",
      "Epoch [960/1000], Loss: 14.4528\n",
      "Epoch [970/1000], Loss: 14.4526\n",
      "Epoch [980/1000], Loss: 14.4525\n",
      "Epoch [990/1000], Loss: 14.4524\n",
      "Epoch [1000/1000], Loss: 14.4522\n",
      "Predicted days_remaining for parent_id 445: [18.429651260375977, 18.798545837402344, 18.800350189208984, 18.79977798461914, 18.792003631591797, 18.79363250732422, 18.794109344482422, 18.794830322265625]\n",
      "Training for parent_id 458...\n",
      "Epoch [10/1000], Loss: 1050.6624\n",
      "Epoch [20/1000], Loss: 927.6439\n",
      "Epoch [30/1000], Loss: 843.1227\n",
      "Epoch [40/1000], Loss: 787.4473\n",
      "Epoch [50/1000], Loss: 741.8611\n",
      "Epoch [60/1000], Loss: 700.6093\n",
      "Epoch [70/1000], Loss: 662.0177\n",
      "Epoch [80/1000], Loss: 625.7429\n",
      "Epoch [90/1000], Loss: 591.5269\n",
      "Epoch [100/1000], Loss: 559.1888\n",
      "Epoch [110/1000], Loss: 528.5849\n",
      "Epoch [120/1000], Loss: 499.5907\n",
      "Epoch [130/1000], Loss: 472.0968\n",
      "Epoch [140/1000], Loss: 446.0070\n",
      "Epoch [150/1000], Loss: 421.2369\n",
      "Epoch [160/1000], Loss: 397.7121\n",
      "Epoch [170/1000], Loss: 375.3661\n",
      "Epoch [180/1000], Loss: 354.1393\n",
      "Epoch [190/1000], Loss: 333.9777\n",
      "Epoch [200/1000], Loss: 314.8315\n",
      "Epoch [210/1000], Loss: 296.6550\n",
      "Epoch [220/1000], Loss: 279.4051\n",
      "Epoch [230/1000], Loss: 263.0414\n",
      "Epoch [240/1000], Loss: 247.5262\n",
      "Epoch [250/1000], Loss: 232.8231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000], Loss: 218.8975\n",
      "Epoch [270/1000], Loss: 205.7164\n",
      "Epoch [280/1000], Loss: 193.2483\n",
      "Epoch [290/1000], Loss: 181.4623\n",
      "Epoch [300/1000], Loss: 170.3293\n",
      "Epoch [310/1000], Loss: 159.8207\n",
      "Epoch [320/1000], Loss: 149.9090\n",
      "Epoch [330/1000], Loss: 140.5678\n",
      "Epoch [340/1000], Loss: 131.7712\n",
      "Epoch [350/1000], Loss: 123.4944\n",
      "Epoch [360/1000], Loss: 115.7133\n",
      "Epoch [370/1000], Loss: 108.4045\n",
      "Epoch [380/1000], Loss: 101.5454\n",
      "Epoch [390/1000], Loss: 95.1142\n",
      "Epoch [400/1000], Loss: 89.0896\n",
      "Epoch [410/1000], Loss: 83.4512\n",
      "Epoch [420/1000], Loss: 78.1793\n",
      "Epoch [430/1000], Loss: 73.2547\n",
      "Epoch [440/1000], Loss: 68.6591\n",
      "Epoch [450/1000], Loss: 64.3747\n",
      "Epoch [460/1000], Loss: 60.3844\n",
      "Epoch [470/1000], Loss: 56.6718\n",
      "Epoch [480/1000], Loss: 53.2211\n",
      "Epoch [490/1000], Loss: 50.0170\n",
      "Epoch [500/1000], Loss: 47.0452\n",
      "Epoch [510/1000], Loss: 44.2916\n",
      "Epoch [520/1000], Loss: 41.7429\n",
      "Epoch [530/1000], Loss: 39.3864\n",
      "Epoch [540/1000], Loss: 37.2099\n",
      "Epoch [550/1000], Loss: 35.2020\n",
      "Epoch [560/1000], Loss: 33.3515\n",
      "Epoch [570/1000], Loss: 31.6480\n",
      "Epoch [580/1000], Loss: 30.0816\n",
      "Epoch [590/1000], Loss: 28.6427\n",
      "Epoch [600/1000], Loss: 27.3227\n",
      "Epoch [610/1000], Loss: 26.1129\n",
      "Epoch [620/1000], Loss: 25.0054\n",
      "Epoch [630/1000], Loss: 23.9928\n",
      "Epoch [640/1000], Loss: 23.0679\n",
      "Epoch [650/1000], Loss: 22.2242\n",
      "Epoch [660/1000], Loss: 21.4554\n",
      "Epoch [670/1000], Loss: 20.7556\n",
      "Epoch [680/1000], Loss: 20.1195\n",
      "Epoch [690/1000], Loss: 19.5418\n",
      "Epoch [700/1000], Loss: 19.0179\n",
      "Epoch [710/1000], Loss: 18.5433\n",
      "Epoch [720/1000], Loss: 18.1138\n",
      "Epoch [730/1000], Loss: 17.7257\n",
      "Epoch [740/1000], Loss: 17.3754\n",
      "Epoch [750/1000], Loss: 17.0595\n",
      "Epoch [760/1000], Loss: 16.7750\n",
      "Epoch [770/1000], Loss: 16.5191\n",
      "Epoch [780/1000], Loss: 16.2892\n",
      "Epoch [790/1000], Loss: 16.0829\n",
      "Epoch [800/1000], Loss: 15.8981\n",
      "Epoch [810/1000], Loss: 15.7326\n",
      "Epoch [820/1000], Loss: 15.5846\n",
      "Epoch [830/1000], Loss: 15.4525\n",
      "Epoch [840/1000], Loss: 15.3346\n",
      "Epoch [850/1000], Loss: 15.2297\n",
      "Epoch [860/1000], Loss: 15.1363\n",
      "Epoch [870/1000], Loss: 15.0533\n",
      "Epoch [880/1000], Loss: 14.9796\n",
      "Epoch [890/1000], Loss: 14.9143\n",
      "Epoch [900/1000], Loss: 14.8565\n",
      "Epoch [910/1000], Loss: 14.8054\n",
      "Epoch [920/1000], Loss: 14.7602\n",
      "Epoch [930/1000], Loss: 14.7204\n",
      "Epoch [940/1000], Loss: 14.6853\n",
      "Epoch [950/1000], Loss: 14.6544\n",
      "Epoch [960/1000], Loss: 14.6273\n",
      "Epoch [970/1000], Loss: 14.6035\n",
      "Epoch [980/1000], Loss: 14.5826\n",
      "Epoch [990/1000], Loss: 14.5643\n",
      "Epoch [1000/1000], Loss: 14.5483\n",
      "Predicted days_remaining for parent_id 458: [33.2017936706543, 33.46601486206055, 33.46314239501953, 33.463199615478516, 33.46418380737305, 33.456844329833984, 33.46468734741211, 33.46693420410156]\n",
      "Training for parent_id 460...\n",
      "Epoch [10/1000], Loss: 329.9822\n",
      "Epoch [20/1000], Loss: 277.2394\n",
      "Epoch [30/1000], Loss: 242.1788\n",
      "Epoch [40/1000], Loss: 216.8238\n",
      "Epoch [50/1000], Loss: 195.5764\n",
      "Epoch [60/1000], Loss: 176.5242\n",
      "Epoch [70/1000], Loss: 159.3276\n",
      "Epoch [80/1000], Loss: 143.8202\n",
      "Epoch [90/1000], Loss: 129.8252\n",
      "Epoch [100/1000], Loss: 117.1873\n",
      "Epoch [110/1000], Loss: 105.7725\n",
      "Epoch [120/1000], Loss: 95.4659\n",
      "Epoch [130/1000], Loss: 86.1677\n",
      "Epoch [140/1000], Loss: 77.7902\n",
      "Epoch [150/1000], Loss: 70.2548\n",
      "Epoch [160/1000], Loss: 63.4904\n",
      "Epoch [170/1000], Loss: 57.4315\n",
      "Epoch [180/1000], Loss: 52.0178\n",
      "Epoch [190/1000], Loss: 47.1933\n",
      "Epoch [200/1000], Loss: 42.9058\n",
      "Epoch [210/1000], Loss: 39.1064\n",
      "Epoch [220/1000], Loss: 35.7498\n",
      "Epoch [230/1000], Loss: 32.7936\n",
      "Epoch [240/1000], Loss: 30.1983\n",
      "Epoch [250/1000], Loss: 27.9273\n",
      "Epoch [260/1000], Loss: 25.9468\n",
      "Epoch [270/1000], Loss: 24.2255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/1000], Loss: 22.7346\n",
      "Epoch [290/1000], Loss: 21.4478\n",
      "Epoch [300/1000], Loss: 20.3410\n",
      "Epoch [310/1000], Loss: 19.3926\n",
      "Epoch [320/1000], Loss: 18.5827\n",
      "Epoch [330/1000], Loss: 17.8937\n",
      "Epoch [340/1000], Loss: 17.3096\n",
      "Epoch [350/1000], Loss: 16.8164\n",
      "Epoch [360/1000], Loss: 16.4013\n",
      "Epoch [370/1000], Loss: 16.0532\n",
      "Epoch [380/1000], Loss: 15.7626\n",
      "Epoch [390/1000], Loss: 15.5207\n",
      "Epoch [400/1000], Loss: 15.3201\n",
      "Epoch [410/1000], Loss: 15.1544\n",
      "Epoch [420/1000], Loss: 15.0181\n",
      "Epoch [430/1000], Loss: 14.9063\n",
      "Epoch [440/1000], Loss: 14.8150\n",
      "Epoch [450/1000], Loss: 14.7407\n",
      "Epoch [460/1000], Loss: 14.6804\n",
      "Epoch [470/1000], Loss: 14.6318\n",
      "Epoch [480/1000], Loss: 14.5926\n",
      "Epoch [490/1000], Loss: 14.5611\n",
      "Epoch [500/1000], Loss: 14.5360\n",
      "Epoch [510/1000], Loss: 14.5161\n",
      "Epoch [520/1000], Loss: 14.5002\n",
      "Epoch [530/1000], Loss: 14.4877\n",
      "Epoch [540/1000], Loss: 14.4778\n",
      "Epoch [550/1000], Loss: 14.4701\n",
      "Epoch [560/1000], Loss: 14.4640\n",
      "Epoch [570/1000], Loss: 14.4593\n",
      "Epoch [580/1000], Loss: 14.4556\n",
      "Epoch [590/1000], Loss: 14.4528\n",
      "Epoch [600/1000], Loss: 14.4506\n",
      "Epoch [610/1000], Loss: 14.4489\n",
      "Epoch [620/1000], Loss: 14.4476\n",
      "Epoch [630/1000], Loss: 14.4466\n",
      "Epoch [640/1000], Loss: 14.4459\n",
      "Epoch [650/1000], Loss: 14.4453\n",
      "Epoch [660/1000], Loss: 14.4449\n",
      "Epoch [670/1000], Loss: 14.4445\n",
      "Epoch [680/1000], Loss: 14.4443\n",
      "Epoch [690/1000], Loss: 14.4441\n",
      "Epoch [700/1000], Loss: 14.4439\n",
      "Epoch [710/1000], Loss: 14.4438\n",
      "Epoch [720/1000], Loss: 14.4437\n",
      "Epoch [730/1000], Loss: 14.4436\n",
      "Epoch [740/1000], Loss: 14.4435\n",
      "Epoch [750/1000], Loss: 14.4435\n",
      "Epoch [760/1000], Loss: 14.4434\n",
      "Epoch [770/1000], Loss: 14.4434\n",
      "Epoch [780/1000], Loss: 14.4433\n",
      "Epoch [790/1000], Loss: 14.4433\n",
      "Epoch [800/1000], Loss: 14.4433\n",
      "Epoch [810/1000], Loss: 14.4432\n",
      "Epoch [820/1000], Loss: 14.4432\n",
      "Epoch [830/1000], Loss: 14.4432\n",
      "Epoch [840/1000], Loss: 14.4431\n",
      "Epoch [850/1000], Loss: 14.4431\n",
      "Epoch [860/1000], Loss: 14.4431\n",
      "Epoch [870/1000], Loss: 14.4431\n",
      "Epoch [880/1000], Loss: 14.4430\n",
      "Epoch [890/1000], Loss: 14.4430\n",
      "Epoch [900/1000], Loss: 14.4430\n",
      "Epoch [910/1000], Loss: 14.4429\n",
      "Epoch [920/1000], Loss: 14.4429\n",
      "Epoch [930/1000], Loss: 14.4429\n",
      "Epoch [940/1000], Loss: 14.4429\n",
      "Epoch [950/1000], Loss: 14.4428\n",
      "Epoch [960/1000], Loss: 14.4428\n",
      "Epoch [970/1000], Loss: 14.4428\n",
      "Epoch [980/1000], Loss: 14.4427\n",
      "Epoch [990/1000], Loss: 14.4427\n",
      "Epoch [1000/1000], Loss: 14.4427\n",
      "Predicted days_remaining for parent_id 460: [19.559724807739258, 19.778675079345703, 19.771808624267578, 19.777956008911133, 19.774755477905273, 19.779075622558594, 19.78021812438965, 19.778942108154297]\n",
      "Training for parent_id 464...\n",
      "Epoch [10/1000], Loss: 763.3058\n",
      "Epoch [20/1000], Loss: 669.6323\n",
      "Epoch [30/1000], Loss: 600.8510\n",
      "Epoch [40/1000], Loss: 553.0402\n",
      "Epoch [50/1000], Loss: 514.5583\n",
      "Epoch [60/1000], Loss: 480.0367\n",
      "Epoch [70/1000], Loss: 447.9580\n",
      "Epoch [80/1000], Loss: 418.0153\n",
      "Epoch [90/1000], Loss: 390.1478\n",
      "Epoch [100/1000], Loss: 364.1609\n",
      "Epoch [110/1000], Loss: 339.8774\n",
      "Epoch [120/1000], Loss: 317.1540\n",
      "Epoch [130/1000], Loss: 295.8704\n",
      "Epoch [140/1000], Loss: 275.9240\n",
      "Epoch [150/1000], Loss: 257.2244\n",
      "Epoch [160/1000], Loss: 239.6922\n",
      "Epoch [170/1000], Loss: 223.2563\n",
      "Epoch [180/1000], Loss: 207.8522\n",
      "Epoch [190/1000], Loss: 193.4213\n",
      "Epoch [200/1000], Loss: 179.9092\n",
      "Epoch [210/1000], Loss: 167.2657\n",
      "Epoch [220/1000], Loss: 155.4435\n",
      "Epoch [230/1000], Loss: 144.3984\n",
      "Epoch [240/1000], Loss: 134.0883\n",
      "Epoch [250/1000], Loss: 124.4736\n",
      "Epoch [260/1000], Loss: 115.5162\n",
      "Epoch [270/1000], Loss: 107.1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/1000], Loss: 99.4304\n",
      "Epoch [290/1000], Loss: 92.2345\n",
      "Epoch [300/1000], Loss: 85.5604\n",
      "Epoch [310/1000], Loss: 79.3778\n",
      "Epoch [320/1000], Loss: 73.6578\n",
      "Epoch [330/1000], Loss: 68.3724\n",
      "Epoch [340/1000], Loss: 63.4949\n",
      "Epoch [350/1000], Loss: 59.0000\n",
      "Epoch [360/1000], Loss: 54.8631\n",
      "Epoch [370/1000], Loss: 51.0611\n",
      "Epoch [380/1000], Loss: 47.5717\n",
      "Epoch [390/1000], Loss: 44.3738\n",
      "Epoch [400/1000], Loss: 41.4472\n",
      "Epoch [410/1000], Loss: 38.7729\n",
      "Epoch [420/1000], Loss: 36.3326\n",
      "Epoch [430/1000], Loss: 34.1093\n",
      "Epoch [440/1000], Loss: 32.0866\n",
      "Epoch [450/1000], Loss: 30.2493\n",
      "Epoch [460/1000], Loss: 28.5830\n",
      "Epoch [470/1000], Loss: 27.0740\n",
      "Epoch [480/1000], Loss: 25.7097\n",
      "Epoch [490/1000], Loss: 24.4781\n",
      "Epoch [500/1000], Loss: 23.3680\n",
      "Epoch [510/1000], Loss: 22.3690\n",
      "Epoch [520/1000], Loss: 21.4715\n",
      "Epoch [530/1000], Loss: 20.6664\n",
      "Epoch [540/1000], Loss: 19.9454\n",
      "Epoch [550/1000], Loss: 19.3007\n",
      "Epoch [560/1000], Loss: 18.7252\n",
      "Epoch [570/1000], Loss: 18.2123\n",
      "Epoch [580/1000], Loss: 17.7559\n",
      "Epoch [590/1000], Loss: 17.3505\n",
      "Epoch [600/1000], Loss: 16.9909\n",
      "Epoch [610/1000], Loss: 16.6726\n",
      "Epoch [620/1000], Loss: 16.3912\n",
      "Epoch [630/1000], Loss: 16.1428\n",
      "Epoch [640/1000], Loss: 15.9240\n",
      "Epoch [650/1000], Loss: 15.7315\n",
      "Epoch [660/1000], Loss: 15.5625\n",
      "Epoch [670/1000], Loss: 15.4143\n",
      "Epoch [680/1000], Loss: 15.2846\n",
      "Epoch [690/1000], Loss: 15.1713\n",
      "Epoch [700/1000], Loss: 15.0724\n",
      "Epoch [710/1000], Loss: 14.9863\n",
      "Epoch [720/1000], Loss: 14.9115\n",
      "Epoch [730/1000], Loss: 14.8465\n",
      "Epoch [740/1000], Loss: 14.7902\n",
      "Epoch [750/1000], Loss: 14.7415\n",
      "Epoch [760/1000], Loss: 14.6994\n",
      "Epoch [770/1000], Loss: 14.6632\n",
      "Epoch [780/1000], Loss: 14.6319\n",
      "Epoch [790/1000], Loss: 14.6051\n",
      "Epoch [800/1000], Loss: 14.5821\n",
      "Epoch [810/1000], Loss: 14.5624\n",
      "Epoch [820/1000], Loss: 14.5455\n",
      "Epoch [830/1000], Loss: 14.5311\n",
      "Epoch [840/1000], Loss: 14.5188\n",
      "Epoch [850/1000], Loss: 14.5084\n",
      "Epoch [860/1000], Loss: 14.4995\n",
      "Epoch [870/1000], Loss: 14.4920\n",
      "Epoch [880/1000], Loss: 14.4856\n",
      "Epoch [890/1000], Loss: 14.4802\n",
      "Epoch [900/1000], Loss: 14.4756\n",
      "Epoch [910/1000], Loss: 14.4718\n",
      "Epoch [920/1000], Loss: 14.4686\n",
      "Epoch [930/1000], Loss: 14.4658\n",
      "Epoch [940/1000], Loss: 14.4636\n",
      "Epoch [950/1000], Loss: 14.4616\n",
      "Epoch [960/1000], Loss: 14.4600\n",
      "Epoch [970/1000], Loss: 14.4587\n",
      "Epoch [980/1000], Loss: 14.4575\n",
      "Epoch [990/1000], Loss: 14.4566\n",
      "Epoch [1000/1000], Loss: 14.4558\n",
      "Predicted days_remaining for parent_id 464: [28.36932373046875, 28.73908042907715, 28.740171432495117, 28.739421844482422, 28.7396183013916, 28.738508224487305, 28.736692428588867, 28.74011993408203]\n",
      "Training for parent_id 481...\n",
      "Epoch [10/1000], Loss: 849.6033\n",
      "Epoch [20/1000], Loss: 760.2790\n",
      "Epoch [30/1000], Loss: 702.7675\n",
      "Epoch [40/1000], Loss: 659.8958\n",
      "Epoch [50/1000], Loss: 621.9197\n",
      "Epoch [60/1000], Loss: 586.4286\n",
      "Epoch [70/1000], Loss: 552.9362\n",
      "Epoch [80/1000], Loss: 521.3604\n",
      "Epoch [90/1000], Loss: 491.5526\n",
      "Epoch [100/1000], Loss: 463.3689\n",
      "Epoch [110/1000], Loss: 436.6988\n",
      "Epoch [120/1000], Loss: 411.4510\n",
      "Epoch [130/1000], Loss: 387.5412\n",
      "Epoch [140/1000], Loss: 364.8907\n",
      "Epoch [150/1000], Loss: 343.4279\n",
      "Epoch [160/1000], Loss: 323.0894\n",
      "Epoch [170/1000], Loss: 303.8180\n",
      "Epoch [180/1000], Loss: 285.5618\n",
      "Epoch [190/1000], Loss: 268.2733\n",
      "Epoch [200/1000], Loss: 251.9085\n",
      "Epoch [210/1000], Loss: 236.4258\n",
      "Epoch [220/1000], Loss: 221.7863\n",
      "Epoch [230/1000], Loss: 207.9526\n",
      "Epoch [240/1000], Loss: 194.8897\n",
      "Epoch [250/1000], Loss: 182.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000], Loss: 170.9411\n",
      "Epoch [270/1000], Loss: 159.9916\n",
      "Epoch [280/1000], Loss: 149.6845\n",
      "Epoch [290/1000], Loss: 139.9904\n",
      "Epoch [300/1000], Loss: 130.8813\n",
      "Epoch [310/1000], Loss: 122.3296\n",
      "Epoch [320/1000], Loss: 114.3087\n",
      "Epoch [330/1000], Loss: 106.7931\n",
      "Epoch [340/1000], Loss: 99.7578\n",
      "Epoch [350/1000], Loss: 93.1788\n",
      "Epoch [360/1000], Loss: 87.0329\n",
      "Epoch [370/1000], Loss: 81.2975\n",
      "Epoch [380/1000], Loss: 75.9508\n",
      "Epoch [390/1000], Loss: 70.9720\n",
      "Epoch [400/1000], Loss: 66.3408\n",
      "Epoch [410/1000], Loss: 62.0377\n",
      "Epoch [420/1000], Loss: 58.0440\n",
      "Epoch [430/1000], Loss: 54.3416\n",
      "Epoch [440/1000], Loss: 50.9133\n",
      "Epoch [450/1000], Loss: 47.7423\n",
      "Epoch [460/1000], Loss: 44.8130\n",
      "Epoch [470/1000], Loss: 42.1100\n",
      "Epoch [480/1000], Loss: 39.6188\n",
      "Epoch [490/1000], Loss: 37.3257\n",
      "Epoch [500/1000], Loss: 35.2175\n",
      "Epoch [510/1000], Loss: 33.2816\n",
      "Epoch [520/1000], Loss: 31.5061\n",
      "Epoch [530/1000], Loss: 29.8798\n",
      "Epoch [540/1000], Loss: 28.3919\n",
      "Epoch [550/1000], Loss: 27.0325\n",
      "Epoch [560/1000], Loss: 25.7920\n",
      "Epoch [570/1000], Loss: 24.6615\n",
      "Epoch [580/1000], Loss: 23.6324\n",
      "Epoch [590/1000], Loss: 22.6970\n",
      "Epoch [600/1000], Loss: 21.8477\n",
      "Epoch [610/1000], Loss: 21.0777\n",
      "Epoch [620/1000], Loss: 20.3805\n",
      "Epoch [630/1000], Loss: 19.7499\n",
      "Epoch [640/1000], Loss: 19.1805\n",
      "Epoch [650/1000], Loss: 18.6668\n",
      "Epoch [660/1000], Loss: 18.2042\n",
      "Epoch [670/1000], Loss: 17.7879\n",
      "Epoch [680/1000], Loss: 17.4140\n",
      "Epoch [690/1000], Loss: 17.0786\n",
      "Epoch [700/1000], Loss: 16.7780\n",
      "Epoch [710/1000], Loss: 16.5090\n",
      "Epoch [720/1000], Loss: 16.2687\n",
      "Epoch [730/1000], Loss: 16.0542\n",
      "Epoch [740/1000], Loss: 15.8630\n",
      "Epoch [750/1000], Loss: 15.6929\n",
      "Epoch [760/1000], Loss: 15.5417\n",
      "Epoch [770/1000], Loss: 15.4075\n",
      "Epoch [780/1000], Loss: 15.2885\n",
      "Epoch [790/1000], Loss: 15.1832\n",
      "Epoch [800/1000], Loss: 15.0901\n",
      "Epoch [810/1000], Loss: 15.0080\n",
      "Epoch [820/1000], Loss: 14.9355\n",
      "Epoch [830/1000], Loss: 14.8717\n",
      "Epoch [840/1000], Loss: 14.8157\n",
      "Epoch [850/1000], Loss: 14.7664\n",
      "Epoch [860/1000], Loss: 14.7233\n",
      "Epoch [870/1000], Loss: 14.6855\n",
      "Epoch [880/1000], Loss: 14.6524\n",
      "Epoch [890/1000], Loss: 14.6236\n",
      "Epoch [900/1000], Loss: 14.5984\n",
      "Epoch [910/1000], Loss: 14.5765\n",
      "Epoch [920/1000], Loss: 14.5575\n",
      "Epoch [930/1000], Loss: 14.5410\n",
      "Epoch [940/1000], Loss: 14.5266\n",
      "Epoch [950/1000], Loss: 14.5142\n",
      "Epoch [960/1000], Loss: 14.5035\n",
      "Epoch [970/1000], Loss: 14.4942\n",
      "Epoch [980/1000], Loss: 14.4862\n",
      "Epoch [990/1000], Loss: 14.4794\n",
      "Epoch [1000/1000], Loss: 14.4734\n",
      "Predicted days_remaining for parent_id 481: [30.447887420654297, 30.584613800048828, 30.584644317626953, 30.584365844726562, 30.584819793701172, 30.585079193115234, 30.582679748535156, 30.58460235595703]\n",
      "Training for parent_id 486...\n",
      "Epoch [10/1000], Loss: 364.8328\n",
      "Epoch [20/1000], Loss: 305.1462\n",
      "Epoch [30/1000], Loss: 266.4083\n",
      "Epoch [40/1000], Loss: 238.5824\n",
      "Epoch [50/1000], Loss: 215.4160\n",
      "Epoch [60/1000], Loss: 194.8121\n",
      "Epoch [70/1000], Loss: 176.2562\n",
      "Epoch [80/1000], Loss: 159.4720\n",
      "Epoch [90/1000], Loss: 144.2621\n",
      "Epoch [100/1000], Loss: 130.4705\n",
      "Epoch [110/1000], Loss: 117.9669\n",
      "Epoch [120/1000], Loss: 106.6389\n",
      "Epoch [130/1000], Loss: 96.3863\n",
      "Epoch [140/1000], Loss: 87.1194\n",
      "Epoch [150/1000], Loss: 78.7567\n",
      "Epoch [160/1000], Loss: 71.2235\n",
      "Epoch [170/1000], Loss: 64.4512\n",
      "Epoch [180/1000], Loss: 58.3763\n",
      "Epoch [190/1000], Loss: 52.9396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 48.0863\n",
      "Epoch [210/1000], Loss: 43.7652\n",
      "Epoch [220/1000], Loss: 39.9283\n",
      "Epoch [230/1000], Loss: 36.5311\n",
      "Epoch [240/1000], Loss: 33.5320\n",
      "Epoch [250/1000], Loss: 30.8923\n",
      "Epoch [260/1000], Loss: 28.5760\n",
      "Epoch [270/1000], Loss: 26.5499\n",
      "Epoch [280/1000], Loss: 24.7833\n",
      "Epoch [290/1000], Loss: 23.2479\n",
      "Epoch [300/1000], Loss: 21.9179\n",
      "Epoch [310/1000], Loss: 20.7696\n",
      "Epoch [320/1000], Loss: 19.7816\n",
      "Epoch [330/1000], Loss: 18.9342\n",
      "Epoch [340/1000], Loss: 18.2101\n",
      "Epoch [350/1000], Loss: 17.5933\n",
      "Epoch [360/1000], Loss: 17.0698\n",
      "Epoch [370/1000], Loss: 16.6269\n",
      "Epoch [380/1000], Loss: 16.2537\n",
      "Epoch [390/1000], Loss: 15.9401\n",
      "Epoch [400/1000], Loss: 15.6776\n",
      "Epoch [410/1000], Loss: 15.4586\n",
      "Epoch [420/1000], Loss: 15.2766\n",
      "Epoch [430/1000], Loss: 15.1258\n",
      "Epoch [440/1000], Loss: 15.0013\n",
      "Epoch [450/1000], Loss: 14.8988\n",
      "Epoch [460/1000], Loss: 14.8148\n",
      "Epoch [470/1000], Loss: 14.7462\n",
      "Epoch [480/1000], Loss: 14.6903\n",
      "Epoch [490/1000], Loss: 14.6449\n",
      "Epoch [500/1000], Loss: 14.6082\n",
      "Epoch [510/1000], Loss: 14.5786\n",
      "Epoch [520/1000], Loss: 14.5548\n",
      "Epoch [530/1000], Loss: 14.5357\n",
      "Epoch [540/1000], Loss: 14.5204\n",
      "Epoch [550/1000], Loss: 14.5083\n",
      "Epoch [560/1000], Loss: 14.4986\n",
      "Epoch [570/1000], Loss: 14.4909\n",
      "Epoch [580/1000], Loss: 14.4849\n",
      "Epoch [590/1000], Loss: 14.4801\n",
      "Epoch [600/1000], Loss: 14.4763\n",
      "Epoch [610/1000], Loss: 14.4733\n",
      "Epoch [620/1000], Loss: 14.4709\n",
      "Epoch [630/1000], Loss: 14.4690\n",
      "Epoch [640/1000], Loss: 14.4675\n",
      "Epoch [650/1000], Loss: 14.4663\n",
      "Epoch [660/1000], Loss: 14.4654\n",
      "Epoch [670/1000], Loss: 14.4646\n",
      "Epoch [680/1000], Loss: 14.4640\n",
      "Epoch [690/1000], Loss: 14.4634\n",
      "Epoch [700/1000], Loss: 14.4630\n",
      "Epoch [710/1000], Loss: 14.4626\n",
      "Epoch [720/1000], Loss: 14.4622\n",
      "Epoch [730/1000], Loss: 14.4619\n",
      "Epoch [740/1000], Loss: 14.4617\n",
      "Epoch [750/1000], Loss: 14.4614\n",
      "Epoch [760/1000], Loss: 14.4612\n",
      "Epoch [770/1000], Loss: 14.4609\n",
      "Epoch [780/1000], Loss: 14.4607\n",
      "Epoch [790/1000], Loss: 14.4605\n",
      "Epoch [800/1000], Loss: 14.4603\n",
      "Epoch [810/1000], Loss: 14.4601\n",
      "Epoch [820/1000], Loss: 14.4599\n",
      "Epoch [830/1000], Loss: 14.4597\n",
      "Epoch [840/1000], Loss: 14.4595\n",
      "Epoch [850/1000], Loss: 14.4593\n",
      "Epoch [860/1000], Loss: 14.4591\n",
      "Epoch [870/1000], Loss: 14.4590\n",
      "Epoch [880/1000], Loss: 14.4588\n",
      "Epoch [890/1000], Loss: 14.4586\n",
      "Epoch [900/1000], Loss: 14.4584\n",
      "Epoch [910/1000], Loss: 14.4582\n",
      "Epoch [920/1000], Loss: 14.4581\n",
      "Epoch [930/1000], Loss: 14.4579\n",
      "Epoch [940/1000], Loss: 14.4577\n",
      "Epoch [950/1000], Loss: 14.4575\n",
      "Epoch [960/1000], Loss: 14.4574\n",
      "Epoch [970/1000], Loss: 14.4572\n",
      "Epoch [980/1000], Loss: 14.4570\n",
      "Epoch [990/1000], Loss: 14.4569\n",
      "Epoch [1000/1000], Loss: 14.4567\n",
      "Predicted days_remaining for parent_id 486: [20.383922576904297, 20.799856185913086, 20.807863235473633, 20.80337142944336, 20.803993225097656, 20.804256439208984, 20.798274993896484, 20.80184555053711]\n",
      "Training for parent_id 511...\n",
      "Epoch [10/1000], Loss: 135.9501\n",
      "Epoch [20/1000], Loss: 100.6882\n",
      "Epoch [30/1000], Loss: 79.9465\n",
      "Epoch [40/1000], Loss: 66.6915\n",
      "Epoch [50/1000], Loss: 56.6198\n",
      "Epoch [60/1000], Loss: 48.4015\n",
      "Epoch [70/1000], Loss: 41.6166\n",
      "Epoch [80/1000], Loss: 36.0270\n",
      "Epoch [90/1000], Loss: 31.4469\n",
      "Epoch [100/1000], Loss: 27.7205\n",
      "Epoch [110/1000], Loss: 24.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000], Loss: 22.3100\n",
      "Epoch [130/1000], Loss: 20.4078\n",
      "Epoch [140/1000], Loss: 18.9185\n",
      "Epoch [150/1000], Loss: 17.7655\n",
      "Epoch [160/1000], Loss: 16.8834\n",
      "Epoch [170/1000], Loss: 16.2165\n",
      "Epoch [180/1000], Loss: 15.7185\n",
      "Epoch [190/1000], Loss: 15.3513\n",
      "Epoch [200/1000], Loss: 15.0840\n",
      "Epoch [210/1000], Loss: 14.8918\n",
      "Epoch [220/1000], Loss: 14.7553\n",
      "Epoch [230/1000], Loss: 14.6597\n",
      "Epoch [240/1000], Loss: 14.5936\n",
      "Epoch [250/1000], Loss: 14.5483\n",
      "Epoch [260/1000], Loss: 14.5178\n",
      "Epoch [270/1000], Loss: 14.4974\n",
      "Epoch [280/1000], Loss: 14.4839\n",
      "Epoch [290/1000], Loss: 14.4750\n",
      "Epoch [300/1000], Loss: 14.4692\n",
      "Epoch [310/1000], Loss: 14.4654\n",
      "Epoch [320/1000], Loss: 14.4630\n",
      "Epoch [330/1000], Loss: 14.4613\n",
      "Epoch [340/1000], Loss: 14.4602\n",
      "Epoch [350/1000], Loss: 14.4593\n",
      "Epoch [360/1000], Loss: 14.4587\n",
      "Epoch [370/1000], Loss: 14.4582\n",
      "Epoch [380/1000], Loss: 14.4577\n",
      "Epoch [390/1000], Loss: 14.4573\n",
      "Epoch [400/1000], Loss: 14.4570\n",
      "Epoch [410/1000], Loss: 14.4566\n",
      "Epoch [420/1000], Loss: 14.4563\n",
      "Epoch [430/1000], Loss: 14.4560\n",
      "Epoch [440/1000], Loss: 14.4557\n",
      "Epoch [450/1000], Loss: 14.4554\n",
      "Epoch [460/1000], Loss: 14.4551\n",
      "Epoch [470/1000], Loss: 14.4548\n",
      "Epoch [480/1000], Loss: 14.4545\n",
      "Epoch [490/1000], Loss: 14.4542\n",
      "Epoch [500/1000], Loss: 14.4539\n",
      "Epoch [510/1000], Loss: 14.4536\n",
      "Epoch [520/1000], Loss: 14.4534\n",
      "Epoch [530/1000], Loss: 14.4531\n",
      "Epoch [540/1000], Loss: 14.4529\n",
      "Epoch [550/1000], Loss: 14.4526\n",
      "Epoch [560/1000], Loss: 14.4524\n",
      "Epoch [570/1000], Loss: 14.4521\n",
      "Epoch [580/1000], Loss: 14.4519\n",
      "Epoch [590/1000], Loss: 14.4517\n",
      "Epoch [600/1000], Loss: 14.4514\n",
      "Epoch [610/1000], Loss: 14.4512\n",
      "Epoch [620/1000], Loss: 14.4510\n",
      "Epoch [630/1000], Loss: 14.4508\n",
      "Epoch [640/1000], Loss: 14.4506\n",
      "Epoch [650/1000], Loss: 14.4504\n",
      "Epoch [660/1000], Loss: 14.4502\n",
      "Epoch [670/1000], Loss: 14.4500\n",
      "Epoch [680/1000], Loss: 14.4498\n",
      "Epoch [690/1000], Loss: 14.4496\n",
      "Epoch [700/1000], Loss: 14.4494\n",
      "Epoch [710/1000], Loss: 14.4493\n",
      "Epoch [720/1000], Loss: 14.4491\n",
      "Epoch [730/1000], Loss: 14.4489\n",
      "Epoch [740/1000], Loss: 14.4487\n",
      "Epoch [750/1000], Loss: 14.4486\n",
      "Epoch [760/1000], Loss: 14.4484\n",
      "Epoch [770/1000], Loss: 14.4482\n",
      "Epoch [780/1000], Loss: 14.4481\n",
      "Epoch [790/1000], Loss: 14.4479\n",
      "Epoch [800/1000], Loss: 14.4478\n",
      "Epoch [810/1000], Loss: 14.4476\n",
      "Epoch [820/1000], Loss: 14.4475\n",
      "Epoch [830/1000], Loss: 14.4474\n",
      "Epoch [840/1000], Loss: 14.4472\n",
      "Epoch [850/1000], Loss: 14.4471\n",
      "Epoch [860/1000], Loss: 14.4469\n",
      "Epoch [870/1000], Loss: 14.4468\n",
      "Epoch [880/1000], Loss: 14.4467\n",
      "Epoch [890/1000], Loss: 14.4466\n",
      "Epoch [900/1000], Loss: 14.4464\n",
      "Epoch [910/1000], Loss: 14.4463\n",
      "Epoch [920/1000], Loss: 14.4462\n",
      "Epoch [930/1000], Loss: 14.4461\n",
      "Epoch [940/1000], Loss: 14.4460\n",
      "Epoch [950/1000], Loss: 14.4459\n",
      "Epoch [960/1000], Loss: 14.4457\n",
      "Epoch [970/1000], Loss: 14.4456\n",
      "Epoch [980/1000], Loss: 14.4455\n",
      "Epoch [990/1000], Loss: 14.4454\n",
      "Epoch [1000/1000], Loss: 14.4453\n",
      "Predicted days_remaining for parent_id 511: [12.516630172729492, 12.775572776794434, 12.790310859680176, 12.783204078674316, 12.78626823425293, 12.778069496154785, 12.792059898376465, 12.777710914611816]\n",
      "Total training time: 47.54 seconds\n",
      "Ensemble Model MSE on test data: 1059.843990903504\n",
      "Total evaluation time: 0.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "input_size = train_df.drop(columns=['parent_id', 'days_remaining']).shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Predicting a single value for each timestep\n",
    "num_epochs = 1000  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()  # Loss function\n",
    "models = []  # To store trained models\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"Models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each parent_id and train a separate model\n",
    "unique_parent_ids = train_df['parent_id'].unique()\n",
    "\n",
    "for parent_id in unique_parent_ids:\n",
    "    print(f\"Training for parent_id {parent_id}...\")\n",
    "    \n",
    "    # Get the data for the current parent_id\n",
    "    features, target = get_data_for_parent(train_df, parent_id)\n",
    "    \n",
    "    # Initialize the model and optimizer\n",
    "    model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop for the current parent_id\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        \n",
    "        # Compute the loss (many-to-many)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(output_dir, f\"model_parent_{parent_id}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    models.append(model)  # Append the trained model to the list\n",
    "\n",
    "    # Optionally, evaluate the model or make predictions\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "        print(f\"Predicted days_remaining for parent_id {parent_id}: {predictions.squeeze().tolist()}\")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total training time\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Assuming df_test is prepared similarly to train_df with 'parent_id' and 'days_remaining'\n",
    "\n",
    "# Extract the relevant features and target from df_test\n",
    "X_test = df_test.drop(columns=['parent_id', 'days_remaining'])\n",
    "y_test = df_test['days_remaining']\n",
    "\n",
    "# Initialize lists for true and predicted values\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each row in the test set and get predictions\n",
    "for i in range(len(X_test)):\n",
    "    # Prepare each test sample with the correct dimensions\n",
    "    test_sample = torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0)  # Shape (1, 1, input_size)\n",
    "    true_value = y_test.iloc[i]  # Actual value for the test sample\n",
    "    \n",
    "    # Get the ensemble prediction\n",
    "    prediction = ensemble_prediction(models, test_sample)\n",
    "    \n",
    "    # Append to lists for MSE calculation\n",
    "    y_true.append(true_value)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate Mean Squared Error for the ensemble model\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Ensemble Model MSE on test data: {mse}\")\n",
    "\n",
    "# Calculate and print the total evaluation time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511426e",
   "metadata": {},
   "source": [
    "### Using minmax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3775166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the dataset: 784\n",
      "Train dataset rows: 584\n",
      "Test dataset rows: 200\n",
      "Parent ID: 14, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 15, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 25, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 40, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 41, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 50, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 51, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 61, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 74, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 80, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 82, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 88, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 91, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 99, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 103, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 105, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 113, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 123, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 124, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 144, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 150, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 166, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 168, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 179, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 199, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 211, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 215, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 218, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 234, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 238, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 256, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 265, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 277, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 281, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 289, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 302, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 310, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 312, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 315, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 316, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 317, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 320, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 321, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 326, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 332, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 340, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 346, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 348, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 360, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 362, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 366, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 371, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 376, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 377, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 387, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 392, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 393, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 395, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 404, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 406, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 412, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 425, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 430, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 433, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 434, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 437, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 445, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 458, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 460, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 464, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 481, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 486, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n",
      "Parent ID: 511, Features Shape: torch.Size([1, 8, 65]), Target Shape: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform only the columns that need scaling\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Step 2: Split based on unique parent_id\n",
    "unique_parent_ids = df['parent_id'].unique()\n",
    "\n",
    "# Randomly shuffle and split into 75% train and 25% test\n",
    "train_parent_ids, test_parent_ids = train_test_split(unique_parent_ids, test_size=0.25, random_state=42)\n",
    "\n",
    "# Filter the dataset based on the split\n",
    "train_df = df[df['parent_id'].isin(train_parent_ids)]\n",
    "test_df = df[df['parent_id'].isin(test_parent_ids)]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total rows in the dataset: {len(df)}\")\n",
    "print(f\"Train dataset rows: {len(train_df)}\")\n",
    "print(f\"Test dataset rows: {len(test_df)}\")\n",
    "\n",
    "# Example: Pass 75% (train_df) to the next function\n",
    "for parent_id in train_df['parent_id'].unique():\n",
    "    features, target = get_data_for_parent(train_df, parent_id)\n",
    "    # Example: Print the features and target shapes\n",
    "    print(f\"Parent ID: {parent_id}, Features Shape: {features.shape}, Target Shape: {target.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0522c8",
   "metadata": {},
   "source": [
    "### Using minmax scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70628948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for parent_id 14...\n",
      "Epoch [10/1000], Loss: 716.6599\n",
      "Epoch [20/1000], Loss: 632.0350\n",
      "Epoch [30/1000], Loss: 559.0217\n",
      "Epoch [40/1000], Loss: 506.6242\n",
      "Epoch [50/1000], Loss: 466.1007\n",
      "Epoch [60/1000], Loss: 431.7722\n",
      "Epoch [70/1000], Loss: 401.1104\n",
      "Epoch [80/1000], Loss: 372.7914\n",
      "Epoch [90/1000], Loss: 346.4568\n",
      "Epoch [100/1000], Loss: 321.8979\n",
      "Epoch [110/1000], Loss: 298.9529\n",
      "Epoch [120/1000], Loss: 277.5013\n",
      "Epoch [130/1000], Loss: 257.4453\n",
      "Epoch [140/1000], Loss: 238.7010\n",
      "Epoch [150/1000], Loss: 221.1935\n",
      "Epoch [160/1000], Loss: 204.8540\n",
      "Epoch [170/1000], Loss: 189.6181\n",
      "Epoch [180/1000], Loss: 175.4243\n",
      "Epoch [190/1000], Loss: 162.2139\n",
      "Epoch [200/1000], Loss: 149.9308\n",
      "Epoch [210/1000], Loss: 138.5213\n",
      "Epoch [220/1000], Loss: 127.9344\n",
      "Epoch [230/1000], Loss: 118.1212\n",
      "Epoch [240/1000], Loss: 109.0357\n",
      "Epoch [250/1000], Loss: 100.6339\n",
      "Epoch [260/1000], Loss: 92.8738\n",
      "Epoch [270/1000], Loss: 85.7157\n",
      "Epoch [280/1000], Loss: 79.1215\n",
      "Epoch [290/1000], Loss: 73.0552\n",
      "Epoch [300/1000], Loss: 67.4824\n",
      "Epoch [310/1000], Loss: 62.3704\n",
      "Epoch [320/1000], Loss: 57.6880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [330/1000], Loss: 53.4056\n",
      "Epoch [340/1000], Loss: 49.4950\n",
      "Epoch [350/1000], Loss: 45.9297\n",
      "Epoch [360/1000], Loss: 42.6844\n",
      "Epoch [370/1000], Loss: 39.7351\n",
      "Epoch [380/1000], Loss: 37.0593\n",
      "Epoch [390/1000], Loss: 34.6357\n",
      "Epoch [400/1000], Loss: 32.4442\n",
      "Epoch [410/1000], Loss: 30.4659\n",
      "Epoch [420/1000], Loss: 28.6834\n",
      "Epoch [430/1000], Loss: 27.0798\n",
      "Epoch [440/1000], Loss: 25.6399\n",
      "Epoch [450/1000], Loss: 24.3492\n",
      "Epoch [460/1000], Loss: 23.1944\n",
      "Epoch [470/1000], Loss: 22.1629\n",
      "Epoch [480/1000], Loss: 21.2432\n",
      "Epoch [490/1000], Loss: 20.4248\n",
      "Epoch [500/1000], Loss: 19.6977\n",
      "Epoch [510/1000], Loss: 19.0530\n",
      "Epoch [520/1000], Loss: 18.4824\n",
      "Epoch [530/1000], Loss: 17.9783\n",
      "Epoch [540/1000], Loss: 17.5337\n",
      "Epoch [550/1000], Loss: 17.1423\n",
      "Epoch [560/1000], Loss: 16.7985\n",
      "Epoch [570/1000], Loss: 16.4969\n",
      "Epoch [580/1000], Loss: 16.2329\n",
      "Epoch [590/1000], Loss: 16.0022\n",
      "Epoch [600/1000], Loss: 15.8009\n",
      "Epoch [610/1000], Loss: 15.6257\n",
      "Epoch [620/1000], Loss: 15.4733\n",
      "Epoch [630/1000], Loss: 15.3412\n",
      "Epoch [640/1000], Loss: 15.2267\n",
      "Epoch [650/1000], Loss: 15.1277\n",
      "Epoch [660/1000], Loss: 15.0423\n",
      "Epoch [670/1000], Loss: 14.9687\n",
      "Epoch [680/1000], Loss: 14.9053\n",
      "Epoch [690/1000], Loss: 14.8509\n",
      "Epoch [700/1000], Loss: 14.8042\n",
      "Epoch [710/1000], Loss: 14.7642\n",
      "Epoch [720/1000], Loss: 14.7300\n",
      "Epoch [730/1000], Loss: 14.7008\n",
      "Epoch [740/1000], Loss: 14.6758\n",
      "Epoch [750/1000], Loss: 14.6546\n",
      "Epoch [760/1000], Loss: 14.6364\n",
      "Epoch [770/1000], Loss: 14.6210\n",
      "Epoch [780/1000], Loss: 14.6079\n",
      "Epoch [790/1000], Loss: 14.5967\n",
      "Epoch [800/1000], Loss: 14.5872\n",
      "Epoch [810/1000], Loss: 14.5791\n",
      "Epoch [820/1000], Loss: 14.5721\n",
      "Epoch [830/1000], Loss: 14.5662\n",
      "Epoch [840/1000], Loss: 14.5611\n",
      "Epoch [850/1000], Loss: 14.5568\n",
      "Epoch [860/1000], Loss: 14.5530\n",
      "Epoch [870/1000], Loss: 14.5498\n",
      "Epoch [880/1000], Loss: 14.5470\n",
      "Epoch [890/1000], Loss: 14.5445\n",
      "Epoch [900/1000], Loss: 14.5423\n",
      "Epoch [910/1000], Loss: 14.5404\n",
      "Epoch [920/1000], Loss: 14.5387\n",
      "Epoch [930/1000], Loss: 14.5371\n",
      "Epoch [940/1000], Loss: 14.5357\n",
      "Epoch [950/1000], Loss: 14.5344\n",
      "Epoch [960/1000], Loss: 14.5332\n",
      "Epoch [970/1000], Loss: 14.5321\n",
      "Epoch [980/1000], Loss: 14.5310\n",
      "Epoch [990/1000], Loss: 14.5300\n",
      "Epoch [1000/1000], Loss: 14.5291\n",
      "Predicted days_remaining for parent_id 14: [26.92854118347168, 27.84023666381836, 27.840343475341797, 27.84096336364746, 27.839664459228516, 27.839645385742188, 27.84103775024414, 27.840986251831055]\n",
      "Training for parent_id 15...\n",
      "Epoch [10/1000], Loss: 767.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 676.1211\n",
      "Epoch [30/1000], Loss: 605.3524\n",
      "Epoch [40/1000], Loss: 558.0620\n",
      "Epoch [50/1000], Loss: 520.0899\n",
      "Epoch [60/1000], Loss: 486.0363\n",
      "Epoch [70/1000], Loss: 454.4067\n",
      "Epoch [80/1000], Loss: 424.8464\n",
      "Epoch [90/1000], Loss: 397.1241\n",
      "Epoch [100/1000], Loss: 371.0561\n",
      "Epoch [110/1000], Loss: 346.5112\n",
      "Epoch [120/1000], Loss: 323.3976\n",
      "Epoch [130/1000], Loss: 301.6460\n",
      "Epoch [140/1000], Loss: 281.1956\n",
      "Epoch [150/1000], Loss: 261.9857\n",
      "Epoch [160/1000], Loss: 243.9552\n",
      "Epoch [170/1000], Loss: 227.0434\n",
      "Epoch [180/1000], Loss: 211.1923\n",
      "Epoch [190/1000], Loss: 196.3462\n",
      "Epoch [200/1000], Loss: 182.4525\n",
      "Epoch [210/1000], Loss: 169.4607\n",
      "Epoch [220/1000], Loss: 157.3230\n",
      "Epoch [230/1000], Loss: 145.9935\n",
      "Epoch [240/1000], Loss: 135.4288\n",
      "Epoch [250/1000], Loss: 125.5871\n",
      "Epoch [260/1000], Loss: 116.4285\n",
      "Epoch [270/1000], Loss: 107.9150\n",
      "Epoch [280/1000], Loss: 100.0100\n",
      "Epoch [290/1000], Loss: 92.6786\n",
      "Epoch [300/1000], Loss: 85.8872\n",
      "Epoch [310/1000], Loss: 79.6039\n",
      "Epoch [320/1000], Loss: 73.7979\n",
      "Epoch [330/1000], Loss: 68.4399\n",
      "Epoch [340/1000], Loss: 63.5019\n",
      "Epoch [350/1000], Loss: 58.9570\n",
      "Epoch [360/1000], Loss: 54.7797\n",
      "Epoch [370/1000], Loss: 50.9455\n",
      "Epoch [380/1000], Loss: 47.4314\n",
      "Epoch [390/1000], Loss: 44.2151\n",
      "Epoch [400/1000], Loss: 41.2758\n",
      "Epoch [410/1000], Loss: 38.5934\n",
      "Epoch [420/1000], Loss: 36.1493\n",
      "Epoch [430/1000], Loss: 33.9256\n",
      "Epoch [440/1000], Loss: 31.9054\n",
      "Epoch [450/1000], Loss: 30.0731\n",
      "Epoch [460/1000], Loss: 28.4136\n",
      "Epoch [470/1000], Loss: 26.9131\n",
      "Epoch [480/1000], Loss: 25.5584\n",
      "Epoch [490/1000], Loss: 24.3372\n",
      "Epoch [500/1000], Loss: 23.2383\n",
      "Epoch [510/1000], Loss: 22.2509\n",
      "Epoch [520/1000], Loss: 21.3651\n",
      "Epoch [530/1000], Loss: 20.5717\n",
      "Epoch [540/1000], Loss: 19.8623\n",
      "Epoch [550/1000], Loss: 19.2290\n",
      "Epoch [560/1000], Loss: 18.6645\n",
      "Epoch [570/1000], Loss: 18.1623\n",
      "Epoch [580/1000], Loss: 17.7160\n",
      "Epoch [590/1000], Loss: 17.3203\n",
      "Epoch [600/1000], Loss: 16.9699\n",
      "Epoch [610/1000], Loss: 16.6601\n",
      "Epoch [620/1000], Loss: 16.3866\n",
      "Epoch [630/1000], Loss: 16.1457\n",
      "Epoch [640/1000], Loss: 15.9338\n",
      "Epoch [650/1000], Loss: 15.7476\n",
      "Epoch [660/1000], Loss: 15.5844\n",
      "Epoch [670/1000], Loss: 15.4415\n",
      "Epoch [680/1000], Loss: 15.3166\n",
      "Epoch [690/1000], Loss: 15.2076\n",
      "Epoch [700/1000], Loss: 15.1127\n",
      "Epoch [710/1000], Loss: 15.0301\n",
      "Epoch [720/1000], Loss: 14.9583\n",
      "Epoch [730/1000], Loss: 14.8961\n",
      "Epoch [740/1000], Loss: 14.8422\n",
      "Epoch [750/1000], Loss: 14.7956\n",
      "Epoch [760/1000], Loss: 14.7554\n",
      "Epoch [770/1000], Loss: 14.7207\n",
      "Epoch [780/1000], Loss: 14.6908\n",
      "Epoch [790/1000], Loss: 14.6651\n",
      "Epoch [800/1000], Loss: 14.6431\n",
      "Epoch [810/1000], Loss: 14.6241\n",
      "Epoch [820/1000], Loss: 14.6079\n",
      "Epoch [830/1000], Loss: 14.5939\n",
      "Epoch [840/1000], Loss: 14.5820\n",
      "Epoch [850/1000], Loss: 14.5718\n",
      "Epoch [860/1000], Loss: 14.5631\n",
      "Epoch [870/1000], Loss: 14.5556\n",
      "Epoch [880/1000], Loss: 14.5492\n",
      "Epoch [890/1000], Loss: 14.5438\n",
      "Epoch [900/1000], Loss: 14.5391\n",
      "Epoch [910/1000], Loss: 14.5350\n",
      "Epoch [920/1000], Loss: 14.5316\n",
      "Epoch [930/1000], Loss: 14.5286\n",
      "Epoch [940/1000], Loss: 14.5260\n",
      "Epoch [950/1000], Loss: 14.5237\n",
      "Epoch [960/1000], Loss: 14.5218\n",
      "Epoch [970/1000], Loss: 14.5201\n",
      "Epoch [980/1000], Loss: 14.5185\n",
      "Epoch [990/1000], Loss: 14.5172\n",
      "Epoch [1000/1000], Loss: 14.5160\n",
      "Predicted days_remaining for parent_id 15: [27.970333099365234, 28.802593231201172, 28.80251693725586, 28.801904678344727, 28.801788330078125, 28.801807403564453, 28.802457809448242, 28.800579071044922]\n",
      "Training for parent_id 25...\n",
      "Epoch [10/1000], Loss: 4263.5513\n",
      "Epoch [20/1000], Loss: 4048.7822\n",
      "Epoch [30/1000], Loss: 3885.5303\n",
      "Epoch [40/1000], Loss: 3767.3652\n",
      "Epoch [50/1000], Loss: 3666.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000], Loss: 3572.8113\n",
      "Epoch [70/1000], Loss: 3483.1030\n",
      "Epoch [80/1000], Loss: 3396.0322\n",
      "Epoch [90/1000], Loss: 3311.4038\n",
      "Epoch [100/1000], Loss: 3229.2295\n",
      "Epoch [110/1000], Loss: 3149.3613\n",
      "Epoch [120/1000], Loss: 3071.6584\n",
      "Epoch [130/1000], Loss: 2996.0159\n",
      "Epoch [140/1000], Loss: 2922.3416\n",
      "Epoch [150/1000], Loss: 2850.5417\n",
      "Epoch [160/1000], Loss: 2780.5212\n",
      "Epoch [170/1000], Loss: 2712.1912\n",
      "Epoch [180/1000], Loss: 2645.4722\n",
      "Epoch [190/1000], Loss: 2580.2944\n",
      "Epoch [200/1000], Loss: 2516.5974\n",
      "Epoch [210/1000], Loss: 2454.3264\n",
      "Epoch [220/1000], Loss: 2393.4326\n",
      "Epoch [230/1000], Loss: 2333.8721\n",
      "Epoch [240/1000], Loss: 2275.6057\n",
      "Epoch [250/1000], Loss: 2218.5972\n",
      "Epoch [260/1000], Loss: 2162.8127\n",
      "Epoch [270/1000], Loss: 2108.2219\n",
      "Epoch [280/1000], Loss: 2054.7961\n",
      "Epoch [290/1000], Loss: 2002.5081\n",
      "Epoch [300/1000], Loss: 1951.3329\n",
      "Epoch [310/1000], Loss: 1901.2462\n",
      "Epoch [320/1000], Loss: 1852.2256\n",
      "Epoch [330/1000], Loss: 1804.2496\n",
      "Epoch [340/1000], Loss: 1757.2975\n",
      "Epoch [350/1000], Loss: 1711.3491\n",
      "Epoch [360/1000], Loss: 1666.3861\n",
      "Epoch [370/1000], Loss: 1622.3900\n",
      "Epoch [380/1000], Loss: 1579.3424\n",
      "Epoch [390/1000], Loss: 1537.2271\n",
      "Epoch [400/1000], Loss: 1496.0267\n",
      "Epoch [410/1000], Loss: 1455.7251\n",
      "Epoch [420/1000], Loss: 1416.3066\n",
      "Epoch [430/1000], Loss: 1377.7561\n",
      "Epoch [440/1000], Loss: 1340.0582\n",
      "Epoch [450/1000], Loss: 1303.1981\n",
      "Epoch [460/1000], Loss: 1267.1616\n",
      "Epoch [470/1000], Loss: 1231.9346\n",
      "Epoch [480/1000], Loss: 1197.5029\n",
      "Epoch [490/1000], Loss: 1163.8533\n",
      "Epoch [500/1000], Loss: 1130.9720\n",
      "Epoch [510/1000], Loss: 1098.8462\n",
      "Epoch [520/1000], Loss: 1067.4625\n",
      "Epoch [530/1000], Loss: 1036.8086\n",
      "Epoch [540/1000], Loss: 1006.8715\n",
      "Epoch [550/1000], Loss: 977.6390\n",
      "Epoch [560/1000], Loss: 949.0989\n",
      "Epoch [570/1000], Loss: 921.2389\n",
      "Epoch [580/1000], Loss: 894.0475\n",
      "Epoch [590/1000], Loss: 867.5127\n",
      "Epoch [600/1000], Loss: 841.6229\n",
      "Epoch [610/1000], Loss: 816.3662\n",
      "Epoch [620/1000], Loss: 791.7320\n",
      "Epoch [630/1000], Loss: 767.7086\n",
      "Epoch [640/1000], Loss: 744.2852\n",
      "Epoch [650/1000], Loss: 721.4507\n",
      "Epoch [660/1000], Loss: 699.1944\n",
      "Epoch [670/1000], Loss: 677.5058\n",
      "Epoch [680/1000], Loss: 656.3738\n",
      "Epoch [690/1000], Loss: 635.7881\n",
      "Epoch [700/1000], Loss: 615.7387\n",
      "Epoch [710/1000], Loss: 596.2149\n",
      "Epoch [720/1000], Loss: 577.2070\n",
      "Epoch [730/1000], Loss: 558.7050\n",
      "Epoch [740/1000], Loss: 540.6988\n",
      "Epoch [750/1000], Loss: 523.1787\n",
      "Epoch [760/1000], Loss: 506.1353\n",
      "Epoch [770/1000], Loss: 489.5583\n",
      "Epoch [780/1000], Loss: 473.4391\n",
      "Epoch [790/1000], Loss: 457.7680\n",
      "Epoch [800/1000], Loss: 442.5359\n",
      "Epoch [810/1000], Loss: 427.7335\n",
      "Epoch [820/1000], Loss: 413.3517\n",
      "Epoch [830/1000], Loss: 399.3821\n",
      "Epoch [840/1000], Loss: 385.8156\n",
      "Epoch [850/1000], Loss: 372.6435\n",
      "Epoch [860/1000], Loss: 359.8572\n",
      "Epoch [870/1000], Loss: 347.4482\n",
      "Epoch [880/1000], Loss: 335.4083\n",
      "Epoch [890/1000], Loss: 323.7292\n",
      "Epoch [900/1000], Loss: 312.4026\n",
      "Epoch [910/1000], Loss: 301.4206\n",
      "Epoch [920/1000], Loss: 290.7752\n",
      "Epoch [930/1000], Loss: 280.4587\n",
      "Epoch [940/1000], Loss: 270.4633\n",
      "Epoch [950/1000], Loss: 260.7814\n",
      "Epoch [960/1000], Loss: 251.4053\n",
      "Epoch [970/1000], Loss: 242.3280\n",
      "Epoch [980/1000], Loss: 233.5420\n",
      "Epoch [990/1000], Loss: 225.0400\n",
      "Epoch [1000/1000], Loss: 216.8150\n",
      "Predicted days_remaining for parent_id 25: [52.42847442626953, 52.57070541381836, 52.57072830200195, 52.57075881958008, 52.56992721557617, 52.57029342651367, 52.56940841674805, 52.570072174072266]\n",
      "Training for parent_id 40...\n",
      "Epoch [10/1000], Loss: 189.4704\n",
      "Epoch [20/1000], Loss: 144.5391\n",
      "Epoch [30/1000], Loss: 116.0238\n",
      "Epoch [40/1000], Loss: 98.1064\n",
      "Epoch [50/1000], Loss: 84.3143\n",
      "Epoch [60/1000], Loss: 72.8414\n",
      "Epoch [70/1000], Loss: 63.1076\n",
      "Epoch [80/1000], Loss: 54.8161\n",
      "Epoch [90/1000], Loss: 47.7606\n",
      "Epoch [100/1000], Loss: 41.7783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 36.7317\n",
      "Epoch [120/1000], Loss: 32.5002\n",
      "Epoch [130/1000], Loss: 28.9760\n",
      "Epoch [140/1000], Loss: 26.0619\n",
      "Epoch [150/1000], Loss: 23.6706\n",
      "Epoch [160/1000], Loss: 21.7236\n",
      "Epoch [170/1000], Loss: 20.1513\n",
      "Epoch [180/1000], Loss: 18.8921\n",
      "Epoch [190/1000], Loss: 17.8922\n",
      "Epoch [200/1000], Loss: 17.1048\n",
      "Epoch [210/1000], Loss: 16.4902\n",
      "Epoch [220/1000], Loss: 16.0145\n",
      "Epoch [230/1000], Loss: 15.6495\n",
      "Epoch [240/1000], Loss: 15.3717\n",
      "Epoch [250/1000], Loss: 15.1619\n",
      "Epoch [260/1000], Loss: 15.0047\n",
      "Epoch [270/1000], Loss: 14.8877\n",
      "Epoch [280/1000], Loss: 14.8012\n",
      "Epoch [290/1000], Loss: 14.7376\n",
      "Epoch [300/1000], Loss: 14.6909\n",
      "Epoch [310/1000], Loss: 14.6567\n",
      "Epoch [320/1000], Loss: 14.6315\n",
      "Epoch [330/1000], Loss: 14.6130\n",
      "Epoch [340/1000], Loss: 14.5991\n",
      "Epoch [350/1000], Loss: 14.5886\n",
      "Epoch [360/1000], Loss: 14.5805\n",
      "Epoch [370/1000], Loss: 14.5741\n",
      "Epoch [380/1000], Loss: 14.5688\n",
      "Epoch [390/1000], Loss: 14.5643\n",
      "Epoch [400/1000], Loss: 14.5604\n",
      "Epoch [410/1000], Loss: 14.5570\n",
      "Epoch [420/1000], Loss: 14.5538\n",
      "Epoch [430/1000], Loss: 14.5509\n",
      "Epoch [440/1000], Loss: 14.5481\n",
      "Epoch [450/1000], Loss: 14.5455\n",
      "Epoch [460/1000], Loss: 14.5430\n",
      "Epoch [470/1000], Loss: 14.5406\n",
      "Epoch [480/1000], Loss: 14.5383\n",
      "Epoch [490/1000], Loss: 14.5360\n",
      "Epoch [500/1000], Loss: 14.5339\n",
      "Epoch [510/1000], Loss: 14.5318\n",
      "Epoch [520/1000], Loss: 14.5298\n",
      "Epoch [530/1000], Loss: 14.5278\n",
      "Epoch [540/1000], Loss: 14.5259\n",
      "Epoch [550/1000], Loss: 14.5240\n",
      "Epoch [560/1000], Loss: 14.5222\n",
      "Epoch [570/1000], Loss: 14.5205\n",
      "Epoch [580/1000], Loss: 14.5188\n",
      "Epoch [590/1000], Loss: 14.5172\n",
      "Epoch [600/1000], Loss: 14.5156\n",
      "Epoch [610/1000], Loss: 14.5140\n",
      "Epoch [620/1000], Loss: 14.5125\n",
      "Epoch [630/1000], Loss: 14.5111\n",
      "Epoch [640/1000], Loss: 14.5096\n",
      "Epoch [650/1000], Loss: 14.5083\n",
      "Epoch [660/1000], Loss: 14.5069\n",
      "Epoch [670/1000], Loss: 14.5056\n",
      "Epoch [680/1000], Loss: 14.5043\n",
      "Epoch [690/1000], Loss: 14.5031\n",
      "Epoch [700/1000], Loss: 14.5019\n",
      "Epoch [710/1000], Loss: 14.5007\n",
      "Epoch [720/1000], Loss: 14.4996\n",
      "Epoch [730/1000], Loss: 14.4985\n",
      "Epoch [740/1000], Loss: 14.4974\n",
      "Epoch [750/1000], Loss: 14.4963\n",
      "Epoch [760/1000], Loss: 14.4953\n",
      "Epoch [770/1000], Loss: 14.4943\n",
      "Epoch [780/1000], Loss: 14.4933\n",
      "Epoch [790/1000], Loss: 14.4924\n",
      "Epoch [800/1000], Loss: 14.4915\n",
      "Epoch [810/1000], Loss: 14.4906\n",
      "Epoch [820/1000], Loss: 14.4897\n",
      "Epoch [830/1000], Loss: 14.4888\n",
      "Epoch [840/1000], Loss: 14.4880\n",
      "Epoch [850/1000], Loss: 14.4872\n",
      "Epoch [860/1000], Loss: 14.4863\n",
      "Epoch [870/1000], Loss: 14.4856\n",
      "Epoch [880/1000], Loss: 14.4848\n",
      "Epoch [890/1000], Loss: 14.4841\n",
      "Epoch [900/1000], Loss: 14.4833\n",
      "Epoch [910/1000], Loss: 14.4826\n",
      "Epoch [920/1000], Loss: 14.4819\n",
      "Epoch [930/1000], Loss: 14.4812\n",
      "Epoch [940/1000], Loss: 14.4806\n",
      "Epoch [950/1000], Loss: 14.4799\n",
      "Epoch [960/1000], Loss: 14.4793\n",
      "Epoch [970/1000], Loss: 14.4787\n",
      "Epoch [980/1000], Loss: 14.4780\n",
      "Epoch [990/1000], Loss: 14.4774\n",
      "Epoch [1000/1000], Loss: 14.4769\n",
      "Predicted days_remaining for parent_id 40: [14.224599838256836, 14.82618522644043, 14.824676513671875, 14.824878692626953, 14.824278831481934, 14.823029518127441, 14.822628021240234, 14.822625160217285]\n",
      "Training for parent_id 41...\n",
      "Epoch [10/1000], Loss: 996.9018\n",
      "Epoch [20/1000], Loss: 888.6733\n",
      "Epoch [30/1000], Loss: 805.1011\n",
      "Epoch [40/1000], Loss: 747.2094\n",
      "Epoch [50/1000], Loss: 701.5532\n",
      "Epoch [60/1000], Loss: 660.9094\n",
      "Epoch [70/1000], Loss: 623.2871\n",
      "Epoch [80/1000], Loss: 587.9724\n",
      "Epoch [90/1000], Loss: 554.7436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 523.4029\n",
      "Epoch [110/1000], Loss: 493.7919\n",
      "Epoch [120/1000], Loss: 465.7792\n",
      "Epoch [130/1000], Loss: 439.2537\n",
      "Epoch [140/1000], Loss: 414.1205\n",
      "Epoch [150/1000], Loss: 390.2974\n",
      "Epoch [160/1000], Loss: 367.7114\n",
      "Epoch [170/1000], Loss: 346.2975\n",
      "Epoch [180/1000], Loss: 325.9965\n",
      "Epoch [190/1000], Loss: 306.7544\n",
      "Epoch [200/1000], Loss: 288.5210\n",
      "Epoch [210/1000], Loss: 271.2498\n",
      "Epoch [220/1000], Loss: 254.8970\n",
      "Epoch [230/1000], Loss: 239.4213\n",
      "Epoch [240/1000], Loss: 224.7838\n",
      "Epoch [250/1000], Loss: 210.9472\n",
      "Epoch [260/1000], Loss: 197.8760\n",
      "Epoch [270/1000], Loss: 185.5363\n",
      "Epoch [280/1000], Loss: 173.8952\n",
      "Epoch [290/1000], Loss: 162.9214\n",
      "Epoch [300/1000], Loss: 152.5848\n",
      "Epoch [310/1000], Loss: 142.8561\n",
      "Epoch [320/1000], Loss: 133.7070\n",
      "Epoch [330/1000], Loss: 125.1105\n",
      "Epoch [340/1000], Loss: 117.0402\n",
      "Epoch [350/1000], Loss: 109.4706\n",
      "Epoch [360/1000], Loss: 102.3772\n",
      "Epoch [370/1000], Loss: 95.7364\n",
      "Epoch [380/1000], Loss: 89.5250\n",
      "Epoch [390/1000], Loss: 83.7212\n",
      "Epoch [400/1000], Loss: 78.3033\n",
      "Epoch [410/1000], Loss: 73.2510\n",
      "Epoch [420/1000], Loss: 68.5442\n",
      "Epoch [430/1000], Loss: 64.1640\n",
      "Epoch [440/1000], Loss: 60.0919\n",
      "Epoch [450/1000], Loss: 56.3103\n",
      "Epoch [460/1000], Loss: 52.8022\n",
      "Epoch [470/1000], Loss: 49.5514\n",
      "Epoch [480/1000], Loss: 46.5423\n",
      "Epoch [490/1000], Loss: 43.7601\n",
      "Epoch [500/1000], Loss: 41.1904\n",
      "Epoch [510/1000], Loss: 38.8199\n",
      "Epoch [520/1000], Loss: 36.6354\n",
      "Epoch [530/1000], Loss: 34.6247\n",
      "Epoch [540/1000], Loss: 32.7762\n",
      "Epoch [550/1000], Loss: 31.0787\n",
      "Epoch [560/1000], Loss: 29.5218\n",
      "Epoch [570/1000], Loss: 28.0954\n",
      "Epoch [580/1000], Loss: 26.7901\n",
      "Epoch [590/1000], Loss: 25.5972\n",
      "Epoch [600/1000], Loss: 24.5082\n",
      "Epoch [610/1000], Loss: 23.5154\n",
      "Epoch [620/1000], Loss: 22.6112\n",
      "Epoch [630/1000], Loss: 21.7888\n",
      "Epoch [640/1000], Loss: 21.0417\n",
      "Epoch [650/1000], Loss: 20.3638\n",
      "Epoch [660/1000], Loss: 19.7496\n",
      "Epoch [670/1000], Loss: 19.1936\n",
      "Epoch [680/1000], Loss: 18.6910\n",
      "Epoch [690/1000], Loss: 18.2372\n",
      "Epoch [700/1000], Loss: 17.8281\n",
      "Epoch [710/1000], Loss: 17.4596\n",
      "Epoch [720/1000], Loss: 17.1281\n",
      "Epoch [730/1000], Loss: 16.8304\n",
      "Epoch [740/1000], Loss: 16.5633\n",
      "Epoch [750/1000], Loss: 16.3239\n",
      "Epoch [760/1000], Loss: 16.1097\n",
      "Epoch [770/1000], Loss: 15.9182\n",
      "Epoch [780/1000], Loss: 15.7473\n",
      "Epoch [790/1000], Loss: 15.5949\n",
      "Epoch [800/1000], Loss: 15.4592\n",
      "Epoch [810/1000], Loss: 15.3386\n",
      "Epoch [820/1000], Loss: 15.2314\n",
      "Epoch [830/1000], Loss: 15.1364\n",
      "Epoch [840/1000], Loss: 15.0522\n",
      "Epoch [850/1000], Loss: 14.9777\n",
      "Epoch [860/1000], Loss: 14.9119\n",
      "Epoch [870/1000], Loss: 14.8538\n",
      "Epoch [880/1000], Loss: 14.8027\n",
      "Epoch [890/1000], Loss: 14.7576\n",
      "Epoch [900/1000], Loss: 14.7180\n",
      "Epoch [910/1000], Loss: 14.6833\n",
      "Epoch [920/1000], Loss: 14.6528\n",
      "Epoch [930/1000], Loss: 14.6261\n",
      "Epoch [940/1000], Loss: 14.6028\n",
      "Epoch [950/1000], Loss: 14.5824\n",
      "Epoch [960/1000], Loss: 14.5646\n",
      "Epoch [970/1000], Loss: 14.5492\n",
      "Epoch [980/1000], Loss: 14.5357\n",
      "Epoch [990/1000], Loss: 14.5240\n",
      "Epoch [1000/1000], Loss: 14.5138\n",
      "Predicted days_remaining for parent_id 41: [32.18227767944336, 32.54976272583008, 32.55017852783203, 32.55059814453125, 32.54960632324219, 32.54982376098633, 32.549560546875, 32.54833221435547]\n",
      "Training for parent_id 50...\n",
      "Epoch [10/1000], Loss: 989.2930\n",
      "Epoch [20/1000], Loss: 886.7352\n",
      "Epoch [30/1000], Loss: 806.1134\n",
      "Epoch [40/1000], Loss: 751.7873\n",
      "Epoch [50/1000], Loss: 706.9933\n",
      "Epoch [60/1000], Loss: 666.8043\n",
      "Epoch [70/1000], Loss: 629.1690\n",
      "Epoch [80/1000], Loss: 593.4716\n",
      "Epoch [90/1000], Loss: 559.7084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 527.7683\n",
      "Epoch [110/1000], Loss: 497.5295\n",
      "Epoch [120/1000], Loss: 468.8917\n",
      "Epoch [130/1000], Loss: 441.7709\n",
      "Epoch [140/1000], Loss: 416.0905\n",
      "Epoch [150/1000], Loss: 391.7759\n",
      "Epoch [160/1000], Loss: 368.7550\n",
      "Epoch [170/1000], Loss: 346.9596\n",
      "Epoch [180/1000], Loss: 326.3259\n",
      "Epoch [190/1000], Loss: 306.7952\n",
      "Epoch [200/1000], Loss: 288.3127\n",
      "Epoch [210/1000], Loss: 270.8274\n",
      "Epoch [220/1000], Loss: 254.2919\n",
      "Epoch [230/1000], Loss: 238.6616\n",
      "Epoch [240/1000], Loss: 223.8942\n",
      "Epoch [250/1000], Loss: 209.9500\n",
      "Epoch [260/1000], Loss: 196.7908\n",
      "Epoch [270/1000], Loss: 184.3808\n",
      "Epoch [280/1000], Loss: 172.6851\n",
      "Epoch [290/1000], Loss: 161.6707\n",
      "Epoch [300/1000], Loss: 151.3057\n",
      "Epoch [310/1000], Loss: 141.5595\n",
      "Epoch [320/1000], Loss: 132.4027\n",
      "Epoch [330/1000], Loss: 123.8069\n",
      "Epoch [340/1000], Loss: 115.7447\n",
      "Epoch [350/1000], Loss: 108.1898\n",
      "Epoch [360/1000], Loss: 101.1167\n",
      "Epoch [370/1000], Loss: 94.5009\n",
      "Epoch [380/1000], Loss: 88.3187\n",
      "Epoch [390/1000], Loss: 82.5474\n",
      "Epoch [400/1000], Loss: 77.1649\n",
      "Epoch [410/1000], Loss: 72.1503\n",
      "Epoch [420/1000], Loss: 67.4831\n",
      "Epoch [430/1000], Loss: 63.1437\n",
      "Epoch [440/1000], Loss: 59.1135\n",
      "Epoch [450/1000], Loss: 55.3744\n",
      "Epoch [460/1000], Loss: 51.9092\n",
      "Epoch [470/1000], Loss: 48.7012\n",
      "Epoch [480/1000], Loss: 45.7347\n",
      "Epoch [490/1000], Loss: 42.9946\n",
      "Epoch [500/1000], Loss: 40.4665\n",
      "Epoch [510/1000], Loss: 38.1366\n",
      "Epoch [520/1000], Loss: 35.9919\n",
      "Epoch [530/1000], Loss: 34.0198\n",
      "Epoch [540/1000], Loss: 32.2088\n",
      "Epoch [550/1000], Loss: 30.5475\n",
      "Epoch [560/1000], Loss: 29.0254\n",
      "Epoch [570/1000], Loss: 27.6325\n",
      "Epoch [580/1000], Loss: 26.3593\n",
      "Epoch [590/1000], Loss: 25.1971\n",
      "Epoch [600/1000], Loss: 24.1373\n",
      "Epoch [610/1000], Loss: 23.1721\n",
      "Epoch [620/1000], Loss: 22.2942\n",
      "Epoch [630/1000], Loss: 21.4967\n",
      "Epoch [640/1000], Loss: 20.7731\n",
      "Epoch [650/1000], Loss: 20.1173\n",
      "Epoch [660/1000], Loss: 19.5238\n",
      "Epoch [670/1000], Loss: 18.9873\n",
      "Epoch [680/1000], Loss: 18.5029\n",
      "Epoch [690/1000], Loss: 18.0661\n",
      "Epoch [700/1000], Loss: 17.6728\n",
      "Epoch [710/1000], Loss: 17.3190\n",
      "Epoch [720/1000], Loss: 17.0012\n",
      "Epoch [730/1000], Loss: 16.7161\n",
      "Epoch [740/1000], Loss: 16.4606\n",
      "Epoch [750/1000], Loss: 16.2321\n",
      "Epoch [760/1000], Loss: 16.0278\n",
      "Epoch [770/1000], Loss: 15.8454\n",
      "Epoch [780/1000], Loss: 15.6829\n",
      "Epoch [790/1000], Loss: 15.5382\n",
      "Epoch [800/1000], Loss: 15.4095\n",
      "Epoch [810/1000], Loss: 15.2952\n",
      "Epoch [820/1000], Loss: 15.1939\n",
      "Epoch [830/1000], Loss: 15.1042\n",
      "Epoch [840/1000], Loss: 15.0248\n",
      "Epoch [850/1000], Loss: 14.9547\n",
      "Epoch [860/1000], Loss: 14.8928\n",
      "Epoch [870/1000], Loss: 14.8383\n",
      "Epoch [880/1000], Loss: 14.7903\n",
      "Epoch [890/1000], Loss: 14.7481\n",
      "Epoch [900/1000], Loss: 14.7111\n",
      "Epoch [910/1000], Loss: 14.6786\n",
      "Epoch [920/1000], Loss: 14.6502\n",
      "Epoch [930/1000], Loss: 14.6254\n",
      "Epoch [940/1000], Loss: 14.6037\n",
      "Epoch [950/1000], Loss: 14.5848\n",
      "Epoch [960/1000], Loss: 14.5683\n",
      "Epoch [970/1000], Loss: 14.5539\n",
      "Epoch [980/1000], Loss: 14.5415\n",
      "Epoch [990/1000], Loss: 14.5307\n",
      "Epoch [1000/1000], Loss: 14.5213\n",
      "Predicted days_remaining for parent_id 50: [32.07183837890625, 32.57901382446289, 32.580074310302734, 32.57957458496094, 32.579261779785156, 32.57963180541992, 32.5789680480957, 32.58012008666992]\n",
      "Training for parent_id 51...\n",
      "Epoch [10/1000], Loss: 151.4334\n",
      "Epoch [20/1000], Loss: 116.8012\n",
      "Epoch [30/1000], Loss: 92.3165\n",
      "Epoch [40/1000], Loss: 76.6638\n",
      "Epoch [50/1000], Loss: 65.0032\n",
      "Epoch [60/1000], Loss: 55.5587\n",
      "Epoch [70/1000], Loss: 47.7362\n",
      "Epoch [80/1000], Loss: 41.2354\n",
      "Epoch [90/1000], Loss: 35.8511\n",
      "Epoch [100/1000], Loss: 31.4200\n",
      "Epoch [110/1000], Loss: 27.8041\n",
      "Epoch [120/1000], Loss: 24.8823\n",
      "Epoch [130/1000], Loss: 22.5464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 20.6994\n",
      "Epoch [150/1000], Loss: 19.2555\n",
      "Epoch [160/1000], Loss: 18.1393\n",
      "Epoch [170/1000], Loss: 17.2861\n",
      "Epoch [180/1000], Loss: 16.6410\n",
      "Epoch [190/1000], Loss: 16.1579\n",
      "Epoch [200/1000], Loss: 15.7995\n",
      "Epoch [210/1000], Loss: 15.5355\n",
      "Epoch [220/1000], Loss: 15.3420\n",
      "Epoch [230/1000], Loss: 15.2005\n",
      "Epoch [240/1000], Loss: 15.0968\n",
      "Epoch [250/1000], Loss: 15.0203\n",
      "Epoch [260/1000], Loss: 14.9631\n",
      "Epoch [270/1000], Loss: 14.9196\n",
      "Epoch [280/1000], Loss: 14.8857\n",
      "Epoch [290/1000], Loss: 14.8584\n",
      "Epoch [300/1000], Loss: 14.8359\n",
      "Epoch [310/1000], Loss: 14.8167\n",
      "Epoch [320/1000], Loss: 14.7999\n",
      "Epoch [330/1000], Loss: 14.7848\n",
      "Epoch [340/1000], Loss: 14.7711\n",
      "Epoch [350/1000], Loss: 14.7585\n",
      "Epoch [360/1000], Loss: 14.7467\n",
      "Epoch [370/1000], Loss: 14.7357\n",
      "Epoch [380/1000], Loss: 14.7253\n",
      "Epoch [390/1000], Loss: 14.7155\n",
      "Epoch [400/1000], Loss: 14.7062\n",
      "Epoch [410/1000], Loss: 14.6973\n",
      "Epoch [420/1000], Loss: 14.6889\n",
      "Epoch [430/1000], Loss: 14.6810\n",
      "Epoch [440/1000], Loss: 14.6734\n",
      "Epoch [450/1000], Loss: 14.6661\n",
      "Epoch [460/1000], Loss: 14.6592\n",
      "Epoch [470/1000], Loss: 14.6526\n",
      "Epoch [480/1000], Loss: 14.6463\n",
      "Epoch [490/1000], Loss: 14.6403\n",
      "Epoch [500/1000], Loss: 14.6346\n",
      "Epoch [510/1000], Loss: 14.6291\n",
      "Epoch [520/1000], Loss: 14.6238\n",
      "Epoch [530/1000], Loss: 14.6187\n",
      "Epoch [540/1000], Loss: 14.6139\n",
      "Epoch [550/1000], Loss: 14.6092\n",
      "Epoch [560/1000], Loss: 14.6047\n",
      "Epoch [570/1000], Loss: 14.6005\n",
      "Epoch [580/1000], Loss: 14.5963\n",
      "Epoch [590/1000], Loss: 14.5924\n",
      "Epoch [600/1000], Loss: 14.5885\n",
      "Epoch [610/1000], Loss: 14.5849\n",
      "Epoch [620/1000], Loss: 14.5813\n",
      "Epoch [630/1000], Loss: 14.5779\n",
      "Epoch [640/1000], Loss: 14.5746\n",
      "Epoch [650/1000], Loss: 14.5715\n",
      "Epoch [660/1000], Loss: 14.5684\n",
      "Epoch [670/1000], Loss: 14.5654\n",
      "Epoch [680/1000], Loss: 14.5626\n",
      "Epoch [690/1000], Loss: 14.5598\n",
      "Epoch [700/1000], Loss: 14.5572\n",
      "Epoch [710/1000], Loss: 14.5546\n",
      "Epoch [720/1000], Loss: 14.5521\n",
      "Epoch [730/1000], Loss: 14.5497\n",
      "Epoch [740/1000], Loss: 14.5473\n",
      "Epoch [750/1000], Loss: 14.5451\n",
      "Epoch [760/1000], Loss: 14.5429\n",
      "Epoch [770/1000], Loss: 14.5408\n",
      "Epoch [780/1000], Loss: 14.5387\n",
      "Epoch [790/1000], Loss: 14.5367\n",
      "Epoch [800/1000], Loss: 14.5348\n",
      "Epoch [810/1000], Loss: 14.5329\n",
      "Epoch [820/1000], Loss: 14.5310\n",
      "Epoch [830/1000], Loss: 14.5293\n",
      "Epoch [840/1000], Loss: 14.5275\n",
      "Epoch [850/1000], Loss: 14.5259\n",
      "Epoch [860/1000], Loss: 14.5242\n",
      "Epoch [870/1000], Loss: 14.5227\n",
      "Epoch [880/1000], Loss: 14.5211\n",
      "Epoch [890/1000], Loss: 14.5196\n",
      "Epoch [900/1000], Loss: 14.5182\n",
      "Epoch [910/1000], Loss: 14.5167\n",
      "Epoch [920/1000], Loss: 14.5154\n",
      "Epoch [930/1000], Loss: 14.5140\n",
      "Epoch [940/1000], Loss: 14.5127\n",
      "Epoch [950/1000], Loss: 14.5114\n",
      "Epoch [960/1000], Loss: 14.5102\n",
      "Epoch [970/1000], Loss: 14.5090\n",
      "Epoch [980/1000], Loss: 14.5078\n",
      "Epoch [990/1000], Loss: 14.5066\n",
      "Epoch [1000/1000], Loss: 14.5055\n",
      "Predicted days_remaining for parent_id 51: [12.05734920501709, 12.843121528625488, 12.846771240234375, 12.845685005187988, 12.845736503601074, 12.845902442932129, 12.843525886535645, 12.844757080078125]\n",
      "Training for parent_id 61...\n",
      "Epoch [10/1000], Loss: 286.5246\n",
      "Epoch [20/1000], Loss: 239.3754\n",
      "Epoch [30/1000], Loss: 204.0793\n",
      "Epoch [40/1000], Loss: 178.8904\n",
      "Epoch [50/1000], Loss: 158.5541\n",
      "Epoch [60/1000], Loss: 140.9302\n",
      "Epoch [70/1000], Loss: 125.2155\n",
      "Epoch [80/1000], Loss: 111.2195\n",
      "Epoch [90/1000], Loss: 98.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 87.7406\n",
      "Epoch [110/1000], Loss: 77.9546\n",
      "Epoch [120/1000], Loss: 69.3012\n",
      "Epoch [130/1000], Loss: 61.6700\n",
      "Epoch [140/1000], Loss: 54.9608\n",
      "Epoch [150/1000], Loss: 49.0816\n",
      "Epoch [160/1000], Loss: 43.9477\n",
      "Epoch [170/1000], Loss: 39.4812\n",
      "Epoch [180/1000], Loss: 35.6104\n",
      "Epoch [190/1000], Loss: 32.2692\n",
      "Epoch [200/1000], Loss: 29.3973\n",
      "Epoch [210/1000], Loss: 26.9393\n",
      "Epoch [220/1000], Loss: 24.8449\n",
      "Epoch [230/1000], Loss: 23.0684\n",
      "Epoch [240/1000], Loss: 21.5684\n",
      "Epoch [250/1000], Loss: 20.3078\n",
      "Epoch [260/1000], Loss: 19.2535\n",
      "Epoch [270/1000], Loss: 18.3758\n",
      "Epoch [280/1000], Loss: 17.6488\n",
      "Epoch [290/1000], Loss: 17.0494\n",
      "Epoch [300/1000], Loss: 16.5577\n",
      "Epoch [310/1000], Loss: 16.1562\n",
      "Epoch [320/1000], Loss: 15.8300\n",
      "Epoch [330/1000], Loss: 15.5661\n",
      "Epoch [340/1000], Loss: 15.3537\n",
      "Epoch [350/1000], Loss: 15.1834\n",
      "Epoch [360/1000], Loss: 15.0476\n",
      "Epoch [370/1000], Loss: 14.9397\n",
      "Epoch [380/1000], Loss: 14.8542\n",
      "Epoch [390/1000], Loss: 14.7868\n",
      "Epoch [400/1000], Loss: 14.7339\n",
      "Epoch [410/1000], Loss: 14.6923\n",
      "Epoch [420/1000], Loss: 14.6598\n",
      "Epoch [430/1000], Loss: 14.6343\n",
      "Epoch [440/1000], Loss: 14.6144\n",
      "Epoch [450/1000], Loss: 14.5988\n",
      "Epoch [460/1000], Loss: 14.5865\n",
      "Epoch [470/1000], Loss: 14.5768\n",
      "Epoch [480/1000], Loss: 14.5690\n",
      "Epoch [490/1000], Loss: 14.5628\n",
      "Epoch [500/1000], Loss: 14.5577\n",
      "Epoch [510/1000], Loss: 14.5534\n",
      "Epoch [520/1000], Loss: 14.5498\n",
      "Epoch [530/1000], Loss: 14.5468\n",
      "Epoch [540/1000], Loss: 14.5441\n",
      "Epoch [550/1000], Loss: 14.5416\n",
      "Epoch [560/1000], Loss: 14.5394\n",
      "Epoch [570/1000], Loss: 14.5374\n",
      "Epoch [580/1000], Loss: 14.5355\n",
      "Epoch [590/1000], Loss: 14.5337\n",
      "Epoch [600/1000], Loss: 14.5320\n",
      "Epoch [610/1000], Loss: 14.5304\n",
      "Epoch [620/1000], Loss: 14.5288\n",
      "Epoch [630/1000], Loss: 14.5273\n",
      "Epoch [640/1000], Loss: 14.5258\n",
      "Epoch [650/1000], Loss: 14.5244\n",
      "Epoch [660/1000], Loss: 14.5230\n",
      "Epoch [670/1000], Loss: 14.5216\n",
      "Epoch [680/1000], Loss: 14.5202\n",
      "Epoch [690/1000], Loss: 14.5189\n",
      "Epoch [700/1000], Loss: 14.5177\n",
      "Epoch [710/1000], Loss: 14.5164\n",
      "Epoch [720/1000], Loss: 14.5152\n",
      "Epoch [730/1000], Loss: 14.5140\n",
      "Epoch [740/1000], Loss: 14.5128\n",
      "Epoch [750/1000], Loss: 14.5117\n",
      "Epoch [760/1000], Loss: 14.5105\n",
      "Epoch [770/1000], Loss: 14.5094\n",
      "Epoch [780/1000], Loss: 14.5083\n",
      "Epoch [790/1000], Loss: 14.5073\n",
      "Epoch [800/1000], Loss: 14.5062\n",
      "Epoch [810/1000], Loss: 14.5052\n",
      "Epoch [820/1000], Loss: 14.5042\n",
      "Epoch [830/1000], Loss: 14.5033\n",
      "Epoch [840/1000], Loss: 14.5023\n",
      "Epoch [850/1000], Loss: 14.5014\n",
      "Epoch [860/1000], Loss: 14.5005\n",
      "Epoch [870/1000], Loss: 14.4996\n",
      "Epoch [880/1000], Loss: 14.4987\n",
      "Epoch [890/1000], Loss: 14.4978\n",
      "Epoch [900/1000], Loss: 14.4970\n",
      "Epoch [910/1000], Loss: 14.4961\n",
      "Epoch [920/1000], Loss: 14.4953\n",
      "Epoch [930/1000], Loss: 14.4945\n",
      "Epoch [940/1000], Loss: 14.4937\n",
      "Epoch [950/1000], Loss: 14.4930\n",
      "Epoch [960/1000], Loss: 14.4922\n",
      "Epoch [970/1000], Loss: 14.4915\n",
      "Epoch [980/1000], Loss: 14.4907\n",
      "Epoch [990/1000], Loss: 14.4900\n",
      "Epoch [1000/1000], Loss: 14.4893\n",
      "Predicted days_remaining for parent_id 61: [17.147966384887695, 17.833940505981445, 17.837570190429688, 17.83527374267578, 17.834482192993164, 17.835527420043945, 17.83986473083496, 17.83470344543457]\n",
      "Training for parent_id 74...\n",
      "Epoch [10/1000], Loss: 88.4713\n",
      "Epoch [20/1000], Loss: 64.7780\n",
      "Epoch [30/1000], Loss: 48.9862\n",
      "Epoch [40/1000], Loss: 39.4812\n",
      "Epoch [50/1000], Loss: 32.7609\n",
      "Epoch [60/1000], Loss: 27.7113\n",
      "Epoch [70/1000], Loss: 23.9243\n",
      "Epoch [80/1000], Loss: 21.1341\n",
      "Epoch [90/1000], Loss: 19.1221\n",
      "Epoch [100/1000], Loss: 17.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 16.7300\n",
      "Epoch [120/1000], Loss: 16.0754\n",
      "Epoch [130/1000], Loss: 15.6450\n",
      "Epoch [140/1000], Loss: 15.3663\n",
      "Epoch [150/1000], Loss: 15.1870\n",
      "Epoch [160/1000], Loss: 15.0707\n",
      "Epoch [170/1000], Loss: 14.9932\n",
      "Epoch [180/1000], Loss: 14.9389\n",
      "Epoch [190/1000], Loss: 14.8986\n",
      "Epoch [200/1000], Loss: 14.8665\n",
      "Epoch [210/1000], Loss: 14.8397\n",
      "Epoch [220/1000], Loss: 14.8163\n",
      "Epoch [230/1000], Loss: 14.7954\n",
      "Epoch [240/1000], Loss: 14.7763\n",
      "Epoch [250/1000], Loss: 14.7589\n",
      "Epoch [260/1000], Loss: 14.7429\n",
      "Epoch [270/1000], Loss: 14.7281\n",
      "Epoch [280/1000], Loss: 14.7144\n",
      "Epoch [290/1000], Loss: 14.7016\n",
      "Epoch [300/1000], Loss: 14.6898\n",
      "Epoch [310/1000], Loss: 14.6788\n",
      "Epoch [320/1000], Loss: 14.6685\n",
      "Epoch [330/1000], Loss: 14.6588\n",
      "Epoch [340/1000], Loss: 14.6498\n",
      "Epoch [350/1000], Loss: 14.6414\n",
      "Epoch [360/1000], Loss: 14.6335\n",
      "Epoch [370/1000], Loss: 14.6260\n",
      "Epoch [380/1000], Loss: 14.6190\n",
      "Epoch [390/1000], Loss: 14.6124\n",
      "Epoch [400/1000], Loss: 14.6061\n",
      "Epoch [410/1000], Loss: 14.6002\n",
      "Epoch [420/1000], Loss: 14.5946\n",
      "Epoch [430/1000], Loss: 14.5893\n",
      "Epoch [440/1000], Loss: 14.5843\n",
      "Epoch [450/1000], Loss: 14.5795\n",
      "Epoch [460/1000], Loss: 14.5750\n",
      "Epoch [470/1000], Loss: 14.5707\n",
      "Epoch [480/1000], Loss: 14.5666\n",
      "Epoch [490/1000], Loss: 14.5627\n",
      "Epoch [500/1000], Loss: 14.5590\n",
      "Epoch [510/1000], Loss: 14.5554\n",
      "Epoch [520/1000], Loss: 14.5520\n",
      "Epoch [530/1000], Loss: 14.5488\n",
      "Epoch [540/1000], Loss: 14.5457\n",
      "Epoch [550/1000], Loss: 14.5427\n",
      "Epoch [560/1000], Loss: 14.5399\n",
      "Epoch [570/1000], Loss: 14.5372\n",
      "Epoch [580/1000], Loss: 14.5345\n",
      "Epoch [590/1000], Loss: 14.5320\n",
      "Epoch [600/1000], Loss: 14.5296\n",
      "Epoch [610/1000], Loss: 14.5273\n",
      "Epoch [620/1000], Loss: 14.5251\n",
      "Epoch [630/1000], Loss: 14.5229\n",
      "Epoch [640/1000], Loss: 14.5209\n",
      "Epoch [650/1000], Loss: 14.5189\n",
      "Epoch [660/1000], Loss: 14.5170\n",
      "Epoch [670/1000], Loss: 14.5151\n",
      "Epoch [680/1000], Loss: 14.5133\n",
      "Epoch [690/1000], Loss: 14.5116\n",
      "Epoch [700/1000], Loss: 14.5100\n",
      "Epoch [710/1000], Loss: 14.5084\n",
      "Epoch [720/1000], Loss: 14.5068\n",
      "Epoch [730/1000], Loss: 14.5053\n",
      "Epoch [740/1000], Loss: 14.5039\n",
      "Epoch [750/1000], Loss: 14.5025\n",
      "Epoch [760/1000], Loss: 14.5011\n",
      "Epoch [770/1000], Loss: 14.4998\n",
      "Epoch [780/1000], Loss: 14.4985\n",
      "Epoch [790/1000], Loss: 14.4973\n",
      "Epoch [800/1000], Loss: 14.4961\n",
      "Epoch [810/1000], Loss: 14.4949\n",
      "Epoch [820/1000], Loss: 14.4938\n",
      "Epoch [830/1000], Loss: 14.4927\n",
      "Epoch [840/1000], Loss: 14.4917\n",
      "Epoch [850/1000], Loss: 14.4906\n",
      "Epoch [860/1000], Loss: 14.4896\n",
      "Epoch [870/1000], Loss: 14.4887\n",
      "Epoch [880/1000], Loss: 14.4877\n",
      "Epoch [890/1000], Loss: 14.4868\n",
      "Epoch [900/1000], Loss: 14.4859\n",
      "Epoch [910/1000], Loss: 14.4851\n",
      "Epoch [920/1000], Loss: 14.4842\n",
      "Epoch [930/1000], Loss: 14.4834\n",
      "Epoch [940/1000], Loss: 14.4826\n",
      "Epoch [950/1000], Loss: 14.4818\n",
      "Epoch [960/1000], Loss: 14.4811\n",
      "Epoch [970/1000], Loss: 14.4803\n",
      "Epoch [980/1000], Loss: 14.4796\n",
      "Epoch [990/1000], Loss: 14.4789\n",
      "Epoch [1000/1000], Loss: 14.4782\n",
      "Predicted days_remaining for parent_id 74: [9.213277816772461, 9.820880889892578, 9.823041915893555, 9.823726654052734, 9.823369979858398, 9.824222564697266, 9.822782516479492, 9.821117401123047]\n",
      "Training for parent_id 80...\n",
      "Epoch [10/1000], Loss: 135.1484\n",
      "Epoch [20/1000], Loss: 104.6578\n",
      "Epoch [30/1000], Loss: 78.6136\n",
      "Epoch [40/1000], Loss: 63.1144\n",
      "Epoch [50/1000], Loss: 52.4222\n",
      "Epoch [60/1000], Loss: 44.1997\n",
      "Epoch [70/1000], Loss: 37.7434\n",
      "Epoch [80/1000], Loss: 32.5485\n",
      "Epoch [90/1000], Loss: 28.3594\n",
      "Epoch [100/1000], Loss: 25.0319\n",
      "Epoch [110/1000], Loss: 22.4357\n",
      "Epoch [120/1000], Loss: 20.4421\n",
      "Epoch [130/1000], Loss: 18.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 17.8095\n",
      "Epoch [150/1000], Loss: 16.9829\n",
      "Epoch [160/1000], Loss: 16.3832\n",
      "Epoch [170/1000], Loss: 15.9531\n",
      "Epoch [180/1000], Loss: 15.6474\n",
      "Epoch [190/1000], Loss: 15.4311\n",
      "Epoch [200/1000], Loss: 15.2781\n",
      "Epoch [210/1000], Loss: 15.1689\n",
      "Epoch [220/1000], Loss: 15.0897\n",
      "Epoch [230/1000], Loss: 15.0307\n",
      "Epoch [240/1000], Loss: 14.9855\n",
      "Epoch [250/1000], Loss: 14.9493\n",
      "Epoch [260/1000], Loss: 14.9194\n",
      "Epoch [270/1000], Loss: 14.8938\n",
      "Epoch [280/1000], Loss: 14.8713\n",
      "Epoch [290/1000], Loss: 14.8511\n",
      "Epoch [300/1000], Loss: 14.8326\n",
      "Epoch [310/1000], Loss: 14.8156\n",
      "Epoch [320/1000], Loss: 14.7998\n",
      "Epoch [330/1000], Loss: 14.7851\n",
      "Epoch [340/1000], Loss: 14.7714\n",
      "Epoch [350/1000], Loss: 14.7584\n",
      "Epoch [360/1000], Loss: 14.7463\n",
      "Epoch [370/1000], Loss: 14.7349\n",
      "Epoch [380/1000], Loss: 14.7241\n",
      "Epoch [390/1000], Loss: 14.7139\n",
      "Epoch [400/1000], Loss: 14.7043\n",
      "Epoch [410/1000], Loss: 14.6952\n",
      "Epoch [420/1000], Loss: 14.6866\n",
      "Epoch [430/1000], Loss: 14.6784\n",
      "Epoch [440/1000], Loss: 14.6706\n",
      "Epoch [450/1000], Loss: 14.6633\n",
      "Epoch [460/1000], Loss: 14.6562\n",
      "Epoch [470/1000], Loss: 14.6495\n",
      "Epoch [480/1000], Loss: 14.6432\n",
      "Epoch [490/1000], Loss: 14.6371\n",
      "Epoch [500/1000], Loss: 14.6313\n",
      "Epoch [510/1000], Loss: 14.6257\n",
      "Epoch [520/1000], Loss: 14.6204\n",
      "Epoch [530/1000], Loss: 14.6154\n",
      "Epoch [540/1000], Loss: 14.6105\n",
      "Epoch [550/1000], Loss: 14.6058\n",
      "Epoch [560/1000], Loss: 14.6014\n",
      "Epoch [570/1000], Loss: 14.5971\n",
      "Epoch [580/1000], Loss: 14.5930\n",
      "Epoch [590/1000], Loss: 14.5890\n",
      "Epoch [600/1000], Loss: 14.5852\n",
      "Epoch [610/1000], Loss: 14.5816\n",
      "Epoch [620/1000], Loss: 14.5781\n",
      "Epoch [630/1000], Loss: 14.5747\n",
      "Epoch [640/1000], Loss: 14.5714\n",
      "Epoch [650/1000], Loss: 14.5683\n",
      "Epoch [660/1000], Loss: 14.5653\n",
      "Epoch [670/1000], Loss: 14.5623\n",
      "Epoch [680/1000], Loss: 14.5595\n",
      "Epoch [690/1000], Loss: 14.5568\n",
      "Epoch [700/1000], Loss: 14.5542\n",
      "Epoch [710/1000], Loss: 14.5516\n",
      "Epoch [720/1000], Loss: 14.5492\n",
      "Epoch [730/1000], Loss: 14.5468\n",
      "Epoch [740/1000], Loss: 14.5445\n",
      "Epoch [750/1000], Loss: 14.5422\n",
      "Epoch [760/1000], Loss: 14.5401\n",
      "Epoch [770/1000], Loss: 14.5380\n",
      "Epoch [780/1000], Loss: 14.5360\n",
      "Epoch [790/1000], Loss: 14.5340\n",
      "Epoch [800/1000], Loss: 14.5321\n",
      "Epoch [810/1000], Loss: 14.5302\n",
      "Epoch [820/1000], Loss: 14.5284\n",
      "Epoch [830/1000], Loss: 14.5267\n",
      "Epoch [840/1000], Loss: 14.5250\n",
      "Epoch [850/1000], Loss: 14.5234\n",
      "Epoch [860/1000], Loss: 14.5218\n",
      "Epoch [870/1000], Loss: 14.5202\n",
      "Epoch [880/1000], Loss: 14.5187\n",
      "Epoch [890/1000], Loss: 14.5172\n",
      "Epoch [900/1000], Loss: 14.5158\n",
      "Epoch [910/1000], Loss: 14.5144\n",
      "Epoch [920/1000], Loss: 14.5131\n",
      "Epoch [930/1000], Loss: 14.5118\n",
      "Epoch [940/1000], Loss: 14.5105\n",
      "Epoch [950/1000], Loss: 14.5092\n",
      "Epoch [960/1000], Loss: 14.5080\n",
      "Epoch [970/1000], Loss: 14.5068\n",
      "Epoch [980/1000], Loss: 14.5057\n",
      "Epoch [990/1000], Loss: 14.5045\n",
      "Epoch [1000/1000], Loss: 14.5034\n",
      "Predicted days_remaining for parent_id 80: [11.067239761352539, 11.839825630187988, 11.84166431427002, 11.84312629699707, 11.844199180603027, 11.84424114227295, 11.843981742858887, 11.843289375305176]\n",
      "Training for parent_id 82...\n",
      "Epoch [10/1000], Loss: 249.2269\n",
      "Epoch [20/1000], Loss: 203.1243\n",
      "Epoch [30/1000], Loss: 169.0972\n",
      "Epoch [40/1000], Loss: 146.0448\n",
      "Epoch [50/1000], Loss: 128.1217\n",
      "Epoch [60/1000], Loss: 112.9027\n",
      "Epoch [70/1000], Loss: 99.6231\n",
      "Epoch [80/1000], Loss: 87.9503\n",
      "Epoch [90/1000], Loss: 77.6778\n",
      "Epoch [100/1000], Loss: 68.6497\n",
      "Epoch [110/1000], Loss: 60.7355\n",
      "Epoch [120/1000], Loss: 53.8203\n",
      "Epoch [130/1000], Loss: 47.8002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 42.5803\n",
      "Epoch [150/1000], Loss: 38.0737\n",
      "Epoch [160/1000], Loss: 34.2006\n",
      "Epoch [170/1000], Loss: 30.8876\n",
      "Epoch [180/1000], Loss: 28.0678\n",
      "Epoch [190/1000], Loss: 25.6800\n",
      "Epoch [200/1000], Loss: 23.6686\n",
      "Epoch [210/1000], Loss: 21.9833\n",
      "Epoch [220/1000], Loss: 20.5791\n",
      "Epoch [230/1000], Loss: 19.4155\n",
      "Epoch [240/1000], Loss: 18.4568\n",
      "Epoch [250/1000], Loss: 17.6714\n",
      "Epoch [260/1000], Loss: 17.0315\n",
      "Epoch [270/1000], Loss: 16.5133\n",
      "Epoch [280/1000], Loss: 16.0959\n",
      "Epoch [290/1000], Loss: 15.7616\n",
      "Epoch [300/1000], Loss: 15.4955\n",
      "Epoch [310/1000], Loss: 15.2846\n",
      "Epoch [320/1000], Loss: 15.1185\n",
      "Epoch [330/1000], Loss: 14.9883\n",
      "Epoch [340/1000], Loss: 14.8867\n",
      "Epoch [350/1000], Loss: 14.8078\n",
      "Epoch [360/1000], Loss: 14.7467\n",
      "Epoch [370/1000], Loss: 14.6997\n",
      "Epoch [380/1000], Loss: 14.6634\n",
      "Epoch [390/1000], Loss: 14.6356\n",
      "Epoch [400/1000], Loss: 14.6142\n",
      "Epoch [410/1000], Loss: 14.5977\n",
      "Epoch [420/1000], Loss: 14.5849\n",
      "Epoch [430/1000], Loss: 14.5750\n",
      "Epoch [440/1000], Loss: 14.5672\n",
      "Epoch [450/1000], Loss: 14.5609\n",
      "Epoch [460/1000], Loss: 14.5559\n",
      "Epoch [470/1000], Loss: 14.5517\n",
      "Epoch [480/1000], Loss: 14.5481\n",
      "Epoch [490/1000], Loss: 14.5451\n",
      "Epoch [500/1000], Loss: 14.5424\n",
      "Epoch [510/1000], Loss: 14.5400\n",
      "Epoch [520/1000], Loss: 14.5378\n",
      "Epoch [530/1000], Loss: 14.5357\n",
      "Epoch [540/1000], Loss: 14.5338\n",
      "Epoch [550/1000], Loss: 14.5319\n",
      "Epoch [560/1000], Loss: 14.5301\n",
      "Epoch [570/1000], Loss: 14.5284\n",
      "Epoch [580/1000], Loss: 14.5268\n",
      "Epoch [590/1000], Loss: 14.5252\n",
      "Epoch [600/1000], Loss: 14.5237\n",
      "Epoch [610/1000], Loss: 14.5221\n",
      "Epoch [620/1000], Loss: 14.5207\n",
      "Epoch [630/1000], Loss: 14.5192\n",
      "Epoch [640/1000], Loss: 14.5178\n",
      "Epoch [650/1000], Loss: 14.5165\n",
      "Epoch [660/1000], Loss: 14.5151\n",
      "Epoch [670/1000], Loss: 14.5138\n",
      "Epoch [680/1000], Loss: 14.5126\n",
      "Epoch [690/1000], Loss: 14.5113\n",
      "Epoch [700/1000], Loss: 14.5101\n",
      "Epoch [710/1000], Loss: 14.5089\n",
      "Epoch [720/1000], Loss: 14.5078\n",
      "Epoch [730/1000], Loss: 14.5066\n",
      "Epoch [740/1000], Loss: 14.5055\n",
      "Epoch [750/1000], Loss: 14.5044\n",
      "Epoch [760/1000], Loss: 14.5034\n",
      "Epoch [770/1000], Loss: 14.5024\n",
      "Epoch [780/1000], Loss: 14.5013\n",
      "Epoch [790/1000], Loss: 14.5003\n",
      "Epoch [800/1000], Loss: 14.4994\n",
      "Epoch [810/1000], Loss: 14.4984\n",
      "Epoch [820/1000], Loss: 14.4975\n",
      "Epoch [830/1000], Loss: 14.4966\n",
      "Epoch [840/1000], Loss: 14.4957\n",
      "Epoch [850/1000], Loss: 14.4948\n",
      "Epoch [860/1000], Loss: 14.4940\n",
      "Epoch [870/1000], Loss: 14.4931\n",
      "Epoch [880/1000], Loss: 14.4923\n",
      "Epoch [890/1000], Loss: 14.4915\n",
      "Epoch [900/1000], Loss: 14.4907\n",
      "Epoch [910/1000], Loss: 14.4900\n",
      "Epoch [920/1000], Loss: 14.4892\n",
      "Epoch [930/1000], Loss: 14.4885\n",
      "Epoch [940/1000], Loss: 14.4878\n",
      "Epoch [950/1000], Loss: 14.4871\n",
      "Epoch [960/1000], Loss: 14.4864\n",
      "Epoch [970/1000], Loss: 14.4857\n",
      "Epoch [980/1000], Loss: 14.4850\n",
      "Epoch [990/1000], Loss: 14.4844\n",
      "Epoch [1000/1000], Loss: 14.4837\n",
      "Predicted days_remaining for parent_id 82: [16.18133544921875, 16.828655242919922, 16.831119537353516, 16.832115173339844, 16.83069610595703, 16.8310604095459, 16.83159065246582, 16.83100700378418]\n",
      "Training for parent_id 88...\n",
      "Epoch [10/1000], Loss: 136.8781\n",
      "Epoch [20/1000], Loss: 107.2242\n",
      "Epoch [30/1000], Loss: 82.2235\n",
      "Epoch [40/1000], Loss: 65.8946\n",
      "Epoch [50/1000], Loss: 54.2729\n",
      "Epoch [60/1000], Loss: 45.4304\n",
      "Epoch [70/1000], Loss: 38.4375\n",
      "Epoch [80/1000], Loss: 32.8735\n",
      "Epoch [90/1000], Loss: 28.4654\n",
      "Epoch [100/1000], Loss: 25.0049\n",
      "Epoch [110/1000], Loss: 22.3201\n",
      "Epoch [120/1000], Loss: 20.2647\n",
      "Epoch [130/1000], Loss: 18.7132\n",
      "Epoch [140/1000], Loss: 17.5591\n",
      "Epoch [150/1000], Loss: 16.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000], Loss: 16.1022\n",
      "Epoch [170/1000], Loss: 15.6671\n",
      "Epoch [180/1000], Loss: 15.3611\n",
      "Epoch [190/1000], Loss: 15.1484\n",
      "Epoch [200/1000], Loss: 15.0017\n",
      "Epoch [210/1000], Loss: 14.9010\n",
      "Epoch [220/1000], Loss: 14.8316\n",
      "Epoch [230/1000], Loss: 14.7834\n",
      "Epoch [240/1000], Loss: 14.7491\n",
      "Epoch [250/1000], Loss: 14.7241\n",
      "Epoch [260/1000], Loss: 14.7051\n",
      "Epoch [270/1000], Loss: 14.6899\n",
      "Epoch [280/1000], Loss: 14.6773\n",
      "Epoch [290/1000], Loss: 14.6664\n",
      "Epoch [300/1000], Loss: 14.6568\n",
      "Epoch [310/1000], Loss: 14.6480\n",
      "Epoch [320/1000], Loss: 14.6398\n",
      "Epoch [330/1000], Loss: 14.6322\n",
      "Epoch [340/1000], Loss: 14.6251\n",
      "Epoch [350/1000], Loss: 14.6184\n",
      "Epoch [360/1000], Loss: 14.6120\n",
      "Epoch [370/1000], Loss: 14.6060\n",
      "Epoch [380/1000], Loss: 14.6003\n",
      "Epoch [390/1000], Loss: 14.5949\n",
      "Epoch [400/1000], Loss: 14.5898\n",
      "Epoch [410/1000], Loss: 14.5849\n",
      "Epoch [420/1000], Loss: 14.5802\n",
      "Epoch [430/1000], Loss: 14.5758\n",
      "Epoch [440/1000], Loss: 14.5716\n",
      "Epoch [450/1000], Loss: 14.5675\n",
      "Epoch [460/1000], Loss: 14.5637\n",
      "Epoch [470/1000], Loss: 14.5600\n",
      "Epoch [480/1000], Loss: 14.5565\n",
      "Epoch [490/1000], Loss: 14.5531\n",
      "Epoch [500/1000], Loss: 14.5499\n",
      "Epoch [510/1000], Loss: 14.5468\n",
      "Epoch [520/1000], Loss: 14.5438\n",
      "Epoch [530/1000], Loss: 14.5410\n",
      "Epoch [540/1000], Loss: 14.5383\n",
      "Epoch [550/1000], Loss: 14.5357\n",
      "Epoch [560/1000], Loss: 14.5332\n",
      "Epoch [570/1000], Loss: 14.5307\n",
      "Epoch [580/1000], Loss: 14.5284\n",
      "Epoch [590/1000], Loss: 14.5262\n",
      "Epoch [600/1000], Loss: 14.5240\n",
      "Epoch [610/1000], Loss: 14.5219\n",
      "Epoch [620/1000], Loss: 14.5199\n",
      "Epoch [630/1000], Loss: 14.5180\n",
      "Epoch [640/1000], Loss: 14.5162\n",
      "Epoch [650/1000], Loss: 14.5144\n",
      "Epoch [660/1000], Loss: 14.5126\n",
      "Epoch [670/1000], Loss: 14.5110\n",
      "Epoch [680/1000], Loss: 14.5093\n",
      "Epoch [690/1000], Loss: 14.5078\n",
      "Epoch [700/1000], Loss: 14.5063\n",
      "Epoch [710/1000], Loss: 14.5048\n",
      "Epoch [720/1000], Loss: 14.5034\n",
      "Epoch [730/1000], Loss: 14.5020\n",
      "Epoch [740/1000], Loss: 14.5007\n",
      "Epoch [750/1000], Loss: 14.4994\n",
      "Epoch [760/1000], Loss: 14.4981\n",
      "Epoch [770/1000], Loss: 14.4969\n",
      "Epoch [780/1000], Loss: 14.4958\n",
      "Epoch [790/1000], Loss: 14.4946\n",
      "Epoch [800/1000], Loss: 14.4935\n",
      "Epoch [810/1000], Loss: 14.4924\n",
      "Epoch [820/1000], Loss: 14.4914\n",
      "Epoch [830/1000], Loss: 14.4904\n",
      "Epoch [840/1000], Loss: 14.4894\n",
      "Epoch [850/1000], Loss: 14.4884\n",
      "Epoch [860/1000], Loss: 14.4875\n",
      "Epoch [870/1000], Loss: 14.4866\n",
      "Epoch [880/1000], Loss: 14.4857\n",
      "Epoch [890/1000], Loss: 14.4849\n",
      "Epoch [900/1000], Loss: 14.4840\n",
      "Epoch [910/1000], Loss: 14.4832\n",
      "Epoch [920/1000], Loss: 14.4824\n",
      "Epoch [930/1000], Loss: 14.4817\n",
      "Epoch [940/1000], Loss: 14.4809\n",
      "Epoch [950/1000], Loss: 14.4802\n",
      "Epoch [960/1000], Loss: 14.4795\n",
      "Epoch [970/1000], Loss: 14.4788\n",
      "Epoch [980/1000], Loss: 14.4781\n",
      "Epoch [990/1000], Loss: 14.4774\n",
      "Epoch [1000/1000], Loss: 14.4768\n",
      "Predicted days_remaining for parent_id 88: [11.224072456359863, 11.81855297088623, 11.824136734008789, 11.824044227600098, 11.82460880279541, 11.823578834533691, 11.823210716247559, 11.821305274963379]\n",
      "Training for parent_id 91...\n",
      "Epoch [10/1000], Loss: 1037.0996\n",
      "Epoch [20/1000], Loss: 925.0197\n",
      "Epoch [30/1000], Loss: 846.7505\n",
      "Epoch [40/1000], Loss: 791.3373\n",
      "Epoch [50/1000], Loss: 745.6807\n",
      "Epoch [60/1000], Loss: 704.3751\n",
      "Epoch [70/1000], Loss: 665.9174\n",
      "Epoch [80/1000], Loss: 629.7634\n",
      "Epoch [90/1000], Loss: 595.6263\n",
      "Epoch [100/1000], Loss: 563.3162\n",
      "Epoch [110/1000], Loss: 532.6895\n",
      "Epoch [120/1000], Loss: 503.6290\n",
      "Epoch [130/1000], Loss: 476.0356\n",
      "Epoch [140/1000], Loss: 449.8227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 424.9144\n",
      "Epoch [160/1000], Loss: 401.2425\n",
      "Epoch [170/1000], Loss: 378.7453\n",
      "Epoch [180/1000], Loss: 357.3664\n",
      "Epoch [190/1000], Loss: 337.0541\n",
      "Epoch [200/1000], Loss: 317.7603\n",
      "Epoch [210/1000], Loss: 299.4400\n",
      "Epoch [220/1000], Loss: 282.0508\n",
      "Epoch [230/1000], Loss: 265.5529\n",
      "Epoch [240/1000], Loss: 249.9085\n",
      "Epoch [250/1000], Loss: 235.0813\n",
      "Epoch [260/1000], Loss: 221.0367\n",
      "Epoch [270/1000], Loss: 207.7419\n",
      "Epoch [280/1000], Loss: 195.1647\n",
      "Epoch [290/1000], Loss: 183.2747\n",
      "Epoch [300/1000], Loss: 172.0422\n",
      "Epoch [310/1000], Loss: 161.4386\n",
      "Epoch [320/1000], Loss: 151.4364\n",
      "Epoch [330/1000], Loss: 142.0088\n",
      "Epoch [340/1000], Loss: 133.1298\n",
      "Epoch [350/1000], Loss: 124.7746\n",
      "Epoch [360/1000], Loss: 116.9188\n",
      "Epoch [370/1000], Loss: 109.5389\n",
      "Epoch [380/1000], Loss: 102.6121\n",
      "Epoch [390/1000], Loss: 96.1165\n",
      "Epoch [400/1000], Loss: 90.0308\n",
      "Epoch [410/1000], Loss: 84.3344\n",
      "Epoch [420/1000], Loss: 79.0074\n",
      "Epoch [430/1000], Loss: 74.0306\n",
      "Epoch [440/1000], Loss: 69.3854\n",
      "Epoch [450/1000], Loss: 65.0541\n",
      "Epoch [460/1000], Loss: 61.0194\n",
      "Epoch [470/1000], Loss: 57.2649\n",
      "Epoch [480/1000], Loss: 53.7744\n",
      "Epoch [490/1000], Loss: 50.5330\n",
      "Epoch [500/1000], Loss: 47.5258\n",
      "Epoch [510/1000], Loss: 44.7389\n",
      "Epoch [520/1000], Loss: 42.1589\n",
      "Epoch [530/1000], Loss: 39.7729\n",
      "Epoch [540/1000], Loss: 37.5688\n",
      "Epoch [550/1000], Loss: 35.5348\n",
      "Epoch [560/1000], Loss: 33.6599\n",
      "Epoch [570/1000], Loss: 31.9336\n",
      "Epoch [580/1000], Loss: 30.3457\n",
      "Epoch [590/1000], Loss: 28.8869\n",
      "Epoch [600/1000], Loss: 27.5482\n",
      "Epoch [610/1000], Loss: 26.3210\n",
      "Epoch [620/1000], Loss: 25.1973\n",
      "Epoch [630/1000], Loss: 24.1695\n",
      "Epoch [640/1000], Loss: 23.2306\n",
      "Epoch [650/1000], Loss: 22.3739\n",
      "Epoch [660/1000], Loss: 21.5929\n",
      "Epoch [670/1000], Loss: 20.8820\n",
      "Epoch [680/1000], Loss: 20.2355\n",
      "Epoch [690/1000], Loss: 19.6482\n",
      "Epoch [700/1000], Loss: 19.1155\n",
      "Epoch [710/1000], Loss: 18.6327\n",
      "Epoch [720/1000], Loss: 18.1957\n",
      "Epoch [730/1000], Loss: 17.8007\n",
      "Epoch [740/1000], Loss: 17.4439\n",
      "Epoch [750/1000], Loss: 17.1222\n",
      "Epoch [760/1000], Loss: 16.8324\n",
      "Epoch [770/1000], Loss: 16.5716\n",
      "Epoch [780/1000], Loss: 16.3373\n",
      "Epoch [790/1000], Loss: 16.1269\n",
      "Epoch [800/1000], Loss: 15.9383\n",
      "Epoch [810/1000], Loss: 15.7694\n",
      "Epoch [820/1000], Loss: 15.6184\n",
      "Epoch [830/1000], Loss: 15.4835\n",
      "Epoch [840/1000], Loss: 15.3631\n",
      "Epoch [850/1000], Loss: 15.2558\n",
      "Epoch [860/1000], Loss: 15.1603\n",
      "Epoch [870/1000], Loss: 15.0754\n",
      "Epoch [880/1000], Loss: 15.0001\n",
      "Epoch [890/1000], Loss: 14.9332\n",
      "Epoch [900/1000], Loss: 14.8740\n",
      "Epoch [910/1000], Loss: 14.8217\n",
      "Epoch [920/1000], Loss: 14.7754\n",
      "Epoch [930/1000], Loss: 14.7346\n",
      "Epoch [940/1000], Loss: 14.6986\n",
      "Epoch [950/1000], Loss: 14.6669\n",
      "Epoch [960/1000], Loss: 14.6390\n",
      "Epoch [970/1000], Loss: 14.6146\n",
      "Epoch [980/1000], Loss: 14.5931\n",
      "Epoch [990/1000], Loss: 14.5743\n",
      "Epoch [1000/1000], Loss: 14.5579\n",
      "Predicted days_remaining for parent_id 91: [33.1134147644043, 33.47138595581055, 33.47108459472656, 33.4717903137207, 33.47060775756836, 33.469905853271484, 33.47023391723633, 33.47074508666992]\n",
      "Training for parent_id 99...\n",
      "Epoch [10/1000], Loss: 97.4441\n",
      "Epoch [20/1000], Loss: 69.7759\n",
      "Epoch [30/1000], Loss: 53.5025\n",
      "Epoch [40/1000], Loss: 43.2081\n",
      "Epoch [50/1000], Loss: 35.9257\n",
      "Epoch [60/1000], Loss: 30.4703\n",
      "Epoch [70/1000], Loss: 26.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000], Loss: 23.1424\n",
      "Epoch [90/1000], Loss: 20.7833\n",
      "Epoch [100/1000], Loss: 19.0601\n",
      "Epoch [110/1000], Loss: 17.8275\n",
      "Epoch [120/1000], Loss: 16.9632\n",
      "Epoch [130/1000], Loss: 16.3677\n",
      "Epoch [140/1000], Loss: 15.9625\n",
      "Epoch [150/1000], Loss: 15.6884\n",
      "Epoch [160/1000], Loss: 15.5019\n",
      "Epoch [170/1000], Loss: 15.3726\n",
      "Epoch [180/1000], Loss: 15.2800\n",
      "Epoch [190/1000], Loss: 15.2105\n",
      "Epoch [200/1000], Loss: 15.1555\n",
      "Epoch [210/1000], Loss: 15.1100\n",
      "Epoch [220/1000], Loss: 15.0708\n",
      "Epoch [230/1000], Loss: 15.0360\n",
      "Epoch [240/1000], Loss: 15.0045\n",
      "Epoch [250/1000], Loss: 14.9758\n",
      "Epoch [260/1000], Loss: 14.9493\n",
      "Epoch [270/1000], Loss: 14.9249\n",
      "Epoch [280/1000], Loss: 14.9022\n",
      "Epoch [290/1000], Loss: 14.8811\n",
      "Epoch [300/1000], Loss: 14.8614\n",
      "Epoch [310/1000], Loss: 14.8431\n",
      "Epoch [320/1000], Loss: 14.8259\n",
      "Epoch [330/1000], Loss: 14.8098\n",
      "Epoch [340/1000], Loss: 14.7948\n",
      "Epoch [350/1000], Loss: 14.7806\n",
      "Epoch [360/1000], Loss: 14.7673\n",
      "Epoch [370/1000], Loss: 14.7548\n",
      "Epoch [380/1000], Loss: 14.7430\n",
      "Epoch [390/1000], Loss: 14.7319\n",
      "Epoch [400/1000], Loss: 14.7214\n",
      "Epoch [410/1000], Loss: 14.7114\n",
      "Epoch [420/1000], Loss: 14.7020\n",
      "Epoch [430/1000], Loss: 14.6931\n",
      "Epoch [440/1000], Loss: 14.6846\n",
      "Epoch [450/1000], Loss: 14.6766\n",
      "Epoch [460/1000], Loss: 14.6689\n",
      "Epoch [470/1000], Loss: 14.6617\n",
      "Epoch [480/1000], Loss: 14.6547\n",
      "Epoch [490/1000], Loss: 14.6481\n",
      "Epoch [500/1000], Loss: 14.6418\n",
      "Epoch [510/1000], Loss: 14.6358\n",
      "Epoch [520/1000], Loss: 14.6301\n",
      "Epoch [530/1000], Loss: 14.6246\n",
      "Epoch [540/1000], Loss: 14.6193\n",
      "Epoch [550/1000], Loss: 14.6143\n",
      "Epoch [560/1000], Loss: 14.6095\n",
      "Epoch [570/1000], Loss: 14.6049\n",
      "Epoch [580/1000], Loss: 14.6004\n",
      "Epoch [590/1000], Loss: 14.5962\n",
      "Epoch [600/1000], Loss: 14.5921\n",
      "Epoch [610/1000], Loss: 14.5882\n",
      "Epoch [620/1000], Loss: 14.5844\n",
      "Epoch [630/1000], Loss: 14.5808\n",
      "Epoch [640/1000], Loss: 14.5773\n",
      "Epoch [650/1000], Loss: 14.5739\n",
      "Epoch [660/1000], Loss: 14.5707\n",
      "Epoch [670/1000], Loss: 14.5676\n",
      "Epoch [680/1000], Loss: 14.5645\n",
      "Epoch [690/1000], Loss: 14.5616\n",
      "Epoch [700/1000], Loss: 14.5588\n",
      "Epoch [710/1000], Loss: 14.5561\n",
      "Epoch [720/1000], Loss: 14.5535\n",
      "Epoch [730/1000], Loss: 14.5510\n",
      "Epoch [740/1000], Loss: 14.5485\n",
      "Epoch [750/1000], Loss: 14.5462\n",
      "Epoch [760/1000], Loss: 14.5439\n",
      "Epoch [770/1000], Loss: 14.5416\n",
      "Epoch [780/1000], Loss: 14.5395\n",
      "Epoch [790/1000], Loss: 14.5374\n",
      "Epoch [800/1000], Loss: 14.5354\n",
      "Epoch [810/1000], Loss: 14.5334\n",
      "Epoch [820/1000], Loss: 14.5315\n",
      "Epoch [830/1000], Loss: 14.5297\n",
      "Epoch [840/1000], Loss: 14.5279\n",
      "Epoch [850/1000], Loss: 14.5262\n",
      "Epoch [860/1000], Loss: 14.5245\n",
      "Epoch [870/1000], Loss: 14.5229\n",
      "Epoch [880/1000], Loss: 14.5213\n",
      "Epoch [890/1000], Loss: 14.5197\n",
      "Epoch [900/1000], Loss: 14.5182\n",
      "Epoch [910/1000], Loss: 14.5168\n",
      "Epoch [920/1000], Loss: 14.5153\n",
      "Epoch [930/1000], Loss: 14.5139\n",
      "Epoch [940/1000], Loss: 14.5126\n",
      "Epoch [950/1000], Loss: 14.5113\n",
      "Epoch [960/1000], Loss: 14.5100\n",
      "Epoch [970/1000], Loss: 14.5088\n",
      "Epoch [980/1000], Loss: 14.5076\n",
      "Epoch [990/1000], Loss: 14.5064\n",
      "Epoch [1000/1000], Loss: 14.5052\n",
      "Predicted days_remaining for parent_id 99: [10.057056427001953, 10.83829116821289, 10.843578338623047, 10.843259811401367, 10.844229698181152, 10.843896865844727, 10.845497131347656, 10.842897415161133]\n",
      "Training for parent_id 103...\n",
      "Epoch [10/1000], Loss: 144.2842\n",
      "Epoch [20/1000], Loss: 107.2767\n",
      "Epoch [30/1000], Loss: 86.0840\n",
      "Epoch [40/1000], Loss: 72.3794\n",
      "Epoch [50/1000], Loss: 61.6501\n",
      "Epoch [60/1000], Loss: 52.7553\n",
      "Epoch [70/1000], Loss: 45.3397\n",
      "Epoch [80/1000], Loss: 39.1838\n",
      "Epoch [90/1000], Loss: 34.1057\n",
      "Epoch [100/1000], Loss: 29.9501\n",
      "Epoch [110/1000], Loss: 26.5815\n",
      "Epoch [120/1000], Loss: 23.8798\n",
      "Epoch [130/1000], Loss: 21.7377\n",
      "Epoch [140/1000], Loss: 20.0594\n",
      "Epoch [150/1000], Loss: 18.7602\n",
      "Epoch [160/1000], Loss: 17.7664\n",
      "Epoch [170/1000], Loss: 17.0148\n",
      "Epoch [180/1000], Loss: 16.4524\n",
      "Epoch [190/1000], Loss: 16.0355\n",
      "Epoch [200/1000], Loss: 15.7287\n",
      "Epoch [210/1000], Loss: 15.5041\n",
      "Epoch [220/1000], Loss: 15.3400\n",
      "Epoch [230/1000], Loss: 15.2198\n",
      "Epoch [240/1000], Loss: 15.1309\n",
      "Epoch [250/1000], Loss: 15.0644\n",
      "Epoch [260/1000], Loss: 15.0135\n",
      "Epoch [270/1000], Loss: 14.9735\n",
      "Epoch [280/1000], Loss: 14.9412\n",
      "Epoch [290/1000], Loss: 14.9143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 14.8911\n",
      "Epoch [310/1000], Loss: 14.8707\n",
      "Epoch [320/1000], Loss: 14.8524\n",
      "Epoch [330/1000], Loss: 14.8357\n",
      "Epoch [340/1000], Loss: 14.8202\n",
      "Epoch [350/1000], Loss: 14.8058\n",
      "Epoch [360/1000], Loss: 14.7923\n",
      "Epoch [370/1000], Loss: 14.7796\n",
      "Epoch [380/1000], Loss: 14.7676\n",
      "Epoch [390/1000], Loss: 14.7562\n",
      "Epoch [400/1000], Loss: 14.7454\n",
      "Epoch [410/1000], Loss: 14.7352\n",
      "Epoch [420/1000], Loss: 14.7255\n",
      "Epoch [430/1000], Loss: 14.7163\n",
      "Epoch [440/1000], Loss: 14.7075\n",
      "Epoch [450/1000], Loss: 14.6991\n",
      "Epoch [460/1000], Loss: 14.6911\n",
      "Epoch [470/1000], Loss: 14.6835\n",
      "Epoch [480/1000], Loss: 14.6763\n",
      "Epoch [490/1000], Loss: 14.6693\n",
      "Epoch [500/1000], Loss: 14.6627\n",
      "Epoch [510/1000], Loss: 14.6563\n",
      "Epoch [520/1000], Loss: 14.6502\n",
      "Epoch [530/1000], Loss: 14.6444\n",
      "Epoch [540/1000], Loss: 14.6388\n",
      "Epoch [550/1000], Loss: 14.6335\n",
      "Epoch [560/1000], Loss: 14.6283\n",
      "Epoch [570/1000], Loss: 14.6234\n",
      "Epoch [580/1000], Loss: 14.6187\n",
      "Epoch [590/1000], Loss: 14.6141\n",
      "Epoch [600/1000], Loss: 14.6097\n",
      "Epoch [610/1000], Loss: 14.6055\n",
      "Epoch [620/1000], Loss: 14.6014\n",
      "Epoch [630/1000], Loss: 14.5975\n",
      "Epoch [640/1000], Loss: 14.5937\n",
      "Epoch [650/1000], Loss: 14.5901\n",
      "Epoch [660/1000], Loss: 14.5866\n",
      "Epoch [670/1000], Loss: 14.5832\n",
      "Epoch [680/1000], Loss: 14.5799\n",
      "Epoch [690/1000], Loss: 14.5768\n",
      "Epoch [700/1000], Loss: 14.5737\n",
      "Epoch [710/1000], Loss: 14.5707\n",
      "Epoch [720/1000], Loss: 14.5679\n",
      "Epoch [730/1000], Loss: 14.5651\n",
      "Epoch [740/1000], Loss: 14.5624\n",
      "Epoch [750/1000], Loss: 14.5598\n",
      "Epoch [760/1000], Loss: 14.5573\n",
      "Epoch [770/1000], Loss: 14.5549\n",
      "Epoch [780/1000], Loss: 14.5525\n",
      "Epoch [790/1000], Loss: 14.5502\n",
      "Epoch [800/1000], Loss: 14.5480\n",
      "Epoch [810/1000], Loss: 14.5459\n",
      "Epoch [820/1000], Loss: 14.5438\n",
      "Epoch [830/1000], Loss: 14.5417\n",
      "Epoch [840/1000], Loss: 14.5398\n",
      "Epoch [850/1000], Loss: 14.5379\n",
      "Epoch [860/1000], Loss: 14.5360\n",
      "Epoch [870/1000], Loss: 14.5342\n",
      "Epoch [880/1000], Loss: 14.5324\n",
      "Epoch [890/1000], Loss: 14.5307\n",
      "Epoch [900/1000], Loss: 14.5290\n",
      "Epoch [910/1000], Loss: 14.5274\n",
      "Epoch [920/1000], Loss: 14.5258\n",
      "Epoch [930/1000], Loss: 14.5243\n",
      "Epoch [940/1000], Loss: 14.5228\n",
      "Epoch [950/1000], Loss: 14.5214\n",
      "Epoch [960/1000], Loss: 14.5199\n",
      "Epoch [970/1000], Loss: 14.5185\n",
      "Epoch [980/1000], Loss: 14.5172\n",
      "Epoch [990/1000], Loss: 14.5159\n",
      "Epoch [1000/1000], Loss: 14.5146\n",
      "Predicted days_remaining for parent_id 103: [12.012205123901367, 12.848225593566895, 12.850820541381836, 12.851179122924805, 12.851936340332031, 12.851083755493164, 12.85128116607666, 12.852153778076172]\n",
      "Training for parent_id 105...\n",
      "Epoch [10/1000], Loss: 257.4721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/1000], Loss: 210.0719\n",
      "Epoch [30/1000], Loss: 173.7994\n",
      "Epoch [40/1000], Loss: 148.9146\n",
      "Epoch [50/1000], Loss: 129.7401\n",
      "Epoch [60/1000], Loss: 113.7347\n",
      "Epoch [70/1000], Loss: 99.9164\n",
      "Epoch [80/1000], Loss: 87.8030\n",
      "Epoch [90/1000], Loss: 77.1789\n",
      "Epoch [100/1000], Loss: 67.8942\n",
      "Epoch [110/1000], Loss: 59.8068\n",
      "Epoch [120/1000], Loss: 52.7873\n",
      "Epoch [130/1000], Loss: 46.7179\n",
      "Epoch [140/1000], Loss: 41.4915\n",
      "Epoch [150/1000], Loss: 37.0104\n",
      "Epoch [160/1000], Loss: 33.1862\n",
      "Epoch [170/1000], Loss: 29.9386\n",
      "Epoch [180/1000], Loss: 27.1945\n",
      "Epoch [190/1000], Loss: 24.8884\n",
      "Epoch [200/1000], Loss: 22.9608\n",
      "Epoch [210/1000], Loss: 21.3587\n",
      "Epoch [220/1000], Loss: 20.0348\n",
      "Epoch [230/1000], Loss: 18.9470\n",
      "Epoch [240/1000], Loss: 18.0586\n",
      "Epoch [250/1000], Loss: 17.3373\n",
      "Epoch [260/1000], Loss: 16.7551\n",
      "Epoch [270/1000], Loss: 16.2879\n",
      "Epoch [280/1000], Loss: 15.9152\n",
      "Epoch [290/1000], Loss: 15.6195\n",
      "Epoch [300/1000], Loss: 15.3863\n",
      "Epoch [310/1000], Loss: 15.2034\n",
      "Epoch [320/1000], Loss: 15.0605\n",
      "Epoch [330/1000], Loss: 14.9495\n",
      "Epoch [340/1000], Loss: 14.8636\n",
      "Epoch [350/1000], Loss: 14.7973\n",
      "Epoch [360/1000], Loss: 14.7463\n",
      "Epoch [370/1000], Loss: 14.7071\n",
      "Epoch [380/1000], Loss: 14.6770\n",
      "Epoch [390/1000], Loss: 14.6538\n",
      "Epoch [400/1000], Loss: 14.6358\n",
      "Epoch [410/1000], Loss: 14.6218\n",
      "Epoch [420/1000], Loss: 14.6108\n",
      "Epoch [430/1000], Loss: 14.6019\n",
      "Epoch [440/1000], Loss: 14.5948\n",
      "Epoch [450/1000], Loss: 14.5888\n",
      "Epoch [460/1000], Loss: 14.5838\n",
      "Epoch [470/1000], Loss: 14.5794\n",
      "Epoch [480/1000], Loss: 14.5756\n",
      "Epoch [490/1000], Loss: 14.5721\n",
      "Epoch [500/1000], Loss: 14.5690\n",
      "Epoch [510/1000], Loss: 14.5660\n",
      "Epoch [520/1000], Loss: 14.5632\n",
      "Epoch [530/1000], Loss: 14.5606\n",
      "Epoch [540/1000], Loss: 14.5581\n",
      "Epoch [550/1000], Loss: 14.5556\n",
      "Epoch [560/1000], Loss: 14.5533\n",
      "Epoch [570/1000], Loss: 14.5511\n",
      "Epoch [580/1000], Loss: 14.5489\n",
      "Epoch [590/1000], Loss: 14.5467\n",
      "Epoch [600/1000], Loss: 14.5447\n",
      "Epoch [610/1000], Loss: 14.5427\n",
      "Epoch [620/1000], Loss: 14.5407\n",
      "Epoch [630/1000], Loss: 14.5388\n",
      "Epoch [640/1000], Loss: 14.5369\n",
      "Epoch [650/1000], Loss: 14.5351\n",
      "Epoch [660/1000], Loss: 14.5334\n",
      "Epoch [670/1000], Loss: 14.5317\n",
      "Epoch [680/1000], Loss: 14.5300\n",
      "Epoch [690/1000], Loss: 14.5284\n",
      "Epoch [700/1000], Loss: 14.5268\n",
      "Epoch [710/1000], Loss: 14.5252\n",
      "Epoch [720/1000], Loss: 14.5237\n",
      "Epoch [730/1000], Loss: 14.5222\n",
      "Epoch [740/1000], Loss: 14.5208\n",
      "Epoch [750/1000], Loss: 14.5194\n",
      "Epoch [760/1000], Loss: 14.5180\n",
      "Epoch [770/1000], Loss: 14.5167\n",
      "Epoch [780/1000], Loss: 14.5154\n",
      "Epoch [790/1000], Loss: 14.5141\n",
      "Epoch [800/1000], Loss: 14.5129\n",
      "Epoch [810/1000], Loss: 14.5116\n",
      "Epoch [820/1000], Loss: 14.5105\n",
      "Epoch [830/1000], Loss: 14.5093\n",
      "Epoch [840/1000], Loss: 14.5082\n",
      "Epoch [850/1000], Loss: 14.5070\n",
      "Epoch [860/1000], Loss: 14.5060\n",
      "Epoch [870/1000], Loss: 14.5049\n",
      "Epoch [880/1000], Loss: 14.5039\n",
      "Epoch [890/1000], Loss: 14.5028\n",
      "Epoch [900/1000], Loss: 14.5019\n",
      "Epoch [910/1000], Loss: 14.5009\n",
      "Epoch [920/1000], Loss: 14.4999\n",
      "Epoch [930/1000], Loss: 14.4990\n",
      "Epoch [940/1000], Loss: 14.4981\n",
      "Epoch [950/1000], Loss: 14.4972\n",
      "Epoch [960/1000], Loss: 14.4963\n",
      "Epoch [970/1000], Loss: 14.4955\n",
      "Epoch [980/1000], Loss: 14.4946\n",
      "Epoch [990/1000], Loss: 14.4938\n",
      "Epoch [1000/1000], Loss: 14.4930\n",
      "Predicted days_remaining for parent_id 105: [16.126401901245117, 16.841014862060547, 16.84059715270996, 16.837995529174805, 16.836532592773438, 16.838306427001953, 16.837825775146484, 16.836029052734375]\n",
      "Training for parent_id 113...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 562.8093\n",
      "Epoch [20/1000], Loss: 489.2695\n",
      "Epoch [30/1000], Loss: 437.8983\n",
      "Epoch [40/1000], Loss: 400.3784\n",
      "Epoch [50/1000], Loss: 368.7736\n",
      "Epoch [60/1000], Loss: 340.5634\n",
      "Epoch [70/1000], Loss: 314.6403\n",
      "Epoch [80/1000], Loss: 290.6411\n",
      "Epoch [90/1000], Loss: 268.3642\n",
      "Epoch [100/1000], Loss: 247.6719\n",
      "Epoch [110/1000], Loss: 228.4617\n",
      "Epoch [120/1000], Loss: 210.6433\n",
      "Epoch [130/1000], Loss: 194.1278\n",
      "Epoch [140/1000], Loss: 178.8281\n",
      "Epoch [150/1000], Loss: 164.6614\n",
      "Epoch [160/1000], Loss: 151.5512\n",
      "Epoch [170/1000], Loss: 139.4270\n",
      "Epoch [180/1000], Loss: 128.2239\n",
      "Epoch [190/1000], Loss: 117.8819\n",
      "Epoch [200/1000], Loss: 108.3452\n",
      "Epoch [210/1000], Loss: 99.5616\n",
      "Epoch [220/1000], Loss: 91.4821\n",
      "Epoch [230/1000], Loss: 84.0605\n",
      "Epoch [240/1000], Loss: 77.2533\n",
      "Epoch [250/1000], Loss: 71.0191\n",
      "Epoch [260/1000], Loss: 65.3188\n",
      "Epoch [270/1000], Loss: 60.1153\n",
      "Epoch [280/1000], Loss: 55.3734\n",
      "Epoch [290/1000], Loss: 51.0596\n",
      "Epoch [300/1000], Loss: 47.1425\n",
      "Epoch [310/1000], Loss: 43.5920\n",
      "Epoch [320/1000], Loss: 40.3800\n",
      "Epoch [330/1000], Loss: 37.4796\n",
      "Epoch [340/1000], Loss: 34.8657\n",
      "Epoch [350/1000], Loss: 32.5146\n",
      "Epoch [360/1000], Loss: 30.4042\n",
      "Epoch [370/1000], Loss: 28.5136\n",
      "Epoch [380/1000], Loss: 26.8234\n",
      "Epoch [390/1000], Loss: 25.3154\n",
      "Epoch [400/1000], Loss: 23.9729\n",
      "Epoch [410/1000], Loss: 22.7801\n",
      "Epoch [420/1000], Loss: 21.7227\n",
      "Epoch [430/1000], Loss: 20.7871\n",
      "Epoch [440/1000], Loss: 19.9612\n",
      "Epoch [450/1000], Loss: 19.2337\n",
      "Epoch [460/1000], Loss: 18.5942\n",
      "Epoch [470/1000], Loss: 18.0332\n",
      "Epoch [480/1000], Loss: 17.5423\n",
      "Epoch [490/1000], Loss: 17.1136\n",
      "Epoch [500/1000], Loss: 16.7400\n",
      "Epoch [510/1000], Loss: 16.4152\n",
      "Epoch [520/1000], Loss: 16.1333\n",
      "Epoch [530/1000], Loss: 15.8893\n",
      "Epoch [540/1000], Loss: 15.6786\n",
      "Epoch [550/1000], Loss: 15.4969\n",
      "Epoch [560/1000], Loss: 15.3407\n",
      "Epoch [570/1000], Loss: 15.2066\n",
      "Epoch [580/1000], Loss: 15.0918\n",
      "Epoch [590/1000], Loss: 14.9937\n",
      "Epoch [600/1000], Loss: 14.9101\n",
      "Epoch [610/1000], Loss: 14.8390\n",
      "Epoch [620/1000], Loss: 14.7786\n",
      "Epoch [630/1000], Loss: 14.7274\n",
      "Epoch [640/1000], Loss: 14.6842\n",
      "Epoch [650/1000], Loss: 14.6477\n",
      "Epoch [660/1000], Loss: 14.6170\n",
      "Epoch [670/1000], Loss: 14.5912\n",
      "Epoch [680/1000], Loss: 14.5696\n",
      "Epoch [690/1000], Loss: 14.5515\n",
      "Epoch [700/1000], Loss: 14.5364\n",
      "Epoch [710/1000], Loss: 14.5238\n",
      "Epoch [720/1000], Loss: 14.5134\n",
      "Epoch [730/1000], Loss: 14.5047\n",
      "Epoch [740/1000], Loss: 14.4975\n",
      "Epoch [750/1000], Loss: 14.4915\n",
      "Epoch [760/1000], Loss: 14.4865\n",
      "Epoch [770/1000], Loss: 14.4825\n",
      "Epoch [780/1000], Loss: 14.4791\n",
      "Epoch [790/1000], Loss: 14.4763\n",
      "Epoch [800/1000], Loss: 14.4740\n",
      "Epoch [810/1000], Loss: 14.4721\n",
      "Epoch [820/1000], Loss: 14.4705\n",
      "Epoch [830/1000], Loss: 14.4692\n",
      "Epoch [840/1000], Loss: 14.4682\n",
      "Epoch [850/1000], Loss: 14.4673\n",
      "Epoch [860/1000], Loss: 14.4665\n",
      "Epoch [870/1000], Loss: 14.4659\n",
      "Epoch [880/1000], Loss: 14.4653\n",
      "Epoch [890/1000], Loss: 14.4649\n",
      "Epoch [900/1000], Loss: 14.4645\n",
      "Epoch [910/1000], Loss: 14.4641\n",
      "Epoch [920/1000], Loss: 14.4638\n",
      "Epoch [930/1000], Loss: 14.4635\n",
      "Epoch [940/1000], Loss: 14.4633\n",
      "Epoch [950/1000], Loss: 14.4630\n",
      "Epoch [960/1000], Loss: 14.4628\n",
      "Epoch [970/1000], Loss: 14.4626\n",
      "Epoch [980/1000], Loss: 14.4624\n",
      "Epoch [990/1000], Loss: 14.4623\n",
      "Epoch [1000/1000], Loss: 14.4621\n",
      "Predicted days_remaining for parent_id 113: [24.328184127807617, 24.80221176147461, 24.801673889160156, 24.801700592041016, 24.801620483398438, 24.801572799682617, 24.801454544067383, 24.80054473876953]\n",
      "Training for parent_id 123...\n",
      "Epoch [10/1000], Loss: 723.8840\n",
      "Epoch [20/1000], Loss: 633.6787\n",
      "Epoch [30/1000], Loss: 564.2111\n",
      "Epoch [40/1000], Loss: 515.5496\n",
      "Epoch [50/1000], Loss: 477.3862\n",
      "Epoch [60/1000], Loss: 443.8503\n",
      "Epoch [70/1000], Loss: 413.1379\n",
      "Epoch [80/1000], Loss: 384.6656\n",
      "Epoch [90/1000], Loss: 358.1173\n",
      "Epoch [100/1000], Loss: 333.2891\n",
      "Epoch [110/1000], Loss: 310.0390\n",
      "Epoch [120/1000], Loss: 288.2607\n",
      "Epoch [130/1000], Loss: 267.8670\n",
      "Epoch [140/1000], Loss: 248.7802\n",
      "Epoch [150/1000], Loss: 230.9277\n",
      "Epoch [160/1000], Loss: 214.2407\n",
      "Epoch [170/1000], Loss: 198.6538\n",
      "Epoch [180/1000], Loss: 184.1053\n",
      "Epoch [190/1000], Loss: 170.5369\n",
      "Epoch [200/1000], Loss: 157.8938\n",
      "Epoch [210/1000], Loss: 146.1238\n",
      "Epoch [220/1000], Loss: 135.1774\n",
      "Epoch [230/1000], Loss: 125.0076\n",
      "Epoch [240/1000], Loss: 115.5698\n",
      "Epoch [250/1000], Loss: 106.8212\n",
      "Epoch [260/1000], Loss: 98.7213\n",
      "Epoch [270/1000], Loss: 91.2312\n",
      "Epoch [280/1000], Loss: 84.3138\n",
      "Epoch [290/1000], Loss: 77.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 72.0576\n",
      "Epoch [310/1000], Loss: 66.6526\n",
      "Epoch [320/1000], Loss: 61.6883\n",
      "Epoch [330/1000], Loss: 57.1354\n",
      "Epoch [340/1000], Loss: 52.9659\n",
      "Epoch [350/1000], Loss: 49.1534\n",
      "Epoch [360/1000], Loss: 45.6726\n",
      "Epoch [370/1000], Loss: 42.4996\n",
      "Epoch [380/1000], Loss: 39.6118\n",
      "Epoch [390/1000], Loss: 36.9878\n",
      "Epoch [400/1000], Loss: 34.6073\n",
      "Epoch [410/1000], Loss: 32.4513\n",
      "Epoch [420/1000], Loss: 30.5019\n",
      "Epoch [430/1000], Loss: 28.7422\n",
      "Epoch [440/1000], Loss: 27.1564\n",
      "Epoch [450/1000], Loss: 25.7298\n",
      "Epoch [460/1000], Loss: 24.4486\n",
      "Epoch [470/1000], Loss: 23.2999\n",
      "Epoch [480/1000], Loss: 22.2719\n",
      "Epoch [490/1000], Loss: 21.3534\n",
      "Epoch [500/1000], Loss: 20.5343\n",
      "Epoch [510/1000], Loss: 19.8050\n",
      "Epoch [520/1000], Loss: 19.1569\n",
      "Epoch [530/1000], Loss: 18.5819\n",
      "Epoch [540/1000], Loss: 18.0727\n",
      "Epoch [550/1000], Loss: 17.6225\n",
      "Epoch [560/1000], Loss: 17.2253\n",
      "Epoch [570/1000], Loss: 16.8753\n",
      "Epoch [580/1000], Loss: 16.5676\n",
      "Epoch [590/1000], Loss: 16.2975\n",
      "Epoch [600/1000], Loss: 16.0608\n",
      "Epoch [610/1000], Loss: 15.8538\n",
      "Epoch [620/1000], Loss: 15.6730\n",
      "Epoch [630/1000], Loss: 15.5154\n",
      "Epoch [640/1000], Loss: 15.3783\n",
      "Epoch [650/1000], Loss: 15.2592\n",
      "Epoch [660/1000], Loss: 15.1559\n",
      "Epoch [670/1000], Loss: 15.0665\n",
      "Epoch [680/1000], Loss: 14.9892\n",
      "Epoch [690/1000], Loss: 14.9225\n",
      "Epoch [700/1000], Loss: 14.8650\n",
      "Epoch [710/1000], Loss: 14.8155\n",
      "Epoch [720/1000], Loss: 14.7731\n",
      "Epoch [730/1000], Loss: 14.7366\n",
      "Epoch [740/1000], Loss: 14.7054\n",
      "Epoch [750/1000], Loss: 14.6787\n",
      "Epoch [760/1000], Loss: 14.6558\n",
      "Epoch [770/1000], Loss: 14.6363\n",
      "Epoch [780/1000], Loss: 14.6197\n",
      "Epoch [790/1000], Loss: 14.6055\n",
      "Epoch [800/1000], Loss: 14.5934\n",
      "Epoch [810/1000], Loss: 14.5831\n",
      "Epoch [820/1000], Loss: 14.5743\n",
      "Epoch [830/1000], Loss: 14.5668\n",
      "Epoch [840/1000], Loss: 14.5604\n",
      "Epoch [850/1000], Loss: 14.5549\n",
      "Epoch [860/1000], Loss: 14.5503\n",
      "Epoch [870/1000], Loss: 14.5462\n",
      "Epoch [880/1000], Loss: 14.5428\n",
      "Epoch [890/1000], Loss: 14.5398\n",
      "Epoch [900/1000], Loss: 14.5371\n",
      "Epoch [910/1000], Loss: 14.5349\n",
      "Epoch [920/1000], Loss: 14.5329\n",
      "Epoch [930/1000], Loss: 14.5311\n",
      "Epoch [940/1000], Loss: 14.5295\n",
      "Epoch [950/1000], Loss: 14.5281\n",
      "Epoch [960/1000], Loss: 14.5268\n",
      "Epoch [970/1000], Loss: 14.5256\n",
      "Epoch [980/1000], Loss: 14.5245\n",
      "Epoch [990/1000], Loss: 14.5235\n",
      "Epoch [1000/1000], Loss: 14.5226\n",
      "Predicted days_remaining for parent_id 123: [26.952743530273438, 27.829492568969727, 27.82997703552246, 27.829328536987305, 27.829486846923828, 27.829374313354492, 27.82949447631836, 27.83012580871582]\n",
      "Training for parent_id 124...\n",
      "Epoch [10/1000], Loss: 188.7450\n",
      "Epoch [20/1000], Loss: 145.6137\n",
      "Epoch [30/1000], Loss: 117.2442\n",
      "Epoch [40/1000], Loss: 98.6111\n",
      "Epoch [50/1000], Loss: 84.6481\n",
      "Epoch [60/1000], Loss: 73.1522\n",
      "Epoch [70/1000], Loss: 63.4092\n",
      "Epoch [80/1000], Loss: 55.0977\n",
      "Epoch [90/1000], Loss: 48.0126\n",
      "Epoch [100/1000], Loss: 41.9970\n",
      "Epoch [110/1000], Loss: 36.9180\n",
      "Epoch [120/1000], Loss: 32.6573\n",
      "Epoch [130/1000], Loss: 29.1076\n",
      "Epoch [140/1000], Loss: 26.1714\n",
      "Epoch [150/1000], Loss: 23.7609\n",
      "Epoch [160/1000], Loss: 21.7974\n",
      "Epoch [170/1000], Loss: 20.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000], Loss: 18.9390\n",
      "Epoch [190/1000], Loss: 17.9283\n",
      "Epoch [200/1000], Loss: 17.1319\n",
      "Epoch [210/1000], Loss: 16.5096\n",
      "Epoch [220/1000], Loss: 16.0276\n",
      "Epoch [230/1000], Loss: 15.6573\n",
      "Epoch [240/1000], Loss: 15.3753\n",
      "Epoch [250/1000], Loss: 15.1622\n",
      "Epoch [260/1000], Loss: 15.0023\n",
      "Epoch [270/1000], Loss: 14.8834\n",
      "Epoch [280/1000], Loss: 14.7954\n",
      "Epoch [290/1000], Loss: 14.7306\n",
      "Epoch [300/1000], Loss: 14.6831\n",
      "Epoch [310/1000], Loss: 14.6484\n",
      "Epoch [320/1000], Loss: 14.6230\n",
      "Epoch [330/1000], Loss: 14.6043\n",
      "Epoch [340/1000], Loss: 14.5904\n",
      "Epoch [350/1000], Loss: 14.5800\n",
      "Epoch [360/1000], Loss: 14.5720\n",
      "Epoch [370/1000], Loss: 14.5657\n",
      "Epoch [380/1000], Loss: 14.5606\n",
      "Epoch [390/1000], Loss: 14.5563\n",
      "Epoch [400/1000], Loss: 14.5526\n",
      "Epoch [410/1000], Loss: 14.5494\n",
      "Epoch [420/1000], Loss: 14.5464\n",
      "Epoch [430/1000], Loss: 14.5437\n",
      "Epoch [440/1000], Loss: 14.5412\n",
      "Epoch [450/1000], Loss: 14.5387\n",
      "Epoch [460/1000], Loss: 14.5364\n",
      "Epoch [470/1000], Loss: 14.5342\n",
      "Epoch [480/1000], Loss: 14.5321\n",
      "Epoch [490/1000], Loss: 14.5300\n",
      "Epoch [500/1000], Loss: 14.5280\n",
      "Epoch [510/1000], Loss: 14.5261\n",
      "Epoch [520/1000], Loss: 14.5242\n",
      "Epoch [530/1000], Loss: 14.5224\n",
      "Epoch [540/1000], Loss: 14.5206\n",
      "Epoch [550/1000], Loss: 14.5189\n",
      "Epoch [560/1000], Loss: 14.5172\n",
      "Epoch [570/1000], Loss: 14.5156\n",
      "Epoch [580/1000], Loss: 14.5141\n",
      "Epoch [590/1000], Loss: 14.5125\n",
      "Epoch [600/1000], Loss: 14.5111\n",
      "Epoch [610/1000], Loss: 14.5096\n",
      "Epoch [620/1000], Loss: 14.5082\n",
      "Epoch [630/1000], Loss: 14.5069\n",
      "Epoch [640/1000], Loss: 14.5055\n",
      "Epoch [650/1000], Loss: 14.5043\n",
      "Epoch [660/1000], Loss: 14.5030\n",
      "Epoch [670/1000], Loss: 14.5018\n",
      "Epoch [680/1000], Loss: 14.5006\n",
      "Epoch [690/1000], Loss: 14.4995\n",
      "Epoch [700/1000], Loss: 14.4983\n",
      "Epoch [710/1000], Loss: 14.4972\n",
      "Epoch [720/1000], Loss: 14.4962\n",
      "Epoch [730/1000], Loss: 14.4951\n",
      "Epoch [740/1000], Loss: 14.4941\n",
      "Epoch [750/1000], Loss: 14.4931\n",
      "Epoch [760/1000], Loss: 14.4922\n",
      "Epoch [770/1000], Loss: 14.4913\n",
      "Epoch [780/1000], Loss: 14.4903\n",
      "Epoch [790/1000], Loss: 14.4894\n",
      "Epoch [800/1000], Loss: 14.4886\n",
      "Epoch [810/1000], Loss: 14.4877\n",
      "Epoch [820/1000], Loss: 14.4869\n",
      "Epoch [830/1000], Loss: 14.4861\n",
      "Epoch [840/1000], Loss: 14.4853\n",
      "Epoch [850/1000], Loss: 14.4845\n",
      "Epoch [860/1000], Loss: 14.4838\n",
      "Epoch [870/1000], Loss: 14.4831\n",
      "Epoch [880/1000], Loss: 14.4823\n",
      "Epoch [890/1000], Loss: 14.4816\n",
      "Epoch [900/1000], Loss: 14.4809\n",
      "Epoch [910/1000], Loss: 14.4803\n",
      "Epoch [920/1000], Loss: 14.4796\n",
      "Epoch [930/1000], Loss: 14.4790\n",
      "Epoch [940/1000], Loss: 14.4784\n",
      "Epoch [950/1000], Loss: 14.4777\n",
      "Epoch [960/1000], Loss: 14.4771\n",
      "Epoch [970/1000], Loss: 14.4766\n",
      "Epoch [980/1000], Loss: 14.4760\n",
      "Epoch [990/1000], Loss: 14.4754\n",
      "Epoch [1000/1000], Loss: 14.4749\n",
      "Predicted days_remaining for parent_id 124: [14.238122940063477, 14.82368278503418, 14.824995040893555, 14.824857711791992, 14.821463584899902, 14.820858001708984, 14.820575714111328, 14.819056510925293]\n",
      "Training for parent_id 144...\n",
      "Epoch [10/1000], Loss: 106.2827\n",
      "Epoch [20/1000], Loss: 78.8294\n",
      "Epoch [30/1000], Loss: 57.8130\n",
      "Epoch [40/1000], Loss: 44.9453\n",
      "Epoch [50/1000], Loss: 36.5273\n",
      "Epoch [60/1000], Loss: 30.6226\n",
      "Epoch [70/1000], Loss: 26.3117\n",
      "Epoch [80/1000], Loss: 23.1258\n",
      "Epoch [90/1000], Loss: 20.7901\n",
      "Epoch [100/1000], Loss: 19.1040\n",
      "Epoch [110/1000], Loss: 17.9077\n",
      "Epoch [120/1000], Loss: 17.0729\n",
      "Epoch [130/1000], Loss: 16.4982\n",
      "Epoch [140/1000], Loss: 16.1054\n",
      "Epoch [150/1000], Loss: 15.8367\n",
      "Epoch [160/1000], Loss: 15.6502\n",
      "Epoch [170/1000], Loss: 15.5172\n",
      "Epoch [180/1000], Loss: 15.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000], Loss: 15.3415\n",
      "Epoch [200/1000], Loss: 15.2787\n",
      "Epoch [210/1000], Loss: 15.2253\n",
      "Epoch [220/1000], Loss: 15.1785\n",
      "Epoch [230/1000], Loss: 15.1367\n",
      "Epoch [240/1000], Loss: 15.0988\n",
      "Epoch [250/1000], Loss: 15.0642\n",
      "Epoch [260/1000], Loss: 15.0324\n",
      "Epoch [270/1000], Loss: 15.0031\n",
      "Epoch [280/1000], Loss: 14.9761\n",
      "Epoch [290/1000], Loss: 14.9510\n",
      "Epoch [300/1000], Loss: 14.9277\n",
      "Epoch [310/1000], Loss: 14.9060\n",
      "Epoch [320/1000], Loss: 14.8858\n",
      "Epoch [330/1000], Loss: 14.8670\n",
      "Epoch [340/1000], Loss: 14.8493\n",
      "Epoch [350/1000], Loss: 14.8328\n",
      "Epoch [360/1000], Loss: 14.8173\n",
      "Epoch [370/1000], Loss: 14.8027\n",
      "Epoch [380/1000], Loss: 14.7890\n",
      "Epoch [390/1000], Loss: 14.7761\n",
      "Epoch [400/1000], Loss: 14.7639\n",
      "Epoch [410/1000], Loss: 14.7524\n",
      "Epoch [420/1000], Loss: 14.7415\n",
      "Epoch [430/1000], Loss: 14.7312\n",
      "Epoch [440/1000], Loss: 14.7214\n",
      "Epoch [450/1000], Loss: 14.7121\n",
      "Epoch [460/1000], Loss: 14.7033\n",
      "Epoch [470/1000], Loss: 14.6949\n",
      "Epoch [480/1000], Loss: 14.6869\n",
      "Epoch [490/1000], Loss: 14.6793\n",
      "Epoch [500/1000], Loss: 14.6721\n",
      "Epoch [510/1000], Loss: 14.6652\n",
      "Epoch [520/1000], Loss: 14.6586\n",
      "Epoch [530/1000], Loss: 14.6523\n",
      "Epoch [540/1000], Loss: 14.6463\n",
      "Epoch [550/1000], Loss: 14.6405\n",
      "Epoch [560/1000], Loss: 14.6350\n",
      "Epoch [570/1000], Loss: 14.6297\n",
      "Epoch [580/1000], Loss: 14.6246\n",
      "Epoch [590/1000], Loss: 14.6197\n",
      "Epoch [600/1000], Loss: 14.6150\n",
      "Epoch [610/1000], Loss: 14.6106\n",
      "Epoch [620/1000], Loss: 14.6062\n",
      "Epoch [630/1000], Loss: 14.6021\n",
      "Epoch [640/1000], Loss: 14.5981\n",
      "Epoch [650/1000], Loss: 14.5942\n",
      "Epoch [660/1000], Loss: 14.5905\n",
      "Epoch [670/1000], Loss: 14.5869\n",
      "Epoch [680/1000], Loss: 14.5835\n",
      "Epoch [690/1000], Loss: 14.5801\n",
      "Epoch [700/1000], Loss: 14.5769\n",
      "Epoch [710/1000], Loss: 14.5738\n",
      "Epoch [720/1000], Loss: 14.5708\n",
      "Epoch [730/1000], Loss: 14.5679\n",
      "Epoch [740/1000], Loss: 14.5651\n",
      "Epoch [750/1000], Loss: 14.5624\n",
      "Epoch [760/1000], Loss: 14.5598\n",
      "Epoch [770/1000], Loss: 14.5572\n",
      "Epoch [780/1000], Loss: 14.5548\n",
      "Epoch [790/1000], Loss: 14.5524\n",
      "Epoch [800/1000], Loss: 14.5501\n",
      "Epoch [810/1000], Loss: 14.5478\n",
      "Epoch [820/1000], Loss: 14.5457\n",
      "Epoch [830/1000], Loss: 14.5435\n",
      "Epoch [840/1000], Loss: 14.5415\n",
      "Epoch [850/1000], Loss: 14.5395\n",
      "Epoch [860/1000], Loss: 14.5376\n",
      "Epoch [870/1000], Loss: 14.5357\n",
      "Epoch [880/1000], Loss: 14.5339\n",
      "Epoch [890/1000], Loss: 14.5321\n",
      "Epoch [900/1000], Loss: 14.5304\n",
      "Epoch [910/1000], Loss: 14.5287\n",
      "Epoch [920/1000], Loss: 14.5271\n",
      "Epoch [930/1000], Loss: 14.5255\n",
      "Epoch [940/1000], Loss: 14.5239\n",
      "Epoch [950/1000], Loss: 14.5224\n",
      "Epoch [960/1000], Loss: 14.5210\n",
      "Epoch [970/1000], Loss: 14.5195\n",
      "Epoch [980/1000], Loss: 14.5181\n",
      "Epoch [990/1000], Loss: 14.5168\n",
      "Epoch [1000/1000], Loss: 14.5155\n",
      "Predicted days_remaining for parent_id 144: [10.006196022033691, 10.845252990722656, 10.85142707824707, 10.850299835205078, 10.851861000061035, 10.848217964172363, 10.850309371948242, 10.849822044372559]\n",
      "Training for parent_id 150...\n",
      "Epoch [10/1000], Loss: 175.4435\n",
      "Epoch [20/1000], Loss: 133.1465\n",
      "Epoch [30/1000], Loss: 105.2617\n",
      "Epoch [40/1000], Loss: 88.1532\n",
      "Epoch [50/1000], Loss: 75.3706\n",
      "Epoch [60/1000], Loss: 64.8801\n",
      "Epoch [70/1000], Loss: 56.0245\n",
      "Epoch [80/1000], Loss: 48.4894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/1000], Loss: 42.1181\n",
      "Epoch [100/1000], Loss: 36.7823\n",
      "Epoch [110/1000], Loss: 32.3498\n",
      "Epoch [120/1000], Loss: 28.6976\n",
      "Epoch [130/1000], Loss: 25.7148\n",
      "Epoch [140/1000], Loss: 23.3015\n",
      "Epoch [150/1000], Loss: 21.3682\n",
      "Epoch [160/1000], Loss: 19.8351\n",
      "Epoch [170/1000], Loss: 18.6317\n",
      "Epoch [180/1000], Loss: 17.6966\n",
      "Epoch [190/1000], Loss: 16.9770\n",
      "Epoch [200/1000], Loss: 16.4284\n",
      "Epoch [210/1000], Loss: 16.0136\n",
      "Epoch [220/1000], Loss: 15.7023\n",
      "Epoch [230/1000], Loss: 15.4702\n",
      "Epoch [240/1000], Loss: 15.2977\n",
      "Epoch [250/1000], Loss: 15.1698\n",
      "Epoch [260/1000], Loss: 15.0747\n",
      "Epoch [270/1000], Loss: 15.0036\n",
      "Epoch [280/1000], Loss: 14.9499\n",
      "Epoch [290/1000], Loss: 14.9088\n",
      "Epoch [300/1000], Loss: 14.8765\n",
      "Epoch [310/1000], Loss: 14.8505\n",
      "Epoch [320/1000], Loss: 14.8291\n",
      "Epoch [330/1000], Loss: 14.8110\n",
      "Epoch [340/1000], Loss: 14.7952\n",
      "Epoch [350/1000], Loss: 14.7811\n",
      "Epoch [360/1000], Loss: 14.7683\n",
      "Epoch [370/1000], Loss: 14.7566\n",
      "Epoch [380/1000], Loss: 14.7456\n",
      "Epoch [390/1000], Loss: 14.7354\n",
      "Epoch [400/1000], Loss: 14.7257\n",
      "Epoch [410/1000], Loss: 14.7166\n",
      "Epoch [420/1000], Loss: 14.7079\n",
      "Epoch [430/1000], Loss: 14.6996\n",
      "Epoch [440/1000], Loss: 14.6917\n",
      "Epoch [450/1000], Loss: 14.6842\n",
      "Epoch [460/1000], Loss: 14.6770\n",
      "Epoch [470/1000], Loss: 14.6701\n",
      "Epoch [480/1000], Loss: 14.6636\n",
      "Epoch [490/1000], Loss: 14.6573\n",
      "Epoch [500/1000], Loss: 14.6512\n",
      "Epoch [510/1000], Loss: 14.6455\n",
      "Epoch [520/1000], Loss: 14.6399\n",
      "Epoch [530/1000], Loss: 14.6346\n",
      "Epoch [540/1000], Loss: 14.6295\n",
      "Epoch [550/1000], Loss: 14.6245\n",
      "Epoch [560/1000], Loss: 14.6198\n",
      "Epoch [570/1000], Loss: 14.6153\n",
      "Epoch [580/1000], Loss: 14.6109\n",
      "Epoch [590/1000], Loss: 14.6067\n",
      "Epoch [600/1000], Loss: 14.6026\n",
      "Epoch [610/1000], Loss: 14.5987\n",
      "Epoch [620/1000], Loss: 14.5949\n",
      "Epoch [630/1000], Loss: 14.5913\n",
      "Epoch [640/1000], Loss: 14.5878\n",
      "Epoch [650/1000], Loss: 14.5844\n",
      "Epoch [660/1000], Loss: 14.5811\n",
      "Epoch [670/1000], Loss: 14.5779\n",
      "Epoch [680/1000], Loss: 14.5749\n",
      "Epoch [690/1000], Loss: 14.5719\n",
      "Epoch [700/1000], Loss: 14.5690\n",
      "Epoch [710/1000], Loss: 14.5663\n",
      "Epoch [720/1000], Loss: 14.5636\n",
      "Epoch [730/1000], Loss: 14.5610\n",
      "Epoch [740/1000], Loss: 14.5584\n",
      "Epoch [750/1000], Loss: 14.5560\n",
      "Epoch [760/1000], Loss: 14.5536\n",
      "Epoch [770/1000], Loss: 14.5513\n",
      "Epoch [780/1000], Loss: 14.5491\n",
      "Epoch [790/1000], Loss: 14.5469\n",
      "Epoch [800/1000], Loss: 14.5448\n",
      "Epoch [810/1000], Loss: 14.5428\n",
      "Epoch [820/1000], Loss: 14.5408\n",
      "Epoch [830/1000], Loss: 14.5389\n",
      "Epoch [840/1000], Loss: 14.5370\n",
      "Epoch [850/1000], Loss: 14.5352\n",
      "Epoch [860/1000], Loss: 14.5334\n",
      "Epoch [870/1000], Loss: 14.5317\n",
      "Epoch [880/1000], Loss: 14.5300\n",
      "Epoch [890/1000], Loss: 14.5284\n",
      "Epoch [900/1000], Loss: 14.5268\n",
      "Epoch [910/1000], Loss: 14.5252\n",
      "Epoch [920/1000], Loss: 14.5237\n",
      "Epoch [930/1000], Loss: 14.5223\n",
      "Epoch [940/1000], Loss: 14.5208\n",
      "Epoch [950/1000], Loss: 14.5194\n",
      "Epoch [960/1000], Loss: 14.5181\n",
      "Epoch [970/1000], Loss: 14.5167\n",
      "Epoch [980/1000], Loss: 14.5154\n",
      "Epoch [990/1000], Loss: 14.5142\n",
      "Epoch [1000/1000], Loss: 14.5129\n",
      "Predicted days_remaining for parent_id 150: [13.020794868469238, 13.848315238952637, 13.852752685546875, 13.851284980773926, 13.850682258605957, 13.849932670593262, 13.850424766540527, 13.850552558898926]\n",
      "Training for parent_id 166...\n",
      "Epoch [10/1000], Loss: 2547.6216\n",
      "Epoch [20/1000], Loss: 2376.0371\n",
      "Epoch [30/1000], Loss: 2248.7397\n",
      "Epoch [40/1000], Loss: 2155.6589\n",
      "Epoch [50/1000], Loss: 2076.1472\n",
      "Epoch [60/1000], Loss: 2003.5116\n",
      "Epoch [70/1000], Loss: 1936.0175\n",
      "Epoch [80/1000], Loss: 1871.5271\n",
      "Epoch [90/1000], Loss: 1809.4716\n",
      "Epoch [100/1000], Loss: 1749.5870\n",
      "Epoch [110/1000], Loss: 1691.6935\n",
      "Epoch [120/1000], Loss: 1635.6570\n",
      "Epoch [130/1000], Loss: 1581.3744\n",
      "Epoch [140/1000], Loss: 1528.7662\n",
      "Epoch [150/1000], Loss: 1477.7698\n",
      "Epoch [160/1000], Loss: 1428.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 1380.4092\n",
      "Epoch [180/1000], Loss: 1333.9467\n",
      "Epoch [190/1000], Loss: 1288.8954\n",
      "Epoch [200/1000], Loss: 1245.2051\n",
      "Epoch [210/1000], Loss: 1202.8276\n",
      "Epoch [220/1000], Loss: 1161.7175\n",
      "Epoch [230/1000], Loss: 1121.8328\n",
      "Epoch [240/1000], Loss: 1083.1343\n",
      "Epoch [250/1000], Loss: 1045.5851\n",
      "Epoch [260/1000], Loss: 1009.1507\n",
      "Epoch [270/1000], Loss: 973.7989\n",
      "Epoch [280/1000], Loss: 939.4993\n",
      "Epoch [290/1000], Loss: 906.2226\n",
      "Epoch [300/1000], Loss: 873.9413\n",
      "Epoch [310/1000], Loss: 842.6292\n",
      "Epoch [320/1000], Loss: 812.2609\n",
      "Epoch [330/1000], Loss: 782.8121\n",
      "Epoch [340/1000], Loss: 754.2594\n",
      "Epoch [350/1000], Loss: 726.5803\n",
      "Epoch [360/1000], Loss: 699.7527\n",
      "Epoch [370/1000], Loss: 673.7559\n",
      "Epoch [380/1000], Loss: 648.5690\n",
      "Epoch [390/1000], Loss: 624.1720\n",
      "Epoch [400/1000], Loss: 600.5452\n",
      "Epoch [410/1000], Loss: 577.6700\n",
      "Epoch [420/1000], Loss: 555.5276\n",
      "Epoch [430/1000], Loss: 534.0999\n",
      "Epoch [440/1000], Loss: 513.3691\n",
      "Epoch [450/1000], Loss: 493.3177\n",
      "Epoch [460/1000], Loss: 473.9289\n",
      "Epoch [470/1000], Loss: 455.1859\n",
      "Epoch [480/1000], Loss: 437.0722\n",
      "Epoch [490/1000], Loss: 419.5720\n",
      "Epoch [500/1000], Loss: 402.6694\n",
      "Epoch [510/1000], Loss: 386.3488\n",
      "Epoch [520/1000], Loss: 370.5951\n",
      "Epoch [530/1000], Loss: 355.3935\n",
      "Epoch [540/1000], Loss: 340.7289\n",
      "Epoch [550/1000], Loss: 326.5875\n",
      "Epoch [560/1000], Loss: 312.9547\n",
      "Epoch [570/1000], Loss: 299.8167\n",
      "Epoch [580/1000], Loss: 287.1599\n",
      "Epoch [590/1000], Loss: 274.9709\n",
      "Epoch [600/1000], Loss: 263.2361\n",
      "Epoch [610/1000], Loss: 251.9431\n",
      "Epoch [620/1000], Loss: 241.0788\n",
      "Epoch [630/1000], Loss: 230.6310\n",
      "Epoch [640/1000], Loss: 220.5873\n",
      "Epoch [650/1000], Loss: 210.9355\n",
      "Epoch [660/1000], Loss: 201.6640\n",
      "Epoch [670/1000], Loss: 192.7611\n",
      "Epoch [680/1000], Loss: 184.2155\n",
      "Epoch [690/1000], Loss: 176.0159\n",
      "Epoch [700/1000], Loss: 168.1516\n",
      "Epoch [710/1000], Loss: 160.6117\n",
      "Epoch [720/1000], Loss: 153.3859\n",
      "Epoch [730/1000], Loss: 146.4637\n",
      "Epoch [740/1000], Loss: 139.8352\n",
      "Epoch [750/1000], Loss: 133.4906\n",
      "Epoch [760/1000], Loss: 127.4201\n",
      "Epoch [770/1000], Loss: 121.6145\n",
      "Epoch [780/1000], Loss: 116.0645\n",
      "Epoch [790/1000], Loss: 110.7611\n",
      "Epoch [800/1000], Loss: 105.6955\n",
      "Epoch [810/1000], Loss: 100.8592\n",
      "Epoch [820/1000], Loss: 96.2439\n",
      "Epoch [830/1000], Loss: 91.8414\n",
      "Epoch [840/1000], Loss: 87.6438\n",
      "Epoch [850/1000], Loss: 83.6433\n",
      "Epoch [860/1000], Loss: 79.8324\n",
      "Epoch [870/1000], Loss: 76.2038\n",
      "Epoch [880/1000], Loss: 72.7503\n",
      "Epoch [890/1000], Loss: 69.4651\n",
      "Epoch [900/1000], Loss: 66.3413\n",
      "Epoch [910/1000], Loss: 63.3725\n",
      "Epoch [920/1000], Loss: 60.5522\n",
      "Epoch [930/1000], Loss: 57.8744\n",
      "Epoch [940/1000], Loss: 55.3330\n",
      "Epoch [950/1000], Loss: 52.9224\n",
      "Epoch [960/1000], Loss: 50.6367\n",
      "Epoch [970/1000], Loss: 48.4707\n",
      "Epoch [980/1000], Loss: 46.4191\n",
      "Epoch [990/1000], Loss: 44.4768\n",
      "Epoch [1000/1000], Loss: 42.6389\n",
      "Predicted days_remaining for parent_id 166: [46.17573165893555, 46.497650146484375, 46.497581481933594, 46.497825622558594, 46.497501373291016, 46.49763870239258, 46.497703552246094, 46.49742126464844]\n",
      "Training for parent_id 168...\n",
      "Epoch [10/1000], Loss: 354.0740\n",
      "Epoch [20/1000], Loss: 295.2351\n",
      "Epoch [30/1000], Loss: 252.5723\n",
      "Epoch [40/1000], Loss: 223.1913\n",
      "Epoch [50/1000], Loss: 200.0953\n",
      "Epoch [60/1000], Loss: 179.9345\n",
      "Epoch [70/1000], Loss: 161.8109\n",
      "Epoch [80/1000], Loss: 145.5118\n",
      "Epoch [90/1000], Loss: 130.8392\n",
      "Epoch [100/1000], Loss: 117.6295\n",
      "Epoch [110/1000], Loss: 105.7443\n",
      "Epoch [120/1000], Loss: 95.0609\n",
      "Epoch [130/1000], Loss: 85.4697\n",
      "Epoch [140/1000], Loss: 76.8717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 69.1776\n",
      "Epoch [160/1000], Loss: 62.3066\n",
      "Epoch [170/1000], Loss: 56.1844\n",
      "Epoch [180/1000], Loss: 50.7429\n",
      "Epoch [190/1000], Loss: 45.9195\n",
      "Epoch [200/1000], Loss: 41.6560\n",
      "Epoch [210/1000], Loss: 37.8987\n",
      "Epoch [220/1000], Loss: 34.5977\n",
      "Epoch [230/1000], Loss: 31.7070\n",
      "Epoch [240/1000], Loss: 29.1839\n",
      "Epoch [250/1000], Loss: 26.9891\n",
      "Epoch [260/1000], Loss: 25.0867\n",
      "Epoch [270/1000], Loss: 23.4435\n",
      "Epoch [280/1000], Loss: 22.0292\n",
      "Epoch [290/1000], Loss: 20.8165\n",
      "Epoch [300/1000], Loss: 19.7804\n",
      "Epoch [310/1000], Loss: 18.8985\n",
      "Epoch [320/1000], Loss: 18.1507\n",
      "Epoch [330/1000], Loss: 17.5190\n",
      "Epoch [340/1000], Loss: 16.9874\n",
      "Epoch [350/1000], Loss: 16.5418\n",
      "Epoch [360/1000], Loss: 16.1696\n",
      "Epoch [370/1000], Loss: 15.8599\n",
      "Epoch [380/1000], Loss: 15.6033\n",
      "Epoch [390/1000], Loss: 15.3913\n",
      "Epoch [400/1000], Loss: 15.2171\n",
      "Epoch [410/1000], Loss: 15.0742\n",
      "Epoch [420/1000], Loss: 14.9576\n",
      "Epoch [430/1000], Loss: 14.8628\n",
      "Epoch [440/1000], Loss: 14.7860\n",
      "Epoch [450/1000], Loss: 14.7239\n",
      "Epoch [460/1000], Loss: 14.6740\n",
      "Epoch [470/1000], Loss: 14.6339\n",
      "Epoch [480/1000], Loss: 14.6019\n",
      "Epoch [490/1000], Loss: 14.5764\n",
      "Epoch [500/1000], Loss: 14.5561\n",
      "Epoch [510/1000], Loss: 14.5400\n",
      "Epoch [520/1000], Loss: 14.5273\n",
      "Epoch [530/1000], Loss: 14.5173\n",
      "Epoch [540/1000], Loss: 14.5093\n",
      "Epoch [550/1000], Loss: 14.5031\n",
      "Epoch [560/1000], Loss: 14.4981\n",
      "Epoch [570/1000], Loss: 14.4942\n",
      "Epoch [580/1000], Loss: 14.4911\n",
      "Epoch [590/1000], Loss: 14.4886\n",
      "Epoch [600/1000], Loss: 14.4867\n",
      "Epoch [610/1000], Loss: 14.4850\n",
      "Epoch [620/1000], Loss: 14.4837\n",
      "Epoch [630/1000], Loss: 14.4826\n",
      "Epoch [640/1000], Loss: 14.4816\n",
      "Epoch [650/1000], Loss: 14.4808\n",
      "Epoch [660/1000], Loss: 14.4801\n",
      "Epoch [670/1000], Loss: 14.4795\n",
      "Epoch [680/1000], Loss: 14.4789\n",
      "Epoch [690/1000], Loss: 14.4784\n",
      "Epoch [700/1000], Loss: 14.4779\n",
      "Epoch [710/1000], Loss: 14.4774\n",
      "Epoch [720/1000], Loss: 14.4769\n",
      "Epoch [730/1000], Loss: 14.4765\n",
      "Epoch [740/1000], Loss: 14.4760\n",
      "Epoch [750/1000], Loss: 14.4756\n",
      "Epoch [760/1000], Loss: 14.4752\n",
      "Epoch [770/1000], Loss: 14.4748\n",
      "Epoch [780/1000], Loss: 14.4744\n",
      "Epoch [790/1000], Loss: 14.4740\n",
      "Epoch [800/1000], Loss: 14.4736\n",
      "Epoch [810/1000], Loss: 14.4733\n",
      "Epoch [820/1000], Loss: 14.4729\n",
      "Epoch [830/1000], Loss: 14.4725\n",
      "Epoch [840/1000], Loss: 14.4722\n",
      "Epoch [850/1000], Loss: 14.4718\n",
      "Epoch [860/1000], Loss: 14.4714\n",
      "Epoch [870/1000], Loss: 14.4711\n",
      "Epoch [880/1000], Loss: 14.4708\n",
      "Epoch [890/1000], Loss: 14.4704\n",
      "Epoch [900/1000], Loss: 14.4701\n",
      "Epoch [910/1000], Loss: 14.4697\n",
      "Epoch [920/1000], Loss: 14.4694\n",
      "Epoch [930/1000], Loss: 14.4691\n",
      "Epoch [940/1000], Loss: 14.4688\n",
      "Epoch [950/1000], Loss: 14.4684\n",
      "Epoch [960/1000], Loss: 14.4681\n",
      "Epoch [970/1000], Loss: 14.4678\n",
      "Epoch [980/1000], Loss: 14.4675\n",
      "Epoch [990/1000], Loss: 14.4672\n",
      "Epoch [1000/1000], Loss: 14.4669\n",
      "Predicted days_remaining for parent_id 168: [19.297025680541992, 19.814348220825195, 19.81606674194336, 19.8150691986084, 19.814908981323242, 19.815902709960938, 19.816667556762695, 19.814001083374023]\n",
      "Training for parent_id 179...\n",
      "Epoch [10/1000], Loss: 161.5207\n",
      "Epoch [20/1000], Loss: 124.0985\n",
      "Epoch [30/1000], Loss: 99.6236\n",
      "Epoch [40/1000], Loss: 82.7577\n",
      "Epoch [50/1000], Loss: 70.1069\n",
      "Epoch [60/1000], Loss: 59.8710\n",
      "Epoch [70/1000], Loss: 51.3712\n",
      "Epoch [80/1000], Loss: 44.2852\n",
      "Epoch [90/1000], Loss: 38.3951\n",
      "Epoch [100/1000], Loss: 33.5267\n",
      "Epoch [110/1000], Loss: 29.5310\n",
      "Epoch [120/1000], Loss: 26.2779\n",
      "Epoch [130/1000], Loss: 23.6522\n",
      "Epoch [140/1000], Loss: 21.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 19.8895\n",
      "Epoch [160/1000], Loss: 18.5856\n",
      "Epoch [170/1000], Loss: 17.5735\n",
      "Epoch [180/1000], Loss: 16.7962\n",
      "Epoch [190/1000], Loss: 16.2054\n",
      "Epoch [200/1000], Loss: 15.7611\n",
      "Epoch [210/1000], Loss: 15.4304\n",
      "Epoch [220/1000], Loss: 15.1866\n",
      "Epoch [230/1000], Loss: 15.0087\n",
      "Epoch [240/1000], Loss: 14.8800\n",
      "Epoch [250/1000], Loss: 14.7876\n",
      "Epoch [260/1000], Loss: 14.7216\n",
      "Epoch [270/1000], Loss: 14.6747\n",
      "Epoch [280/1000], Loss: 14.6414\n",
      "Epoch [290/1000], Loss: 14.6176\n",
      "Epoch [300/1000], Loss: 14.6005\n",
      "Epoch [310/1000], Loss: 14.5879\n",
      "Epoch [320/1000], Loss: 14.5785\n",
      "Epoch [330/1000], Loss: 14.5712\n",
      "Epoch [340/1000], Loss: 14.5654\n",
      "Epoch [350/1000], Loss: 14.5606\n",
      "Epoch [360/1000], Loss: 14.5564\n",
      "Epoch [370/1000], Loss: 14.5527\n",
      "Epoch [380/1000], Loss: 14.5493\n",
      "Epoch [390/1000], Loss: 14.5462\n",
      "Epoch [400/1000], Loss: 14.5432\n",
      "Epoch [410/1000], Loss: 14.5404\n",
      "Epoch [420/1000], Loss: 14.5377\n",
      "Epoch [430/1000], Loss: 14.5352\n",
      "Epoch [440/1000], Loss: 14.5327\n",
      "Epoch [450/1000], Loss: 14.5303\n",
      "Epoch [460/1000], Loss: 14.5281\n",
      "Epoch [470/1000], Loss: 14.5259\n",
      "Epoch [480/1000], Loss: 14.5237\n",
      "Epoch [490/1000], Loss: 14.5217\n",
      "Epoch [500/1000], Loss: 14.5197\n",
      "Epoch [510/1000], Loss: 14.5178\n",
      "Epoch [520/1000], Loss: 14.5159\n",
      "Epoch [530/1000], Loss: 14.5142\n",
      "Epoch [540/1000], Loss: 14.5124\n",
      "Epoch [550/1000], Loss: 14.5107\n",
      "Epoch [560/1000], Loss: 14.5091\n",
      "Epoch [570/1000], Loss: 14.5076\n",
      "Epoch [580/1000], Loss: 14.5060\n",
      "Epoch [590/1000], Loss: 14.5046\n",
      "Epoch [600/1000], Loss: 14.5032\n",
      "Epoch [610/1000], Loss: 14.5018\n",
      "Epoch [620/1000], Loss: 14.5004\n",
      "Epoch [630/1000], Loss: 14.4991\n",
      "Epoch [640/1000], Loss: 14.4979\n",
      "Epoch [650/1000], Loss: 14.4967\n",
      "Epoch [660/1000], Loss: 14.4955\n",
      "Epoch [670/1000], Loss: 14.4943\n",
      "Epoch [680/1000], Loss: 14.4932\n",
      "Epoch [690/1000], Loss: 14.4921\n",
      "Epoch [700/1000], Loss: 14.4911\n",
      "Epoch [710/1000], Loss: 14.4900\n",
      "Epoch [720/1000], Loss: 14.4891\n",
      "Epoch [730/1000], Loss: 14.4881\n",
      "Epoch [740/1000], Loss: 14.4871\n",
      "Epoch [750/1000], Loss: 14.4862\n",
      "Epoch [760/1000], Loss: 14.4853\n",
      "Epoch [770/1000], Loss: 14.4845\n",
      "Epoch [780/1000], Loss: 14.4836\n",
      "Epoch [790/1000], Loss: 14.4828\n",
      "Epoch [800/1000], Loss: 14.4820\n",
      "Epoch [810/1000], Loss: 14.4812\n",
      "Epoch [820/1000], Loss: 14.4805\n",
      "Epoch [830/1000], Loss: 14.4797\n",
      "Epoch [840/1000], Loss: 14.4790\n",
      "Epoch [850/1000], Loss: 14.4783\n",
      "Epoch [860/1000], Loss: 14.4776\n",
      "Epoch [870/1000], Loss: 14.4769\n",
      "Epoch [880/1000], Loss: 14.4763\n",
      "Epoch [890/1000], Loss: 14.4756\n",
      "Epoch [900/1000], Loss: 14.4750\n",
      "Epoch [910/1000], Loss: 14.4744\n",
      "Epoch [920/1000], Loss: 14.4738\n",
      "Epoch [930/1000], Loss: 14.4732\n",
      "Epoch [940/1000], Loss: 14.4727\n",
      "Epoch [950/1000], Loss: 14.4721\n",
      "Epoch [960/1000], Loss: 14.4716\n",
      "Epoch [970/1000], Loss: 14.4711\n",
      "Epoch [980/1000], Loss: 14.4705\n",
      "Epoch [990/1000], Loss: 14.4700\n",
      "Epoch [1000/1000], Loss: 14.4696\n",
      "Predicted days_remaining for parent_id 179: [13.27595329284668, 13.817728042602539, 13.821209907531738, 13.820128440856934, 13.815189361572266, 13.817773818969727, 13.813764572143555, 13.811664581298828]\n",
      "Training for parent_id 199...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 294.4554\n",
      "Epoch [20/1000], Loss: 244.5602\n",
      "Epoch [30/1000], Loss: 204.0217\n",
      "Epoch [40/1000], Loss: 176.6497\n",
      "Epoch [50/1000], Loss: 155.8771\n",
      "Epoch [60/1000], Loss: 138.3752\n",
      "Epoch [70/1000], Loss: 123.0559\n",
      "Epoch [80/1000], Loss: 109.4780\n",
      "Epoch [90/1000], Loss: 97.3907\n",
      "Epoch [100/1000], Loss: 86.6235\n",
      "Epoch [110/1000], Loss: 77.0457\n",
      "Epoch [120/1000], Loss: 68.5483\n",
      "Epoch [130/1000], Loss: 61.0345\n",
      "Epoch [140/1000], Loss: 54.4153\n",
      "Epoch [150/1000], Loss: 48.6077\n",
      "Epoch [160/1000], Loss: 43.5334\n",
      "Epoch [170/1000], Loss: 39.1190\n",
      "Epoch [180/1000], Loss: 35.2957\n",
      "Epoch [190/1000], Loss: 31.9994\n",
      "Epoch [200/1000], Loss: 29.1705\n",
      "Epoch [210/1000], Loss: 26.7540\n",
      "Epoch [220/1000], Loss: 24.6996\n",
      "Epoch [230/1000], Loss: 22.9614\n",
      "Epoch [240/1000], Loss: 21.4978\n",
      "Epoch [250/1000], Loss: 20.2713\n",
      "Epoch [260/1000], Loss: 19.2485\n",
      "Epoch [270/1000], Loss: 18.3997\n",
      "Epoch [280/1000], Loss: 17.6986\n",
      "Epoch [290/1000], Loss: 17.1223\n",
      "Epoch [300/1000], Loss: 16.6506\n",
      "Epoch [310/1000], Loss: 16.2664\n",
      "Epoch [320/1000], Loss: 15.9548\n",
      "Epoch [330/1000], Loss: 15.7030\n",
      "Epoch [340/1000], Loss: 15.5003\n",
      "Epoch [350/1000], Loss: 15.3377\n",
      "Epoch [360/1000], Loss: 15.2076\n",
      "Epoch [370/1000], Loss: 15.1037\n",
      "Epoch [380/1000], Loss: 15.0209\n",
      "Epoch [390/1000], Loss: 14.9549\n",
      "Epoch [400/1000], Loss: 14.9022\n",
      "Epoch [410/1000], Loss: 14.8601\n",
      "Epoch [420/1000], Loss: 14.8264\n",
      "Epoch [430/1000], Loss: 14.7991\n",
      "Epoch [440/1000], Loss: 14.7769\n",
      "Epoch [450/1000], Loss: 14.7587\n",
      "Epoch [460/1000], Loss: 14.7435\n",
      "Epoch [470/1000], Loss: 14.7307\n",
      "Epoch [480/1000], Loss: 14.7198\n",
      "Epoch [490/1000], Loss: 14.7103\n",
      "Epoch [500/1000], Loss: 14.7019\n",
      "Epoch [510/1000], Loss: 14.6943\n",
      "Epoch [520/1000], Loss: 14.6875\n",
      "Epoch [530/1000], Loss: 14.6812\n",
      "Epoch [540/1000], Loss: 14.6753\n",
      "Epoch [550/1000], Loss: 14.6698\n",
      "Epoch [560/1000], Loss: 14.6645\n",
      "Epoch [570/1000], Loss: 14.6595\n",
      "Epoch [580/1000], Loss: 14.6548\n",
      "Epoch [590/1000], Loss: 14.6502\n",
      "Epoch [600/1000], Loss: 14.6457\n",
      "Epoch [610/1000], Loss: 14.6415\n",
      "Epoch [620/1000], Loss: 14.6373\n",
      "Epoch [630/1000], Loss: 14.6333\n",
      "Epoch [640/1000], Loss: 14.6295\n",
      "Epoch [650/1000], Loss: 14.6257\n",
      "Epoch [660/1000], Loss: 14.6220\n",
      "Epoch [670/1000], Loss: 14.6185\n",
      "Epoch [680/1000], Loss: 14.6150\n",
      "Epoch [690/1000], Loss: 14.6117\n",
      "Epoch [700/1000], Loss: 14.6084\n",
      "Epoch [710/1000], Loss: 14.6052\n",
      "Epoch [720/1000], Loss: 14.6021\n",
      "Epoch [730/1000], Loss: 14.5991\n",
      "Epoch [740/1000], Loss: 14.5962\n",
      "Epoch [750/1000], Loss: 14.5933\n",
      "Epoch [760/1000], Loss: 14.5906\n",
      "Epoch [770/1000], Loss: 14.5879\n",
      "Epoch [780/1000], Loss: 14.5852\n",
      "Epoch [790/1000], Loss: 14.5827\n",
      "Epoch [800/1000], Loss: 14.5801\n",
      "Epoch [810/1000], Loss: 14.5777\n",
      "Epoch [820/1000], Loss: 14.5753\n",
      "Epoch [830/1000], Loss: 14.5730\n",
      "Epoch [840/1000], Loss: 14.5707\n",
      "Epoch [850/1000], Loss: 14.5685\n",
      "Epoch [860/1000], Loss: 14.5664\n",
      "Epoch [870/1000], Loss: 14.5642\n",
      "Epoch [880/1000], Loss: 14.5622\n",
      "Epoch [890/1000], Loss: 14.5602\n",
      "Epoch [900/1000], Loss: 14.5582\n",
      "Epoch [910/1000], Loss: 14.5563\n",
      "Epoch [920/1000], Loss: 14.5544\n",
      "Epoch [930/1000], Loss: 14.5526\n",
      "Epoch [940/1000], Loss: 14.5508\n",
      "Epoch [950/1000], Loss: 14.5491\n",
      "Epoch [960/1000], Loss: 14.5473\n",
      "Epoch [970/1000], Loss: 14.5457\n",
      "Epoch [980/1000], Loss: 14.5440\n",
      "Epoch [990/1000], Loss: 14.5424\n",
      "Epoch [1000/1000], Loss: 14.5409\n",
      "Predicted days_remaining for parent_id 199: [16.898420333862305, 17.869291305541992, 17.870786666870117, 17.8697452545166, 17.86957359313965, 17.869516372680664, 17.87004280090332, 17.87000274658203]\n",
      "Training for parent_id 211...\n",
      "Epoch [10/1000], Loss: 229.4418\n",
      "Epoch [20/1000], Loss: 182.8901\n",
      "Epoch [30/1000], Loss: 147.4499\n",
      "Epoch [40/1000], Loss: 124.0410\n",
      "Epoch [50/1000], Loss: 106.8830\n",
      "Epoch [60/1000], Loss: 93.1152\n",
      "Epoch [70/1000], Loss: 81.4778\n",
      "Epoch [80/1000], Loss: 71.4469\n",
      "Epoch [90/1000], Loss: 62.7585\n",
      "Epoch [100/1000], Loss: 55.2300\n",
      "Epoch [110/1000], Loss: 48.7173\n",
      "Epoch [120/1000], Loss: 43.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/1000], Loss: 38.2531\n",
      "Epoch [140/1000], Loss: 34.1009\n",
      "Epoch [150/1000], Loss: 30.5791\n",
      "Epoch [160/1000], Loss: 27.6249\n",
      "Epoch [170/1000], Loss: 25.1686\n",
      "Epoch [180/1000], Loss: 23.1421\n",
      "Epoch [190/1000], Loss: 21.4823\n",
      "Epoch [200/1000], Loss: 20.1324\n",
      "Epoch [210/1000], Loss: 19.0422\n",
      "Epoch [220/1000], Loss: 18.1675\n",
      "Epoch [230/1000], Loss: 17.4703\n",
      "Epoch [240/1000], Loss: 16.9178\n",
      "Epoch [250/1000], Loss: 16.4824\n",
      "Epoch [260/1000], Loss: 16.1409\n",
      "Epoch [270/1000], Loss: 15.8740\n",
      "Epoch [280/1000], Loss: 15.6660\n",
      "Epoch [290/1000], Loss: 15.5041\n",
      "Epoch [300/1000], Loss: 15.3778\n",
      "Epoch [310/1000], Loss: 15.2791\n",
      "Epoch [320/1000], Loss: 15.2014\n",
      "Epoch [330/1000], Loss: 15.1398\n",
      "Epoch [340/1000], Loss: 15.0904\n",
      "Epoch [350/1000], Loss: 15.0501\n",
      "Epoch [360/1000], Loss: 15.0167\n",
      "Epoch [370/1000], Loss: 14.9885\n",
      "Epoch [380/1000], Loss: 14.9642\n",
      "Epoch [390/1000], Loss: 14.9430\n",
      "Epoch [400/1000], Loss: 14.9240\n",
      "Epoch [410/1000], Loss: 14.9068\n",
      "Epoch [420/1000], Loss: 14.8911\n",
      "Epoch [430/1000], Loss: 14.8765\n",
      "Epoch [440/1000], Loss: 14.8628\n",
      "Epoch [450/1000], Loss: 14.8500\n",
      "Epoch [460/1000], Loss: 14.8378\n",
      "Epoch [470/1000], Loss: 14.8262\n",
      "Epoch [480/1000], Loss: 14.8152\n",
      "Epoch [490/1000], Loss: 14.8047\n",
      "Epoch [500/1000], Loss: 14.7946\n",
      "Epoch [510/1000], Loss: 14.7850\n",
      "Epoch [520/1000], Loss: 14.7757\n",
      "Epoch [530/1000], Loss: 14.7669\n",
      "Epoch [540/1000], Loss: 14.7583\n",
      "Epoch [550/1000], Loss: 14.7501\n",
      "Epoch [560/1000], Loss: 14.7423\n",
      "Epoch [570/1000], Loss: 14.7347\n",
      "Epoch [580/1000], Loss: 14.7274\n",
      "Epoch [590/1000], Loss: 14.7203\n",
      "Epoch [600/1000], Loss: 14.7136\n",
      "Epoch [610/1000], Loss: 14.7070\n",
      "Epoch [620/1000], Loss: 14.7007\n",
      "Epoch [630/1000], Loss: 14.6946\n",
      "Epoch [640/1000], Loss: 14.6888\n",
      "Epoch [650/1000], Loss: 14.6831\n",
      "Epoch [660/1000], Loss: 14.6776\n",
      "Epoch [670/1000], Loss: 14.6723\n",
      "Epoch [680/1000], Loss: 14.6672\n",
      "Epoch [690/1000], Loss: 14.6622\n",
      "Epoch [700/1000], Loss: 14.6574\n",
      "Epoch [710/1000], Loss: 14.6528\n",
      "Epoch [720/1000], Loss: 14.6483\n",
      "Epoch [730/1000], Loss: 14.6439\n",
      "Epoch [740/1000], Loss: 14.6397\n",
      "Epoch [750/1000], Loss: 14.6356\n",
      "Epoch [760/1000], Loss: 14.6317\n",
      "Epoch [770/1000], Loss: 14.6278\n",
      "Epoch [780/1000], Loss: 14.6241\n",
      "Epoch [790/1000], Loss: 14.6205\n",
      "Epoch [800/1000], Loss: 14.6169\n",
      "Epoch [810/1000], Loss: 14.6135\n",
      "Epoch [820/1000], Loss: 14.6102\n",
      "Epoch [830/1000], Loss: 14.6070\n",
      "Epoch [840/1000], Loss: 14.6038\n",
      "Epoch [850/1000], Loss: 14.6008\n",
      "Epoch [860/1000], Loss: 14.5978\n",
      "Epoch [870/1000], Loss: 14.5949\n",
      "Epoch [880/1000], Loss: 14.5921\n",
      "Epoch [890/1000], Loss: 14.5894\n",
      "Epoch [900/1000], Loss: 14.5867\n",
      "Epoch [910/1000], Loss: 14.5841\n",
      "Epoch [920/1000], Loss: 14.5816\n",
      "Epoch [930/1000], Loss: 14.5791\n",
      "Epoch [940/1000], Loss: 14.5767\n",
      "Epoch [950/1000], Loss: 14.5744\n",
      "Epoch [960/1000], Loss: 14.5721\n",
      "Epoch [970/1000], Loss: 14.5699\n",
      "Epoch [980/1000], Loss: 14.5677\n",
      "Epoch [990/1000], Loss: 14.5656\n",
      "Epoch [1000/1000], Loss: 14.5635\n",
      "Predicted days_remaining for parent_id 211: [14.807172775268555, 15.878005981445312, 15.880182266235352, 15.880060195922852, 15.880334854125977, 15.880401611328125, 15.879316329956055, 15.878969192504883]\n",
      "Training for parent_id 215...\n",
      "Epoch [10/1000], Loss: 319.9106\n",
      "Epoch [20/1000], Loss: 262.7895\n",
      "Epoch [30/1000], Loss: 222.1174\n",
      "Epoch [40/1000], Loss: 193.7550\n",
      "Epoch [50/1000], Loss: 171.7151\n",
      "Epoch [60/1000], Loss: 152.9616\n",
      "Epoch [70/1000], Loss: 136.4076\n",
      "Epoch [80/1000], Loss: 121.6944\n",
      "Epoch [90/1000], Loss: 108.5793\n",
      "Epoch [100/1000], Loss: 96.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 86.4561\n",
      "Epoch [120/1000], Loss: 77.1786\n",
      "Epoch [130/1000], Loss: 68.9394\n",
      "Epoch [140/1000], Loss: 61.6395\n",
      "Epoch [150/1000], Loss: 55.1893\n",
      "Epoch [160/1000], Loss: 49.5065\n",
      "Epoch [170/1000], Loss: 44.5158\n",
      "Epoch [180/1000], Loss: 40.1475\n",
      "Epoch [190/1000], Loss: 36.3376\n",
      "Epoch [200/1000], Loss: 33.0269\n",
      "Epoch [210/1000], Loss: 30.1611\n",
      "Epoch [220/1000], Loss: 27.6901\n",
      "Epoch [230/1000], Loss: 25.5683\n",
      "Epoch [240/1000], Loss: 23.7538\n",
      "Epoch [250/1000], Loss: 22.2086\n",
      "Epoch [260/1000], Loss: 20.8984\n",
      "Epoch [270/1000], Loss: 19.7922\n",
      "Epoch [280/1000], Loss: 18.8625\n",
      "Epoch [290/1000], Loss: 18.0844\n",
      "Epoch [300/1000], Loss: 17.4360\n",
      "Epoch [310/1000], Loss: 16.8982\n",
      "Epoch [320/1000], Loss: 16.4541\n",
      "Epoch [330/1000], Loss: 16.0889\n",
      "Epoch [340/1000], Loss: 15.7898\n",
      "Epoch [350/1000], Loss: 15.5461\n",
      "Epoch [360/1000], Loss: 15.3481\n",
      "Epoch [370/1000], Loss: 15.1881\n",
      "Epoch [380/1000], Loss: 15.0592\n",
      "Epoch [390/1000], Loss: 14.9558\n",
      "Epoch [400/1000], Loss: 14.8731\n",
      "Epoch [410/1000], Loss: 14.8071\n",
      "Epoch [420/1000], Loss: 14.7547\n",
      "Epoch [430/1000], Loss: 14.7131\n",
      "Epoch [440/1000], Loss: 14.6802\n",
      "Epoch [450/1000], Loss: 14.6541\n",
      "Epoch [460/1000], Loss: 14.6335\n",
      "Epoch [470/1000], Loss: 14.6171\n",
      "Epoch [480/1000], Loss: 14.6041\n",
      "Epoch [490/1000], Loss: 14.5936\n",
      "Epoch [500/1000], Loss: 14.5852\n",
      "Epoch [510/1000], Loss: 14.5784\n",
      "Epoch [520/1000], Loss: 14.5728\n",
      "Epoch [530/1000], Loss: 14.5681\n",
      "Epoch [540/1000], Loss: 14.5642\n",
      "Epoch [550/1000], Loss: 14.5608\n",
      "Epoch [560/1000], Loss: 14.5578\n",
      "Epoch [570/1000], Loss: 14.5551\n",
      "Epoch [580/1000], Loss: 14.5527\n",
      "Epoch [590/1000], Loss: 14.5505\n",
      "Epoch [600/1000], Loss: 14.5484\n",
      "Epoch [610/1000], Loss: 14.5465\n",
      "Epoch [620/1000], Loss: 14.5446\n",
      "Epoch [630/1000], Loss: 14.5429\n",
      "Epoch [640/1000], Loss: 14.5412\n",
      "Epoch [650/1000], Loss: 14.5395\n",
      "Epoch [660/1000], Loss: 14.5379\n",
      "Epoch [670/1000], Loss: 14.5363\n",
      "Epoch [680/1000], Loss: 14.5348\n",
      "Epoch [690/1000], Loss: 14.5333\n",
      "Epoch [700/1000], Loss: 14.5319\n",
      "Epoch [710/1000], Loss: 14.5305\n",
      "Epoch [720/1000], Loss: 14.5291\n",
      "Epoch [730/1000], Loss: 14.5277\n",
      "Epoch [740/1000], Loss: 14.5264\n",
      "Epoch [750/1000], Loss: 14.5251\n",
      "Epoch [760/1000], Loss: 14.5238\n",
      "Epoch [770/1000], Loss: 14.5226\n",
      "Epoch [780/1000], Loss: 14.5213\n",
      "Epoch [790/1000], Loss: 14.5201\n",
      "Epoch [800/1000], Loss: 14.5189\n",
      "Epoch [810/1000], Loss: 14.5178\n",
      "Epoch [820/1000], Loss: 14.5166\n",
      "Epoch [830/1000], Loss: 14.5155\n",
      "Epoch [840/1000], Loss: 14.5144\n",
      "Epoch [850/1000], Loss: 14.5134\n",
      "Epoch [860/1000], Loss: 14.5123\n",
      "Epoch [870/1000], Loss: 14.5113\n",
      "Epoch [880/1000], Loss: 14.5103\n",
      "Epoch [890/1000], Loss: 14.5093\n",
      "Epoch [900/1000], Loss: 14.5083\n",
      "Epoch [910/1000], Loss: 14.5073\n",
      "Epoch [920/1000], Loss: 14.5064\n",
      "Epoch [930/1000], Loss: 14.5055\n",
      "Epoch [940/1000], Loss: 14.5046\n",
      "Epoch [950/1000], Loss: 14.5037\n",
      "Epoch [960/1000], Loss: 14.5028\n",
      "Epoch [970/1000], Loss: 14.5020\n",
      "Epoch [980/1000], Loss: 14.5011\n",
      "Epoch [990/1000], Loss: 14.5003\n",
      "Epoch [1000/1000], Loss: 14.4995\n",
      "Predicted days_remaining for parent_id 215: [18.091815948486328, 18.844097137451172, 18.845781326293945, 18.845592498779297, 18.845884323120117, 18.842803955078125, 18.842853546142578, 18.841110229492188]\n",
      "Training for parent_id 218...\n",
      "Epoch [10/1000], Loss: 824.1401\n",
      "Epoch [20/1000], Loss: 737.1603\n",
      "Epoch [30/1000], Loss: 665.6807\n",
      "Epoch [40/1000], Loss: 614.0901\n",
      "Epoch [50/1000], Loss: 572.5873\n",
      "Epoch [60/1000], Loss: 535.7078\n",
      "Epoch [70/1000], Loss: 501.7659\n",
      "Epoch [80/1000], Loss: 470.1432\n",
      "Epoch [90/1000], Loss: 440.5240\n",
      "Epoch [100/1000], Loss: 412.7094\n",
      "Epoch [110/1000], Loss: 386.5550\n",
      "Epoch [120/1000], Loss: 361.9447\n",
      "Epoch [130/1000], Loss: 338.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/1000], Loss: 316.9710\n",
      "Epoch [150/1000], Loss: 296.4406\n",
      "Epoch [160/1000], Loss: 277.1163\n",
      "Epoch [170/1000], Loss: 258.9319\n",
      "Epoch [180/1000], Loss: 241.8264\n",
      "Epoch [190/1000], Loss: 225.7428\n",
      "Epoch [200/1000], Loss: 210.6283\n",
      "Epoch [210/1000], Loss: 196.4328\n",
      "Epoch [220/1000], Loss: 183.1096\n",
      "Epoch [230/1000], Loss: 170.6140\n",
      "Epoch [240/1000], Loss: 158.9038\n",
      "Epoch [250/1000], Loss: 147.9389\n",
      "Epoch [260/1000], Loss: 137.6808\n",
      "Epoch [270/1000], Loss: 128.0928\n",
      "Epoch [280/1000], Loss: 119.1398\n",
      "Epoch [290/1000], Loss: 110.7880\n",
      "Epoch [300/1000], Loss: 103.0052\n",
      "Epoch [310/1000], Loss: 95.7604\n",
      "Epoch [320/1000], Loss: 89.0238\n",
      "Epoch [330/1000], Loss: 82.7667\n",
      "Epoch [340/1000], Loss: 76.9618\n",
      "Epoch [350/1000], Loss: 71.5827\n",
      "Epoch [360/1000], Loss: 66.6043\n",
      "Epoch [370/1000], Loss: 62.0022\n",
      "Epoch [380/1000], Loss: 57.7534\n",
      "Epoch [390/1000], Loss: 53.8358\n",
      "Epoch [400/1000], Loss: 50.2280\n",
      "Epoch [410/1000], Loss: 46.9101\n",
      "Epoch [420/1000], Loss: 43.8627\n",
      "Epoch [430/1000], Loss: 41.0674\n",
      "Epoch [440/1000], Loss: 38.5070\n",
      "Epoch [450/1000], Loss: 36.1649\n",
      "Epoch [460/1000], Loss: 34.0254\n",
      "Epoch [470/1000], Loss: 32.0737\n",
      "Epoch [480/1000], Loss: 30.2959\n",
      "Epoch [490/1000], Loss: 28.6787\n",
      "Epoch [500/1000], Loss: 27.2097\n",
      "Epoch [510/1000], Loss: 25.8773\n",
      "Epoch [520/1000], Loss: 24.6705\n",
      "Epoch [530/1000], Loss: 23.5790\n",
      "Epoch [540/1000], Loss: 22.5933\n",
      "Epoch [550/1000], Loss: 21.7044\n",
      "Epoch [560/1000], Loss: 20.9040\n",
      "Epoch [570/1000], Loss: 20.1843\n",
      "Epoch [580/1000], Loss: 19.5382\n",
      "Epoch [590/1000], Loss: 18.9590\n",
      "Epoch [600/1000], Loss: 18.4406\n",
      "Epoch [610/1000], Loss: 17.9772\n",
      "Epoch [620/1000], Loss: 17.5637\n",
      "Epoch [630/1000], Loss: 17.1952\n",
      "Epoch [640/1000], Loss: 16.8674\n",
      "Epoch [650/1000], Loss: 16.5761\n",
      "Epoch [660/1000], Loss: 16.3178\n",
      "Epoch [670/1000], Loss: 16.0890\n",
      "Epoch [680/1000], Loss: 15.8866\n",
      "Epoch [690/1000], Loss: 15.7079\n",
      "Epoch [700/1000], Loss: 15.5503\n",
      "Epoch [710/1000], Loss: 15.4116\n",
      "Epoch [720/1000], Loss: 15.2897\n",
      "Epoch [730/1000], Loss: 15.1827\n",
      "Epoch [740/1000], Loss: 15.0889\n",
      "Epoch [750/1000], Loss: 15.0069\n",
      "Epoch [760/1000], Loss: 14.9352\n",
      "Epoch [770/1000], Loss: 14.8726\n",
      "Epoch [780/1000], Loss: 14.8181\n",
      "Epoch [790/1000], Loss: 14.7707\n",
      "Epoch [800/1000], Loss: 14.7296\n",
      "Epoch [810/1000], Loss: 14.6939\n",
      "Epoch [820/1000], Loss: 14.6629\n",
      "Epoch [830/1000], Loss: 14.6362\n",
      "Epoch [840/1000], Loss: 14.6131\n",
      "Epoch [850/1000], Loss: 14.5932\n",
      "Epoch [860/1000], Loss: 14.5761\n",
      "Epoch [870/1000], Loss: 14.5613\n",
      "Epoch [880/1000], Loss: 14.5486\n",
      "Epoch [890/1000], Loss: 14.5378\n",
      "Epoch [900/1000], Loss: 14.5285\n",
      "Epoch [910/1000], Loss: 14.5205\n",
      "Epoch [920/1000], Loss: 14.5137\n",
      "Epoch [930/1000], Loss: 14.5078\n",
      "Epoch [940/1000], Loss: 14.5028\n",
      "Epoch [950/1000], Loss: 14.4986\n",
      "Epoch [960/1000], Loss: 14.4950\n",
      "Epoch [970/1000], Loss: 14.4919\n",
      "Epoch [980/1000], Loss: 14.4892\n",
      "Epoch [990/1000], Loss: 14.4870\n",
      "Epoch [1000/1000], Loss: 14.4851\n",
      "Predicted days_remaining for parent_id 218: [29.135814666748047, 29.73361587524414, 29.734243392944336, 29.734302520751953, 29.734216690063477, 29.734575271606445, 29.734607696533203, 29.732885360717773]\n",
      "Training for parent_id 234...\n",
      "Epoch [10/1000], Loss: 1198.7527\n",
      "Epoch [20/1000], Loss: 1082.6392\n",
      "Epoch [30/1000], Loss: 987.3798\n",
      "Epoch [40/1000], Loss: 918.4155\n",
      "Epoch [50/1000], Loss: 864.6473\n",
      "Epoch [60/1000], Loss: 817.5681\n",
      "Epoch [70/1000], Loss: 774.6221\n",
      "Epoch [80/1000], Loss: 734.3639\n",
      "Epoch [90/1000], Loss: 696.2373\n",
      "Epoch [100/1000], Loss: 660.0402\n",
      "Epoch [110/1000], Loss: 625.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/1000], Loss: 592.8657\n",
      "Epoch [130/1000], Loss: 561.6631\n",
      "Epoch [140/1000], Loss: 531.9340\n",
      "Epoch [150/1000], Loss: 503.6115\n",
      "Epoch [160/1000], Loss: 476.6372\n",
      "Epoch [170/1000], Loss: 450.9549\n",
      "Epoch [180/1000], Loss: 426.5084\n",
      "Epoch [190/1000], Loss: 403.2423\n",
      "Epoch [200/1000], Loss: 381.1041\n",
      "Epoch [210/1000], Loss: 360.0434\n",
      "Epoch [220/1000], Loss: 340.0131\n",
      "Epoch [230/1000], Loss: 320.9686\n",
      "Epoch [240/1000], Loss: 302.8679\n",
      "Epoch [250/1000], Loss: 285.6711\n",
      "Epoch [260/1000], Loss: 269.3401\n",
      "Epoch [270/1000], Loss: 253.8390\n",
      "Epoch [280/1000], Loss: 239.1329\n",
      "Epoch [290/1000], Loss: 225.1886\n",
      "Epoch [300/1000], Loss: 211.9743\n",
      "Epoch [310/1000], Loss: 199.4591\n",
      "Epoch [320/1000], Loss: 187.6135\n",
      "Epoch [330/1000], Loss: 176.4090\n",
      "Epoch [340/1000], Loss: 165.8178\n",
      "Epoch [350/1000], Loss: 155.8134\n",
      "Epoch [360/1000], Loss: 146.3700\n",
      "Epoch [370/1000], Loss: 137.4626\n",
      "Epoch [380/1000], Loss: 129.0671\n",
      "Epoch [390/1000], Loss: 121.1601\n",
      "Epoch [400/1000], Loss: 113.7190\n",
      "Epoch [410/1000], Loss: 106.7219\n",
      "Epoch [420/1000], Loss: 100.1477\n",
      "Epoch [430/1000], Loss: 93.9760\n",
      "Epoch [440/1000], Loss: 88.1870\n",
      "Epoch [450/1000], Loss: 82.7616\n",
      "Epoch [460/1000], Loss: 77.6813\n",
      "Epoch [470/1000], Loss: 72.9284\n",
      "Epoch [480/1000], Loss: 68.4857\n",
      "Epoch [490/1000], Loss: 64.3369\n",
      "Epoch [500/1000], Loss: 60.4658\n",
      "Epoch [510/1000], Loss: 56.8574\n",
      "Epoch [520/1000], Loss: 53.4969\n",
      "Epoch [530/1000], Loss: 50.3702\n",
      "Epoch [540/1000], Loss: 47.4638\n",
      "Epoch [550/1000], Loss: 44.7648\n",
      "Epoch [560/1000], Loss: 42.2609\n",
      "Epoch [570/1000], Loss: 39.9402\n",
      "Epoch [580/1000], Loss: 37.7914\n",
      "Epoch [590/1000], Loss: 35.8038\n",
      "Epoch [600/1000], Loss: 33.9671\n",
      "Epoch [610/1000], Loss: 32.2716\n",
      "Epoch [620/1000], Loss: 30.7079\n",
      "Epoch [630/1000], Loss: 29.2674\n",
      "Epoch [640/1000], Loss: 27.9417\n",
      "Epoch [650/1000], Loss: 26.7228\n",
      "Epoch [660/1000], Loss: 25.6034\n",
      "Epoch [670/1000], Loss: 24.5764\n",
      "Epoch [680/1000], Loss: 23.6351\n",
      "Epoch [690/1000], Loss: 22.7734\n",
      "Epoch [700/1000], Loss: 21.9852\n",
      "Epoch [710/1000], Loss: 21.2652\n",
      "Epoch [720/1000], Loss: 20.6080\n",
      "Epoch [730/1000], Loss: 20.0089\n",
      "Epoch [740/1000], Loss: 19.4633\n",
      "Epoch [750/1000], Loss: 18.9670\n",
      "Epoch [760/1000], Loss: 18.5160\n",
      "Epoch [770/1000], Loss: 18.1066\n",
      "Epoch [780/1000], Loss: 17.7354\n",
      "Epoch [790/1000], Loss: 17.3992\n",
      "Epoch [800/1000], Loss: 17.0950\n",
      "Epoch [810/1000], Loss: 16.8200\n",
      "Epoch [820/1000], Loss: 16.5718\n",
      "Epoch [830/1000], Loss: 16.3480\n",
      "Epoch [840/1000], Loss: 16.1464\n",
      "Epoch [850/1000], Loss: 15.9649\n",
      "Epoch [860/1000], Loss: 15.8018\n",
      "Epoch [870/1000], Loss: 15.6554\n",
      "Epoch [880/1000], Loss: 15.5241\n",
      "Epoch [890/1000], Loss: 15.4065\n",
      "Epoch [900/1000], Loss: 15.3012\n",
      "Epoch [910/1000], Loss: 15.2071\n",
      "Epoch [920/1000], Loss: 15.1231\n",
      "Epoch [930/1000], Loss: 15.0482\n",
      "Epoch [940/1000], Loss: 14.9814\n",
      "Epoch [950/1000], Loss: 14.9220\n",
      "Epoch [960/1000], Loss: 14.8692\n",
      "Epoch [970/1000], Loss: 14.8223\n",
      "Epoch [980/1000], Loss: 14.7807\n",
      "Epoch [990/1000], Loss: 14.7439\n",
      "Epoch [1000/1000], Loss: 14.7113\n",
      "Predicted days_remaining for parent_id 234: [34.716773986816406, 35.35416030883789, 35.353668212890625, 35.3545036315918, 35.35463333129883, 35.354042053222656, 35.35401916503906, 35.352378845214844]\n",
      "Training for parent_id 238...\n",
      "Epoch [10/1000], Loss: 763.7023\n",
      "Epoch [20/1000], Loss: 672.8649\n",
      "Epoch [30/1000], Loss: 602.2313\n",
      "Epoch [40/1000], Loss: 553.7481\n",
      "Epoch [50/1000], Loss: 515.2254\n",
      "Epoch [60/1000], Loss: 480.9695\n",
      "Epoch [70/1000], Loss: 449.4086\n",
      "Epoch [80/1000], Loss: 420.0056\n",
      "Epoch [90/1000], Loss: 392.4956\n",
      "Epoch [100/1000], Loss: 366.7006\n",
      "Epoch [110/1000], Loss: 342.4878\n",
      "Epoch [120/1000], Loss: 319.7502\n",
      "Epoch [130/1000], Loss: 298.3958\n",
      "Epoch [140/1000], Loss: 278.3427\n",
      "Epoch [150/1000], Loss: 259.5156\n",
      "Epoch [160/1000], Loss: 241.8456\n",
      "Epoch [170/1000], Loss: 225.2684\n",
      "Epoch [180/1000], Loss: 209.7242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000], Loss: 195.1572\n",
      "Epoch [200/1000], Loss: 181.5148\n",
      "Epoch [210/1000], Loss: 168.7477\n",
      "Epoch [220/1000], Loss: 156.8092\n",
      "Epoch [230/1000], Loss: 145.6550\n",
      "Epoch [240/1000], Loss: 135.2432\n",
      "Epoch [250/1000], Loss: 125.5338\n",
      "Epoch [260/1000], Loss: 116.4885\n",
      "Epoch [270/1000], Loss: 108.0708\n",
      "Epoch [280/1000], Loss: 100.2460\n",
      "Epoch [290/1000], Loss: 92.9805\n",
      "Epoch [300/1000], Loss: 86.2422\n",
      "Epoch [310/1000], Loss: 80.0006\n",
      "Epoch [320/1000], Loss: 74.2261\n",
      "Epoch [330/1000], Loss: 68.8907\n",
      "Epoch [340/1000], Loss: 63.9674\n",
      "Epoch [350/1000], Loss: 59.4303\n",
      "Epoch [360/1000], Loss: 55.2549\n",
      "Epoch [370/1000], Loss: 51.4175\n",
      "Epoch [380/1000], Loss: 47.8958\n",
      "Epoch [390/1000], Loss: 44.6684\n",
      "Epoch [400/1000], Loss: 41.7148\n",
      "Epoch [410/1000], Loss: 39.0160\n",
      "Epoch [420/1000], Loss: 36.5534\n",
      "Epoch [430/1000], Loss: 34.3097\n",
      "Epoch [440/1000], Loss: 32.2686\n",
      "Epoch [450/1000], Loss: 30.4146\n",
      "Epoch [460/1000], Loss: 28.7331\n",
      "Epoch [470/1000], Loss: 27.2104\n",
      "Epoch [480/1000], Loss: 25.8336\n",
      "Epoch [490/1000], Loss: 24.5907\n",
      "Epoch [500/1000], Loss: 23.4705\n",
      "Epoch [510/1000], Loss: 22.4624\n",
      "Epoch [520/1000], Loss: 21.5567\n",
      "Epoch [530/1000], Loss: 20.7443\n",
      "Epoch [540/1000], Loss: 20.0166\n",
      "Epoch [550/1000], Loss: 19.3660\n",
      "Epoch [560/1000], Loss: 18.7852\n",
      "Epoch [570/1000], Loss: 18.2675\n",
      "Epoch [580/1000], Loss: 17.8069\n",
      "Epoch [590/1000], Loss: 17.3977\n",
      "Epoch [600/1000], Loss: 17.0347\n",
      "Epoch [610/1000], Loss: 16.7133\n",
      "Epoch [620/1000], Loss: 16.4292\n",
      "Epoch [630/1000], Loss: 16.1785\n",
      "Epoch [640/1000], Loss: 15.9576\n",
      "Epoch [650/1000], Loss: 15.7632\n",
      "Epoch [660/1000], Loss: 15.5925\n",
      "Epoch [670/1000], Loss: 15.4429\n",
      "Epoch [680/1000], Loss: 15.3118\n",
      "Epoch [690/1000], Loss: 15.1974\n",
      "Epoch [700/1000], Loss: 15.0975\n",
      "Epoch [710/1000], Loss: 15.0105\n",
      "Epoch [720/1000], Loss: 14.9348\n",
      "Epoch [730/1000], Loss: 14.8691\n",
      "Epoch [740/1000], Loss: 14.8122\n",
      "Epoch [750/1000], Loss: 14.7629\n",
      "Epoch [760/1000], Loss: 14.7203\n",
      "Epoch [770/1000], Loss: 14.6836\n",
      "Epoch [780/1000], Loss: 14.6520\n",
      "Epoch [790/1000], Loss: 14.6248\n",
      "Epoch [800/1000], Loss: 14.6014\n",
      "Epoch [810/1000], Loss: 14.5814\n",
      "Epoch [820/1000], Loss: 14.5643\n",
      "Epoch [830/1000], Loss: 14.5496\n",
      "Epoch [840/1000], Loss: 14.5371\n",
      "Epoch [850/1000], Loss: 14.5265\n",
      "Epoch [860/1000], Loss: 14.5174\n",
      "Epoch [870/1000], Loss: 14.5097\n",
      "Epoch [880/1000], Loss: 14.5031\n",
      "Epoch [890/1000], Loss: 14.4976\n",
      "Epoch [900/1000], Loss: 14.4929\n",
      "Epoch [910/1000], Loss: 14.4889\n",
      "Epoch [920/1000], Loss: 14.4855\n",
      "Epoch [930/1000], Loss: 14.4827\n",
      "Epoch [940/1000], Loss: 14.4802\n",
      "Epoch [950/1000], Loss: 14.4782\n",
      "Epoch [960/1000], Loss: 14.4765\n",
      "Epoch [970/1000], Loss: 14.4750\n",
      "Epoch [980/1000], Loss: 14.4737\n",
      "Epoch [990/1000], Loss: 14.4727\n",
      "Epoch [1000/1000], Loss: 14.4718\n",
      "Predicted days_remaining for parent_id 238: [28.227542877197266, 28.760536193847656, 28.758947372436523, 28.758726119995117, 28.75884437561035, 28.759437561035156, 28.75940704345703, 28.760644912719727]\n",
      "Training for parent_id 256...\n",
      "Epoch [10/1000], Loss: 85.9618\n",
      "Epoch [20/1000], Loss: 62.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 47.2628\n",
      "Epoch [40/1000], Loss: 38.2851\n",
      "Epoch [50/1000], Loss: 31.8404\n",
      "Epoch [60/1000], Loss: 27.0019\n",
      "Epoch [70/1000], Loss: 23.3833\n",
      "Epoch [80/1000], Loss: 20.7202\n",
      "Epoch [90/1000], Loss: 18.8013\n",
      "Epoch [100/1000], Loss: 17.4515\n",
      "Epoch [110/1000], Loss: 16.5261\n",
      "Epoch [120/1000], Loss: 15.9077\n",
      "Epoch [130/1000], Loss: 15.5040\n",
      "Epoch [140/1000], Loss: 15.2452\n",
      "Epoch [150/1000], Loss: 15.0810\n",
      "Epoch [160/1000], Loss: 14.9763\n",
      "Epoch [170/1000], Loss: 14.9080\n",
      "Epoch [180/1000], Loss: 14.8614\n",
      "Epoch [190/1000], Loss: 14.8274\n",
      "Epoch [200/1000], Loss: 14.8008\n",
      "Epoch [210/1000], Loss: 14.7788\n",
      "Epoch [220/1000], Loss: 14.7597\n",
      "Epoch [230/1000], Loss: 14.7426\n",
      "Epoch [240/1000], Loss: 14.7270\n",
      "Epoch [250/1000], Loss: 14.7128\n",
      "Epoch [260/1000], Loss: 14.6996\n",
      "Epoch [270/1000], Loss: 14.6874\n",
      "Epoch [280/1000], Loss: 14.6760\n",
      "Epoch [290/1000], Loss: 14.6654\n",
      "Epoch [300/1000], Loss: 14.6556\n",
      "Epoch [310/1000], Loss: 14.6464\n",
      "Epoch [320/1000], Loss: 14.6378\n",
      "Epoch [330/1000], Loss: 14.6297\n",
      "Epoch [340/1000], Loss: 14.6221\n",
      "Epoch [350/1000], Loss: 14.6150\n",
      "Epoch [360/1000], Loss: 14.6083\n",
      "Epoch [370/1000], Loss: 14.6020\n",
      "Epoch [380/1000], Loss: 14.5961\n",
      "Epoch [390/1000], Loss: 14.5904\n",
      "Epoch [400/1000], Loss: 14.5851\n",
      "Epoch [410/1000], Loss: 14.5801\n",
      "Epoch [420/1000], Loss: 14.5753\n",
      "Epoch [430/1000], Loss: 14.5708\n",
      "Epoch [440/1000], Loss: 14.5665\n",
      "Epoch [450/1000], Loss: 14.5624\n",
      "Epoch [460/1000], Loss: 14.5585\n",
      "Epoch [470/1000], Loss: 14.5548\n",
      "Epoch [480/1000], Loss: 14.5513\n",
      "Epoch [490/1000], Loss: 14.5479\n",
      "Epoch [500/1000], Loss: 14.5447\n",
      "Epoch [510/1000], Loss: 14.5416\n",
      "Epoch [520/1000], Loss: 14.5387\n",
      "Epoch [530/1000], Loss: 14.5359\n",
      "Epoch [540/1000], Loss: 14.5332\n",
      "Epoch [550/1000], Loss: 14.5306\n",
      "Epoch [560/1000], Loss: 14.5282\n",
      "Epoch [570/1000], Loss: 14.5258\n",
      "Epoch [580/1000], Loss: 14.5235\n",
      "Epoch [590/1000], Loss: 14.5213\n",
      "Epoch [600/1000], Loss: 14.5192\n",
      "Epoch [610/1000], Loss: 14.5172\n",
      "Epoch [620/1000], Loss: 14.5153\n",
      "Epoch [630/1000], Loss: 14.5134\n",
      "Epoch [640/1000], Loss: 14.5116\n",
      "Epoch [650/1000], Loss: 14.5099\n",
      "Epoch [660/1000], Loss: 14.5082\n",
      "Epoch [670/1000], Loss: 14.5066\n",
      "Epoch [680/1000], Loss: 14.5050\n",
      "Epoch [690/1000], Loss: 14.5035\n",
      "Epoch [700/1000], Loss: 14.5020\n",
      "Epoch [710/1000], Loss: 14.5006\n",
      "Epoch [720/1000], Loss: 14.4993\n",
      "Epoch [730/1000], Loss: 14.4980\n",
      "Epoch [740/1000], Loss: 14.4967\n",
      "Epoch [750/1000], Loss: 14.4955\n",
      "Epoch [760/1000], Loss: 14.4943\n",
      "Epoch [770/1000], Loss: 14.4931\n",
      "Epoch [780/1000], Loss: 14.4920\n",
      "Epoch [790/1000], Loss: 14.4909\n",
      "Epoch [800/1000], Loss: 14.4898\n",
      "Epoch [810/1000], Loss: 14.4888\n",
      "Epoch [820/1000], Loss: 14.4878\n",
      "Epoch [830/1000], Loss: 14.4869\n",
      "Epoch [840/1000], Loss: 14.4859\n",
      "Epoch [850/1000], Loss: 14.4850\n",
      "Epoch [860/1000], Loss: 14.4841\n",
      "Epoch [870/1000], Loss: 14.4833\n",
      "Epoch [880/1000], Loss: 14.4825\n",
      "Epoch [890/1000], Loss: 14.4816\n",
      "Epoch [900/1000], Loss: 14.4809\n",
      "Epoch [910/1000], Loss: 14.4801\n",
      "Epoch [920/1000], Loss: 14.4793\n",
      "Epoch [930/1000], Loss: 14.4786\n",
      "Epoch [940/1000], Loss: 14.4779\n",
      "Epoch [950/1000], Loss: 14.4772\n",
      "Epoch [960/1000], Loss: 14.4765\n",
      "Epoch [970/1000], Loss: 14.4759\n",
      "Epoch [980/1000], Loss: 14.4752\n",
      "Epoch [990/1000], Loss: 14.4746\n",
      "Epoch [1000/1000], Loss: 14.4740\n",
      "Predicted days_remaining for parent_id 256: [9.24211597442627, 9.816946983337402, 9.819412231445312, 9.820351600646973, 9.821043014526367, 9.820843696594238, 9.819141387939453, 9.817427635192871]\n",
      "Training for parent_id 265...\n",
      "Epoch [10/1000], Loss: 83.2978\n",
      "Epoch [20/1000], Loss: 56.1316\n",
      "Epoch [30/1000], Loss: 41.3538\n",
      "Epoch [40/1000], Loss: 33.0777\n",
      "Epoch [50/1000], Loss: 27.5857\n",
      "Epoch [60/1000], Loss: 23.6546\n",
      "Epoch [70/1000], Loss: 20.8160\n",
      "Epoch [80/1000], Loss: 18.7974\n",
      "Epoch [90/1000], Loss: 17.3944\n",
      "Epoch [100/1000], Loss: 16.4459\n",
      "Epoch [110/1000], Loss: 15.8239\n",
      "Epoch [120/1000], Loss: 15.4276\n",
      "Epoch [130/1000], Loss: 15.1808\n",
      "Epoch [140/1000], Loss: 15.0288\n",
      "Epoch [150/1000], Loss: 14.9345\n",
      "Epoch [160/1000], Loss: 14.8741\n",
      "Epoch [170/1000], Loss: 14.8329\n",
      "Epoch [180/1000], Loss: 14.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/1000], Loss: 14.7782\n",
      "Epoch [200/1000], Loss: 14.7576\n",
      "Epoch [210/1000], Loss: 14.7394\n",
      "Epoch [220/1000], Loss: 14.7229\n",
      "Epoch [230/1000], Loss: 14.7079\n",
      "Epoch [240/1000], Loss: 14.6941\n",
      "Epoch [250/1000], Loss: 14.6813\n",
      "Epoch [260/1000], Loss: 14.6695\n",
      "Epoch [270/1000], Loss: 14.6586\n",
      "Epoch [280/1000], Loss: 14.6484\n",
      "Epoch [290/1000], Loss: 14.6390\n",
      "Epoch [300/1000], Loss: 14.6302\n",
      "Epoch [310/1000], Loss: 14.6219\n",
      "Epoch [320/1000], Loss: 14.6142\n",
      "Epoch [330/1000], Loss: 14.6070\n",
      "Epoch [340/1000], Loss: 14.6003\n",
      "Epoch [350/1000], Loss: 14.5939\n",
      "Epoch [360/1000], Loss: 14.5879\n",
      "Epoch [370/1000], Loss: 14.5823\n",
      "Epoch [380/1000], Loss: 14.5770\n",
      "Epoch [390/1000], Loss: 14.5720\n",
      "Epoch [400/1000], Loss: 14.5673\n",
      "Epoch [410/1000], Loss: 14.5628\n",
      "Epoch [420/1000], Loss: 14.5585\n",
      "Epoch [430/1000], Loss: 14.5545\n",
      "Epoch [440/1000], Loss: 14.5507\n",
      "Epoch [450/1000], Loss: 14.5471\n",
      "Epoch [460/1000], Loss: 14.5436\n",
      "Epoch [470/1000], Loss: 14.5403\n",
      "Epoch [480/1000], Loss: 14.5372\n",
      "Epoch [490/1000], Loss: 14.5342\n",
      "Epoch [500/1000], Loss: 14.5313\n",
      "Epoch [510/1000], Loss: 14.5286\n",
      "Epoch [520/1000], Loss: 14.5260\n",
      "Epoch [530/1000], Loss: 14.5235\n",
      "Epoch [540/1000], Loss: 14.5212\n",
      "Epoch [550/1000], Loss: 14.5189\n",
      "Epoch [560/1000], Loss: 14.5167\n",
      "Epoch [570/1000], Loss: 14.5146\n",
      "Epoch [580/1000], Loss: 14.5126\n",
      "Epoch [590/1000], Loss: 14.5107\n",
      "Epoch [600/1000], Loss: 14.5088\n",
      "Epoch [610/1000], Loss: 14.5070\n",
      "Epoch [620/1000], Loss: 14.5053\n",
      "Epoch [630/1000], Loss: 14.5036\n",
      "Epoch [640/1000], Loss: 14.5021\n",
      "Epoch [650/1000], Loss: 14.5005\n",
      "Epoch [660/1000], Loss: 14.4991\n",
      "Epoch [670/1000], Loss: 14.4976\n",
      "Epoch [680/1000], Loss: 14.4963\n",
      "Epoch [690/1000], Loss: 14.4949\n",
      "Epoch [700/1000], Loss: 14.4936\n",
      "Epoch [710/1000], Loss: 14.4924\n",
      "Epoch [720/1000], Loss: 14.4912\n",
      "Epoch [730/1000], Loss: 14.4901\n",
      "Epoch [740/1000], Loss: 14.4889\n",
      "Epoch [750/1000], Loss: 14.4879\n",
      "Epoch [760/1000], Loss: 14.4868\n",
      "Epoch [770/1000], Loss: 14.4858\n",
      "Epoch [780/1000], Loss: 14.4848\n",
      "Epoch [790/1000], Loss: 14.4839\n",
      "Epoch [800/1000], Loss: 14.4829\n",
      "Epoch [810/1000], Loss: 14.4820\n",
      "Epoch [820/1000], Loss: 14.4812\n",
      "Epoch [830/1000], Loss: 14.4803\n",
      "Epoch [840/1000], Loss: 14.4795\n",
      "Epoch [850/1000], Loss: 14.4787\n",
      "Epoch [860/1000], Loss: 14.4779\n",
      "Epoch [870/1000], Loss: 14.4772\n",
      "Epoch [880/1000], Loss: 14.4764\n",
      "Epoch [890/1000], Loss: 14.4757\n",
      "Epoch [900/1000], Loss: 14.4750\n",
      "Epoch [910/1000], Loss: 14.4744\n",
      "Epoch [920/1000], Loss: 14.4737\n",
      "Epoch [930/1000], Loss: 14.4731\n",
      "Epoch [940/1000], Loss: 14.4725\n",
      "Epoch [950/1000], Loss: 14.4719\n",
      "Epoch [960/1000], Loss: 14.4713\n",
      "Epoch [970/1000], Loss: 14.4707\n",
      "Epoch [980/1000], Loss: 14.4701\n",
      "Epoch [990/1000], Loss: 14.4696\n",
      "Epoch [1000/1000], Loss: 14.4691\n",
      "Predicted days_remaining for parent_id 265: [9.277735710144043, 9.818553924560547, 9.816789627075195, 9.815939903259277, 9.813244819641113, 9.81227970123291, 9.813942909240723, 9.810078620910645]\n",
      "Training for parent_id 277...\n",
      "Epoch [10/1000], Loss: 536.5276\n",
      "Epoch [20/1000], Loss: 469.9014\n",
      "Epoch [30/1000], Loss: 411.7242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000], Loss: 370.7356\n",
      "Epoch [50/1000], Loss: 338.4209\n",
      "Epoch [60/1000], Loss: 310.5888\n",
      "Epoch [70/1000], Loss: 285.4360\n",
      "Epoch [80/1000], Loss: 262.3442\n",
      "Epoch [90/1000], Loss: 241.0437\n",
      "Epoch [100/1000], Loss: 221.3520\n",
      "Epoch [110/1000], Loss: 203.1302\n",
      "Epoch [120/1000], Loss: 186.2686\n",
      "Epoch [130/1000], Loss: 170.6765\n",
      "Epoch [140/1000], Loss: 156.2751\n",
      "Epoch [150/1000], Loss: 142.9931\n",
      "Epoch [160/1000], Loss: 130.7636\n",
      "Epoch [170/1000], Loss: 119.5224\n",
      "Epoch [180/1000], Loss: 109.2074\n",
      "Epoch [190/1000], Loss: 99.7585\n",
      "Epoch [200/1000], Loss: 91.1178\n",
      "Epoch [210/1000], Loss: 83.2301\n",
      "Epoch [220/1000], Loss: 76.0425\n",
      "Epoch [230/1000], Loss: 69.5048\n",
      "Epoch [240/1000], Loss: 63.5695\n",
      "Epoch [250/1000], Loss: 58.1914\n",
      "Epoch [260/1000], Loss: 53.3278\n",
      "Epoch [270/1000], Loss: 48.9384\n",
      "Epoch [280/1000], Loss: 44.9851\n",
      "Epoch [290/1000], Loss: 41.4322\n",
      "Epoch [300/1000], Loss: 38.2461\n",
      "Epoch [310/1000], Loss: 35.3950\n",
      "Epoch [320/1000], Loss: 32.8494\n",
      "Epoch [330/1000], Loss: 30.5818\n",
      "Epoch [340/1000], Loss: 28.5664\n",
      "Epoch [350/1000], Loss: 26.7792\n",
      "Epoch [360/1000], Loss: 25.1982\n",
      "Epoch [370/1000], Loss: 23.8029\n",
      "Epoch [380/1000], Loss: 22.5743\n",
      "Epoch [390/1000], Loss: 21.4952\n",
      "Epoch [400/1000], Loss: 20.5495\n",
      "Epoch [410/1000], Loss: 19.7229\n",
      "Epoch [420/1000], Loss: 19.0019\n",
      "Epoch [430/1000], Loss: 18.3747\n",
      "Epoch [440/1000], Loss: 17.8304\n",
      "Epoch [450/1000], Loss: 17.3591\n",
      "Epoch [460/1000], Loss: 16.9520\n",
      "Epoch [470/1000], Loss: 16.6012\n",
      "Epoch [480/1000], Loss: 16.2996\n",
      "Epoch [490/1000], Loss: 16.0409\n",
      "Epoch [500/1000], Loss: 15.8196\n",
      "Epoch [510/1000], Loss: 15.6306\n",
      "Epoch [520/1000], Loss: 15.4695\n",
      "Epoch [530/1000], Loss: 15.3326\n",
      "Epoch [540/1000], Loss: 15.2164\n",
      "Epoch [550/1000], Loss: 15.1180\n",
      "Epoch [560/1000], Loss: 15.0348\n",
      "Epoch [570/1000], Loss: 14.9647\n",
      "Epoch [580/1000], Loss: 14.9055\n",
      "Epoch [590/1000], Loss: 14.8557\n",
      "Epoch [600/1000], Loss: 14.8139\n",
      "Epoch [610/1000], Loss: 14.7787\n",
      "Epoch [620/1000], Loss: 14.7492\n",
      "Epoch [630/1000], Loss: 14.7244\n",
      "Epoch [640/1000], Loss: 14.7036\n",
      "Epoch [650/1000], Loss: 14.6861\n",
      "Epoch [660/1000], Loss: 14.6713\n",
      "Epoch [670/1000], Loss: 14.6589\n",
      "Epoch [680/1000], Loss: 14.6483\n",
      "Epoch [690/1000], Loss: 14.6393\n",
      "Epoch [700/1000], Loss: 14.6316\n",
      "Epoch [710/1000], Loss: 14.6250\n",
      "Epoch [720/1000], Loss: 14.6193\n",
      "Epoch [730/1000], Loss: 14.6143\n",
      "Epoch [740/1000], Loss: 14.6099\n",
      "Epoch [750/1000], Loss: 14.6060\n",
      "Epoch [760/1000], Loss: 14.6025\n",
      "Epoch [770/1000], Loss: 14.5994\n",
      "Epoch [780/1000], Loss: 14.5965\n",
      "Epoch [790/1000], Loss: 14.5938\n",
      "Epoch [800/1000], Loss: 14.5913\n",
      "Epoch [810/1000], Loss: 14.5890\n",
      "Epoch [820/1000], Loss: 14.5868\n",
      "Epoch [830/1000], Loss: 14.5847\n",
      "Epoch [840/1000], Loss: 14.5827\n",
      "Epoch [850/1000], Loss: 14.5807\n",
      "Epoch [860/1000], Loss: 14.5789\n",
      "Epoch [870/1000], Loss: 14.5771\n",
      "Epoch [880/1000], Loss: 14.5753\n",
      "Epoch [890/1000], Loss: 14.5736\n",
      "Epoch [900/1000], Loss: 14.5719\n",
      "Epoch [910/1000], Loss: 14.5703\n",
      "Epoch [920/1000], Loss: 14.5687\n",
      "Epoch [930/1000], Loss: 14.5671\n",
      "Epoch [940/1000], Loss: 14.5655\n",
      "Epoch [950/1000], Loss: 14.5640\n",
      "Epoch [960/1000], Loss: 14.5625\n",
      "Epoch [970/1000], Loss: 14.5610\n",
      "Epoch [980/1000], Loss: 14.5596\n",
      "Epoch [990/1000], Loss: 14.5582\n",
      "Epoch [1000/1000], Loss: 14.5568\n",
      "Predicted days_remaining for parent_id 277: [22.835397720336914, 23.87790870666504, 23.879281997680664, 23.878829956054688, 23.879201889038086, 23.880006790161133, 23.880020141601562, 23.87757682800293]\n",
      "Training for parent_id 281...\n",
      "Epoch [10/1000], Loss: 216.3583\n",
      "Epoch [20/1000], Loss: 172.3781\n",
      "Epoch [30/1000], Loss: 143.5558\n",
      "Epoch [40/1000], Loss: 124.4206\n",
      "Epoch [50/1000], Loss: 109.0330\n",
      "Epoch [60/1000], Loss: 95.8557\n",
      "Epoch [70/1000], Loss: 84.3720\n",
      "Epoch [80/1000], Loss: 74.3018\n",
      "Epoch [90/1000], Loss: 65.4596\n",
      "Epoch [100/1000], Loss: 57.7173\n",
      "Epoch [110/1000], Loss: 50.9684\n",
      "Epoch [120/1000], Loss: 45.1157\n",
      "Epoch [130/1000], Loss: 40.0685\n",
      "Epoch [140/1000], Loss: 35.7417\n",
      "Epoch [150/1000], Loss: 32.0554\n",
      "Epoch [160/1000], Loss: 28.9348\n",
      "Epoch [170/1000], Loss: 26.3100\n",
      "Epoch [180/1000], Loss: 24.1166\n",
      "Epoch [190/1000], Loss: 22.2958\n",
      "Epoch [200/1000], Loss: 20.7942\n",
      "Epoch [210/1000], Loss: 19.5641\n",
      "Epoch [220/1000], Loss: 18.5630\n",
      "Epoch [230/1000], Loss: 17.7536\n",
      "Epoch [240/1000], Loss: 17.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/1000], Loss: 16.5841\n",
      "Epoch [260/1000], Loss: 16.1721\n",
      "Epoch [270/1000], Loss: 15.8470\n",
      "Epoch [280/1000], Loss: 15.5918\n",
      "Epoch [290/1000], Loss: 15.3924\n",
      "Epoch [300/1000], Loss: 15.2373\n",
      "Epoch [310/1000], Loss: 15.1169\n",
      "Epoch [320/1000], Loss: 15.0236\n",
      "Epoch [330/1000], Loss: 14.9514\n",
      "Epoch [340/1000], Loss: 14.8953\n",
      "Epoch [350/1000], Loss: 14.8517\n",
      "Epoch [360/1000], Loss: 14.8174\n",
      "Epoch [370/1000], Loss: 14.7903\n",
      "Epoch [380/1000], Loss: 14.7686\n",
      "Epoch [390/1000], Loss: 14.7508\n",
      "Epoch [400/1000], Loss: 14.7361\n",
      "Epoch [410/1000], Loss: 14.7237\n",
      "Epoch [420/1000], Loss: 14.7129\n",
      "Epoch [430/1000], Loss: 14.7035\n",
      "Epoch [440/1000], Loss: 14.6950\n",
      "Epoch [450/1000], Loss: 14.6873\n",
      "Epoch [460/1000], Loss: 14.6801\n",
      "Epoch [470/1000], Loss: 14.6735\n",
      "Epoch [480/1000], Loss: 14.6672\n",
      "Epoch [490/1000], Loss: 14.6612\n",
      "Epoch [500/1000], Loss: 14.6556\n",
      "Epoch [510/1000], Loss: 14.6502\n",
      "Epoch [520/1000], Loss: 14.6450\n",
      "Epoch [530/1000], Loss: 14.6400\n",
      "Epoch [540/1000], Loss: 14.6352\n",
      "Epoch [550/1000], Loss: 14.6305\n",
      "Epoch [560/1000], Loss: 14.6261\n",
      "Epoch [570/1000], Loss: 14.6217\n",
      "Epoch [580/1000], Loss: 14.6176\n",
      "Epoch [590/1000], Loss: 14.6135\n",
      "Epoch [600/1000], Loss: 14.6096\n",
      "Epoch [610/1000], Loss: 14.6059\n",
      "Epoch [620/1000], Loss: 14.6022\n",
      "Epoch [630/1000], Loss: 14.5987\n",
      "Epoch [640/1000], Loss: 14.5953\n",
      "Epoch [650/1000], Loss: 14.5919\n",
      "Epoch [660/1000], Loss: 14.5887\n",
      "Epoch [670/1000], Loss: 14.5856\n",
      "Epoch [680/1000], Loss: 14.5826\n",
      "Epoch [690/1000], Loss: 14.5797\n",
      "Epoch [700/1000], Loss: 14.5768\n",
      "Epoch [710/1000], Loss: 14.5741\n",
      "Epoch [720/1000], Loss: 14.5714\n",
      "Epoch [730/1000], Loss: 14.5688\n",
      "Epoch [740/1000], Loss: 14.5662\n",
      "Epoch [750/1000], Loss: 14.5638\n",
      "Epoch [760/1000], Loss: 14.5614\n",
      "Epoch [770/1000], Loss: 14.5591\n",
      "Epoch [780/1000], Loss: 14.5568\n",
      "Epoch [790/1000], Loss: 14.5546\n",
      "Epoch [800/1000], Loss: 14.5525\n",
      "Epoch [810/1000], Loss: 14.5504\n",
      "Epoch [820/1000], Loss: 14.5484\n",
      "Epoch [830/1000], Loss: 14.5464\n",
      "Epoch [840/1000], Loss: 14.5445\n",
      "Epoch [850/1000], Loss: 14.5426\n",
      "Epoch [860/1000], Loss: 14.5408\n",
      "Epoch [870/1000], Loss: 14.5390\n",
      "Epoch [880/1000], Loss: 14.5373\n",
      "Epoch [890/1000], Loss: 14.5356\n",
      "Epoch [900/1000], Loss: 14.5340\n",
      "Epoch [910/1000], Loss: 14.5324\n",
      "Epoch [920/1000], Loss: 14.5308\n",
      "Epoch [930/1000], Loss: 14.5293\n",
      "Epoch [940/1000], Loss: 14.5278\n",
      "Epoch [950/1000], Loss: 14.5264\n",
      "Epoch [960/1000], Loss: 14.5249\n",
      "Epoch [970/1000], Loss: 14.5235\n",
      "Epoch [980/1000], Loss: 14.5222\n",
      "Epoch [990/1000], Loss: 14.5209\n",
      "Epoch [1000/1000], Loss: 14.5196\n",
      "Predicted days_remaining for parent_id 281: [14.990559577941895, 15.85488510131836, 15.85628604888916, 15.857246398925781, 15.85599136352539, 15.85605239868164, 15.856293678283691, 15.856066703796387]\n",
      "Training for parent_id 289...\n",
      "Epoch [10/1000], Loss: 146.8761\n",
      "Epoch [20/1000], Loss: 111.8280\n",
      "Epoch [30/1000], Loss: 87.8932\n",
      "Epoch [40/1000], Loss: 72.4530\n",
      "Epoch [50/1000], Loss: 61.0265\n",
      "Epoch [60/1000], Loss: 51.8509\n",
      "Epoch [70/1000], Loss: 44.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/1000], Loss: 38.1270\n",
      "Epoch [90/1000], Loss: 33.0576\n",
      "Epoch [100/1000], Loss: 28.9400\n",
      "Epoch [110/1000], Loss: 25.6288\n",
      "Epoch [120/1000], Loss: 22.9953\n",
      "Epoch [130/1000], Loss: 20.9251\n",
      "Epoch [140/1000], Loss: 19.3169\n",
      "Epoch [150/1000], Loss: 18.0826\n",
      "Epoch [160/1000], Loss: 17.1468\n",
      "Epoch [170/1000], Loss: 16.4459\n",
      "Epoch [180/1000], Loss: 15.9274\n",
      "Epoch [190/1000], Loss: 15.5483\n",
      "Epoch [200/1000], Loss: 15.2743\n",
      "Epoch [210/1000], Loss: 15.0782\n",
      "Epoch [220/1000], Loss: 14.9393\n",
      "Epoch [230/1000], Loss: 14.8414\n",
      "Epoch [240/1000], Loss: 14.7728\n",
      "Epoch [250/1000], Loss: 14.7246\n",
      "Epoch [260/1000], Loss: 14.6906\n",
      "Epoch [270/1000], Loss: 14.6663\n",
      "Epoch [280/1000], Loss: 14.6485\n",
      "Epoch [290/1000], Loss: 14.6351\n",
      "Epoch [300/1000], Loss: 14.6246\n",
      "Epoch [310/1000], Loss: 14.6160\n",
      "Epoch [320/1000], Loss: 14.6087\n",
      "Epoch [330/1000], Loss: 14.6024\n",
      "Epoch [340/1000], Loss: 14.5967\n",
      "Epoch [350/1000], Loss: 14.5914\n",
      "Epoch [360/1000], Loss: 14.5865\n",
      "Epoch [370/1000], Loss: 14.5819\n",
      "Epoch [380/1000], Loss: 14.5776\n",
      "Epoch [390/1000], Loss: 14.5734\n",
      "Epoch [400/1000], Loss: 14.5694\n",
      "Epoch [410/1000], Loss: 14.5656\n",
      "Epoch [420/1000], Loss: 14.5620\n",
      "Epoch [430/1000], Loss: 14.5585\n",
      "Epoch [440/1000], Loss: 14.5552\n",
      "Epoch [450/1000], Loss: 14.5520\n",
      "Epoch [460/1000], Loss: 14.5489\n",
      "Epoch [470/1000], Loss: 14.5460\n",
      "Epoch [480/1000], Loss: 14.5432\n",
      "Epoch [490/1000], Loss: 14.5404\n",
      "Epoch [500/1000], Loss: 14.5378\n",
      "Epoch [510/1000], Loss: 14.5353\n",
      "Epoch [520/1000], Loss: 14.5329\n",
      "Epoch [530/1000], Loss: 14.5305\n",
      "Epoch [540/1000], Loss: 14.5283\n",
      "Epoch [550/1000], Loss: 14.5261\n",
      "Epoch [560/1000], Loss: 14.5240\n",
      "Epoch [570/1000], Loss: 14.5220\n",
      "Epoch [580/1000], Loss: 14.5201\n",
      "Epoch [590/1000], Loss: 14.5182\n",
      "Epoch [600/1000], Loss: 14.5164\n",
      "Epoch [610/1000], Loss: 14.5146\n",
      "Epoch [620/1000], Loss: 14.5129\n",
      "Epoch [630/1000], Loss: 14.5113\n",
      "Epoch [640/1000], Loss: 14.5097\n",
      "Epoch [650/1000], Loss: 14.5081\n",
      "Epoch [660/1000], Loss: 14.5066\n",
      "Epoch [670/1000], Loss: 14.5052\n",
      "Epoch [680/1000], Loss: 14.5038\n",
      "Epoch [690/1000], Loss: 14.5025\n",
      "Epoch [700/1000], Loss: 14.5011\n",
      "Epoch [710/1000], Loss: 14.4999\n",
      "Epoch [720/1000], Loss: 14.4986\n",
      "Epoch [730/1000], Loss: 14.4974\n",
      "Epoch [740/1000], Loss: 14.4963\n",
      "Epoch [750/1000], Loss: 14.4951\n",
      "Epoch [760/1000], Loss: 14.4940\n",
      "Epoch [770/1000], Loss: 14.4930\n",
      "Epoch [780/1000], Loss: 14.4919\n",
      "Epoch [790/1000], Loss: 14.4909\n",
      "Epoch [800/1000], Loss: 14.4899\n",
      "Epoch [810/1000], Loss: 14.4890\n",
      "Epoch [820/1000], Loss: 14.4881\n",
      "Epoch [830/1000], Loss: 14.4872\n",
      "Epoch [840/1000], Loss: 14.4863\n",
      "Epoch [850/1000], Loss: 14.4854\n",
      "Epoch [860/1000], Loss: 14.4846\n",
      "Epoch [870/1000], Loss: 14.4838\n",
      "Epoch [880/1000], Loss: 14.4830\n",
      "Epoch [890/1000], Loss: 14.4822\n",
      "Epoch [900/1000], Loss: 14.4815\n",
      "Epoch [910/1000], Loss: 14.4807\n",
      "Epoch [920/1000], Loss: 14.4800\n",
      "Epoch [930/1000], Loss: 14.4793\n",
      "Epoch [940/1000], Loss: 14.4786\n",
      "Epoch [950/1000], Loss: 14.4780\n",
      "Epoch [960/1000], Loss: 14.4773\n",
      "Epoch [970/1000], Loss: 14.4767\n",
      "Epoch [980/1000], Loss: 14.4761\n",
      "Epoch [990/1000], Loss: 14.4755\n",
      "Epoch [1000/1000], Loss: 14.4749\n",
      "Predicted days_remaining for parent_id 289: [12.237401962280273, 12.819278717041016, 12.821268081665039, 12.820016860961914, 12.821433067321777, 12.822626113891602, 12.827116012573242, 12.819084167480469]\n",
      "Training for parent_id 302...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 109.8915\n",
      "Epoch [20/1000], Loss: 80.0649\n",
      "Epoch [30/1000], Loss: 57.8005\n",
      "Epoch [40/1000], Loss: 44.9729\n",
      "Epoch [50/1000], Loss: 36.7134\n",
      "Epoch [60/1000], Loss: 30.7326\n",
      "Epoch [70/1000], Loss: 26.2501\n",
      "Epoch [80/1000], Loss: 22.8970\n",
      "Epoch [90/1000], Loss: 20.4223\n",
      "Epoch [100/1000], Loss: 18.6295\n",
      "Epoch [110/1000], Loss: 17.3574\n",
      "Epoch [120/1000], Loss: 16.4744\n",
      "Epoch [130/1000], Loss: 15.8749\n",
      "Epoch [140/1000], Loss: 15.4760\n",
      "Epoch [150/1000], Loss: 15.2153\n",
      "Epoch [160/1000], Loss: 15.0468\n",
      "Epoch [170/1000], Loss: 14.9383\n",
      "Epoch [180/1000], Loss: 14.8675\n",
      "Epoch [190/1000], Loss: 14.8201\n",
      "Epoch [200/1000], Loss: 14.7868\n",
      "Epoch [210/1000], Loss: 14.7619\n",
      "Epoch [220/1000], Loss: 14.7421\n",
      "Epoch [230/1000], Loss: 14.7255\n",
      "Epoch [240/1000], Loss: 14.7109\n",
      "Epoch [250/1000], Loss: 14.6978\n",
      "Epoch [260/1000], Loss: 14.6858\n",
      "Epoch [270/1000], Loss: 14.6747\n",
      "Epoch [280/1000], Loss: 14.6643\n",
      "Epoch [290/1000], Loss: 14.6547\n",
      "Epoch [300/1000], Loss: 14.6457\n",
      "Epoch [310/1000], Loss: 14.6372\n",
      "Epoch [320/1000], Loss: 14.6293\n",
      "Epoch [330/1000], Loss: 14.6218\n",
      "Epoch [340/1000], Loss: 14.6148\n",
      "Epoch [350/1000], Loss: 14.6082\n",
      "Epoch [360/1000], Loss: 14.6019\n",
      "Epoch [370/1000], Loss: 14.5960\n",
      "Epoch [380/1000], Loss: 14.5904\n",
      "Epoch [390/1000], Loss: 14.5852\n",
      "Epoch [400/1000], Loss: 14.5802\n",
      "Epoch [410/1000], Loss: 14.5754\n",
      "Epoch [420/1000], Loss: 14.5709\n",
      "Epoch [430/1000], Loss: 14.5667\n",
      "Epoch [440/1000], Loss: 14.5626\n",
      "Epoch [450/1000], Loss: 14.5587\n",
      "Epoch [460/1000], Loss: 14.5550\n",
      "Epoch [470/1000], Loss: 14.5515\n",
      "Epoch [480/1000], Loss: 14.5482\n",
      "Epoch [490/1000], Loss: 14.5450\n",
      "Epoch [500/1000], Loss: 14.5419\n",
      "Epoch [510/1000], Loss: 14.5390\n",
      "Epoch [520/1000], Loss: 14.5362\n",
      "Epoch [530/1000], Loss: 14.5335\n",
      "Epoch [540/1000], Loss: 14.5309\n",
      "Epoch [550/1000], Loss: 14.5284\n",
      "Epoch [560/1000], Loss: 14.5261\n",
      "Epoch [570/1000], Loss: 14.5238\n",
      "Epoch [580/1000], Loss: 14.5216\n",
      "Epoch [590/1000], Loss: 14.5195\n",
      "Epoch [600/1000], Loss: 14.5175\n",
      "Epoch [610/1000], Loss: 14.5155\n",
      "Epoch [620/1000], Loss: 14.5137\n",
      "Epoch [630/1000], Loss: 14.5119\n",
      "Epoch [640/1000], Loss: 14.5101\n",
      "Epoch [650/1000], Loss: 14.5084\n",
      "Epoch [660/1000], Loss: 14.5068\n",
      "Epoch [670/1000], Loss: 14.5052\n",
      "Epoch [680/1000], Loss: 14.5037\n",
      "Epoch [690/1000], Loss: 14.5023\n",
      "Epoch [700/1000], Loss: 14.5009\n",
      "Epoch [710/1000], Loss: 14.4995\n",
      "Epoch [720/1000], Loss: 14.4982\n",
      "Epoch [730/1000], Loss: 14.4969\n",
      "Epoch [740/1000], Loss: 14.4957\n",
      "Epoch [750/1000], Loss: 14.4945\n",
      "Epoch [760/1000], Loss: 14.4933\n",
      "Epoch [770/1000], Loss: 14.4922\n",
      "Epoch [780/1000], Loss: 14.4911\n",
      "Epoch [790/1000], Loss: 14.4900\n",
      "Epoch [800/1000], Loss: 14.4890\n",
      "Epoch [810/1000], Loss: 14.4880\n",
      "Epoch [820/1000], Loss: 14.4870\n",
      "Epoch [830/1000], Loss: 14.4861\n",
      "Epoch [840/1000], Loss: 14.4852\n",
      "Epoch [850/1000], Loss: 14.4843\n",
      "Epoch [860/1000], Loss: 14.4834\n",
      "Epoch [870/1000], Loss: 14.4826\n",
      "Epoch [880/1000], Loss: 14.4818\n",
      "Epoch [890/1000], Loss: 14.4810\n",
      "Epoch [900/1000], Loss: 14.4802\n",
      "Epoch [910/1000], Loss: 14.4795\n",
      "Epoch [920/1000], Loss: 14.4787\n",
      "Epoch [930/1000], Loss: 14.4780\n",
      "Epoch [940/1000], Loss: 14.4773\n",
      "Epoch [950/1000], Loss: 14.4766\n",
      "Epoch [960/1000], Loss: 14.4760\n",
      "Epoch [970/1000], Loss: 14.4753\n",
      "Epoch [980/1000], Loss: 14.4747\n",
      "Epoch [990/1000], Loss: 14.4741\n",
      "Epoch [1000/1000], Loss: 14.4735\n",
      "Predicted days_remaining for parent_id 302: [10.245977401733398, 10.818242073059082, 10.822047233581543, 10.818035125732422, 10.82030963897705, 10.821264266967773, 10.82071304321289, 10.813336372375488]\n",
      "Training for parent_id 310...\n",
      "Epoch [10/1000], Loss: 2827.8118\n",
      "Epoch [20/1000], Loss: 2653.0090\n",
      "Epoch [30/1000], Loss: 2526.3606\n",
      "Epoch [40/1000], Loss: 2435.1794\n",
      "Epoch [50/1000], Loss: 2355.9861\n",
      "Epoch [60/1000], Loss: 2281.5439\n",
      "Epoch [70/1000], Loss: 2210.5466\n",
      "Epoch [80/1000], Loss: 2142.3230\n",
      "Epoch [90/1000], Loss: 2076.4866\n",
      "Epoch [100/1000], Loss: 2012.8135\n",
      "Epoch [110/1000], Loss: 1951.1611\n",
      "Epoch [120/1000], Loss: 1891.4192\n",
      "Epoch [130/1000], Loss: 1833.4911\n",
      "Epoch [140/1000], Loss: 1777.2870\n",
      "Epoch [150/1000], Loss: 1722.7245\n",
      "Epoch [160/1000], Loss: 1669.7289\n",
      "Epoch [170/1000], Loss: 1618.2334\n",
      "Epoch [180/1000], Loss: 1568.1772\n",
      "Epoch [190/1000], Loss: 1519.5065\n",
      "Epoch [200/1000], Loss: 1472.1715\n",
      "Epoch [210/1000], Loss: 1426.1278\n",
      "Epoch [220/1000], Loss: 1381.3339\n",
      "Epoch [230/1000], Loss: 1337.7512\n",
      "Epoch [240/1000], Loss: 1295.3452\n",
      "Epoch [250/1000], Loss: 1254.0828\n",
      "Epoch [260/1000], Loss: 1213.9325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 1174.8656\n",
      "Epoch [280/1000], Loss: 1136.8544\n",
      "Epoch [290/1000], Loss: 1099.8724\n",
      "Epoch [300/1000], Loss: 1063.8950\n",
      "Epoch [310/1000], Loss: 1028.8977\n",
      "Epoch [320/1000], Loss: 994.8577\n",
      "Epoch [330/1000], Loss: 961.7532\n",
      "Epoch [340/1000], Loss: 929.5622\n",
      "Epoch [350/1000], Loss: 898.2643\n",
      "Epoch [360/1000], Loss: 867.8396\n",
      "Epoch [370/1000], Loss: 838.2682\n",
      "Epoch [380/1000], Loss: 809.5316\n",
      "Epoch [390/1000], Loss: 781.6108\n",
      "Epoch [400/1000], Loss: 754.4882\n",
      "Epoch [410/1000], Loss: 728.1458\n",
      "Epoch [420/1000], Loss: 702.5667\n",
      "Epoch [430/1000], Loss: 677.7335\n",
      "Epoch [440/1000], Loss: 653.6302\n",
      "Epoch [450/1000], Loss: 630.2403\n",
      "Epoch [460/1000], Loss: 607.5477\n",
      "Epoch [470/1000], Loss: 585.5370\n",
      "Epoch [480/1000], Loss: 564.1927\n",
      "Epoch [490/1000], Loss: 543.4998\n",
      "Epoch [500/1000], Loss: 523.4432\n",
      "Epoch [510/1000], Loss: 504.0084\n",
      "Epoch [520/1000], Loss: 485.1808\n",
      "Epoch [530/1000], Loss: 466.9467\n",
      "Epoch [540/1000], Loss: 449.2917\n",
      "Epoch [550/1000], Loss: 432.2021\n",
      "Epoch [560/1000], Loss: 415.6646\n",
      "Epoch [570/1000], Loss: 399.6656\n",
      "Epoch [580/1000], Loss: 384.1921\n",
      "Epoch [590/1000], Loss: 369.2313\n",
      "Epoch [600/1000], Loss: 354.7703\n",
      "Epoch [610/1000], Loss: 340.7966\n",
      "Epoch [620/1000], Loss: 327.2978\n",
      "Epoch [630/1000], Loss: 314.2618\n",
      "Epoch [640/1000], Loss: 301.6765\n",
      "Epoch [650/1000], Loss: 289.5302\n",
      "Epoch [660/1000], Loss: 277.8115\n",
      "Epoch [670/1000], Loss: 266.5087\n",
      "Epoch [680/1000], Loss: 255.6106\n",
      "Epoch [690/1000], Loss: 245.1064\n",
      "Epoch [700/1000], Loss: 234.9849\n",
      "Epoch [710/1000], Loss: 225.2357\n",
      "Epoch [720/1000], Loss: 215.8480\n",
      "Epoch [730/1000], Loss: 206.8117\n",
      "Epoch [740/1000], Loss: 198.1166\n",
      "Epoch [750/1000], Loss: 189.7527\n",
      "Epoch [760/1000], Loss: 181.7103\n",
      "Epoch [770/1000], Loss: 173.9798\n",
      "Epoch [780/1000], Loss: 166.5515\n",
      "Epoch [790/1000], Loss: 159.4165\n",
      "Epoch [800/1000], Loss: 152.5658\n",
      "Epoch [810/1000], Loss: 145.9902\n",
      "Epoch [820/1000], Loss: 139.6812\n",
      "Epoch [830/1000], Loss: 133.6301\n",
      "Epoch [840/1000], Loss: 127.8288\n",
      "Epoch [850/1000], Loss: 122.2687\n",
      "Epoch [860/1000], Loss: 116.9424\n",
      "Epoch [870/1000], Loss: 111.8416\n",
      "Epoch [880/1000], Loss: 106.9589\n",
      "Epoch [890/1000], Loss: 102.2867\n",
      "Epoch [900/1000], Loss: 97.8177\n",
      "Epoch [910/1000], Loss: 93.5449\n",
      "Epoch [920/1000], Loss: 89.4612\n",
      "Epoch [930/1000], Loss: 85.5600\n",
      "Epoch [940/1000], Loss: 81.8344\n",
      "Epoch [950/1000], Loss: 78.2783\n",
      "Epoch [960/1000], Loss: 74.8850\n",
      "Epoch [970/1000], Loss: 71.6487\n",
      "Epoch [980/1000], Loss: 68.5634\n",
      "Epoch [990/1000], Loss: 65.6233\n",
      "Epoch [1000/1000], Loss: 62.8226\n",
      "Predicted days_remaining for parent_id 310: [47.60493850708008, 47.8438720703125, 47.84419631958008, 47.84431076049805, 47.844154357910156, 47.8439826965332, 47.84408950805664, 47.8434944152832]\n",
      "Training for parent_id 312...\n",
      "Epoch [10/1000], Loss: 1184.1749\n",
      "Epoch [20/1000], Loss: 1058.9833\n",
      "Epoch [30/1000], Loss: 968.1605\n",
      "Epoch [40/1000], Loss: 908.3118\n",
      "Epoch [50/1000], Loss: 859.3524\n",
      "Epoch [60/1000], Loss: 814.7239\n",
      "Epoch [70/1000], Loss: 773.0189\n",
      "Epoch [80/1000], Loss: 733.7013\n",
      "Epoch [90/1000], Loss: 696.4556\n",
      "Epoch [100/1000], Loss: 661.0803\n",
      "Epoch [110/1000], Loss: 627.4314\n",
      "Epoch [120/1000], Loss: 595.3945\n",
      "Epoch [130/1000], Loss: 564.8731\n",
      "Epoch [140/1000], Loss: 535.7820\n",
      "Epoch [150/1000], Loss: 508.0454\n",
      "Epoch [160/1000], Loss: 481.5955\n",
      "Epoch [170/1000], Loss: 456.3704\n",
      "Epoch [180/1000], Loss: 432.3137\n",
      "Epoch [190/1000], Loss: 409.3734\n",
      "Epoch [200/1000], Loss: 387.5016\n",
      "Epoch [210/1000], Loss: 366.6532\n",
      "Epoch [220/1000], Loss: 346.7860\n",
      "Epoch [230/1000], Loss: 327.8603\n",
      "Epoch [240/1000], Loss: 309.8383\n",
      "Epoch [250/1000], Loss: 292.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000], Loss: 276.3640\n",
      "Epoch [270/1000], Loss: 260.8443\n",
      "Epoch [280/1000], Loss: 246.0939\n",
      "Epoch [290/1000], Loss: 232.0824\n",
      "Epoch [300/1000], Loss: 218.7803\n",
      "Epoch [310/1000], Loss: 206.1596\n",
      "Epoch [320/1000], Loss: 194.1927\n",
      "Epoch [330/1000], Loss: 182.8530\n",
      "Epoch [340/1000], Loss: 172.1151\n",
      "Epoch [350/1000], Loss: 161.9537\n",
      "Epoch [360/1000], Loss: 152.3448\n",
      "Epoch [370/1000], Loss: 143.2649\n",
      "Epoch [380/1000], Loss: 134.6911\n",
      "Epoch [390/1000], Loss: 126.6015\n",
      "Epoch [400/1000], Loss: 118.9745\n",
      "Epoch [410/1000], Loss: 111.7891\n",
      "Epoch [420/1000], Loss: 105.0254\n",
      "Epoch [430/1000], Loss: 98.6637\n",
      "Epoch [440/1000], Loss: 92.6849\n",
      "Epoch [450/1000], Loss: 87.0708\n",
      "Epoch [460/1000], Loss: 81.8035\n",
      "Epoch [470/1000], Loss: 76.8658\n",
      "Epoch [480/1000], Loss: 72.2412\n",
      "Epoch [490/1000], Loss: 67.9136\n",
      "Epoch [500/1000], Loss: 63.8674\n",
      "Epoch [510/1000], Loss: 60.0879\n",
      "Epoch [520/1000], Loss: 56.5606\n",
      "Epoch [530/1000], Loss: 53.2717\n",
      "Epoch [540/1000], Loss: 50.2078\n",
      "Epoch [550/1000], Loss: 47.3564\n",
      "Epoch [560/1000], Loss: 44.7052\n",
      "Epoch [570/1000], Loss: 42.2423\n",
      "Epoch [580/1000], Loss: 39.9567\n",
      "Epoch [590/1000], Loss: 37.8376\n",
      "Epoch [600/1000], Loss: 35.8749\n",
      "Epoch [610/1000], Loss: 34.0586\n",
      "Epoch [620/1000], Loss: 32.3796\n",
      "Epoch [630/1000], Loss: 30.8290\n",
      "Epoch [640/1000], Loss: 29.3984\n",
      "Epoch [650/1000], Loss: 28.0799\n",
      "Epoch [660/1000], Loss: 26.8659\n",
      "Epoch [670/1000], Loss: 25.7492\n",
      "Epoch [680/1000], Loss: 24.7230\n",
      "Epoch [690/1000], Loss: 23.7811\n",
      "Epoch [700/1000], Loss: 22.9174\n",
      "Epoch [710/1000], Loss: 22.1261\n",
      "Epoch [720/1000], Loss: 21.4020\n",
      "Epoch [730/1000], Loss: 20.7401\n",
      "Epoch [740/1000], Loss: 20.1356\n",
      "Epoch [750/1000], Loss: 19.5841\n",
      "Epoch [760/1000], Loss: 19.0816\n",
      "Epoch [770/1000], Loss: 18.6241\n",
      "Epoch [780/1000], Loss: 18.2080\n",
      "Epoch [790/1000], Loss: 17.8301\n",
      "Epoch [800/1000], Loss: 17.4871\n",
      "Epoch [810/1000], Loss: 17.1762\n",
      "Epoch [820/1000], Loss: 16.8947\n",
      "Epoch [830/1000], Loss: 16.6400\n",
      "Epoch [840/1000], Loss: 16.4099\n",
      "Epoch [850/1000], Loss: 16.2023\n",
      "Epoch [860/1000], Loss: 16.0150\n",
      "Epoch [870/1000], Loss: 15.8463\n",
      "Epoch [880/1000], Loss: 15.6946\n",
      "Epoch [890/1000], Loss: 15.5582\n",
      "Epoch [900/1000], Loss: 15.4358\n",
      "Epoch [910/1000], Loss: 15.3260\n",
      "Epoch [920/1000], Loss: 15.2277\n",
      "Epoch [930/1000], Loss: 15.1397\n",
      "Epoch [940/1000], Loss: 15.0610\n",
      "Epoch [950/1000], Loss: 14.9908\n",
      "Epoch [960/1000], Loss: 14.9282\n",
      "Epoch [970/1000], Loss: 14.8724\n",
      "Epoch [980/1000], Loss: 14.8228\n",
      "Epoch [990/1000], Loss: 14.7786\n",
      "Epoch [1000/1000], Loss: 14.7394\n",
      "Predicted days_remaining for parent_id 312: [34.88561248779297, 35.26642608642578, 35.26650619506836, 35.266624450683594, 35.26766586303711, 35.2653694152832, 35.26546096801758, 35.26472473144531]\n",
      "Training for parent_id 315...\n",
      "Epoch [10/1000], Loss: 1422.0648\n",
      "Epoch [20/1000], Loss: 1302.6377\n",
      "Epoch [30/1000], Loss: 1209.0447\n",
      "Epoch [40/1000], Loss: 1141.0056\n",
      "Epoch [50/1000], Loss: 1084.5339\n",
      "Epoch [60/1000], Loss: 1033.3541\n",
      "Epoch [70/1000], Loss: 985.4568\n",
      "Epoch [80/1000], Loss: 940.1232\n",
      "Epoch [90/1000], Loss: 896.9948\n",
      "Epoch [100/1000], Loss: 855.8520\n",
      "Epoch [110/1000], Loss: 816.5418\n",
      "Epoch [120/1000], Loss: 778.9471\n",
      "Epoch [130/1000], Loss: 742.9705\n",
      "Epoch [140/1000], Loss: 708.5266\n",
      "Epoch [150/1000], Loss: 675.5394\n",
      "Epoch [160/1000], Loss: 643.9397\n",
      "Epoch [170/1000], Loss: 613.6639\n",
      "Epoch [180/1000], Loss: 584.6536\n",
      "Epoch [190/1000], Loss: 556.8551\n",
      "Epoch [200/1000], Loss: 530.2183\n",
      "Epoch [210/1000], Loss: 504.6964\n",
      "Epoch [220/1000], Loss: 480.2458\n",
      "Epoch [230/1000], Loss: 456.8257\n",
      "Epoch [240/1000], Loss: 434.3974\n",
      "Epoch [250/1000], Loss: 412.9243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/1000], Loss: 392.3714\n",
      "Epoch [270/1000], Loss: 372.7057\n",
      "Epoch [280/1000], Loss: 353.8953\n",
      "Epoch [290/1000], Loss: 335.9098\n",
      "Epoch [300/1000], Loss: 318.7198\n",
      "Epoch [310/1000], Loss: 302.2970\n",
      "Epoch [320/1000], Loss: 286.6143\n",
      "Epoch [330/1000], Loss: 271.6449\n",
      "Epoch [340/1000], Loss: 257.3635\n",
      "Epoch [350/1000], Loss: 243.7452\n",
      "Epoch [360/1000], Loss: 230.7660\n",
      "Epoch [370/1000], Loss: 218.4022\n",
      "Epoch [380/1000], Loss: 206.6312\n",
      "Epoch [390/1000], Loss: 195.4309\n",
      "Epoch [400/1000], Loss: 184.7796\n",
      "Epoch [410/1000], Loss: 174.6565\n",
      "Epoch [420/1000], Loss: 165.0410\n",
      "Epoch [430/1000], Loss: 155.9132\n",
      "Epoch [440/1000], Loss: 147.2540\n",
      "Epoch [450/1000], Loss: 139.0442\n",
      "Epoch [460/1000], Loss: 131.2657\n",
      "Epoch [470/1000], Loss: 123.9006\n",
      "Epoch [480/1000], Loss: 116.9316\n",
      "Epoch [490/1000], Loss: 110.3418\n",
      "Epoch [500/1000], Loss: 104.1148\n",
      "Epoch [510/1000], Loss: 98.2347\n",
      "Epoch [520/1000], Loss: 92.6861\n",
      "Epoch [530/1000], Loss: 87.4541\n",
      "Epoch [540/1000], Loss: 82.5240\n",
      "Epoch [550/1000], Loss: 77.8818\n",
      "Epoch [560/1000], Loss: 73.5140\n",
      "Epoch [570/1000], Loss: 69.4073\n",
      "Epoch [580/1000], Loss: 65.5490\n",
      "Epoch [590/1000], Loss: 61.9269\n",
      "Epoch [600/1000], Loss: 58.5291\n",
      "Epoch [610/1000], Loss: 55.3441\n",
      "Epoch [620/1000], Loss: 52.3609\n",
      "Epoch [630/1000], Loss: 49.5690\n",
      "Epoch [640/1000], Loss: 46.9581\n",
      "Epoch [650/1000], Loss: 44.5184\n",
      "Epoch [660/1000], Loss: 42.2405\n",
      "Epoch [670/1000], Loss: 40.1155\n",
      "Epoch [680/1000], Loss: 38.1346\n",
      "Epoch [690/1000], Loss: 36.2895\n",
      "Epoch [700/1000], Loss: 34.5725\n",
      "Epoch [710/1000], Loss: 32.9758\n",
      "Epoch [720/1000], Loss: 31.4924\n",
      "Epoch [730/1000], Loss: 30.1154\n",
      "Epoch [740/1000], Loss: 28.8380\n",
      "Epoch [750/1000], Loss: 27.6543\n",
      "Epoch [760/1000], Loss: 26.5581\n",
      "Epoch [770/1000], Loss: 25.5441\n",
      "Epoch [780/1000], Loss: 24.6066\n",
      "Epoch [790/1000], Loss: 23.7408\n",
      "Epoch [800/1000], Loss: 22.9419\n",
      "Epoch [810/1000], Loss: 22.2053\n",
      "Epoch [820/1000], Loss: 21.5267\n",
      "Epoch [830/1000], Loss: 20.9021\n",
      "Epoch [840/1000], Loss: 20.3278\n",
      "Epoch [850/1000], Loss: 19.8002\n",
      "Epoch [860/1000], Loss: 19.3158\n",
      "Epoch [870/1000], Loss: 18.8716\n",
      "Epoch [880/1000], Loss: 18.4646\n",
      "Epoch [890/1000], Loss: 18.0919\n",
      "Epoch [900/1000], Loss: 17.7511\n",
      "Epoch [910/1000], Loss: 17.4396\n",
      "Epoch [920/1000], Loss: 17.1552\n",
      "Epoch [930/1000], Loss: 16.8958\n",
      "Epoch [940/1000], Loss: 16.6594\n",
      "Epoch [950/1000], Loss: 16.4441\n",
      "Epoch [960/1000], Loss: 16.2482\n",
      "Epoch [970/1000], Loss: 16.0702\n",
      "Epoch [980/1000], Loss: 15.9086\n",
      "Epoch [990/1000], Loss: 15.7620\n",
      "Epoch [1000/1000], Loss: 15.6291\n",
      "Predicted days_remaining for parent_id 315: [37.3203125, 37.72271728515625, 37.72282409667969, 37.72264862060547, 37.72242736816406, 37.722721099853516, 37.72256851196289, 37.72266387939453]\n",
      "Training for parent_id 316...\n",
      "Epoch [10/1000], Loss: 259.4237\n",
      "Epoch [20/1000], Loss: 209.2160\n",
      "Epoch [30/1000], Loss: 170.1142\n",
      "Epoch [40/1000], Loss: 144.5411\n",
      "Epoch [50/1000], Loss: 125.6105\n",
      "Epoch [60/1000], Loss: 110.2612\n",
      "Epoch [70/1000], Loss: 97.0807\n",
      "Epoch [80/1000], Loss: 85.5517\n",
      "Epoch [90/1000], Loss: 75.4334\n",
      "Epoch [100/1000], Loss: 66.5515\n",
      "Epoch [110/1000], Loss: 58.7728\n",
      "Epoch [120/1000], Loss: 51.9857\n",
      "Epoch [130/1000], Loss: 46.0912\n",
      "Epoch [140/1000], Loss: 40.9978\n",
      "Epoch [150/1000], Loss: 36.6207\n",
      "Epoch [160/1000], Loss: 32.8801\n",
      "Epoch [170/1000], Loss: 29.7020\n",
      "Epoch [180/1000], Loss: 27.0174\n",
      "Epoch [190/1000], Loss: 24.7630\n",
      "Epoch [200/1000], Loss: 22.8813\n",
      "Epoch [210/1000], Loss: 21.3198\n",
      "Epoch [220/1000], Loss: 20.0320\n",
      "Epoch [230/1000], Loss: 18.9761\n",
      "Epoch [240/1000], Loss: 18.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/1000], Loss: 17.4181\n",
      "Epoch [260/1000], Loss: 16.8562\n",
      "Epoch [270/1000], Loss: 16.4060\n",
      "Epoch [280/1000], Loss: 16.0472\n",
      "Epoch [290/1000], Loss: 15.7625\n",
      "Epoch [300/1000], Loss: 15.5377\n",
      "Epoch [310/1000], Loss: 15.3608\n",
      "Epoch [320/1000], Loss: 15.2221\n",
      "Epoch [330/1000], Loss: 15.1135\n",
      "Epoch [340/1000], Loss: 15.0286\n",
      "Epoch [350/1000], Loss: 14.9621\n",
      "Epoch [360/1000], Loss: 14.9099\n",
      "Epoch [370/1000], Loss: 14.8688\n",
      "Epoch [380/1000], Loss: 14.8361\n",
      "Epoch [390/1000], Loss: 14.8099\n",
      "Epoch [400/1000], Loss: 14.7886\n",
      "Epoch [410/1000], Loss: 14.7710\n",
      "Epoch [420/1000], Loss: 14.7563\n",
      "Epoch [430/1000], Loss: 14.7437\n",
      "Epoch [440/1000], Loss: 14.7327\n",
      "Epoch [450/1000], Loss: 14.7230\n",
      "Epoch [460/1000], Loss: 14.7142\n",
      "Epoch [470/1000], Loss: 14.7062\n",
      "Epoch [480/1000], Loss: 14.6988\n",
      "Epoch [490/1000], Loss: 14.6919\n",
      "Epoch [500/1000], Loss: 14.6853\n",
      "Epoch [510/1000], Loss: 14.6791\n",
      "Epoch [520/1000], Loss: 14.6732\n",
      "Epoch [530/1000], Loss: 14.6676\n",
      "Epoch [540/1000], Loss: 14.6621\n",
      "Epoch [550/1000], Loss: 14.6569\n",
      "Epoch [560/1000], Loss: 14.6519\n",
      "Epoch [570/1000], Loss: 14.6470\n",
      "Epoch [580/1000], Loss: 14.6423\n",
      "Epoch [590/1000], Loss: 14.6378\n",
      "Epoch [600/1000], Loss: 14.6334\n",
      "Epoch [610/1000], Loss: 14.6292\n",
      "Epoch [620/1000], Loss: 14.6250\n",
      "Epoch [630/1000], Loss: 14.6211\n",
      "Epoch [640/1000], Loss: 14.6172\n",
      "Epoch [650/1000], Loss: 14.6135\n",
      "Epoch [660/1000], Loss: 14.6099\n",
      "Epoch [670/1000], Loss: 14.6063\n",
      "Epoch [680/1000], Loss: 14.6029\n",
      "Epoch [690/1000], Loss: 14.5996\n",
      "Epoch [700/1000], Loss: 14.5964\n",
      "Epoch [710/1000], Loss: 14.5933\n",
      "Epoch [720/1000], Loss: 14.5903\n",
      "Epoch [730/1000], Loss: 14.5873\n",
      "Epoch [740/1000], Loss: 14.5845\n",
      "Epoch [750/1000], Loss: 14.5817\n",
      "Epoch [760/1000], Loss: 14.5790\n",
      "Epoch [770/1000], Loss: 14.5764\n",
      "Epoch [780/1000], Loss: 14.5738\n",
      "Epoch [790/1000], Loss: 14.5713\n",
      "Epoch [800/1000], Loss: 14.5689\n",
      "Epoch [810/1000], Loss: 14.5666\n",
      "Epoch [820/1000], Loss: 14.5643\n",
      "Epoch [830/1000], Loss: 14.5621\n",
      "Epoch [840/1000], Loss: 14.5599\n",
      "Epoch [850/1000], Loss: 14.5578\n",
      "Epoch [860/1000], Loss: 14.5557\n",
      "Epoch [870/1000], Loss: 14.5537\n",
      "Epoch [880/1000], Loss: 14.5517\n",
      "Epoch [890/1000], Loss: 14.5498\n",
      "Epoch [900/1000], Loss: 14.5479\n",
      "Epoch [910/1000], Loss: 14.5461\n",
      "Epoch [920/1000], Loss: 14.5443\n",
      "Epoch [930/1000], Loss: 14.5426\n",
      "Epoch [940/1000], Loss: 14.5409\n",
      "Epoch [950/1000], Loss: 14.5393\n",
      "Epoch [960/1000], Loss: 14.5377\n",
      "Epoch [970/1000], Loss: 14.5361\n",
      "Epoch [980/1000], Loss: 14.5345\n",
      "Epoch [990/1000], Loss: 14.5330\n",
      "Epoch [1000/1000], Loss: 14.5316\n",
      "Predicted days_remaining for parent_id 316: [15.93708324432373, 16.865663528442383, 16.863855361938477, 16.863758087158203, 16.862693786621094, 16.862882614135742, 16.863367080688477, 16.86385154724121]\n",
      "Training for parent_id 317...\n",
      "Epoch [10/1000], Loss: 1341.0536\n",
      "Epoch [20/1000], Loss: 1230.7512\n",
      "Epoch [30/1000], Loss: 1134.9487\n",
      "Epoch [40/1000], Loss: 1064.5249\n",
      "Epoch [50/1000], Loss: 1008.4655\n",
      "Epoch [60/1000], Loss: 958.6733\n",
      "Epoch [70/1000], Loss: 912.4131\n",
      "Epoch [80/1000], Loss: 868.7722\n",
      "Epoch [90/1000], Loss: 827.3216\n",
      "Epoch [100/1000], Loss: 787.8149\n",
      "Epoch [110/1000], Loss: 750.0916\n",
      "Epoch [120/1000], Loss: 714.0363\n",
      "Epoch [130/1000], Loss: 679.5585\n",
      "Epoch [140/1000], Loss: 646.5809\n",
      "Epoch [150/1000], Loss: 615.0354\n",
      "Epoch [160/1000], Loss: 584.8595\n",
      "Epoch [170/1000], Loss: 555.9952\n",
      "Epoch [180/1000], Loss: 528.3880\n",
      "Epoch [190/1000], Loss: 501.9865\n",
      "Epoch [200/1000], Loss: 476.7424\n",
      "Epoch [210/1000], Loss: 452.6097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 429.5448\n",
      "Epoch [230/1000], Loss: 407.5063\n",
      "Epoch [240/1000], Loss: 386.4547\n",
      "Epoch [250/1000], Loss: 366.3524\n",
      "Epoch [260/1000], Loss: 347.1633\n",
      "Epoch [270/1000], Loss: 328.8530\n",
      "Epoch [280/1000], Loss: 311.3883\n",
      "Epoch [290/1000], Loss: 294.7373\n",
      "Epoch [300/1000], Loss: 278.8694\n",
      "Epoch [310/1000], Loss: 263.7550\n",
      "Epoch [320/1000], Loss: 249.3654\n",
      "Epoch [330/1000], Loss: 235.6731\n",
      "Epoch [340/1000], Loss: 222.6512\n",
      "Epoch [350/1000], Loss: 210.2740\n",
      "Epoch [360/1000], Loss: 198.5161\n",
      "Epoch [370/1000], Loss: 187.3531\n",
      "Epoch [380/1000], Loss: 176.7615\n",
      "Epoch [390/1000], Loss: 166.7183\n",
      "Epoch [400/1000], Loss: 157.2009\n",
      "Epoch [410/1000], Loss: 148.1879\n",
      "Epoch [420/1000], Loss: 139.6582\n",
      "Epoch [430/1000], Loss: 131.5912\n",
      "Epoch [440/1000], Loss: 123.9671\n",
      "Epoch [450/1000], Loss: 116.7668\n",
      "Epoch [460/1000], Loss: 109.9714\n",
      "Epoch [470/1000], Loss: 103.5629\n",
      "Epoch [480/1000], Loss: 97.5236\n",
      "Epoch [490/1000], Loss: 91.8365\n",
      "Epoch [500/1000], Loss: 86.4851\n",
      "Epoch [510/1000], Loss: 81.4535\n",
      "Epoch [520/1000], Loss: 76.7261\n",
      "Epoch [530/1000], Loss: 72.2882\n",
      "Epoch [540/1000], Loss: 68.1252\n",
      "Epoch [550/1000], Loss: 64.2232\n",
      "Epoch [560/1000], Loss: 60.5689\n",
      "Epoch [570/1000], Loss: 57.1493\n",
      "Epoch [580/1000], Loss: 53.9520\n",
      "Epoch [590/1000], Loss: 50.9650\n",
      "Epoch [600/1000], Loss: 48.1768\n",
      "Epoch [610/1000], Loss: 45.5764\n",
      "Epoch [620/1000], Loss: 43.1532\n",
      "Epoch [630/1000], Loss: 40.8971\n",
      "Epoch [640/1000], Loss: 38.7983\n",
      "Epoch [650/1000], Loss: 36.8477\n",
      "Epoch [660/1000], Loss: 35.0363\n",
      "Epoch [670/1000], Loss: 33.3556\n",
      "Epoch [680/1000], Loss: 31.7977\n",
      "Epoch [690/1000], Loss: 30.3548\n",
      "Epoch [700/1000], Loss: 29.0197\n",
      "Epoch [710/1000], Loss: 27.7854\n",
      "Epoch [720/1000], Loss: 26.6454\n",
      "Epoch [730/1000], Loss: 25.5934\n",
      "Epoch [740/1000], Loss: 24.6234\n",
      "Epoch [750/1000], Loss: 23.7300\n",
      "Epoch [760/1000], Loss: 22.9079\n",
      "Epoch [770/1000], Loss: 22.1520\n",
      "Epoch [780/1000], Loss: 21.4577\n",
      "Epoch [790/1000], Loss: 20.8205\n",
      "Epoch [800/1000], Loss: 20.2363\n",
      "Epoch [810/1000], Loss: 19.7013\n",
      "Epoch [820/1000], Loss: 19.2116\n",
      "Epoch [830/1000], Loss: 18.7640\n",
      "Epoch [840/1000], Loss: 18.3552\n",
      "Epoch [850/1000], Loss: 17.9821\n",
      "Epoch [860/1000], Loss: 17.6420\n",
      "Epoch [870/1000], Loss: 17.3323\n",
      "Epoch [880/1000], Loss: 17.0505\n",
      "Epoch [890/1000], Loss: 16.7944\n",
      "Epoch [900/1000], Loss: 16.5618\n",
      "Epoch [910/1000], Loss: 16.3508\n",
      "Epoch [920/1000], Loss: 16.1595\n",
      "Epoch [930/1000], Loss: 15.9864\n",
      "Epoch [940/1000], Loss: 15.8298\n",
      "Epoch [950/1000], Loss: 15.6882\n",
      "Epoch [960/1000], Loss: 15.5604\n",
      "Epoch [970/1000], Loss: 15.4452\n",
      "Epoch [980/1000], Loss: 15.3413\n",
      "Epoch [990/1000], Loss: 15.2479\n",
      "Epoch [1000/1000], Loss: 15.1638\n",
      "Predicted days_remaining for parent_id 317: [36.391117095947266, 37.00366973876953, 37.00382614135742, 37.00375747680664, 37.003379821777344, 37.00346755981445, 37.003170013427734, 37.003299713134766]\n",
      "Training for parent_id 320...\n",
      "Epoch [10/1000], Loss: 1827.2517\n",
      "Epoch [20/1000], Loss: 1696.2479\n",
      "Epoch [30/1000], Loss: 1586.3947\n",
      "Epoch [40/1000], Loss: 1506.9585\n",
      "Epoch [50/1000], Loss: 1441.9559\n",
      "Epoch [60/1000], Loss: 1382.5098\n",
      "Epoch [70/1000], Loss: 1326.0560\n",
      "Epoch [80/1000], Loss: 1272.1302\n",
      "Epoch [90/1000], Loss: 1220.6787\n",
      "Epoch [100/1000], Loss: 1171.4799\n",
      "Epoch [110/1000], Loss: 1124.3447\n",
      "Epoch [120/1000], Loss: 1079.1233\n",
      "Epoch [130/1000], Loss: 1035.6920\n",
      "Epoch [140/1000], Loss: 993.9455\n",
      "Epoch [150/1000], Loss: 953.7908\n",
      "Epoch [160/1000], Loss: 915.1455\n",
      "Epoch [170/1000], Loss: 877.9362\n",
      "Epoch [180/1000], Loss: 842.0963\n",
      "Epoch [190/1000], Loss: 807.5665\n",
      "Epoch [200/1000], Loss: 774.2925\n",
      "Epoch [210/1000], Loss: 742.2244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 711.3168\n",
      "Epoch [230/1000], Loss: 681.5276\n",
      "Epoch [240/1000], Loss: 652.8170\n",
      "Epoch [250/1000], Loss: 625.1485\n",
      "Epoch [260/1000], Loss: 598.4872\n",
      "Epoch [270/1000], Loss: 572.8007\n",
      "Epoch [280/1000], Loss: 548.0573\n",
      "Epoch [290/1000], Loss: 524.2274\n",
      "Epoch [300/1000], Loss: 501.2827\n",
      "Epoch [310/1000], Loss: 479.1958\n",
      "Epoch [320/1000], Loss: 457.9405\n",
      "Epoch [330/1000], Loss: 437.4911\n",
      "Epoch [340/1000], Loss: 417.8235\n",
      "Epoch [350/1000], Loss: 398.9136\n",
      "Epoch [360/1000], Loss: 380.7385\n",
      "Epoch [370/1000], Loss: 363.2759\n",
      "Epoch [380/1000], Loss: 346.5036\n",
      "Epoch [390/1000], Loss: 330.4008\n",
      "Epoch [400/1000], Loss: 314.9465\n",
      "Epoch [410/1000], Loss: 300.1206\n",
      "Epoch [420/1000], Loss: 285.9033\n",
      "Epoch [430/1000], Loss: 272.2755\n",
      "Epoch [440/1000], Loss: 259.2181\n",
      "Epoch [450/1000], Loss: 246.7129\n",
      "Epoch [460/1000], Loss: 234.7419\n",
      "Epoch [470/1000], Loss: 223.2875\n",
      "Epoch [480/1000], Loss: 212.3325\n",
      "Epoch [490/1000], Loss: 201.8600\n",
      "Epoch [500/1000], Loss: 191.8535\n",
      "Epoch [510/1000], Loss: 182.2972\n",
      "Epoch [520/1000], Loss: 173.1752\n",
      "Epoch [530/1000], Loss: 164.4720\n",
      "Epoch [540/1000], Loss: 156.1729\n",
      "Epoch [550/1000], Loss: 148.2630\n",
      "Epoch [560/1000], Loss: 140.7281\n",
      "Epoch [570/1000], Loss: 133.5542\n",
      "Epoch [580/1000], Loss: 126.7277\n",
      "Epoch [590/1000], Loss: 120.2352\n",
      "Epoch [600/1000], Loss: 114.0639\n",
      "Epoch [610/1000], Loss: 108.2010\n",
      "Epoch [620/1000], Loss: 102.6344\n",
      "Epoch [630/1000], Loss: 97.3519\n",
      "Epoch [640/1000], Loss: 92.3421\n",
      "Epoch [650/1000], Loss: 87.5935\n",
      "Epoch [660/1000], Loss: 83.0952\n",
      "Epoch [670/1000], Loss: 78.8366\n",
      "Epoch [680/1000], Loss: 74.8071\n",
      "Epoch [690/1000], Loss: 70.9967\n",
      "Epoch [700/1000], Loss: 67.3958\n",
      "Epoch [710/1000], Loss: 63.9950\n",
      "Epoch [720/1000], Loss: 60.7849\n",
      "Epoch [730/1000], Loss: 57.7569\n",
      "Epoch [740/1000], Loss: 54.9024\n",
      "Epoch [750/1000], Loss: 52.2131\n",
      "Epoch [760/1000], Loss: 49.6811\n",
      "Epoch [770/1000], Loss: 47.2987\n",
      "Epoch [780/1000], Loss: 45.0584\n",
      "Epoch [790/1000], Loss: 42.9533\n",
      "Epoch [800/1000], Loss: 40.9763\n",
      "Epoch [810/1000], Loss: 39.1209\n",
      "Epoch [820/1000], Loss: 37.3809\n",
      "Epoch [830/1000], Loss: 35.7500\n",
      "Epoch [840/1000], Loss: 34.2225\n",
      "Epoch [850/1000], Loss: 32.7927\n",
      "Epoch [860/1000], Loss: 31.4553\n",
      "Epoch [870/1000], Loss: 30.2053\n",
      "Epoch [880/1000], Loss: 29.0375\n",
      "Epoch [890/1000], Loss: 27.9476\n",
      "Epoch [900/1000], Loss: 26.9307\n",
      "Epoch [910/1000], Loss: 25.9829\n",
      "Epoch [920/1000], Loss: 25.0999\n",
      "Epoch [930/1000], Loss: 24.2780\n",
      "Epoch [940/1000], Loss: 23.5135\n",
      "Epoch [950/1000], Loss: 22.8027\n",
      "Epoch [960/1000], Loss: 22.1425\n",
      "Epoch [970/1000], Loss: 21.5296\n",
      "Epoch [980/1000], Loss: 20.9611\n",
      "Epoch [990/1000], Loss: 20.4341\n",
      "Epoch [1000/1000], Loss: 19.9460\n",
      "Predicted days_remaining for parent_id 320: [41.15690994262695, 41.45166778564453, 41.45207595825195, 41.45204544067383, 41.451778411865234, 41.451995849609375, 41.4521598815918, 41.45144271850586]\n",
      "Training for parent_id 321...\n",
      "Epoch [10/1000], Loss: 690.6506\n",
      "Epoch [20/1000], Loss: 613.4501\n",
      "Epoch [30/1000], Loss: 559.5523\n",
      "Epoch [40/1000], Loss: 519.9860\n",
      "Epoch [50/1000], Loss: 485.6496\n",
      "Epoch [60/1000], Loss: 454.1696\n",
      "Epoch [70/1000], Loss: 424.8302\n",
      "Epoch [80/1000], Loss: 397.3060\n",
      "Epoch [90/1000], Loss: 371.4102\n",
      "Epoch [100/1000], Loss: 347.0205\n",
      "Epoch [110/1000], Loss: 324.0460\n",
      "Epoch [120/1000], Loss: 302.4120\n",
      "Epoch [130/1000], Loss: 282.0520\n",
      "Epoch [140/1000], Loss: 262.9043\n",
      "Epoch [150/1000], Loss: 244.9102\n",
      "Epoch [160/1000], Loss: 228.0135\n",
      "Epoch [170/1000], Loss: 212.1600\n",
      "Epoch [180/1000], Loss: 197.2978\n",
      "Epoch [190/1000], Loss: 183.3773\n",
      "Epoch [200/1000], Loss: 170.3503\n",
      "Epoch [210/1000], Loss: 158.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/1000], Loss: 146.7959\n",
      "Epoch [230/1000], Loss: 136.1821\n",
      "Epoch [240/1000], Loss: 126.2895\n",
      "Epoch [250/1000], Loss: 117.0790\n",
      "Epoch [260/1000], Loss: 108.5135\n",
      "Epoch [270/1000], Loss: 100.5569\n",
      "Epoch [280/1000], Loss: 93.1750\n",
      "Epoch [290/1000], Loss: 86.3348\n",
      "Epoch [300/1000], Loss: 80.0045\n",
      "Epoch [310/1000], Loss: 74.1536\n",
      "Epoch [320/1000], Loss: 68.7532\n",
      "Epoch [330/1000], Loss: 63.7753\n",
      "Epoch [340/1000], Loss: 59.1932\n",
      "Epoch [350/1000], Loss: 54.9813\n",
      "Epoch [360/1000], Loss: 51.1152\n",
      "Epoch [370/1000], Loss: 47.5718\n",
      "Epoch [380/1000], Loss: 44.3289\n",
      "Epoch [390/1000], Loss: 41.3654\n",
      "Epoch [400/1000], Loss: 38.6613\n",
      "Epoch [410/1000], Loss: 36.1977\n",
      "Epoch [420/1000], Loss: 33.9567\n",
      "Epoch [430/1000], Loss: 31.9214\n",
      "Epoch [440/1000], Loss: 30.0757\n",
      "Epoch [450/1000], Loss: 28.4046\n",
      "Epoch [460/1000], Loss: 26.8942\n",
      "Epoch [470/1000], Loss: 25.5310\n",
      "Epoch [480/1000], Loss: 24.3028\n",
      "Epoch [490/1000], Loss: 23.1979\n",
      "Epoch [500/1000], Loss: 22.2057\n",
      "Epoch [510/1000], Loss: 21.3161\n",
      "Epoch [520/1000], Loss: 20.5198\n",
      "Epoch [530/1000], Loss: 19.8082\n",
      "Epoch [540/1000], Loss: 19.1734\n",
      "Epoch [550/1000], Loss: 18.6079\n",
      "Epoch [560/1000], Loss: 18.1052\n",
      "Epoch [570/1000], Loss: 17.6589\n",
      "Epoch [580/1000], Loss: 17.2634\n",
      "Epoch [590/1000], Loss: 16.9135\n",
      "Epoch [600/1000], Loss: 16.6045\n",
      "Epoch [610/1000], Loss: 16.3321\n",
      "Epoch [620/1000], Loss: 16.0922\n",
      "Epoch [630/1000], Loss: 15.8815\n",
      "Epoch [640/1000], Loss: 15.6966\n",
      "Epoch [650/1000], Loss: 15.5347\n",
      "Epoch [660/1000], Loss: 15.3931\n",
      "Epoch [670/1000], Loss: 15.2696\n",
      "Epoch [680/1000], Loss: 15.1619\n",
      "Epoch [690/1000], Loss: 15.0682\n",
      "Epoch [700/1000], Loss: 14.9869\n",
      "Epoch [710/1000], Loss: 14.9163\n",
      "Epoch [720/1000], Loss: 14.8553\n",
      "Epoch [730/1000], Loss: 14.8025\n",
      "Epoch [740/1000], Loss: 14.7569\n",
      "Epoch [750/1000], Loss: 14.7177\n",
      "Epoch [760/1000], Loss: 14.6839\n",
      "Epoch [770/1000], Loss: 14.6549\n",
      "Epoch [780/1000], Loss: 14.6301\n",
      "Epoch [790/1000], Loss: 14.6088\n",
      "Epoch [800/1000], Loss: 14.5905\n",
      "Epoch [810/1000], Loss: 14.5750\n",
      "Epoch [820/1000], Loss: 14.5617\n",
      "Epoch [830/1000], Loss: 14.5504\n",
      "Epoch [840/1000], Loss: 14.5407\n",
      "Epoch [850/1000], Loss: 14.5325\n",
      "Epoch [860/1000], Loss: 14.5255\n",
      "Epoch [870/1000], Loss: 14.5196\n",
      "Epoch [880/1000], Loss: 14.5146\n",
      "Epoch [890/1000], Loss: 14.5103\n",
      "Epoch [900/1000], Loss: 14.5066\n",
      "Epoch [910/1000], Loss: 14.5035\n",
      "Epoch [920/1000], Loss: 14.5009\n",
      "Epoch [930/1000], Loss: 14.4986\n",
      "Epoch [940/1000], Loss: 14.4967\n",
      "Epoch [950/1000], Loss: 14.4951\n",
      "Epoch [960/1000], Loss: 14.4936\n",
      "Epoch [970/1000], Loss: 14.4924\n",
      "Epoch [980/1000], Loss: 14.4913\n",
      "Epoch [990/1000], Loss: 14.4904\n",
      "Epoch [1000/1000], Loss: 14.4895\n",
      "Predicted days_remaining for parent_id 321: [27.113658905029297, 27.789016723632812, 27.789308547973633, 27.789037704467773, 27.789478302001953, 27.789081573486328, 27.788400650024414, 27.788135528564453]\n",
      "Training for parent_id 326...\n",
      "Epoch [10/1000], Loss: 194.5797\n",
      "Epoch [20/1000], Loss: 153.6262\n",
      "Epoch [30/1000], Loss: 127.2517\n",
      "Epoch [40/1000], Loss: 109.3076\n",
      "Epoch [50/1000], Loss: 94.8905\n",
      "Epoch [60/1000], Loss: 82.6054\n",
      "Epoch [70/1000], Loss: 71.9899\n",
      "Epoch [80/1000], Loss: 62.8129\n",
      "Epoch [90/1000], Loss: 54.8946\n",
      "Epoch [100/1000], Loss: 48.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 42.2648\n",
      "Epoch [120/1000], Loss: 37.3150\n",
      "Epoch [130/1000], Loss: 33.1352\n",
      "Epoch [140/1000], Loss: 29.6306\n",
      "Epoch [150/1000], Loss: 26.7139\n",
      "Epoch [160/1000], Loss: 24.3047\n",
      "Epoch [170/1000], Loss: 22.3299\n",
      "Epoch [180/1000], Loss: 20.7236\n",
      "Epoch [190/1000], Loss: 19.4273\n",
      "Epoch [200/1000], Loss: 18.3892\n",
      "Epoch [210/1000], Loss: 17.5641\n",
      "Epoch [220/1000], Loss: 16.9134\n",
      "Epoch [230/1000], Loss: 16.4038\n",
      "Epoch [240/1000], Loss: 16.0075\n",
      "Epoch [250/1000], Loss: 15.7014\n",
      "Epoch [260/1000], Loss: 15.4661\n",
      "Epoch [270/1000], Loss: 15.2863\n",
      "Epoch [280/1000], Loss: 15.1493\n",
      "Epoch [290/1000], Loss: 15.0451\n",
      "Epoch [300/1000], Loss: 14.9660\n",
      "Epoch [310/1000], Loss: 14.9056\n",
      "Epoch [320/1000], Loss: 14.8594\n",
      "Epoch [330/1000], Loss: 14.8236\n",
      "Epoch [340/1000], Loss: 14.7956\n",
      "Epoch [350/1000], Loss: 14.7733\n",
      "Epoch [360/1000], Loss: 14.7552\n",
      "Epoch [370/1000], Loss: 14.7402\n",
      "Epoch [380/1000], Loss: 14.7274\n",
      "Epoch [390/1000], Loss: 14.7163\n",
      "Epoch [400/1000], Loss: 14.7064\n",
      "Epoch [410/1000], Loss: 14.6974\n",
      "Epoch [420/1000], Loss: 14.6892\n",
      "Epoch [430/1000], Loss: 14.6816\n",
      "Epoch [440/1000], Loss: 14.6745\n",
      "Epoch [450/1000], Loss: 14.6677\n",
      "Epoch [460/1000], Loss: 14.6613\n",
      "Epoch [470/1000], Loss: 14.6552\n",
      "Epoch [480/1000], Loss: 14.6494\n",
      "Epoch [490/1000], Loss: 14.6438\n",
      "Epoch [500/1000], Loss: 14.6384\n",
      "Epoch [510/1000], Loss: 14.6332\n",
      "Epoch [520/1000], Loss: 14.6283\n",
      "Epoch [530/1000], Loss: 14.6235\n",
      "Epoch [540/1000], Loss: 14.6189\n",
      "Epoch [550/1000], Loss: 14.6145\n",
      "Epoch [560/1000], Loss: 14.6102\n",
      "Epoch [570/1000], Loss: 14.6061\n",
      "Epoch [580/1000], Loss: 14.6021\n",
      "Epoch [590/1000], Loss: 14.5983\n",
      "Epoch [600/1000], Loss: 14.5946\n",
      "Epoch [610/1000], Loss: 14.5910\n",
      "Epoch [620/1000], Loss: 14.5876\n",
      "Epoch [630/1000], Loss: 14.5843\n",
      "Epoch [640/1000], Loss: 14.5810\n",
      "Epoch [650/1000], Loss: 14.5779\n",
      "Epoch [660/1000], Loss: 14.5749\n",
      "Epoch [670/1000], Loss: 14.5720\n",
      "Epoch [680/1000], Loss: 14.5691\n",
      "Epoch [690/1000], Loss: 14.5664\n",
      "Epoch [700/1000], Loss: 14.5637\n",
      "Epoch [710/1000], Loss: 14.5612\n",
      "Epoch [720/1000], Loss: 14.5587\n",
      "Epoch [730/1000], Loss: 14.5563\n",
      "Epoch [740/1000], Loss: 14.5539\n",
      "Epoch [750/1000], Loss: 14.5516\n",
      "Epoch [760/1000], Loss: 14.5494\n",
      "Epoch [770/1000], Loss: 14.5473\n",
      "Epoch [780/1000], Loss: 14.5452\n",
      "Epoch [790/1000], Loss: 14.5431\n",
      "Epoch [800/1000], Loss: 14.5412\n",
      "Epoch [810/1000], Loss: 14.5392\n",
      "Epoch [820/1000], Loss: 14.5374\n",
      "Epoch [830/1000], Loss: 14.5356\n",
      "Epoch [840/1000], Loss: 14.5338\n",
      "Epoch [850/1000], Loss: 14.5321\n",
      "Epoch [860/1000], Loss: 14.5304\n",
      "Epoch [870/1000], Loss: 14.5288\n",
      "Epoch [880/1000], Loss: 14.5272\n",
      "Epoch [890/1000], Loss: 14.5256\n",
      "Epoch [900/1000], Loss: 14.5241\n",
      "Epoch [910/1000], Loss: 14.5227\n",
      "Epoch [920/1000], Loss: 14.5212\n",
      "Epoch [930/1000], Loss: 14.5198\n",
      "Epoch [940/1000], Loss: 14.5185\n",
      "Epoch [950/1000], Loss: 14.5171\n",
      "Epoch [960/1000], Loss: 14.5159\n",
      "Epoch [970/1000], Loss: 14.5146\n",
      "Epoch [980/1000], Loss: 14.5133\n",
      "Epoch [990/1000], Loss: 14.5121\n",
      "Epoch [1000/1000], Loss: 14.5110\n",
      "Predicted days_remaining for parent_id 326: [14.0311918258667, 14.848862648010254, 14.84986400604248, 14.8502836227417, 14.851252555847168, 14.849987983703613, 14.850127220153809, 14.850130081176758]\n",
      "Training for parent_id 332...\n",
      "Epoch [10/1000], Loss: 119.4759\n",
      "Epoch [20/1000], Loss: 87.0875\n",
      "Epoch [30/1000], Loss: 67.5407\n",
      "Epoch [40/1000], Loss: 55.2691\n",
      "Epoch [50/1000], Loss: 46.1075\n",
      "Epoch [60/1000], Loss: 38.8975\n",
      "Epoch [70/1000], Loss: 33.1606\n",
      "Epoch [80/1000], Loss: 28.6155\n",
      "Epoch [90/1000], Loss: 25.0504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 22.2889\n",
      "Epoch [110/1000], Loss: 20.1795\n",
      "Epoch [120/1000], Loss: 18.5924\n",
      "Epoch [130/1000], Loss: 17.4170\n",
      "Epoch [140/1000], Loss: 16.5606\n",
      "Epoch [150/1000], Loss: 15.9468\n",
      "Epoch [160/1000], Loss: 15.5142\n",
      "Epoch [170/1000], Loss: 15.2140\n",
      "Epoch [180/1000], Loss: 15.0088\n",
      "Epoch [190/1000], Loss: 14.8704\n",
      "Epoch [200/1000], Loss: 14.7779\n",
      "Epoch [210/1000], Loss: 14.7165\n",
      "Epoch [220/1000], Loss: 14.6755\n",
      "Epoch [230/1000], Loss: 14.6479\n",
      "Epoch [240/1000], Loss: 14.6289\n",
      "Epoch [250/1000], Loss: 14.6152\n",
      "Epoch [260/1000], Loss: 14.6049\n",
      "Epoch [270/1000], Loss: 14.5967\n",
      "Epoch [280/1000], Loss: 14.5898\n",
      "Epoch [290/1000], Loss: 14.5838\n",
      "Epoch [300/1000], Loss: 14.5783\n",
      "Epoch [310/1000], Loss: 14.5733\n",
      "Epoch [320/1000], Loss: 14.5686\n",
      "Epoch [330/1000], Loss: 14.5642\n",
      "Epoch [340/1000], Loss: 14.5600\n",
      "Epoch [350/1000], Loss: 14.5561\n",
      "Epoch [360/1000], Loss: 14.5523\n",
      "Epoch [370/1000], Loss: 14.5487\n",
      "Epoch [380/1000], Loss: 14.5452\n",
      "Epoch [390/1000], Loss: 14.5419\n",
      "Epoch [400/1000], Loss: 14.5388\n",
      "Epoch [410/1000], Loss: 14.5358\n",
      "Epoch [420/1000], Loss: 14.5329\n",
      "Epoch [430/1000], Loss: 14.5302\n",
      "Epoch [440/1000], Loss: 14.5276\n",
      "Epoch [450/1000], Loss: 14.5250\n",
      "Epoch [460/1000], Loss: 14.5226\n",
      "Epoch [470/1000], Loss: 14.5203\n",
      "Epoch [480/1000], Loss: 14.5181\n",
      "Epoch [490/1000], Loss: 14.5159\n",
      "Epoch [500/1000], Loss: 14.5139\n",
      "Epoch [510/1000], Loss: 14.5119\n",
      "Epoch [520/1000], Loss: 14.5100\n",
      "Epoch [530/1000], Loss: 14.5082\n",
      "Epoch [540/1000], Loss: 14.5065\n",
      "Epoch [550/1000], Loss: 14.5048\n",
      "Epoch [560/1000], Loss: 14.5031\n",
      "Epoch [570/1000], Loss: 14.5016\n",
      "Epoch [580/1000], Loss: 14.5001\n",
      "Epoch [590/1000], Loss: 14.4986\n",
      "Epoch [600/1000], Loss: 14.4972\n",
      "Epoch [610/1000], Loss: 14.4958\n",
      "Epoch [620/1000], Loss: 14.4945\n",
      "Epoch [630/1000], Loss: 14.4932\n",
      "Epoch [640/1000], Loss: 14.4920\n",
      "Epoch [650/1000], Loss: 14.4908\n",
      "Epoch [660/1000], Loss: 14.4897\n",
      "Epoch [670/1000], Loss: 14.4886\n",
      "Epoch [680/1000], Loss: 14.4875\n",
      "Epoch [690/1000], Loss: 14.4864\n",
      "Epoch [700/1000], Loss: 14.4854\n",
      "Epoch [710/1000], Loss: 14.4844\n",
      "Epoch [720/1000], Loss: 14.4835\n",
      "Epoch [730/1000], Loss: 14.4826\n",
      "Epoch [740/1000], Loss: 14.4817\n",
      "Epoch [750/1000], Loss: 14.4808\n",
      "Epoch [760/1000], Loss: 14.4800\n",
      "Epoch [770/1000], Loss: 14.4792\n",
      "Epoch [780/1000], Loss: 14.4784\n",
      "Epoch [790/1000], Loss: 14.4776\n",
      "Epoch [800/1000], Loss: 14.4768\n",
      "Epoch [810/1000], Loss: 14.4761\n",
      "Epoch [820/1000], Loss: 14.4754\n",
      "Epoch [830/1000], Loss: 14.4747\n",
      "Epoch [840/1000], Loss: 14.4740\n",
      "Epoch [850/1000], Loss: 14.4734\n",
      "Epoch [860/1000], Loss: 14.4728\n",
      "Epoch [870/1000], Loss: 14.4721\n",
      "Epoch [880/1000], Loss: 14.4715\n",
      "Epoch [890/1000], Loss: 14.4709\n",
      "Epoch [900/1000], Loss: 14.4704\n",
      "Epoch [910/1000], Loss: 14.4698\n",
      "Epoch [920/1000], Loss: 14.4693\n",
      "Epoch [930/1000], Loss: 14.4687\n",
      "Epoch [940/1000], Loss: 14.4682\n",
      "Epoch [950/1000], Loss: 14.4677\n",
      "Epoch [960/1000], Loss: 14.4672\n",
      "Epoch [970/1000], Loss: 14.4667\n",
      "Epoch [980/1000], Loss: 14.4663\n",
      "Epoch [990/1000], Loss: 14.4658\n",
      "Epoch [1000/1000], Loss: 14.4654\n",
      "Predicted days_remaining for parent_id 332: [11.307480812072754, 11.813456535339355, 11.816933631896973, 11.815386772155762, 11.80855941772461, 11.81196117401123, 11.809120178222656, 11.80679988861084]\n",
      "Training for parent_id 340...\n",
      "Epoch [10/1000], Loss: 218.5363\n",
      "Epoch [20/1000], Loss: 173.0429\n",
      "Epoch [30/1000], Loss: 140.8837\n",
      "Epoch [40/1000], Loss: 119.5396\n",
      "Epoch [50/1000], Loss: 103.4935\n",
      "Epoch [60/1000], Loss: 90.1407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/1000], Loss: 78.6723\n",
      "Epoch [80/1000], Loss: 68.7437\n",
      "Epoch [90/1000], Loss: 60.1407\n",
      "Epoch [100/1000], Loss: 52.7071\n",
      "Epoch [110/1000], Loss: 46.3148\n",
      "Epoch [120/1000], Loss: 40.8489\n",
      "Epoch [130/1000], Loss: 36.2024\n",
      "Epoch [140/1000], Loss: 32.2754\n",
      "Epoch [150/1000], Loss: 28.9762\n",
      "Epoch [160/1000], Loss: 26.2212\n",
      "Epoch [170/1000], Loss: 23.9351\n",
      "Epoch [180/1000], Loss: 22.0505\n",
      "Epoch [190/1000], Loss: 20.5071\n",
      "Epoch [200/1000], Loss: 19.2518\n",
      "Epoch [210/1000], Loss: 18.2380\n",
      "Epoch [220/1000], Loss: 17.4249\n",
      "Epoch [230/1000], Loss: 16.7775\n",
      "Epoch [240/1000], Loss: 16.2656\n",
      "Epoch [250/1000], Loss: 15.8638\n",
      "Epoch [260/1000], Loss: 15.5505\n",
      "Epoch [270/1000], Loss: 15.3080\n",
      "Epoch [280/1000], Loss: 15.1215\n",
      "Epoch [290/1000], Loss: 14.9790\n",
      "Epoch [300/1000], Loss: 14.8708\n",
      "Epoch [310/1000], Loss: 14.7890\n",
      "Epoch [320/1000], Loss: 14.7275\n",
      "Epoch [330/1000], Loss: 14.6814\n",
      "Epoch [340/1000], Loss: 14.6470\n",
      "Epoch [350/1000], Loss: 14.6212\n",
      "Epoch [360/1000], Loss: 14.6019\n",
      "Epoch [370/1000], Loss: 14.5874\n",
      "Epoch [380/1000], Loss: 14.5763\n",
      "Epoch [390/1000], Loss: 14.5678\n",
      "Epoch [400/1000], Loss: 14.5612\n",
      "Epoch [410/1000], Loss: 14.5558\n",
      "Epoch [420/1000], Loss: 14.5515\n",
      "Epoch [430/1000], Loss: 14.5478\n",
      "Epoch [440/1000], Loss: 14.5446\n",
      "Epoch [450/1000], Loss: 14.5417\n",
      "Epoch [460/1000], Loss: 14.5392\n",
      "Epoch [470/1000], Loss: 14.5368\n",
      "Epoch [480/1000], Loss: 14.5346\n",
      "Epoch [490/1000], Loss: 14.5324\n",
      "Epoch [500/1000], Loss: 14.5304\n",
      "Epoch [510/1000], Loss: 14.5285\n",
      "Epoch [520/1000], Loss: 14.5266\n",
      "Epoch [530/1000], Loss: 14.5248\n",
      "Epoch [540/1000], Loss: 14.5231\n",
      "Epoch [550/1000], Loss: 14.5214\n",
      "Epoch [560/1000], Loss: 14.5197\n",
      "Epoch [570/1000], Loss: 14.5181\n",
      "Epoch [580/1000], Loss: 14.5166\n",
      "Epoch [590/1000], Loss: 14.5151\n",
      "Epoch [600/1000], Loss: 14.5136\n",
      "Epoch [610/1000], Loss: 14.5122\n",
      "Epoch [620/1000], Loss: 14.5108\n",
      "Epoch [630/1000], Loss: 14.5094\n",
      "Epoch [640/1000], Loss: 14.5081\n",
      "Epoch [650/1000], Loss: 14.5068\n",
      "Epoch [660/1000], Loss: 14.5056\n",
      "Epoch [670/1000], Loss: 14.5043\n",
      "Epoch [680/1000], Loss: 14.5031\n",
      "Epoch [690/1000], Loss: 14.5020\n",
      "Epoch [700/1000], Loss: 14.5008\n",
      "Epoch [710/1000], Loss: 14.4997\n",
      "Epoch [720/1000], Loss: 14.4987\n",
      "Epoch [730/1000], Loss: 14.4976\n",
      "Epoch [740/1000], Loss: 14.4966\n",
      "Epoch [750/1000], Loss: 14.4956\n",
      "Epoch [760/1000], Loss: 14.4946\n",
      "Epoch [770/1000], Loss: 14.4937\n",
      "Epoch [780/1000], Loss: 14.4927\n",
      "Epoch [790/1000], Loss: 14.4918\n",
      "Epoch [800/1000], Loss: 14.4909\n",
      "Epoch [810/1000], Loss: 14.4901\n",
      "Epoch [820/1000], Loss: 14.4892\n",
      "Epoch [830/1000], Loss: 14.4884\n",
      "Epoch [840/1000], Loss: 14.4876\n",
      "Epoch [850/1000], Loss: 14.4868\n",
      "Epoch [860/1000], Loss: 14.4860\n",
      "Epoch [870/1000], Loss: 14.4853\n",
      "Epoch [880/1000], Loss: 14.4845\n",
      "Epoch [890/1000], Loss: 14.4838\n",
      "Epoch [900/1000], Loss: 14.4831\n",
      "Epoch [910/1000], Loss: 14.4824\n",
      "Epoch [920/1000], Loss: 14.4817\n",
      "Epoch [930/1000], Loss: 14.4811\n",
      "Epoch [940/1000], Loss: 14.4804\n",
      "Epoch [950/1000], Loss: 14.4798\n",
      "Epoch [960/1000], Loss: 14.4792\n",
      "Epoch [970/1000], Loss: 14.4786\n",
      "Epoch [980/1000], Loss: 14.4780\n",
      "Epoch [990/1000], Loss: 14.4774\n",
      "Epoch [1000/1000], Loss: 14.4768\n",
      "Predicted days_remaining for parent_id 340: [15.225098609924316, 15.825053215026855, 15.82374095916748, 15.823719024658203, 15.824007034301758, 15.826742172241211, 15.824129104614258, 15.823302268981934]\n",
      "Training for parent_id 346...\n",
      "Epoch [10/1000], Loss: 320.6069\n",
      "Epoch [20/1000], Loss: 267.1008\n",
      "Epoch [30/1000], Loss: 228.5348\n",
      "Epoch [40/1000], Loss: 202.3650\n",
      "Epoch [50/1000], Loss: 181.3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/1000], Loss: 163.1523\n",
      "Epoch [70/1000], Loss: 146.8093\n",
      "Epoch [80/1000], Loss: 131.9778\n",
      "Epoch [90/1000], Loss: 118.4462\n",
      "Epoch [100/1000], Loss: 106.1264\n",
      "Epoch [110/1000], Loss: 94.9976\n",
      "Epoch [120/1000], Loss: 84.9984\n",
      "Epoch [130/1000], Loss: 76.0464\n",
      "Epoch [140/1000], Loss: 68.0571\n",
      "Epoch [150/1000], Loss: 60.9502\n",
      "Epoch [160/1000], Loss: 54.6507\n",
      "Epoch [170/1000], Loss: 49.0884\n",
      "Epoch [180/1000], Loss: 44.1961\n",
      "Epoch [190/1000], Loss: 39.9097\n",
      "Epoch [200/1000], Loss: 36.1683\n",
      "Epoch [210/1000], Loss: 32.9147\n",
      "Epoch [220/1000], Loss: 30.0959\n",
      "Epoch [230/1000], Loss: 27.6629\n",
      "Epoch [240/1000], Loss: 25.5711\n",
      "Epoch [250/1000], Loss: 23.7795\n",
      "Epoch [260/1000], Loss: 22.2512\n",
      "Epoch [270/1000], Loss: 20.9528\n",
      "Epoch [280/1000], Loss: 19.8542\n",
      "Epoch [290/1000], Loss: 18.9285\n",
      "Epoch [300/1000], Loss: 18.1516\n",
      "Epoch [310/1000], Loss: 17.5025\n",
      "Epoch [320/1000], Loss: 16.9622\n",
      "Epoch [330/1000], Loss: 16.5145\n",
      "Epoch [340/1000], Loss: 16.1450\n",
      "Epoch [350/1000], Loss: 15.8414\n",
      "Epoch [360/1000], Loss: 15.5927\n",
      "Epoch [370/1000], Loss: 15.3900\n",
      "Epoch [380/1000], Loss: 15.2254\n",
      "Epoch [390/1000], Loss: 15.0921\n",
      "Epoch [400/1000], Loss: 14.9847\n",
      "Epoch [410/1000], Loss: 14.8983\n",
      "Epoch [420/1000], Loss: 14.8291\n",
      "Epoch [430/1000], Loss: 14.7737\n",
      "Epoch [440/1000], Loss: 14.7296\n",
      "Epoch [450/1000], Loss: 14.6944\n",
      "Epoch [460/1000], Loss: 14.6665\n",
      "Epoch [470/1000], Loss: 14.6442\n",
      "Epoch [480/1000], Loss: 14.6264\n",
      "Epoch [490/1000], Loss: 14.6122\n",
      "Epoch [500/1000], Loss: 14.6009\n",
      "Epoch [510/1000], Loss: 14.5917\n",
      "Epoch [520/1000], Loss: 14.5842\n",
      "Epoch [530/1000], Loss: 14.5780\n",
      "Epoch [540/1000], Loss: 14.5729\n",
      "Epoch [550/1000], Loss: 14.5685\n",
      "Epoch [560/1000], Loss: 14.5648\n",
      "Epoch [570/1000], Loss: 14.5616\n",
      "Epoch [580/1000], Loss: 14.5587\n",
      "Epoch [590/1000], Loss: 14.5561\n",
      "Epoch [600/1000], Loss: 14.5537\n",
      "Epoch [610/1000], Loss: 14.5515\n",
      "Epoch [620/1000], Loss: 14.5495\n",
      "Epoch [630/1000], Loss: 14.5475\n",
      "Epoch [640/1000], Loss: 14.5457\n",
      "Epoch [650/1000], Loss: 14.5439\n",
      "Epoch [660/1000], Loss: 14.5422\n",
      "Epoch [670/1000], Loss: 14.5405\n",
      "Epoch [680/1000], Loss: 14.5389\n",
      "Epoch [690/1000], Loss: 14.5373\n",
      "Epoch [700/1000], Loss: 14.5358\n",
      "Epoch [710/1000], Loss: 14.5343\n",
      "Epoch [720/1000], Loss: 14.5328\n",
      "Epoch [730/1000], Loss: 14.5314\n",
      "Epoch [740/1000], Loss: 14.5300\n",
      "Epoch [750/1000], Loss: 14.5286\n",
      "Epoch [760/1000], Loss: 14.5273\n",
      "Epoch [770/1000], Loss: 14.5260\n",
      "Epoch [780/1000], Loss: 14.5247\n",
      "Epoch [790/1000], Loss: 14.5234\n",
      "Epoch [800/1000], Loss: 14.5222\n",
      "Epoch [810/1000], Loss: 14.5210\n",
      "Epoch [820/1000], Loss: 14.5198\n",
      "Epoch [830/1000], Loss: 14.5186\n",
      "Epoch [840/1000], Loss: 14.5174\n",
      "Epoch [850/1000], Loss: 14.5163\n",
      "Epoch [860/1000], Loss: 14.5152\n",
      "Epoch [870/1000], Loss: 14.5141\n",
      "Epoch [880/1000], Loss: 14.5131\n",
      "Epoch [890/1000], Loss: 14.5120\n",
      "Epoch [900/1000], Loss: 14.5110\n",
      "Epoch [910/1000], Loss: 14.5100\n",
      "Epoch [920/1000], Loss: 14.5090\n",
      "Epoch [930/1000], Loss: 14.5081\n",
      "Epoch [940/1000], Loss: 14.5071\n",
      "Epoch [950/1000], Loss: 14.5062\n",
      "Epoch [960/1000], Loss: 14.5053\n",
      "Epoch [970/1000], Loss: 14.5044\n",
      "Epoch [980/1000], Loss: 14.5035\n",
      "Epoch [990/1000], Loss: 14.5026\n",
      "Epoch [1000/1000], Loss: 14.5018\n",
      "Predicted days_remaining for parent_id 346: [18.07959747314453, 18.84597396850586, 18.845497131347656, 18.845611572265625, 18.84664535522461, 18.84701919555664, 18.845308303833008, 18.84469985961914]\n",
      "Training for parent_id 348...\n",
      "Epoch [10/1000], Loss: 930.7064\n",
      "Epoch [20/1000], Loss: 829.4763\n",
      "Epoch [30/1000], Loss: 756.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000], Loss: 704.9672\n",
      "Epoch [50/1000], Loss: 662.5964\n",
      "Epoch [60/1000], Loss: 624.2004\n",
      "Epoch [70/1000], Loss: 588.4448\n",
      "Epoch [80/1000], Loss: 554.8331\n",
      "Epoch [90/1000], Loss: 523.1066\n",
      "Epoch [100/1000], Loss: 493.0985\n",
      "Epoch [110/1000], Loss: 464.6855\n",
      "Epoch [120/1000], Loss: 437.7682\n",
      "Epoch [130/1000], Loss: 412.2616\n",
      "Epoch [140/1000], Loss: 388.0900\n",
      "Epoch [150/1000], Loss: 365.1854\n",
      "Epoch [160/1000], Loss: 343.4849\n",
      "Epoch [170/1000], Loss: 322.9303\n",
      "Epoch [180/1000], Loss: 303.4672\n",
      "Epoch [190/1000], Loss: 285.0447\n",
      "Epoch [200/1000], Loss: 267.6149\n",
      "Epoch [210/1000], Loss: 251.1323\n",
      "Epoch [220/1000], Loss: 235.5540\n",
      "Epoch [230/1000], Loss: 220.8390\n",
      "Epoch [240/1000], Loss: 206.9481\n",
      "Epoch [250/1000], Loss: 193.8442\n",
      "Epoch [260/1000], Loss: 181.4913\n",
      "Epoch [270/1000], Loss: 169.8552\n",
      "Epoch [280/1000], Loss: 158.9028\n",
      "Epoch [290/1000], Loss: 148.6022\n",
      "Epoch [300/1000], Loss: 138.9228\n",
      "Epoch [310/1000], Loss: 129.8350\n",
      "Epoch [320/1000], Loss: 121.3102\n",
      "Epoch [330/1000], Loss: 113.3210\n",
      "Epoch [340/1000], Loss: 105.8406\n",
      "Epoch [350/1000], Loss: 98.8434\n",
      "Epoch [360/1000], Loss: 92.3046\n",
      "Epoch [370/1000], Loss: 86.2004\n",
      "Epoch [380/1000], Loss: 80.5076\n",
      "Epoch [390/1000], Loss: 75.2040\n",
      "Epoch [400/1000], Loss: 70.2682\n",
      "Epoch [410/1000], Loss: 65.6797\n",
      "Epoch [420/1000], Loss: 61.4187\n",
      "Epoch [430/1000], Loss: 57.4661\n",
      "Epoch [440/1000], Loss: 53.8037\n",
      "Epoch [450/1000], Loss: 50.4140\n",
      "Epoch [460/1000], Loss: 47.2804\n",
      "Epoch [470/1000], Loss: 44.3868\n",
      "Epoch [480/1000], Loss: 41.7179\n",
      "Epoch [490/1000], Loss: 39.2592\n",
      "Epoch [500/1000], Loss: 36.9969\n",
      "Epoch [510/1000], Loss: 34.9176\n",
      "Epoch [520/1000], Loss: 33.0091\n",
      "Epoch [530/1000], Loss: 31.2592\n",
      "Epoch [540/1000], Loss: 29.6569\n",
      "Epoch [550/1000], Loss: 28.1915\n",
      "Epoch [560/1000], Loss: 26.8529\n",
      "Epoch [570/1000], Loss: 25.6317\n",
      "Epoch [580/1000], Loss: 24.5190\n",
      "Epoch [590/1000], Loss: 23.5065\n",
      "Epoch [600/1000], Loss: 22.5862\n",
      "Epoch [610/1000], Loss: 21.7509\n",
      "Epoch [620/1000], Loss: 20.9937\n",
      "Epoch [630/1000], Loss: 20.3081\n",
      "Epoch [640/1000], Loss: 19.6882\n",
      "Epoch [650/1000], Loss: 19.1283\n",
      "Epoch [660/1000], Loss: 18.6235\n",
      "Epoch [670/1000], Loss: 18.1687\n",
      "Epoch [680/1000], Loss: 17.7596\n",
      "Epoch [690/1000], Loss: 17.3922\n",
      "Epoch [700/1000], Loss: 17.0625\n",
      "Epoch [710/1000], Loss: 16.7671\n",
      "Epoch [720/1000], Loss: 16.5027\n",
      "Epoch [730/1000], Loss: 16.2665\n",
      "Epoch [740/1000], Loss: 16.0557\n",
      "Epoch [750/1000], Loss: 15.8678\n",
      "Epoch [760/1000], Loss: 15.7005\n",
      "Epoch [770/1000], Loss: 15.5518\n",
      "Epoch [780/1000], Loss: 15.4198\n",
      "Epoch [790/1000], Loss: 15.3028\n",
      "Epoch [800/1000], Loss: 15.1992\n",
      "Epoch [810/1000], Loss: 15.1076\n",
      "Epoch [820/1000], Loss: 15.0267\n",
      "Epoch [830/1000], Loss: 14.9554\n",
      "Epoch [840/1000], Loss: 14.8926\n",
      "Epoch [850/1000], Loss: 14.8373\n",
      "Epoch [860/1000], Loss: 14.7888\n",
      "Epoch [870/1000], Loss: 14.7462\n",
      "Epoch [880/1000], Loss: 14.7089\n",
      "Epoch [890/1000], Loss: 14.6763\n",
      "Epoch [900/1000], Loss: 14.6478\n",
      "Epoch [910/1000], Loss: 14.6229\n",
      "Epoch [920/1000], Loss: 14.6012\n",
      "Epoch [930/1000], Loss: 14.5823\n",
      "Epoch [940/1000], Loss: 14.5659\n",
      "Epoch [950/1000], Loss: 14.5517\n",
      "Epoch [960/1000], Loss: 14.5393\n",
      "Epoch [970/1000], Loss: 14.5287\n",
      "Epoch [980/1000], Loss: 14.5194\n",
      "Epoch [990/1000], Loss: 14.5114\n",
      "Epoch [1000/1000], Loss: 14.5045\n",
      "Predicted days_remaining for parent_id 348: [31.112232208251953, 31.61590576171875, 31.61634635925293, 31.616390228271484, 31.616601943969727, 31.618404388427734, 31.61668586730957, 31.616775512695312]\n",
      "Training for parent_id 360...\n",
      "Epoch [10/1000], Loss: 429.7154\n",
      "Epoch [20/1000], Loss: 366.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 318.4461\n",
      "Epoch [40/1000], Loss: 284.7062\n",
      "Epoch [50/1000], Loss: 258.0950\n",
      "Epoch [60/1000], Loss: 234.8176\n",
      "Epoch [70/1000], Loss: 213.7442\n",
      "Epoch [80/1000], Loss: 194.5032\n",
      "Epoch [90/1000], Loss: 176.9188\n",
      "Epoch [100/1000], Loss: 160.8465\n",
      "Epoch [110/1000], Loss: 146.1591\n",
      "Epoch [120/1000], Loss: 132.7454\n",
      "Epoch [130/1000], Loss: 120.5068\n",
      "Epoch [140/1000], Loss: 109.3543\n",
      "Epoch [150/1000], Loss: 99.2069\n",
      "Epoch [160/1000], Loss: 89.9896\n",
      "Epoch [170/1000], Loss: 81.6327\n",
      "Epoch [180/1000], Loss: 74.0710\n",
      "Epoch [190/1000], Loss: 67.2430\n",
      "Epoch [200/1000], Loss: 61.0910\n",
      "Epoch [210/1000], Loss: 55.5607\n",
      "Epoch [220/1000], Loss: 50.6007\n",
      "Epoch [230/1000], Loss: 46.1629\n",
      "Epoch [240/1000], Loss: 42.2023\n",
      "Epoch [250/1000], Loss: 38.6764\n",
      "Epoch [260/1000], Loss: 35.5458\n",
      "Epoch [270/1000], Loss: 32.7735\n",
      "Epoch [280/1000], Loss: 30.3252\n",
      "Epoch [290/1000], Loss: 28.1691\n",
      "Epoch [300/1000], Loss: 26.2756\n",
      "Epoch [310/1000], Loss: 24.6174\n",
      "Epoch [320/1000], Loss: 23.1696\n",
      "Epoch [330/1000], Loss: 21.9091\n",
      "Epoch [340/1000], Loss: 20.8150\n",
      "Epoch [350/1000], Loss: 19.8681\n",
      "Epoch [360/1000], Loss: 19.0510\n",
      "Epoch [370/1000], Loss: 18.3480\n",
      "Epoch [380/1000], Loss: 17.7451\n",
      "Epoch [390/1000], Loss: 17.2296\n",
      "Epoch [400/1000], Loss: 16.7901\n",
      "Epoch [410/1000], Loss: 16.4165\n",
      "Epoch [420/1000], Loss: 16.1000\n",
      "Epoch [430/1000], Loss: 15.8326\n",
      "Epoch [440/1000], Loss: 15.6073\n",
      "Epoch [450/1000], Loss: 15.4181\n",
      "Epoch [460/1000], Loss: 15.2597\n",
      "Epoch [470/1000], Loss: 15.1275\n",
      "Epoch [480/1000], Loss: 15.0174\n",
      "Epoch [490/1000], Loss: 14.9260\n",
      "Epoch [500/1000], Loss: 14.8504\n",
      "Epoch [510/1000], Loss: 14.7879\n",
      "Epoch [520/1000], Loss: 14.7365\n",
      "Epoch [530/1000], Loss: 14.6942\n",
      "Epoch [540/1000], Loss: 14.6595\n",
      "Epoch [550/1000], Loss: 14.6312\n",
      "Epoch [560/1000], Loss: 14.6081\n",
      "Epoch [570/1000], Loss: 14.5892\n",
      "Epoch [580/1000], Loss: 14.5738\n",
      "Epoch [590/1000], Loss: 14.5613\n",
      "Epoch [600/1000], Loss: 14.5511\n",
      "Epoch [610/1000], Loss: 14.5428\n",
      "Epoch [620/1000], Loss: 14.5360\n",
      "Epoch [630/1000], Loss: 14.5305\n",
      "Epoch [640/1000], Loss: 14.5260\n",
      "Epoch [650/1000], Loss: 14.5222\n",
      "Epoch [660/1000], Loss: 14.5191\n",
      "Epoch [670/1000], Loss: 14.5165\n",
      "Epoch [680/1000], Loss: 14.5143\n",
      "Epoch [690/1000], Loss: 14.5124\n",
      "Epoch [700/1000], Loss: 14.5108\n",
      "Epoch [710/1000], Loss: 14.5093\n",
      "Epoch [720/1000], Loss: 14.5081\n",
      "Epoch [730/1000], Loss: 14.5069\n",
      "Epoch [740/1000], Loss: 14.5059\n",
      "Epoch [750/1000], Loss: 14.5049\n",
      "Epoch [760/1000], Loss: 14.5040\n",
      "Epoch [770/1000], Loss: 14.5031\n",
      "Epoch [780/1000], Loss: 14.5023\n",
      "Epoch [790/1000], Loss: 14.5016\n",
      "Epoch [800/1000], Loss: 14.5008\n",
      "Epoch [810/1000], Loss: 14.5001\n",
      "Epoch [820/1000], Loss: 14.4994\n",
      "Epoch [830/1000], Loss: 14.4987\n",
      "Epoch [840/1000], Loss: 14.4980\n",
      "Epoch [850/1000], Loss: 14.4973\n",
      "Epoch [860/1000], Loss: 14.4967\n",
      "Epoch [870/1000], Loss: 14.4960\n",
      "Epoch [880/1000], Loss: 14.4954\n",
      "Epoch [890/1000], Loss: 14.4948\n",
      "Epoch [900/1000], Loss: 14.4942\n",
      "Epoch [910/1000], Loss: 14.4936\n",
      "Epoch [920/1000], Loss: 14.4930\n",
      "Epoch [930/1000], Loss: 14.4924\n",
      "Epoch [940/1000], Loss: 14.4918\n",
      "Epoch [950/1000], Loss: 14.4912\n",
      "Epoch [960/1000], Loss: 14.4907\n",
      "Epoch [970/1000], Loss: 14.4901\n",
      "Epoch [980/1000], Loss: 14.4895\n",
      "Epoch [990/1000], Loss: 14.4890\n",
      "Epoch [1000/1000], Loss: 14.4885\n",
      "Predicted days_remaining for parent_id 360: [21.15350341796875, 21.834562301635742, 21.83542823791504, 21.835430145263672, 21.83836555480957, 21.83584213256836, 21.836238861083984, 21.8339900970459]\n",
      "Training for parent_id 362...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 1054.3733\n",
      "Epoch [20/1000], Loss: 944.4636\n",
      "Epoch [30/1000], Loss: 870.5999\n",
      "Epoch [40/1000], Loss: 818.3845\n",
      "Epoch [50/1000], Loss: 773.5258\n",
      "Epoch [60/1000], Loss: 732.2755\n",
      "Epoch [70/1000], Loss: 693.5728\n",
      "Epoch [80/1000], Loss: 656.9595\n",
      "Epoch [90/1000], Loss: 622.1746\n",
      "Epoch [100/1000], Loss: 589.0560\n",
      "Epoch [110/1000], Loss: 557.4955\n",
      "Epoch [120/1000], Loss: 527.4141\n",
      "Epoch [130/1000], Loss: 498.7471\n",
      "Epoch [140/1000], Loss: 471.4374\n",
      "Epoch [150/1000], Loss: 445.4317\n",
      "Epoch [160/1000], Loss: 420.6788\n",
      "Epoch [170/1000], Loss: 397.1284\n",
      "Epoch [180/1000], Loss: 374.7321\n",
      "Epoch [190/1000], Loss: 353.4429\n",
      "Epoch [200/1000], Loss: 333.2151\n",
      "Epoch [210/1000], Loss: 314.0049\n",
      "Epoch [220/1000], Loss: 295.7699\n",
      "Epoch [230/1000], Loss: 278.4694\n",
      "Epoch [240/1000], Loss: 262.0641\n",
      "Epoch [250/1000], Loss: 246.5166\n",
      "Epoch [260/1000], Loss: 231.7905\n",
      "Epoch [270/1000], Loss: 217.8509\n",
      "Epoch [280/1000], Loss: 204.6643\n",
      "Epoch [290/1000], Loss: 192.1981\n",
      "Epoch [300/1000], Loss: 180.4212\n",
      "Epoch [310/1000], Loss: 169.3032\n",
      "Epoch [320/1000], Loss: 158.8150\n",
      "Epoch [330/1000], Loss: 148.9284\n",
      "Epoch [340/1000], Loss: 139.6161\n",
      "Epoch [350/1000], Loss: 130.8516\n",
      "Epoch [360/1000], Loss: 122.6095\n",
      "Epoch [370/1000], Loss: 114.8651\n",
      "Epoch [380/1000], Loss: 107.5945\n",
      "Epoch [390/1000], Loss: 100.7747\n",
      "Epoch [400/1000], Loss: 94.3833\n",
      "Epoch [410/1000], Loss: 88.3988\n",
      "Epoch [420/1000], Loss: 82.8004\n",
      "Epoch [430/1000], Loss: 77.5682\n",
      "Epoch [440/1000], Loss: 72.6827\n",
      "Epoch [450/1000], Loss: 68.1253\n",
      "Epoch [460/1000], Loss: 63.8781\n",
      "Epoch [470/1000], Loss: 59.9239\n",
      "Epoch [480/1000], Loss: 56.2461\n",
      "Epoch [490/1000], Loss: 52.8288\n",
      "Epoch [500/1000], Loss: 49.6569\n",
      "Epoch [510/1000], Loss: 46.7156\n",
      "Epoch [520/1000], Loss: 43.9911\n",
      "Epoch [530/1000], Loss: 41.4700\n",
      "Epoch [540/1000], Loss: 39.1396\n",
      "Epoch [550/1000], Loss: 36.9877\n",
      "Epoch [560/1000], Loss: 35.0028\n",
      "Epoch [570/1000], Loss: 33.1739\n",
      "Epoch [580/1000], Loss: 31.4906\n",
      "Epoch [590/1000], Loss: 29.9430\n",
      "Epoch [600/1000], Loss: 28.5216\n",
      "Epoch [610/1000], Loss: 27.2177\n",
      "Epoch [620/1000], Loss: 26.0229\n",
      "Epoch [630/1000], Loss: 24.9292\n",
      "Epoch [640/1000], Loss: 23.9293\n",
      "Epoch [650/1000], Loss: 23.0160\n",
      "Epoch [660/1000], Loss: 22.1830\n",
      "Epoch [670/1000], Loss: 21.4239\n",
      "Epoch [680/1000], Loss: 20.7330\n",
      "Epoch [690/1000], Loss: 20.1049\n",
      "Epoch [700/1000], Loss: 19.5345\n",
      "Epoch [710/1000], Loss: 19.0171\n",
      "Epoch [720/1000], Loss: 18.5484\n",
      "Epoch [730/1000], Loss: 18.1243\n",
      "Epoch [740/1000], Loss: 17.7409\n",
      "Epoch [750/1000], Loss: 17.3948\n",
      "Epoch [760/1000], Loss: 17.0827\n",
      "Epoch [770/1000], Loss: 16.8016\n",
      "Epoch [780/1000], Loss: 16.5486\n",
      "Epoch [790/1000], Loss: 16.3214\n",
      "Epoch [800/1000], Loss: 16.1174\n",
      "Epoch [810/1000], Loss: 15.9345\n",
      "Epoch [820/1000], Loss: 15.7707\n",
      "Epoch [830/1000], Loss: 15.6243\n",
      "Epoch [840/1000], Loss: 15.4935\n",
      "Epoch [850/1000], Loss: 15.3767\n",
      "Epoch [860/1000], Loss: 15.2727\n",
      "Epoch [870/1000], Loss: 15.1801\n",
      "Epoch [880/1000], Loss: 15.0977\n",
      "Epoch [890/1000], Loss: 15.0246\n",
      "Epoch [900/1000], Loss: 14.9597\n",
      "Epoch [910/1000], Loss: 14.9022\n",
      "Epoch [920/1000], Loss: 14.8514\n",
      "Epoch [930/1000], Loss: 14.8064\n",
      "Epoch [940/1000], Loss: 14.7667\n",
      "Epoch [950/1000], Loss: 14.7317\n",
      "Epoch [960/1000], Loss: 14.7008\n",
      "Epoch [970/1000], Loss: 14.6737\n",
      "Epoch [980/1000], Loss: 14.6498\n",
      "Epoch [990/1000], Loss: 14.6288\n",
      "Epoch [1000/1000], Loss: 14.6105\n",
      "Predicted days_remaining for parent_id 362: [32.789249420166016, 33.49734115600586, 33.49680709838867, 33.497100830078125, 33.49707794189453, 33.4978141784668, 33.49746322631836, 33.49773025512695]\n",
      "Training for parent_id 366...\n",
      "Epoch [10/1000], Loss: 897.0277\n",
      "Epoch [20/1000], Loss: 807.6487\n",
      "Epoch [30/1000], Loss: 732.7520\n",
      "Epoch [40/1000], Loss: 681.1804\n",
      "Epoch [50/1000], Loss: 638.5657\n",
      "Epoch [60/1000], Loss: 600.0130\n",
      "Epoch [70/1000], Loss: 564.2548\n",
      "Epoch [80/1000], Loss: 530.7687\n",
      "Epoch [90/1000], Loss: 499.2502\n",
      "Epoch [100/1000], Loss: 469.5028\n",
      "Epoch [110/1000], Loss: 441.3889\n",
      "Epoch [120/1000], Loss: 414.8040\n",
      "Epoch [130/1000], Loss: 389.6617\n",
      "Epoch [140/1000], Loss: 365.8865\n",
      "Epoch [150/1000], Loss: 343.4094\n",
      "Epoch [160/1000], Loss: 322.1664\n",
      "Epoch [170/1000], Loss: 302.0970\n",
      "Epoch [180/1000], Loss: 283.1446\n",
      "Epoch [190/1000], Loss: 265.2549\n",
      "Epoch [200/1000], Loss: 248.3772\n",
      "Epoch [210/1000], Loss: 232.4629\n",
      "Epoch [220/1000], Loss: 217.4660\n",
      "Epoch [230/1000], Loss: 203.3426\n",
      "Epoch [240/1000], Loss: 190.0511\n",
      "Epoch [250/1000], Loss: 177.5513\n",
      "Epoch [260/1000], Loss: 165.8052\n",
      "Epoch [270/1000], Loss: 154.7760\n",
      "Epoch [280/1000], Loss: 144.4287\n",
      "Epoch [290/1000], Loss: 134.7294\n",
      "Epoch [300/1000], Loss: 125.6459\n",
      "Epoch [310/1000], Loss: 117.1468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/1000], Loss: 109.2022\n",
      "Epoch [330/1000], Loss: 101.7831\n",
      "Epoch [340/1000], Loss: 94.8619\n",
      "Epoch [350/1000], Loss: 88.4116\n",
      "Epoch [360/1000], Loss: 82.4067\n",
      "Epoch [370/1000], Loss: 76.8223\n",
      "Epoch [380/1000], Loss: 71.6348\n",
      "Epoch [390/1000], Loss: 66.8212\n",
      "Epoch [400/1000], Loss: 62.3597\n",
      "Epoch [410/1000], Loss: 58.2293\n",
      "Epoch [420/1000], Loss: 54.4099\n",
      "Epoch [430/1000], Loss: 50.8822\n",
      "Epoch [440/1000], Loss: 47.6278\n",
      "Epoch [450/1000], Loss: 44.6293\n",
      "Epoch [460/1000], Loss: 41.8699\n",
      "Epoch [470/1000], Loss: 39.3336\n",
      "Epoch [480/1000], Loss: 37.0054\n",
      "Epoch [490/1000], Loss: 34.8709\n",
      "Epoch [500/1000], Loss: 32.9164\n",
      "Epoch [510/1000], Loss: 31.1290\n",
      "Epoch [520/1000], Loss: 29.4967\n",
      "Epoch [530/1000], Loss: 28.0077\n",
      "Epoch [540/1000], Loss: 26.6515\n",
      "Epoch [550/1000], Loss: 25.4177\n",
      "Epoch [560/1000], Loss: 24.2968\n",
      "Epoch [570/1000], Loss: 23.2798\n",
      "Epoch [580/1000], Loss: 22.3583\n",
      "Epoch [590/1000], Loss: 21.5244\n",
      "Epoch [600/1000], Loss: 20.7710\n",
      "Epoch [610/1000], Loss: 20.0910\n",
      "Epoch [620/1000], Loss: 19.4782\n",
      "Epoch [630/1000], Loss: 18.9267\n",
      "Epoch [640/1000], Loss: 18.4311\n",
      "Epoch [650/1000], Loss: 17.9863\n",
      "Epoch [660/1000], Loss: 17.5876\n",
      "Epoch [670/1000], Loss: 17.2308\n",
      "Epoch [680/1000], Loss: 16.9118\n",
      "Epoch [690/1000], Loss: 16.6271\n",
      "Epoch [700/1000], Loss: 16.3734\n",
      "Epoch [710/1000], Loss: 16.1475\n",
      "Epoch [720/1000], Loss: 15.9468\n",
      "Epoch [730/1000], Loss: 15.7686\n",
      "Epoch [740/1000], Loss: 15.6107\n",
      "Epoch [750/1000], Loss: 15.4708\n",
      "Epoch [760/1000], Loss: 15.3473\n",
      "Epoch [770/1000], Loss: 15.2382\n",
      "Epoch [780/1000], Loss: 15.1420\n",
      "Epoch [790/1000], Loss: 15.0574\n",
      "Epoch [800/1000], Loss: 14.9830\n",
      "Epoch [810/1000], Loss: 14.9176\n",
      "Epoch [820/1000], Loss: 14.8604\n",
      "Epoch [830/1000], Loss: 14.8102\n",
      "Epoch [840/1000], Loss: 14.7664\n",
      "Epoch [850/1000], Loss: 14.7281\n",
      "Epoch [860/1000], Loss: 14.6948\n",
      "Epoch [870/1000], Loss: 14.6657\n",
      "Epoch [880/1000], Loss: 14.6404\n",
      "Epoch [890/1000], Loss: 14.6185\n",
      "Epoch [900/1000], Loss: 14.5994\n",
      "Epoch [910/1000], Loss: 14.5829\n",
      "Epoch [920/1000], Loss: 14.5686\n",
      "Epoch [930/1000], Loss: 14.5563\n",
      "Epoch [940/1000], Loss: 14.5456\n",
      "Epoch [950/1000], Loss: 14.5364\n",
      "Epoch [960/1000], Loss: 14.5285\n",
      "Epoch [970/1000], Loss: 14.5217\n",
      "Epoch [980/1000], Loss: 14.5158\n",
      "Epoch [990/1000], Loss: 14.5107\n",
      "Epoch [1000/1000], Loss: 14.5064\n",
      "Predicted days_remaining for parent_id 366: [30.030818939208984, 30.68454933166504, 30.684579849243164, 30.68423843383789, 30.684223175048828, 30.68436622619629, 30.684345245361328, 30.682373046875]\n",
      "Training for parent_id 371...\n",
      "Epoch [10/1000], Loss: 248.1652\n",
      "Epoch [20/1000], Loss: 202.9148\n",
      "Epoch [30/1000], Loss: 169.0993\n",
      "Epoch [40/1000], Loss: 146.0941\n",
      "Epoch [50/1000], Loss: 128.0698\n",
      "Epoch [60/1000], Loss: 112.6811\n",
      "Epoch [70/1000], Loss: 99.1966\n",
      "Epoch [80/1000], Loss: 87.3424\n",
      "Epoch [90/1000], Loss: 76.9302\n",
      "Epoch [100/1000], Loss: 67.8038\n",
      "Epoch [110/1000], Loss: 59.8285\n",
      "Epoch [120/1000], Loss: 52.8846\n",
      "Epoch [130/1000], Loss: 46.8634\n",
      "Epoch [140/1000], Loss: 41.6650\n",
      "Epoch [150/1000], Loss: 37.1975\n",
      "Epoch [160/1000], Loss: 33.3764\n",
      "Epoch [170/1000], Loss: 30.1245\n",
      "Epoch [180/1000], Loss: 27.3711\n",
      "Epoch [190/1000], Loss: 25.0522\n",
      "Epoch [200/1000], Loss: 23.1099\n",
      "Epoch [210/1000], Loss: 21.4921\n",
      "Epoch [220/1000], Loss: 20.1522\n",
      "Epoch [230/1000], Loss: 19.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/1000], Loss: 18.1456\n",
      "Epoch [250/1000], Loss: 17.4105\n",
      "Epoch [260/1000], Loss: 16.8156\n",
      "Epoch [270/1000], Loss: 16.3372\n",
      "Epoch [280/1000], Loss: 15.9545\n",
      "Epoch [290/1000], Loss: 15.6502\n",
      "Epoch [300/1000], Loss: 15.4096\n",
      "Epoch [310/1000], Loss: 15.2203\n",
      "Epoch [320/1000], Loss: 15.0722\n",
      "Epoch [330/1000], Loss: 14.9568\n",
      "Epoch [340/1000], Loss: 14.8673\n",
      "Epoch [350/1000], Loss: 14.7982\n",
      "Epoch [360/1000], Loss: 14.7449\n",
      "Epoch [370/1000], Loss: 14.7040\n",
      "Epoch [380/1000], Loss: 14.6725\n",
      "Epoch [390/1000], Loss: 14.6482\n",
      "Epoch [400/1000], Loss: 14.6295\n",
      "Epoch [410/1000], Loss: 14.6149\n",
      "Epoch [420/1000], Loss: 14.6035\n",
      "Epoch [430/1000], Loss: 14.5945\n",
      "Epoch [440/1000], Loss: 14.5872\n",
      "Epoch [450/1000], Loss: 14.5812\n",
      "Epoch [460/1000], Loss: 14.5762\n",
      "Epoch [470/1000], Loss: 14.5719\n",
      "Epoch [480/1000], Loss: 14.5682\n",
      "Epoch [490/1000], Loss: 14.5648\n",
      "Epoch [500/1000], Loss: 14.5618\n",
      "Epoch [510/1000], Loss: 14.5590\n",
      "Epoch [520/1000], Loss: 14.5563\n",
      "Epoch [530/1000], Loss: 14.5538\n",
      "Epoch [540/1000], Loss: 14.5515\n",
      "Epoch [550/1000], Loss: 14.5492\n",
      "Epoch [560/1000], Loss: 14.5470\n",
      "Epoch [570/1000], Loss: 14.5449\n",
      "Epoch [580/1000], Loss: 14.5428\n",
      "Epoch [590/1000], Loss: 14.5408\n",
      "Epoch [600/1000], Loss: 14.5389\n",
      "Epoch [610/1000], Loss: 14.5370\n",
      "Epoch [620/1000], Loss: 14.5352\n",
      "Epoch [630/1000], Loss: 14.5334\n",
      "Epoch [640/1000], Loss: 14.5317\n",
      "Epoch [650/1000], Loss: 14.5300\n",
      "Epoch [660/1000], Loss: 14.5284\n",
      "Epoch [670/1000], Loss: 14.5267\n",
      "Epoch [680/1000], Loss: 14.5252\n",
      "Epoch [690/1000], Loss: 14.5237\n",
      "Epoch [700/1000], Loss: 14.5222\n",
      "Epoch [710/1000], Loss: 14.5207\n",
      "Epoch [720/1000], Loss: 14.5193\n",
      "Epoch [730/1000], Loss: 14.5179\n",
      "Epoch [740/1000], Loss: 14.5166\n",
      "Epoch [750/1000], Loss: 14.5152\n",
      "Epoch [760/1000], Loss: 14.5139\n",
      "Epoch [770/1000], Loss: 14.5127\n",
      "Epoch [780/1000], Loss: 14.5115\n",
      "Epoch [790/1000], Loss: 14.5103\n",
      "Epoch [800/1000], Loss: 14.5091\n",
      "Epoch [810/1000], Loss: 14.5079\n",
      "Epoch [820/1000], Loss: 14.5068\n",
      "Epoch [830/1000], Loss: 14.5057\n",
      "Epoch [840/1000], Loss: 14.5047\n",
      "Epoch [850/1000], Loss: 14.5036\n",
      "Epoch [860/1000], Loss: 14.5026\n",
      "Epoch [870/1000], Loss: 14.5016\n",
      "Epoch [880/1000], Loss: 14.5006\n",
      "Epoch [890/1000], Loss: 14.4997\n",
      "Epoch [900/1000], Loss: 14.4987\n",
      "Epoch [910/1000], Loss: 14.4978\n",
      "Epoch [920/1000], Loss: 14.4969\n",
      "Epoch [930/1000], Loss: 14.4960\n",
      "Epoch [940/1000], Loss: 14.4952\n",
      "Epoch [950/1000], Loss: 14.4943\n",
      "Epoch [960/1000], Loss: 14.4935\n",
      "Epoch [970/1000], Loss: 14.4927\n",
      "Epoch [980/1000], Loss: 14.4919\n",
      "Epoch [990/1000], Loss: 14.4911\n",
      "Epoch [1000/1000], Loss: 14.4904\n",
      "Predicted days_remaining for parent_id 371: [16.141508102416992, 16.83633041381836, 16.835308074951172, 16.836618423461914, 16.837787628173828, 16.836193084716797, 16.837326049804688, 16.834558486938477]\n",
      "Training for parent_id 376...\n",
      "Epoch [10/1000], Loss: 728.8369\n",
      "Epoch [20/1000], Loss: 633.1426\n",
      "Epoch [30/1000], Loss: 558.6587\n",
      "Epoch [40/1000], Loss: 510.0689\n",
      "Epoch [50/1000], Loss: 471.9552\n",
      "Epoch [60/1000], Loss: 438.4038\n",
      "Epoch [70/1000], Loss: 407.9315\n",
      "Epoch [80/1000], Loss: 379.7956\n",
      "Epoch [90/1000], Loss: 353.5932\n",
      "Epoch [100/1000], Loss: 329.0865\n",
      "Epoch [110/1000], Loss: 306.1208\n",
      "Epoch [120/1000], Loss: 284.5862\n",
      "Epoch [130/1000], Loss: 264.3963\n",
      "Epoch [140/1000], Loss: 245.4776\n",
      "Epoch [150/1000], Loss: 227.7637\n",
      "Epoch [160/1000], Loss: 211.1929\n",
      "Epoch [170/1000], Loss: 195.7068\n",
      "Epoch [180/1000], Loss: 181.2492\n",
      "Epoch [190/1000], Loss: 167.7663\n",
      "Epoch [200/1000], Loss: 155.2062\n",
      "Epoch [210/1000], Loss: 143.5189\n",
      "Epoch [220/1000], Loss: 132.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/1000], Loss: 122.5725\n",
      "Epoch [240/1000], Loss: 113.2228\n",
      "Epoch [250/1000], Loss: 104.5647\n",
      "Epoch [260/1000], Loss: 96.5572\n",
      "Epoch [270/1000], Loss: 89.1613\n",
      "Epoch [280/1000], Loss: 82.3394\n",
      "Epoch [290/1000], Loss: 76.0557\n",
      "Epoch [300/1000], Loss: 70.2759\n",
      "Epoch [310/1000], Loss: 64.9673\n",
      "Epoch [320/1000], Loss: 60.0985\n",
      "Epoch [330/1000], Loss: 55.6400\n",
      "Epoch [340/1000], Loss: 51.5634\n",
      "Epoch [350/1000], Loss: 47.8416\n",
      "Epoch [360/1000], Loss: 44.4493\n",
      "Epoch [370/1000], Loss: 41.3622\n",
      "Epoch [380/1000], Loss: 38.5573\n",
      "Epoch [390/1000], Loss: 36.0131\n",
      "Epoch [400/1000], Loss: 33.7092\n",
      "Epoch [410/1000], Loss: 31.6264\n",
      "Epoch [420/1000], Loss: 29.7466\n",
      "Epoch [430/1000], Loss: 28.0530\n",
      "Epoch [440/1000], Loss: 26.5297\n",
      "Epoch [450/1000], Loss: 25.1621\n",
      "Epoch [460/1000], Loss: 23.9362\n",
      "Epoch [470/1000], Loss: 22.8394\n",
      "Epoch [480/1000], Loss: 21.8598\n",
      "Epoch [490/1000], Loss: 20.9863\n",
      "Epoch [500/1000], Loss: 20.2090\n",
      "Epoch [510/1000], Loss: 19.5184\n",
      "Epoch [520/1000], Loss: 18.9059\n",
      "Epoch [530/1000], Loss: 18.3637\n",
      "Epoch [540/1000], Loss: 17.8845\n",
      "Epoch [550/1000], Loss: 17.4619\n",
      "Epoch [560/1000], Loss: 17.0897\n",
      "Epoch [570/1000], Loss: 16.7625\n",
      "Epoch [580/1000], Loss: 16.4754\n",
      "Epoch [590/1000], Loss: 16.2239\n",
      "Epoch [600/1000], Loss: 16.0039\n",
      "Epoch [610/1000], Loss: 15.8119\n",
      "Epoch [620/1000], Loss: 15.6446\n",
      "Epoch [630/1000], Loss: 15.4990\n",
      "Epoch [640/1000], Loss: 15.3726\n",
      "Epoch [650/1000], Loss: 15.2629\n",
      "Epoch [660/1000], Loss: 15.1679\n",
      "Epoch [670/1000], Loss: 15.0858\n",
      "Epoch [680/1000], Loss: 15.0148\n",
      "Epoch [690/1000], Loss: 14.9537\n",
      "Epoch [700/1000], Loss: 14.9010\n",
      "Epoch [710/1000], Loss: 14.8557\n",
      "Epoch [720/1000], Loss: 14.8168\n",
      "Epoch [730/1000], Loss: 14.7833\n",
      "Epoch [740/1000], Loss: 14.7546\n",
      "Epoch [750/1000], Loss: 14.7300\n",
      "Epoch [760/1000], Loss: 14.7089\n",
      "Epoch [770/1000], Loss: 14.6908\n",
      "Epoch [780/1000], Loss: 14.6753\n",
      "Epoch [790/1000], Loss: 14.6620\n",
      "Epoch [800/1000], Loss: 14.6505\n",
      "Epoch [810/1000], Loss: 14.6406\n",
      "Epoch [820/1000], Loss: 14.6321\n",
      "Epoch [830/1000], Loss: 14.6248\n",
      "Epoch [840/1000], Loss: 14.6184\n",
      "Epoch [850/1000], Loss: 14.6128\n",
      "Epoch [860/1000], Loss: 14.6079\n",
      "Epoch [870/1000], Loss: 14.6037\n",
      "Epoch [880/1000], Loss: 14.5999\n",
      "Epoch [890/1000], Loss: 14.5965\n",
      "Epoch [900/1000], Loss: 14.5934\n",
      "Epoch [910/1000], Loss: 14.5907\n",
      "Epoch [920/1000], Loss: 14.5882\n",
      "Epoch [930/1000], Loss: 14.5859\n",
      "Epoch [940/1000], Loss: 14.5838\n",
      "Epoch [950/1000], Loss: 14.5818\n",
      "Epoch [960/1000], Loss: 14.5800\n",
      "Epoch [970/1000], Loss: 14.5782\n",
      "Epoch [980/1000], Loss: 14.5766\n",
      "Epoch [990/1000], Loss: 14.5750\n",
      "Epoch [1000/1000], Loss: 14.5735\n",
      "Predicted days_remaining for parent_id 376: [26.75143814086914, 27.861820220947266, 27.863197326660156, 27.863391876220703, 27.863723754882812, 27.863821029663086, 27.863155364990234, 27.862701416015625]\n",
      "Training for parent_id 377...\n",
      "Epoch [10/1000], Loss: 94.3565\n",
      "Epoch [20/1000], Loss: 72.1513\n",
      "Epoch [30/1000], Loss: 52.3324\n",
      "Epoch [40/1000], Loss: 39.8325\n",
      "Epoch [50/1000], Loss: 32.1277\n",
      "Epoch [60/1000], Loss: 26.9162\n",
      "Epoch [70/1000], Loss: 23.2303\n",
      "Epoch [80/1000], Loss: 20.6285\n",
      "Epoch [90/1000], Loss: 18.8236\n",
      "Epoch [100/1000], Loss: 17.5971\n",
      "Epoch [110/1000], Loss: 16.7804\n",
      "Epoch [120/1000], Loss: 16.2449\n",
      "Epoch [130/1000], Loss: 15.8961\n",
      "Epoch [140/1000], Loss: 15.6669\n",
      "Epoch [150/1000], Loss: 15.5120\n",
      "Epoch [160/1000], Loss: 15.4022\n",
      "Epoch [170/1000], Loss: 15.3194\n",
      "Epoch [180/1000], Loss: 15.2529\n",
      "Epoch [190/1000], Loss: 15.1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/1000], Loss: 15.1480\n",
      "Epoch [210/1000], Loss: 15.1044\n",
      "Epoch [220/1000], Loss: 15.0649\n",
      "Epoch [230/1000], Loss: 15.0291\n",
      "Epoch [240/1000], Loss: 14.9963\n",
      "Epoch [250/1000], Loss: 14.9662\n",
      "Epoch [260/1000], Loss: 14.9385\n",
      "Epoch [270/1000], Loss: 14.9131\n",
      "Epoch [280/1000], Loss: 14.8895\n",
      "Epoch [290/1000], Loss: 14.8678\n",
      "Epoch [300/1000], Loss: 14.8476\n",
      "Epoch [310/1000], Loss: 14.8289\n",
      "Epoch [320/1000], Loss: 14.8114\n",
      "Epoch [330/1000], Loss: 14.7952\n",
      "Epoch [340/1000], Loss: 14.7800\n",
      "Epoch [350/1000], Loss: 14.7658\n",
      "Epoch [360/1000], Loss: 14.7524\n",
      "Epoch [370/1000], Loss: 14.7400\n",
      "Epoch [380/1000], Loss: 14.7282\n",
      "Epoch [390/1000], Loss: 14.7172\n",
      "Epoch [400/1000], Loss: 14.7068\n",
      "Epoch [410/1000], Loss: 14.6970\n",
      "Epoch [420/1000], Loss: 14.6877\n",
      "Epoch [430/1000], Loss: 14.6789\n",
      "Epoch [440/1000], Loss: 14.6706\n",
      "Epoch [450/1000], Loss: 14.6628\n",
      "Epoch [460/1000], Loss: 14.6553\n",
      "Epoch [470/1000], Loss: 14.6483\n",
      "Epoch [480/1000], Loss: 14.6415\n",
      "Epoch [490/1000], Loss: 14.6351\n",
      "Epoch [500/1000], Loss: 14.6290\n",
      "Epoch [510/1000], Loss: 14.6232\n",
      "Epoch [520/1000], Loss: 14.6177\n",
      "Epoch [530/1000], Loss: 14.6124\n",
      "Epoch [540/1000], Loss: 14.6074\n",
      "Epoch [550/1000], Loss: 14.6026\n",
      "Epoch [560/1000], Loss: 14.5979\n",
      "Epoch [570/1000], Loss: 14.5935\n",
      "Epoch [580/1000], Loss: 14.5893\n",
      "Epoch [590/1000], Loss: 14.5852\n",
      "Epoch [600/1000], Loss: 14.5813\n",
      "Epoch [610/1000], Loss: 14.5776\n",
      "Epoch [620/1000], Loss: 14.5740\n",
      "Epoch [630/1000], Loss: 14.5705\n",
      "Epoch [640/1000], Loss: 14.5672\n",
      "Epoch [650/1000], Loss: 14.5640\n",
      "Epoch [660/1000], Loss: 14.5610\n",
      "Epoch [670/1000], Loss: 14.5580\n",
      "Epoch [680/1000], Loss: 14.5551\n",
      "Epoch [690/1000], Loss: 14.5524\n",
      "Epoch [700/1000], Loss: 14.5497\n",
      "Epoch [710/1000], Loss: 14.5472\n",
      "Epoch [720/1000], Loss: 14.5447\n",
      "Epoch [730/1000], Loss: 14.5423\n",
      "Epoch [740/1000], Loss: 14.5400\n",
      "Epoch [750/1000], Loss: 14.5378\n",
      "Epoch [760/1000], Loss: 14.5356\n",
      "Epoch [770/1000], Loss: 14.5335\n",
      "Epoch [780/1000], Loss: 14.5315\n",
      "Epoch [790/1000], Loss: 14.5296\n",
      "Epoch [800/1000], Loss: 14.5277\n",
      "Epoch [810/1000], Loss: 14.5258\n",
      "Epoch [820/1000], Loss: 14.5240\n",
      "Epoch [830/1000], Loss: 14.5223\n",
      "Epoch [840/1000], Loss: 14.5206\n",
      "Epoch [850/1000], Loss: 14.5190\n",
      "Epoch [860/1000], Loss: 14.5174\n",
      "Epoch [870/1000], Loss: 14.5159\n",
      "Epoch [880/1000], Loss: 14.5144\n",
      "Epoch [890/1000], Loss: 14.5129\n",
      "Epoch [900/1000], Loss: 14.5115\n",
      "Epoch [910/1000], Loss: 14.5102\n",
      "Epoch [920/1000], Loss: 14.5088\n",
      "Epoch [930/1000], Loss: 14.5075\n",
      "Epoch [940/1000], Loss: 14.5063\n",
      "Epoch [950/1000], Loss: 14.5050\n",
      "Epoch [960/1000], Loss: 14.5038\n",
      "Epoch [970/1000], Loss: 14.5027\n",
      "Epoch [980/1000], Loss: 14.5015\n",
      "Epoch [990/1000], Loss: 14.5004\n",
      "Epoch [1000/1000], Loss: 14.4994\n",
      "Predicted days_remaining for parent_id 377: [9.086801528930664, 9.837177276611328, 9.838702201843262, 9.836589813232422, 9.836687088012695, 9.838964462280273, 9.839622497558594, 9.83774185180664]\n",
      "Training for parent_id 387...\n",
      "Epoch [10/1000], Loss: 352.4152\n",
      "Epoch [20/1000], Loss: 296.7462\n",
      "Epoch [30/1000], Loss: 255.6691\n",
      "Epoch [40/1000], Loss: 226.5096\n",
      "Epoch [50/1000], Loss: 203.2866\n",
      "Epoch [60/1000], Loss: 183.1473\n",
      "Epoch [70/1000], Loss: 165.1328\n",
      "Epoch [80/1000], Loss: 148.8836\n",
      "Epoch [90/1000], Loss: 134.1928\n",
      "Epoch [100/1000], Loss: 120.9080\n",
      "Epoch [110/1000], Loss: 108.9040\n",
      "Epoch [120/1000], Loss: 98.0712\n",
      "Epoch [130/1000], Loss: 88.3109\n",
      "Epoch [140/1000], Loss: 79.5330\n",
      "Epoch [150/1000], Loss: 71.6548\n",
      "Epoch [160/1000], Loss: 64.5996\n",
      "Epoch [170/1000], Loss: 58.2966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000], Loss: 52.6799\n",
      "Epoch [190/1000], Loss: 47.6883\n",
      "Epoch [200/1000], Loss: 43.2647\n",
      "Epoch [210/1000], Loss: 39.3560\n",
      "Epoch [220/1000], Loss: 35.9129\n",
      "Epoch [230/1000], Loss: 32.8895\n",
      "Epoch [240/1000], Loss: 30.2431\n",
      "Epoch [250/1000], Loss: 27.9345\n",
      "Epoch [260/1000], Loss: 25.9274\n",
      "Epoch [270/1000], Loss: 24.1884\n",
      "Epoch [280/1000], Loss: 22.6870\n",
      "Epoch [290/1000], Loss: 21.3953\n",
      "Epoch [300/1000], Loss: 20.2880\n",
      "Epoch [310/1000], Loss: 19.3423\n",
      "Epoch [320/1000], Loss: 18.5375\n",
      "Epoch [330/1000], Loss: 17.8551\n",
      "Epoch [340/1000], Loss: 17.2787\n",
      "Epoch [350/1000], Loss: 16.7935\n",
      "Epoch [360/1000], Loss: 16.3867\n",
      "Epoch [370/1000], Loss: 16.0469\n",
      "Epoch [380/1000], Loss: 15.7641\n",
      "Epoch [390/1000], Loss: 15.5295\n",
      "Epoch [400/1000], Loss: 15.3357\n",
      "Epoch [410/1000], Loss: 15.1762\n",
      "Epoch [420/1000], Loss: 15.0453\n",
      "Epoch [430/1000], Loss: 14.9383\n",
      "Epoch [440/1000], Loss: 14.8512\n",
      "Epoch [450/1000], Loss: 14.7805\n",
      "Epoch [460/1000], Loss: 14.7233\n",
      "Epoch [470/1000], Loss: 14.6771\n",
      "Epoch [480/1000], Loss: 14.6400\n",
      "Epoch [490/1000], Loss: 14.6102\n",
      "Epoch [500/1000], Loss: 14.5864\n",
      "Epoch [510/1000], Loss: 14.5674\n",
      "Epoch [520/1000], Loss: 14.5523\n",
      "Epoch [530/1000], Loss: 14.5402\n",
      "Epoch [540/1000], Loss: 14.5306\n",
      "Epoch [550/1000], Loss: 14.5230\n",
      "Epoch [560/1000], Loss: 14.5169\n",
      "Epoch [570/1000], Loss: 14.5120\n",
      "Epoch [580/1000], Loss: 14.5081\n",
      "Epoch [590/1000], Loss: 14.5050\n",
      "Epoch [600/1000], Loss: 14.5024\n",
      "Epoch [610/1000], Loss: 14.5003\n",
      "Epoch [620/1000], Loss: 14.4985\n",
      "Epoch [630/1000], Loss: 14.4970\n",
      "Epoch [640/1000], Loss: 14.4958\n",
      "Epoch [650/1000], Loss: 14.4946\n",
      "Epoch [660/1000], Loss: 14.4936\n",
      "Epoch [670/1000], Loss: 14.4927\n",
      "Epoch [680/1000], Loss: 14.4919\n",
      "Epoch [690/1000], Loss: 14.4912\n",
      "Epoch [700/1000], Loss: 14.4904\n",
      "Epoch [710/1000], Loss: 14.4898\n",
      "Epoch [720/1000], Loss: 14.4891\n",
      "Epoch [730/1000], Loss: 14.4885\n",
      "Epoch [740/1000], Loss: 14.4879\n",
      "Epoch [750/1000], Loss: 14.4873\n",
      "Epoch [760/1000], Loss: 14.4867\n",
      "Epoch [770/1000], Loss: 14.4861\n",
      "Epoch [780/1000], Loss: 14.4855\n",
      "Epoch [790/1000], Loss: 14.4850\n",
      "Epoch [800/1000], Loss: 14.4845\n",
      "Epoch [810/1000], Loss: 14.4839\n",
      "Epoch [820/1000], Loss: 14.4834\n",
      "Epoch [830/1000], Loss: 14.4829\n",
      "Epoch [840/1000], Loss: 14.4824\n",
      "Epoch [850/1000], Loss: 14.4819\n",
      "Epoch [860/1000], Loss: 14.4814\n",
      "Epoch [870/1000], Loss: 14.4809\n",
      "Epoch [880/1000], Loss: 14.4804\n",
      "Epoch [890/1000], Loss: 14.4799\n",
      "Epoch [900/1000], Loss: 14.4795\n",
      "Epoch [910/1000], Loss: 14.4790\n",
      "Epoch [920/1000], Loss: 14.4785\n",
      "Epoch [930/1000], Loss: 14.4781\n",
      "Epoch [940/1000], Loss: 14.4776\n",
      "Epoch [950/1000], Loss: 14.4772\n",
      "Epoch [960/1000], Loss: 14.4768\n",
      "Epoch [970/1000], Loss: 14.4763\n",
      "Epoch [980/1000], Loss: 14.4759\n",
      "Epoch [990/1000], Loss: 14.4755\n",
      "Epoch [1000/1000], Loss: 14.4751\n",
      "Predicted days_remaining for parent_id 387: [19.237770080566406, 19.824811935424805, 19.824020385742188, 19.82424545288086, 19.82370376586914, 19.824583053588867, 19.82120132446289, 19.82383155822754]\n",
      "Training for parent_id 392...\n",
      "Epoch [10/1000], Loss: 110.6758\n",
      "Epoch [20/1000], Loss: 84.0984\n",
      "Epoch [30/1000], Loss: 62.5981\n",
      "Epoch [40/1000], Loss: 49.4454\n",
      "Epoch [50/1000], Loss: 40.5362\n",
      "Epoch [60/1000], Loss: 33.8994\n",
      "Epoch [70/1000], Loss: 28.8026\n",
      "Epoch [80/1000], Loss: 24.9127\n",
      "Epoch [90/1000], Loss: 21.9878\n",
      "Epoch [100/1000], Loss: 19.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/1000], Loss: 18.2575\n",
      "Epoch [120/1000], Loss: 17.1418\n",
      "Epoch [130/1000], Loss: 16.3639\n",
      "Epoch [140/1000], Loss: 15.8321\n",
      "Epoch [150/1000], Loss: 15.4747\n",
      "Epoch [160/1000], Loss: 15.2379\n",
      "Epoch [170/1000], Loss: 15.0819\n",
      "Epoch [180/1000], Loss: 14.9789\n",
      "Epoch [190/1000], Loss: 14.9096\n",
      "Epoch [200/1000], Loss: 14.8615\n",
      "Epoch [210/1000], Loss: 14.8265\n",
      "Epoch [220/1000], Loss: 14.7995\n",
      "Epoch [230/1000], Loss: 14.7775\n",
      "Epoch [240/1000], Loss: 14.7588\n",
      "Epoch [250/1000], Loss: 14.7423\n",
      "Epoch [260/1000], Loss: 14.7275\n",
      "Epoch [270/1000], Loss: 14.7139\n",
      "Epoch [280/1000], Loss: 14.7014\n",
      "Epoch [290/1000], Loss: 14.6898\n",
      "Epoch [300/1000], Loss: 14.6791\n",
      "Epoch [310/1000], Loss: 14.6690\n",
      "Epoch [320/1000], Loss: 14.6596\n",
      "Epoch [330/1000], Loss: 14.6508\n",
      "Epoch [340/1000], Loss: 14.6425\n",
      "Epoch [350/1000], Loss: 14.6347\n",
      "Epoch [360/1000], Loss: 14.6274\n",
      "Epoch [370/1000], Loss: 14.6205\n",
      "Epoch [380/1000], Loss: 14.6140\n",
      "Epoch [390/1000], Loss: 14.6079\n",
      "Epoch [400/1000], Loss: 14.6020\n",
      "Epoch [410/1000], Loss: 14.5965\n",
      "Epoch [420/1000], Loss: 14.5913\n",
      "Epoch [430/1000], Loss: 14.5863\n",
      "Epoch [440/1000], Loss: 14.5816\n",
      "Epoch [450/1000], Loss: 14.5772\n",
      "Epoch [460/1000], Loss: 14.5729\n",
      "Epoch [470/1000], Loss: 14.5688\n",
      "Epoch [480/1000], Loss: 14.5649\n",
      "Epoch [490/1000], Loss: 14.5612\n",
      "Epoch [500/1000], Loss: 14.5577\n",
      "Epoch [510/1000], Loss: 14.5543\n",
      "Epoch [520/1000], Loss: 14.5511\n",
      "Epoch [530/1000], Loss: 14.5480\n",
      "Epoch [540/1000], Loss: 14.5450\n",
      "Epoch [550/1000], Loss: 14.5422\n",
      "Epoch [560/1000], Loss: 14.5394\n",
      "Epoch [570/1000], Loss: 14.5368\n",
      "Epoch [580/1000], Loss: 14.5343\n",
      "Epoch [590/1000], Loss: 14.5319\n",
      "Epoch [600/1000], Loss: 14.5296\n",
      "Epoch [610/1000], Loss: 14.5273\n",
      "Epoch [620/1000], Loss: 14.5252\n",
      "Epoch [630/1000], Loss: 14.5231\n",
      "Epoch [640/1000], Loss: 14.5211\n",
      "Epoch [650/1000], Loss: 14.5192\n",
      "Epoch [660/1000], Loss: 14.5173\n",
      "Epoch [670/1000], Loss: 14.5155\n",
      "Epoch [680/1000], Loss: 14.5138\n",
      "Epoch [690/1000], Loss: 14.5121\n",
      "Epoch [700/1000], Loss: 14.5105\n",
      "Epoch [710/1000], Loss: 14.5089\n",
      "Epoch [720/1000], Loss: 14.5074\n",
      "Epoch [730/1000], Loss: 14.5059\n",
      "Epoch [740/1000], Loss: 14.5045\n",
      "Epoch [750/1000], Loss: 14.5031\n",
      "Epoch [760/1000], Loss: 14.5018\n",
      "Epoch [770/1000], Loss: 14.5005\n",
      "Epoch [780/1000], Loss: 14.4992\n",
      "Epoch [790/1000], Loss: 14.4980\n",
      "Epoch [800/1000], Loss: 14.4968\n",
      "Epoch [810/1000], Loss: 14.4957\n",
      "Epoch [820/1000], Loss: 14.4946\n",
      "Epoch [830/1000], Loss: 14.4935\n",
      "Epoch [840/1000], Loss: 14.4925\n",
      "Epoch [850/1000], Loss: 14.4914\n",
      "Epoch [860/1000], Loss: 14.4904\n",
      "Epoch [870/1000], Loss: 14.4895\n",
      "Epoch [880/1000], Loss: 14.4885\n",
      "Epoch [890/1000], Loss: 14.4876\n",
      "Epoch [900/1000], Loss: 14.4867\n",
      "Epoch [910/1000], Loss: 14.4859\n",
      "Epoch [920/1000], Loss: 14.4850\n",
      "Epoch [930/1000], Loss: 14.4842\n",
      "Epoch [940/1000], Loss: 14.4834\n",
      "Epoch [950/1000], Loss: 14.4826\n",
      "Epoch [960/1000], Loss: 14.4819\n",
      "Epoch [970/1000], Loss: 14.4811\n",
      "Epoch [980/1000], Loss: 14.4804\n",
      "Epoch [990/1000], Loss: 14.4797\n",
      "Epoch [1000/1000], Loss: 14.4790\n",
      "Predicted days_remaining for parent_id 392: [10.20849323272705, 10.825836181640625, 10.824234008789062, 10.824457168579102, 10.824918746948242, 10.824531555175781, 10.824691772460938, 10.820647239685059]\n",
      "Training for parent_id 393...\n",
      "Epoch [10/1000], Loss: 374.9626\n",
      "Epoch [20/1000], Loss: 322.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 267.9902\n",
      "Epoch [40/1000], Loss: 230.9675\n",
      "Epoch [50/1000], Loss: 205.0170\n",
      "Epoch [60/1000], Loss: 183.6045\n",
      "Epoch [70/1000], Loss: 164.8282\n",
      "Epoch [80/1000], Loss: 148.0551\n",
      "Epoch [90/1000], Loss: 132.9709\n",
      "Epoch [100/1000], Loss: 119.3762\n",
      "Epoch [110/1000], Loss: 107.1253\n",
      "Epoch [120/1000], Loss: 96.1000\n",
      "Epoch [130/1000], Loss: 86.1974\n",
      "Epoch [140/1000], Loss: 77.3247\n",
      "Epoch [150/1000], Loss: 69.3955\n",
      "Epoch [160/1000], Loss: 62.3295\n",
      "Epoch [170/1000], Loss: 56.0510\n",
      "Epoch [180/1000], Loss: 50.4894\n",
      "Epoch [190/1000], Loss: 45.5782\n",
      "Epoch [200/1000], Loss: 41.2555\n",
      "Epoch [210/1000], Loss: 37.4634\n",
      "Epoch [220/1000], Loss: 34.1481\n",
      "Epoch [230/1000], Loss: 31.2599\n",
      "Epoch [240/1000], Loss: 28.7526\n",
      "Epoch [250/1000], Loss: 26.5839\n",
      "Epoch [260/1000], Loss: 24.7150\n",
      "Epoch [270/1000], Loss: 23.1105\n",
      "Epoch [280/1000], Loss: 21.7381\n",
      "Epoch [290/1000], Loss: 20.5687\n",
      "Epoch [300/1000], Loss: 19.5760\n",
      "Epoch [310/1000], Loss: 18.7366\n",
      "Epoch [320/1000], Loss: 18.0295\n",
      "Epoch [330/1000], Loss: 17.4361\n",
      "Epoch [340/1000], Loss: 16.9400\n",
      "Epoch [350/1000], Loss: 16.5267\n",
      "Epoch [360/1000], Loss: 16.1837\n",
      "Epoch [370/1000], Loss: 15.9001\n",
      "Epoch [380/1000], Loss: 15.6663\n",
      "Epoch [390/1000], Loss: 15.4742\n",
      "Epoch [400/1000], Loss: 15.3169\n",
      "Epoch [410/1000], Loss: 15.1885\n",
      "Epoch [420/1000], Loss: 15.0838\n",
      "Epoch [430/1000], Loss: 14.9988\n",
      "Epoch [440/1000], Loss: 14.9298\n",
      "Epoch [450/1000], Loss: 14.8738\n",
      "Epoch [460/1000], Loss: 14.8285\n",
      "Epoch [470/1000], Loss: 14.7917\n",
      "Epoch [480/1000], Loss: 14.7619\n",
      "Epoch [490/1000], Loss: 14.7376\n",
      "Epoch [500/1000], Loss: 14.7178\n",
      "Epoch [510/1000], Loss: 14.7015\n",
      "Epoch [520/1000], Loss: 14.6880\n",
      "Epoch [530/1000], Loss: 14.6768\n",
      "Epoch [540/1000], Loss: 14.6673\n",
      "Epoch [550/1000], Loss: 14.6592\n",
      "Epoch [560/1000], Loss: 14.6522\n",
      "Epoch [570/1000], Loss: 14.6461\n",
      "Epoch [580/1000], Loss: 14.6406\n",
      "Epoch [590/1000], Loss: 14.6357\n",
      "Epoch [600/1000], Loss: 14.6312\n",
      "Epoch [610/1000], Loss: 14.6271\n",
      "Epoch [620/1000], Loss: 14.6233\n",
      "Epoch [630/1000], Loss: 14.6197\n",
      "Epoch [640/1000], Loss: 14.6162\n",
      "Epoch [650/1000], Loss: 14.6129\n",
      "Epoch [660/1000], Loss: 14.6098\n",
      "Epoch [670/1000], Loss: 14.6068\n",
      "Epoch [680/1000], Loss: 14.6038\n",
      "Epoch [690/1000], Loss: 14.6010\n",
      "Epoch [700/1000], Loss: 14.5982\n",
      "Epoch [710/1000], Loss: 14.5956\n",
      "Epoch [720/1000], Loss: 14.5930\n",
      "Epoch [730/1000], Loss: 14.5904\n",
      "Epoch [740/1000], Loss: 14.5879\n",
      "Epoch [750/1000], Loss: 14.5855\n",
      "Epoch [760/1000], Loss: 14.5831\n",
      "Epoch [770/1000], Loss: 14.5808\n",
      "Epoch [780/1000], Loss: 14.5785\n",
      "Epoch [790/1000], Loss: 14.5763\n",
      "Epoch [800/1000], Loss: 14.5742\n",
      "Epoch [810/1000], Loss: 14.5720\n",
      "Epoch [820/1000], Loss: 14.5700\n",
      "Epoch [830/1000], Loss: 14.5679\n",
      "Epoch [840/1000], Loss: 14.5660\n",
      "Epoch [850/1000], Loss: 14.5640\n",
      "Epoch [860/1000], Loss: 14.5621\n",
      "Epoch [870/1000], Loss: 14.5603\n",
      "Epoch [880/1000], Loss: 14.5584\n",
      "Epoch [890/1000], Loss: 14.5567\n",
      "Epoch [900/1000], Loss: 14.5549\n",
      "Epoch [910/1000], Loss: 14.5532\n",
      "Epoch [920/1000], Loss: 14.5515\n",
      "Epoch [930/1000], Loss: 14.5499\n",
      "Epoch [940/1000], Loss: 14.5483\n",
      "Epoch [950/1000], Loss: 14.5467\n",
      "Epoch [960/1000], Loss: 14.5452\n",
      "Epoch [970/1000], Loss: 14.5437\n",
      "Epoch [980/1000], Loss: 14.5422\n",
      "Epoch [990/1000], Loss: 14.5407\n",
      "Epoch [1000/1000], Loss: 14.5393\n",
      "Predicted days_remaining for parent_id 393: [18.906021118164062, 19.869789123535156, 19.86990737915039, 19.869686126708984, 19.870094299316406, 19.869735717773438, 19.87047004699707, 19.870746612548828]\n",
      "Training for parent_id 395...\n",
      "Epoch [10/1000], Loss: 81.3908\n",
      "Epoch [20/1000], Loss: 58.3307\n",
      "Epoch [30/1000], Loss: 43.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/1000], Loss: 34.2293\n",
      "Epoch [50/1000], Loss: 28.2497\n",
      "Epoch [60/1000], Loss: 24.0115\n",
      "Epoch [70/1000], Loss: 20.9855\n",
      "Epoch [80/1000], Loss: 18.8666\n",
      "Epoch [90/1000], Loss: 17.4269\n",
      "Epoch [100/1000], Loss: 16.4786\n",
      "Epoch [110/1000], Loss: 15.8721\n",
      "Epoch [120/1000], Loss: 15.4934\n",
      "Epoch [130/1000], Loss: 15.2604\n",
      "Epoch [140/1000], Loss: 15.1164\n",
      "Epoch [150/1000], Loss: 15.0248\n",
      "Epoch [160/1000], Loss: 14.9630\n",
      "Epoch [170/1000], Loss: 14.9179\n",
      "Epoch [180/1000], Loss: 14.8823\n",
      "Epoch [190/1000], Loss: 14.8524\n",
      "Epoch [200/1000], Loss: 14.8262\n",
      "Epoch [210/1000], Loss: 14.8029\n",
      "Epoch [220/1000], Loss: 14.7818\n",
      "Epoch [230/1000], Loss: 14.7625\n",
      "Epoch [240/1000], Loss: 14.7449\n",
      "Epoch [250/1000], Loss: 14.7288\n",
      "Epoch [260/1000], Loss: 14.7140\n",
      "Epoch [270/1000], Loss: 14.7003\n",
      "Epoch [280/1000], Loss: 14.6877\n",
      "Epoch [290/1000], Loss: 14.6760\n",
      "Epoch [300/1000], Loss: 14.6652\n",
      "Epoch [310/1000], Loss: 14.6551\n",
      "Epoch [320/1000], Loss: 14.6457\n",
      "Epoch [330/1000], Loss: 14.6369\n",
      "Epoch [340/1000], Loss: 14.6287\n",
      "Epoch [350/1000], Loss: 14.6210\n",
      "Epoch [360/1000], Loss: 14.6138\n",
      "Epoch [370/1000], Loss: 14.6070\n",
      "Epoch [380/1000], Loss: 14.6007\n",
      "Epoch [390/1000], Loss: 14.5947\n",
      "Epoch [400/1000], Loss: 14.5890\n",
      "Epoch [410/1000], Loss: 14.5837\n",
      "Epoch [420/1000], Loss: 14.5786\n",
      "Epoch [430/1000], Loss: 14.5738\n",
      "Epoch [440/1000], Loss: 14.5693\n",
      "Epoch [450/1000], Loss: 14.5650\n",
      "Epoch [460/1000], Loss: 14.5609\n",
      "Epoch [470/1000], Loss: 14.5570\n",
      "Epoch [480/1000], Loss: 14.5533\n",
      "Epoch [490/1000], Loss: 14.5498\n",
      "Epoch [500/1000], Loss: 14.5465\n",
      "Epoch [510/1000], Loss: 14.5433\n",
      "Epoch [520/1000], Loss: 14.5402\n",
      "Epoch [530/1000], Loss: 14.5373\n",
      "Epoch [540/1000], Loss: 14.5345\n",
      "Epoch [550/1000], Loss: 14.5318\n",
      "Epoch [560/1000], Loss: 14.5293\n",
      "Epoch [570/1000], Loss: 14.5268\n",
      "Epoch [580/1000], Loss: 14.5245\n",
      "Epoch [590/1000], Loss: 14.5222\n",
      "Epoch [600/1000], Loss: 14.5200\n",
      "Epoch [610/1000], Loss: 14.5180\n",
      "Epoch [620/1000], Loss: 14.5160\n",
      "Epoch [630/1000], Loss: 14.5140\n",
      "Epoch [640/1000], Loss: 14.5122\n",
      "Epoch [650/1000], Loss: 14.5104\n",
      "Epoch [660/1000], Loss: 14.5087\n",
      "Epoch [670/1000], Loss: 14.5070\n",
      "Epoch [680/1000], Loss: 14.5054\n",
      "Epoch [690/1000], Loss: 14.5039\n",
      "Epoch [700/1000], Loss: 14.5024\n",
      "Epoch [710/1000], Loss: 14.5010\n",
      "Epoch [720/1000], Loss: 14.4996\n",
      "Epoch [730/1000], Loss: 14.4982\n",
      "Epoch [740/1000], Loss: 14.4969\n",
      "Epoch [750/1000], Loss: 14.4957\n",
      "Epoch [760/1000], Loss: 14.4944\n",
      "Epoch [770/1000], Loss: 14.4933\n",
      "Epoch [780/1000], Loss: 14.4921\n",
      "Epoch [790/1000], Loss: 14.4910\n",
      "Epoch [800/1000], Loss: 14.4899\n",
      "Epoch [810/1000], Loss: 14.4889\n",
      "Epoch [820/1000], Loss: 14.4879\n",
      "Epoch [830/1000], Loss: 14.4869\n",
      "Epoch [840/1000], Loss: 14.4860\n",
      "Epoch [850/1000], Loss: 14.4850\n",
      "Epoch [860/1000], Loss: 14.4841\n",
      "Epoch [870/1000], Loss: 14.4833\n",
      "Epoch [880/1000], Loss: 14.4824\n",
      "Epoch [890/1000], Loss: 14.4816\n",
      "Epoch [900/1000], Loss: 14.4808\n",
      "Epoch [910/1000], Loss: 14.4800\n",
      "Epoch [920/1000], Loss: 14.4792\n",
      "Epoch [930/1000], Loss: 14.4785\n",
      "Epoch [940/1000], Loss: 14.4778\n",
      "Epoch [950/1000], Loss: 14.4771\n",
      "Epoch [960/1000], Loss: 14.4764\n",
      "Epoch [970/1000], Loss: 14.4757\n",
      "Epoch [980/1000], Loss: 14.4751\n",
      "Epoch [990/1000], Loss: 14.4744\n",
      "Epoch [1000/1000], Loss: 14.4738\n",
      "Predicted days_remaining for parent_id 395: [9.243117332458496, 9.822112083435059, 9.823805809020996, 9.820086479187012, 9.816475868225098, 9.817122459411621, 9.817249298095703, 9.8148775100708]\n",
      "Training for parent_id 404...\n",
      "Epoch [10/1000], Loss: 125.4952\n",
      "Epoch [20/1000], Loss: 97.2524\n",
      "Epoch [30/1000], Loss: 76.7114\n",
      "Epoch [40/1000], Loss: 63.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 53.0930\n",
      "Epoch [60/1000], Loss: 44.9879\n",
      "Epoch [70/1000], Loss: 38.3996\n",
      "Epoch [80/1000], Loss: 33.0600\n",
      "Epoch [90/1000], Loss: 28.7667\n",
      "Epoch [100/1000], Loss: 25.3488\n",
      "Epoch [110/1000], Loss: 22.6582\n",
      "Epoch [120/1000], Loss: 20.5657\n",
      "Epoch [130/1000], Loss: 18.9594\n",
      "Epoch [140/1000], Loss: 17.7429\n",
      "Epoch [150/1000], Loss: 16.8344\n",
      "Epoch [160/1000], Loss: 16.1655\n",
      "Epoch [170/1000], Loss: 15.6801\n",
      "Epoch [180/1000], Loss: 15.3327\n",
      "Epoch [190/1000], Loss: 15.0876\n",
      "Epoch [200/1000], Loss: 14.9168\n",
      "Epoch [210/1000], Loss: 14.7993\n",
      "Epoch [220/1000], Loss: 14.7192\n",
      "Epoch [230/1000], Loss: 14.6650\n",
      "Epoch [240/1000], Loss: 14.6284\n",
      "Epoch [250/1000], Loss: 14.6036\n",
      "Epoch [260/1000], Loss: 14.5865\n",
      "Epoch [270/1000], Loss: 14.5745\n",
      "Epoch [280/1000], Loss: 14.5657\n",
      "Epoch [290/1000], Loss: 14.5591\n",
      "Epoch [300/1000], Loss: 14.5537\n",
      "Epoch [310/1000], Loss: 14.5492\n",
      "Epoch [320/1000], Loss: 14.5453\n",
      "Epoch [330/1000], Loss: 14.5417\n",
      "Epoch [340/1000], Loss: 14.5385\n",
      "Epoch [350/1000], Loss: 14.5354\n",
      "Epoch [360/1000], Loss: 14.5325\n",
      "Epoch [370/1000], Loss: 14.5297\n",
      "Epoch [380/1000], Loss: 14.5270\n",
      "Epoch [390/1000], Loss: 14.5245\n",
      "Epoch [400/1000], Loss: 14.5221\n",
      "Epoch [410/1000], Loss: 14.5197\n",
      "Epoch [420/1000], Loss: 14.5175\n",
      "Epoch [430/1000], Loss: 14.5153\n",
      "Epoch [440/1000], Loss: 14.5133\n",
      "Epoch [450/1000], Loss: 14.5113\n",
      "Epoch [460/1000], Loss: 14.5094\n",
      "Epoch [470/1000], Loss: 14.5075\n",
      "Epoch [480/1000], Loss: 14.5058\n",
      "Epoch [490/1000], Loss: 14.5041\n",
      "Epoch [500/1000], Loss: 14.5024\n",
      "Epoch [510/1000], Loss: 14.5008\n",
      "Epoch [520/1000], Loss: 14.4993\n",
      "Epoch [530/1000], Loss: 14.4978\n",
      "Epoch [540/1000], Loss: 14.4964\n",
      "Epoch [550/1000], Loss: 14.4951\n",
      "Epoch [560/1000], Loss: 14.4937\n",
      "Epoch [570/1000], Loss: 14.4925\n",
      "Epoch [580/1000], Loss: 14.4912\n",
      "Epoch [590/1000], Loss: 14.4900\n",
      "Epoch [600/1000], Loss: 14.4889\n",
      "Epoch [610/1000], Loss: 14.4877\n",
      "Epoch [620/1000], Loss: 14.4867\n",
      "Epoch [630/1000], Loss: 14.4856\n",
      "Epoch [640/1000], Loss: 14.4846\n",
      "Epoch [650/1000], Loss: 14.4836\n",
      "Epoch [660/1000], Loss: 14.4827\n",
      "Epoch [670/1000], Loss: 14.4817\n",
      "Epoch [680/1000], Loss: 14.4809\n",
      "Epoch [690/1000], Loss: 14.4800\n",
      "Epoch [700/1000], Loss: 14.4791\n",
      "Epoch [710/1000], Loss: 14.4783\n",
      "Epoch [720/1000], Loss: 14.4775\n",
      "Epoch [730/1000], Loss: 14.4768\n",
      "Epoch [740/1000], Loss: 14.4760\n",
      "Epoch [750/1000], Loss: 14.4753\n",
      "Epoch [760/1000], Loss: 14.4746\n",
      "Epoch [770/1000], Loss: 14.4739\n",
      "Epoch [780/1000], Loss: 14.4732\n",
      "Epoch [790/1000], Loss: 14.4726\n",
      "Epoch [800/1000], Loss: 14.4719\n",
      "Epoch [810/1000], Loss: 14.4713\n",
      "Epoch [820/1000], Loss: 14.4707\n",
      "Epoch [830/1000], Loss: 14.4701\n",
      "Epoch [840/1000], Loss: 14.4696\n",
      "Epoch [850/1000], Loss: 14.4690\n",
      "Epoch [860/1000], Loss: 14.4685\n",
      "Epoch [870/1000], Loss: 14.4679\n",
      "Epoch [880/1000], Loss: 14.4674\n",
      "Epoch [890/1000], Loss: 14.4669\n",
      "Epoch [900/1000], Loss: 14.4664\n",
      "Epoch [910/1000], Loss: 14.4660\n",
      "Epoch [920/1000], Loss: 14.4655\n",
      "Epoch [930/1000], Loss: 14.4650\n",
      "Epoch [940/1000], Loss: 14.4646\n",
      "Epoch [950/1000], Loss: 14.4642\n",
      "Epoch [960/1000], Loss: 14.4638\n",
      "Epoch [970/1000], Loss: 14.4633\n",
      "Epoch [980/1000], Loss: 14.4629\n",
      "Epoch [990/1000], Loss: 14.4625\n",
      "Epoch [1000/1000], Loss: 14.4622\n",
      "Predicted days_remaining for parent_id 404: [11.333917617797852, 11.80681037902832, 11.812870025634766, 11.808847427368164, 11.811708450317383, 11.807092666625977, 11.806228637695312, 11.804558753967285]\n",
      "Training for parent_id 406...\n",
      "Epoch [10/1000], Loss: 478.6155\n",
      "Epoch [20/1000], Loss: 410.0944\n",
      "Epoch [30/1000], Loss: 362.3126\n",
      "Epoch [40/1000], Loss: 329.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/1000], Loss: 301.4456\n",
      "Epoch [60/1000], Loss: 276.3653\n",
      "Epoch [70/1000], Loss: 253.3862\n",
      "Epoch [80/1000], Loss: 232.2625\n",
      "Epoch [90/1000], Loss: 212.8222\n",
      "Epoch [100/1000], Loss: 194.9219\n",
      "Epoch [110/1000], Loss: 178.4405\n",
      "Epoch [120/1000], Loss: 163.2720\n",
      "Epoch [130/1000], Loss: 149.3207\n",
      "Epoch [140/1000], Loss: 136.4995\n",
      "Epoch [150/1000], Loss: 124.7281\n",
      "Epoch [160/1000], Loss: 113.9324\n",
      "Epoch [170/1000], Loss: 104.0440\n",
      "Epoch [180/1000], Loss: 94.9988\n",
      "Epoch [190/1000], Loss: 86.7374\n",
      "Epoch [200/1000], Loss: 79.2039\n",
      "Epoch [210/1000], Loss: 72.3458\n",
      "Epoch [220/1000], Loss: 66.1137\n",
      "Epoch [230/1000], Loss: 60.4612\n",
      "Epoch [240/1000], Loss: 55.3443\n",
      "Epoch [250/1000], Loss: 50.7218\n",
      "Epoch [260/1000], Loss: 46.5545\n",
      "Epoch [270/1000], Loss: 42.8058\n",
      "Epoch [280/1000], Loss: 39.4409\n",
      "Epoch [290/1000], Loss: 36.4275\n",
      "Epoch [300/1000], Loss: 33.7349\n",
      "Epoch [310/1000], Loss: 31.3347\n",
      "Epoch [320/1000], Loss: 29.2002\n",
      "Epoch [330/1000], Loss: 27.3066\n",
      "Epoch [340/1000], Loss: 25.6307\n",
      "Epoch [350/1000], Loss: 24.1513\n",
      "Epoch [360/1000], Loss: 22.8484\n",
      "Epoch [370/1000], Loss: 21.7040\n",
      "Epoch [380/1000], Loss: 20.7013\n",
      "Epoch [390/1000], Loss: 19.8250\n",
      "Epoch [400/1000], Loss: 19.0612\n",
      "Epoch [410/1000], Loss: 18.3970\n",
      "Epoch [420/1000], Loss: 17.8210\n",
      "Epoch [430/1000], Loss: 17.3228\n",
      "Epoch [440/1000], Loss: 16.8930\n",
      "Epoch [450/1000], Loss: 16.5232\n",
      "Epoch [460/1000], Loss: 16.2058\n",
      "Epoch [470/1000], Loss: 15.9341\n",
      "Epoch [480/1000], Loss: 15.7021\n",
      "Epoch [490/1000], Loss: 15.5046\n",
      "Epoch [500/1000], Loss: 15.3368\n",
      "Epoch [510/1000], Loss: 15.1947\n",
      "Epoch [520/1000], Loss: 15.0746\n",
      "Epoch [530/1000], Loss: 14.9734\n",
      "Epoch [540/1000], Loss: 14.8883\n",
      "Epoch [550/1000], Loss: 14.8170\n",
      "Epoch [560/1000], Loss: 14.7573\n",
      "Epoch [570/1000], Loss: 14.7075\n",
      "Epoch [580/1000], Loss: 14.6661\n",
      "Epoch [590/1000], Loss: 14.6317\n",
      "Epoch [600/1000], Loss: 14.6032\n",
      "Epoch [610/1000], Loss: 14.5796\n",
      "Epoch [620/1000], Loss: 14.5602\n",
      "Epoch [630/1000], Loss: 14.5442\n",
      "Epoch [640/1000], Loss: 14.5310\n",
      "Epoch [650/1000], Loss: 14.5203\n",
      "Epoch [660/1000], Loss: 14.5115\n",
      "Epoch [670/1000], Loss: 14.5042\n",
      "Epoch [680/1000], Loss: 14.4984\n",
      "Epoch [690/1000], Loss: 14.4935\n",
      "Epoch [700/1000], Loss: 14.4896\n",
      "Epoch [710/1000], Loss: 14.4864\n",
      "Epoch [720/1000], Loss: 14.4838\n",
      "Epoch [730/1000], Loss: 14.4816\n",
      "Epoch [740/1000], Loss: 14.4798\n",
      "Epoch [750/1000], Loss: 14.4784\n",
      "Epoch [760/1000], Loss: 14.4772\n",
      "Epoch [770/1000], Loss: 14.4761\n",
      "Epoch [780/1000], Loss: 14.4753\n",
      "Epoch [790/1000], Loss: 14.4745\n",
      "Epoch [800/1000], Loss: 14.4739\n",
      "Epoch [810/1000], Loss: 14.4733\n",
      "Epoch [820/1000], Loss: 14.4728\n",
      "Epoch [830/1000], Loss: 14.4724\n",
      "Epoch [840/1000], Loss: 14.4720\n",
      "Epoch [850/1000], Loss: 14.4716\n",
      "Epoch [860/1000], Loss: 14.4713\n",
      "Epoch [870/1000], Loss: 14.4710\n",
      "Epoch [880/1000], Loss: 14.4707\n",
      "Epoch [890/1000], Loss: 14.4704\n",
      "Epoch [900/1000], Loss: 14.4701\n",
      "Epoch [910/1000], Loss: 14.4698\n",
      "Epoch [920/1000], Loss: 14.4695\n",
      "Epoch [930/1000], Loss: 14.4693\n",
      "Epoch [940/1000], Loss: 14.4690\n",
      "Epoch [950/1000], Loss: 14.4687\n",
      "Epoch [960/1000], Loss: 14.4685\n",
      "Epoch [970/1000], Loss: 14.4682\n",
      "Epoch [980/1000], Loss: 14.4680\n",
      "Epoch [990/1000], Loss: 14.4677\n",
      "Epoch [1000/1000], Loss: 14.4675\n",
      "Predicted days_remaining for parent_id 406: [22.290790557861328, 22.815216064453125, 22.814464569091797, 22.81342887878418, 22.813308715820312, 22.81458854675293, 22.81439208984375, 22.814708709716797]\n",
      "Training for parent_id 412...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 175.3245\n",
      "Epoch [20/1000], Loss: 139.6369\n",
      "Epoch [30/1000], Loss: 109.5809\n",
      "Epoch [40/1000], Loss: 89.6750\n",
      "Epoch [50/1000], Loss: 75.4705\n",
      "Epoch [60/1000], Loss: 64.2791\n",
      "Epoch [70/1000], Loss: 55.0690\n",
      "Epoch [80/1000], Loss: 47.4088\n",
      "Epoch [90/1000], Loss: 41.0338\n",
      "Epoch [100/1000], Loss: 35.7483\n",
      "Epoch [110/1000], Loss: 31.3935\n",
      "Epoch [120/1000], Loss: 27.8335\n",
      "Epoch [130/1000], Loss: 24.9488\n",
      "Epoch [140/1000], Loss: 22.6331\n",
      "Epoch [150/1000], Loss: 20.7926\n",
      "Epoch [160/1000], Loss: 19.3445\n",
      "Epoch [170/1000], Loss: 18.2166\n",
      "Epoch [180/1000], Loss: 17.3470\n",
      "Epoch [190/1000], Loss: 16.6833\n",
      "Epoch [200/1000], Loss: 16.1814\n",
      "Epoch [210/1000], Loss: 15.8053\n",
      "Epoch [220/1000], Loss: 15.5256\n",
      "Epoch [230/1000], Loss: 15.3190\n",
      "Epoch [240/1000], Loss: 15.1670\n",
      "Epoch [250/1000], Loss: 15.0554\n",
      "Epoch [260/1000], Loss: 14.9734\n",
      "Epoch [270/1000], Loss: 14.9127\n",
      "Epoch [280/1000], Loss: 14.8673\n",
      "Epoch [290/1000], Loss: 14.8328\n",
      "Epoch [300/1000], Loss: 14.8059\n",
      "Epoch [310/1000], Loss: 14.7845\n",
      "Epoch [320/1000], Loss: 14.7668\n",
      "Epoch [330/1000], Loss: 14.7519\n",
      "Epoch [340/1000], Loss: 14.7389\n",
      "Epoch [350/1000], Loss: 14.7273\n",
      "Epoch [360/1000], Loss: 14.7168\n",
      "Epoch [370/1000], Loss: 14.7071\n",
      "Epoch [380/1000], Loss: 14.6980\n",
      "Epoch [390/1000], Loss: 14.6895\n",
      "Epoch [400/1000], Loss: 14.6815\n",
      "Epoch [410/1000], Loss: 14.6739\n",
      "Epoch [420/1000], Loss: 14.6666\n",
      "Epoch [430/1000], Loss: 14.6597\n",
      "Epoch [440/1000], Loss: 14.6531\n",
      "Epoch [450/1000], Loss: 14.6468\n",
      "Epoch [460/1000], Loss: 14.6408\n",
      "Epoch [470/1000], Loss: 14.6351\n",
      "Epoch [480/1000], Loss: 14.6296\n",
      "Epoch [490/1000], Loss: 14.6243\n",
      "Epoch [500/1000], Loss: 14.6192\n",
      "Epoch [510/1000], Loss: 14.6144\n",
      "Epoch [520/1000], Loss: 14.6097\n",
      "Epoch [530/1000], Loss: 14.6052\n",
      "Epoch [540/1000], Loss: 14.6009\n",
      "Epoch [550/1000], Loss: 14.5968\n",
      "Epoch [560/1000], Loss: 14.5928\n",
      "Epoch [570/1000], Loss: 14.5890\n",
      "Epoch [580/1000], Loss: 14.5853\n",
      "Epoch [590/1000], Loss: 14.5818\n",
      "Epoch [600/1000], Loss: 14.5783\n",
      "Epoch [610/1000], Loss: 14.5750\n",
      "Epoch [620/1000], Loss: 14.5718\n",
      "Epoch [630/1000], Loss: 14.5688\n",
      "Epoch [640/1000], Loss: 14.5658\n",
      "Epoch [650/1000], Loss: 14.5629\n",
      "Epoch [660/1000], Loss: 14.5602\n",
      "Epoch [670/1000], Loss: 14.5575\n",
      "Epoch [680/1000], Loss: 14.5549\n",
      "Epoch [690/1000], Loss: 14.5524\n",
      "Epoch [700/1000], Loss: 14.5500\n",
      "Epoch [710/1000], Loss: 14.5476\n",
      "Epoch [720/1000], Loss: 14.5453\n",
      "Epoch [730/1000], Loss: 14.5431\n",
      "Epoch [740/1000], Loss: 14.5410\n",
      "Epoch [750/1000], Loss: 14.5389\n",
      "Epoch [760/1000], Loss: 14.5369\n",
      "Epoch [770/1000], Loss: 14.5350\n",
      "Epoch [780/1000], Loss: 14.5331\n",
      "Epoch [790/1000], Loss: 14.5312\n",
      "Epoch [800/1000], Loss: 14.5294\n",
      "Epoch [810/1000], Loss: 14.5277\n",
      "Epoch [820/1000], Loss: 14.5260\n",
      "Epoch [830/1000], Loss: 14.5244\n",
      "Epoch [840/1000], Loss: 14.5228\n",
      "Epoch [850/1000], Loss: 14.5212\n",
      "Epoch [860/1000], Loss: 14.5197\n",
      "Epoch [870/1000], Loss: 14.5183\n",
      "Epoch [880/1000], Loss: 14.5168\n",
      "Epoch [890/1000], Loss: 14.5155\n",
      "Epoch [900/1000], Loss: 14.5141\n",
      "Epoch [910/1000], Loss: 14.5128\n",
      "Epoch [920/1000], Loss: 14.5115\n",
      "Epoch [930/1000], Loss: 14.5102\n",
      "Epoch [940/1000], Loss: 14.5090\n",
      "Epoch [950/1000], Loss: 14.5078\n",
      "Epoch [960/1000], Loss: 14.5067\n",
      "Epoch [970/1000], Loss: 14.5055\n",
      "Epoch [980/1000], Loss: 14.5044\n",
      "Epoch [990/1000], Loss: 14.5034\n",
      "Epoch [1000/1000], Loss: 14.5023\n",
      "Predicted days_remaining for parent_id 412: [13.07431411743164, 13.840652465820312, 13.84494686126709, 13.844827651977539, 13.842708587646484, 13.843387603759766, 13.84283447265625, 13.84427261352539]\n",
      "Training for parent_id 425...\n",
      "Epoch [10/1000], Loss: 1422.8401\n",
      "Epoch [20/1000], Loss: 1307.4224\n",
      "Epoch [30/1000], Loss: 1216.6982\n",
      "Epoch [40/1000], Loss: 1145.7633\n",
      "Epoch [50/1000], Loss: 1086.0077\n",
      "Epoch [60/1000], Loss: 1032.4799\n",
      "Epoch [70/1000], Loss: 982.6242\n",
      "Epoch [80/1000], Loss: 935.7219\n",
      "Epoch [90/1000], Loss: 891.3489\n",
      "Epoch [100/1000], Loss: 849.2079\n",
      "Epoch [110/1000], Loss: 809.0902\n",
      "Epoch [120/1000], Loss: 770.8361\n",
      "Epoch [130/1000], Loss: 734.3182\n",
      "Epoch [140/1000], Loss: 699.4321\n",
      "Epoch [150/1000], Loss: 666.0901\n",
      "Epoch [160/1000], Loss: 634.2152\n",
      "Epoch [170/1000], Loss: 603.7369\n",
      "Epoch [180/1000], Loss: 574.5891\n",
      "Epoch [190/1000], Loss: 546.7094\n",
      "Epoch [200/1000], Loss: 520.0405\n",
      "Epoch [210/1000], Loss: 494.5285\n",
      "Epoch [220/1000], Loss: 470.1237\n",
      "Epoch [230/1000], Loss: 446.7799\n",
      "Epoch [240/1000], Loss: 424.4541\n",
      "Epoch [250/1000], Loss: 403.1057\n",
      "Epoch [260/1000], Loss: 382.6969\n",
      "Epoch [270/1000], Loss: 363.1917\n",
      "Epoch [280/1000], Loss: 344.5559\n",
      "Epoch [290/1000], Loss: 326.7570\n",
      "Epoch [300/1000], Loss: 309.7638\n",
      "Epoch [310/1000], Loss: 293.5462\n",
      "Epoch [320/1000], Loss: 278.0758\n",
      "Epoch [330/1000], Loss: 263.3246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [340/1000], Loss: 249.2660\n",
      "Epoch [350/1000], Loss: 235.8741\n",
      "Epoch [360/1000], Loss: 223.1238\n",
      "Epoch [370/1000], Loss: 210.9908\n",
      "Epoch [380/1000], Loss: 199.4516\n",
      "Epoch [390/1000], Loss: 188.4832\n",
      "Epoch [400/1000], Loss: 178.0636\n",
      "Epoch [410/1000], Loss: 168.1711\n",
      "Epoch [420/1000], Loss: 158.7847\n",
      "Epoch [430/1000], Loss: 149.8841\n",
      "Epoch [440/1000], Loss: 141.4495\n",
      "Epoch [450/1000], Loss: 133.4616\n",
      "Epoch [460/1000], Loss: 125.9017\n",
      "Epoch [470/1000], Loss: 118.7517\n",
      "Epoch [480/1000], Loss: 111.9939\n",
      "Epoch [490/1000], Loss: 105.6112\n",
      "Epoch [500/1000], Loss: 99.5869\n",
      "Epoch [510/1000], Loss: 93.9051\n",
      "Epoch [520/1000], Loss: 88.5499\n",
      "Epoch [530/1000], Loss: 83.5064\n",
      "Epoch [540/1000], Loss: 78.7598\n",
      "Epoch [550/1000], Loss: 74.2960\n",
      "Epoch [560/1000], Loss: 70.1013\n",
      "Epoch [570/1000], Loss: 66.1625\n",
      "Epoch [580/1000], Loss: 62.4668\n",
      "Epoch [590/1000], Loss: 59.0018\n",
      "Epoch [600/1000], Loss: 55.7558\n",
      "Epoch [610/1000], Loss: 52.7172\n",
      "Epoch [620/1000], Loss: 49.8751\n",
      "Epoch [630/1000], Loss: 47.2188\n",
      "Epoch [640/1000], Loss: 44.7383\n",
      "Epoch [650/1000], Loss: 42.4238\n",
      "Epoch [660/1000], Loss: 40.2659\n",
      "Epoch [670/1000], Loss: 38.2558\n",
      "Epoch [680/1000], Loss: 36.3847\n",
      "Epoch [690/1000], Loss: 34.6447\n",
      "Epoch [700/1000], Loss: 33.0279\n",
      "Epoch [710/1000], Loss: 31.5267\n",
      "Epoch [720/1000], Loss: 30.1342\n",
      "Epoch [730/1000], Loss: 28.8435\n",
      "Epoch [740/1000], Loss: 27.6483\n",
      "Epoch [750/1000], Loss: 26.5425\n",
      "Epoch [760/1000], Loss: 25.5202\n",
      "Epoch [770/1000], Loss: 24.5759\n",
      "Epoch [780/1000], Loss: 23.7045\n",
      "Epoch [790/1000], Loss: 22.9011\n",
      "Epoch [800/1000], Loss: 22.1610\n",
      "Epoch [810/1000], Loss: 21.4798\n",
      "Epoch [820/1000], Loss: 20.8534\n",
      "Epoch [830/1000], Loss: 20.2779\n",
      "Epoch [840/1000], Loss: 19.7497\n",
      "Epoch [850/1000], Loss: 19.2652\n",
      "Epoch [860/1000], Loss: 18.8213\n",
      "Epoch [870/1000], Loss: 18.4150\n",
      "Epoch [880/1000], Loss: 18.0434\n",
      "Epoch [890/1000], Loss: 17.7038\n",
      "Epoch [900/1000], Loss: 17.3938\n",
      "Epoch [910/1000], Loss: 17.1110\n",
      "Epoch [920/1000], Loss: 16.8534\n",
      "Epoch [930/1000], Loss: 16.6188\n",
      "Epoch [940/1000], Loss: 16.4055\n",
      "Epoch [950/1000], Loss: 16.2116\n",
      "Epoch [960/1000], Loss: 16.0356\n",
      "Epoch [970/1000], Loss: 15.8760\n",
      "Epoch [980/1000], Loss: 15.7313\n",
      "Epoch [990/1000], Loss: 15.6004\n",
      "Epoch [1000/1000], Loss: 15.4819\n",
      "Predicted days_remaining for parent_id 425: [37.36752700805664, 37.79729080200195, 37.797428131103516, 37.79753112792969, 37.79755783081055, 37.7974967956543, 37.79696273803711, 37.796260833740234]\n",
      "Training for parent_id 430...\n",
      "Epoch [10/1000], Loss: 125.4861\n",
      "Epoch [20/1000], Loss: 95.4448\n",
      "Epoch [30/1000], Loss: 73.9508\n",
      "Epoch [40/1000], Loss: 60.4283\n",
      "Epoch [50/1000], Loss: 50.5648\n",
      "Epoch [60/1000], Loss: 42.7111\n",
      "Epoch [70/1000], Loss: 36.3649\n",
      "Epoch [80/1000], Loss: 31.2680\n",
      "Epoch [90/1000], Loss: 27.2184\n",
      "Epoch [100/1000], Loss: 24.0411\n",
      "Epoch [110/1000], Loss: 21.5820\n",
      "Epoch [120/1000], Loss: 19.7063\n",
      "Epoch [130/1000], Loss: 18.2971\n",
      "Epoch [140/1000], Loss: 17.2548\n",
      "Epoch [150/1000], Loss: 16.4956\n",
      "Epoch [160/1000], Loss: 15.9508\n",
      "Epoch [170/1000], Loss: 15.5655\n",
      "Epoch [180/1000], Loss: 15.2964\n",
      "Epoch [190/1000], Loss: 15.1102\n",
      "Epoch [200/1000], Loss: 14.9823\n",
      "Epoch [210/1000], Loss: 14.8944\n",
      "Epoch [220/1000], Loss: 14.8336\n",
      "Epoch [230/1000], Loss: 14.7909\n",
      "Epoch [240/1000], Loss: 14.7600\n",
      "Epoch [250/1000], Loss: 14.7369\n",
      "Epoch [260/1000], Loss: 14.7187\n",
      "Epoch [270/1000], Loss: 14.7039\n",
      "Epoch [280/1000], Loss: 14.6912\n",
      "Epoch [290/1000], Loss: 14.6800\n",
      "Epoch [300/1000], Loss: 14.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [310/1000], Loss: 14.6606\n",
      "Epoch [320/1000], Loss: 14.6520\n",
      "Epoch [330/1000], Loss: 14.6440\n",
      "Epoch [340/1000], Loss: 14.6364\n",
      "Epoch [350/1000], Loss: 14.6292\n",
      "Epoch [360/1000], Loss: 14.6224\n",
      "Epoch [370/1000], Loss: 14.6160\n",
      "Epoch [380/1000], Loss: 14.6100\n",
      "Epoch [390/1000], Loss: 14.6042\n",
      "Epoch [400/1000], Loss: 14.5987\n",
      "Epoch [410/1000], Loss: 14.5935\n",
      "Epoch [420/1000], Loss: 14.5886\n",
      "Epoch [430/1000], Loss: 14.5839\n",
      "Epoch [440/1000], Loss: 14.5794\n",
      "Epoch [450/1000], Loss: 14.5751\n",
      "Epoch [460/1000], Loss: 14.5710\n",
      "Epoch [470/1000], Loss: 14.5671\n",
      "Epoch [480/1000], Loss: 14.5634\n",
      "Epoch [490/1000], Loss: 14.5598\n",
      "Epoch [500/1000], Loss: 14.5564\n",
      "Epoch [510/1000], Loss: 14.5531\n",
      "Epoch [520/1000], Loss: 14.5500\n",
      "Epoch [530/1000], Loss: 14.5470\n",
      "Epoch [540/1000], Loss: 14.5441\n",
      "Epoch [550/1000], Loss: 14.5413\n",
      "Epoch [560/1000], Loss: 14.5387\n",
      "Epoch [570/1000], Loss: 14.5361\n",
      "Epoch [580/1000], Loss: 14.5337\n",
      "Epoch [590/1000], Loss: 14.5313\n",
      "Epoch [600/1000], Loss: 14.5290\n",
      "Epoch [610/1000], Loss: 14.5268\n",
      "Epoch [620/1000], Loss: 14.5247\n",
      "Epoch [630/1000], Loss: 14.5227\n",
      "Epoch [640/1000], Loss: 14.5207\n",
      "Epoch [650/1000], Loss: 14.5188\n",
      "Epoch [660/1000], Loss: 14.5170\n",
      "Epoch [670/1000], Loss: 14.5152\n",
      "Epoch [680/1000], Loss: 14.5135\n",
      "Epoch [690/1000], Loss: 14.5119\n",
      "Epoch [700/1000], Loss: 14.5103\n",
      "Epoch [710/1000], Loss: 14.5087\n",
      "Epoch [720/1000], Loss: 14.5072\n",
      "Epoch [730/1000], Loss: 14.5058\n",
      "Epoch [740/1000], Loss: 14.5044\n",
      "Epoch [750/1000], Loss: 14.5030\n",
      "Epoch [760/1000], Loss: 14.5017\n",
      "Epoch [770/1000], Loss: 14.5004\n",
      "Epoch [780/1000], Loss: 14.4992\n",
      "Epoch [790/1000], Loss: 14.4980\n",
      "Epoch [800/1000], Loss: 14.4968\n",
      "Epoch [810/1000], Loss: 14.4957\n",
      "Epoch [820/1000], Loss: 14.4946\n",
      "Epoch [830/1000], Loss: 14.4935\n",
      "Epoch [840/1000], Loss: 14.4925\n",
      "Epoch [850/1000], Loss: 14.4914\n",
      "Epoch [860/1000], Loss: 14.4905\n",
      "Epoch [870/1000], Loss: 14.4895\n",
      "Epoch [880/1000], Loss: 14.4886\n",
      "Epoch [890/1000], Loss: 14.4877\n",
      "Epoch [900/1000], Loss: 14.4868\n",
      "Epoch [910/1000], Loss: 14.4859\n",
      "Epoch [920/1000], Loss: 14.4851\n",
      "Epoch [930/1000], Loss: 14.4843\n",
      "Epoch [940/1000], Loss: 14.4835\n",
      "Epoch [950/1000], Loss: 14.4827\n",
      "Epoch [960/1000], Loss: 14.4820\n",
      "Epoch [970/1000], Loss: 14.4812\n",
      "Epoch [980/1000], Loss: 14.4805\n",
      "Epoch [990/1000], Loss: 14.4798\n",
      "Epoch [1000/1000], Loss: 14.4791\n",
      "Predicted days_remaining for parent_id 430: [11.20837688446045, 11.821232795715332, 11.82485580444336, 11.826509475708008, 11.82666015625, 11.825772285461426, 11.825957298278809, 11.822102546691895]\n",
      "Training for parent_id 433...\n",
      "Epoch [10/1000], Loss: 471.8238\n",
      "Epoch [20/1000], Loss: 401.8769\n",
      "Epoch [30/1000], Loss: 349.7550\n",
      "Epoch [40/1000], Loss: 314.6910\n",
      "Epoch [50/1000], Loss: 286.4879\n",
      "Epoch [60/1000], Loss: 261.6757\n",
      "Epoch [70/1000], Loss: 239.2482\n",
      "Epoch [80/1000], Loss: 218.7703\n",
      "Epoch [90/1000], Loss: 199.9852\n",
      "Epoch [100/1000], Loss: 182.7182\n",
      "Epoch [110/1000], Loss: 166.8389\n",
      "Epoch [120/1000], Loss: 152.2413\n",
      "Epoch [130/1000], Loss: 138.8345\n",
      "Epoch [140/1000], Loss: 126.5367\n",
      "Epoch [150/1000], Loss: 115.2727\n",
      "Epoch [160/1000], Loss: 104.9723\n",
      "Epoch [170/1000], Loss: 95.5692\n",
      "Epoch [180/1000], Loss: 87.0008\n",
      "Epoch [190/1000], Loss: 79.2079\n",
      "Epoch [200/1000], Loss: 72.1343\n",
      "Epoch [210/1000], Loss: 65.7267\n",
      "Epoch [220/1000], Loss: 59.9347\n",
      "Epoch [230/1000], Loss: 54.7104\n",
      "Epoch [240/1000], Loss: 50.0089\n",
      "Epoch [250/1000], Loss: 45.7875\n",
      "Epoch [260/1000], Loss: 42.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 38.6271\n",
      "Epoch [280/1000], Loss: 35.6149\n",
      "Epoch [290/1000], Loss: 32.9364\n",
      "Epoch [300/1000], Loss: 30.5607\n",
      "Epoch [310/1000], Loss: 28.4590\n",
      "Epoch [320/1000], Loss: 26.6045\n",
      "Epoch [330/1000], Loss: 24.9724\n",
      "Epoch [340/1000], Loss: 23.5399\n",
      "Epoch [350/1000], Loss: 22.2859\n",
      "Epoch [360/1000], Loss: 21.1912\n",
      "Epoch [370/1000], Loss: 20.2380\n",
      "Epoch [380/1000], Loss: 19.4103\n",
      "Epoch [390/1000], Loss: 18.6936\n",
      "Epoch [400/1000], Loss: 18.0747\n",
      "Epoch [410/1000], Loss: 17.5417\n",
      "Epoch [420/1000], Loss: 17.0839\n",
      "Epoch [430/1000], Loss: 16.6917\n",
      "Epoch [440/1000], Loss: 16.3567\n",
      "Epoch [450/1000], Loss: 16.0713\n",
      "Epoch [460/1000], Loss: 15.8288\n",
      "Epoch [470/1000], Loss: 15.6232\n",
      "Epoch [480/1000], Loss: 15.4495\n",
      "Epoch [490/1000], Loss: 15.3030\n",
      "Epoch [500/1000], Loss: 15.1797\n",
      "Epoch [510/1000], Loss: 15.0763\n",
      "Epoch [520/1000], Loss: 14.9896\n",
      "Epoch [530/1000], Loss: 14.9173\n",
      "Epoch [540/1000], Loss: 14.8569\n",
      "Epoch [550/1000], Loss: 14.8067\n",
      "Epoch [560/1000], Loss: 14.7649\n",
      "Epoch [570/1000], Loss: 14.7303\n",
      "Epoch [580/1000], Loss: 14.7015\n",
      "Epoch [590/1000], Loss: 14.6777\n",
      "Epoch [600/1000], Loss: 14.6579\n",
      "Epoch [610/1000], Loss: 14.6415\n",
      "Epoch [620/1000], Loss: 14.6279\n",
      "Epoch [630/1000], Loss: 14.6166\n",
      "Epoch [640/1000], Loss: 14.6071\n",
      "Epoch [650/1000], Loss: 14.5992\n",
      "Epoch [660/1000], Loss: 14.5926\n",
      "Epoch [670/1000], Loss: 14.5869\n",
      "Epoch [680/1000], Loss: 14.5821\n",
      "Epoch [690/1000], Loss: 14.5779\n",
      "Epoch [700/1000], Loss: 14.5743\n",
      "Epoch [710/1000], Loss: 14.5712\n",
      "Epoch [720/1000], Loss: 14.5683\n",
      "Epoch [730/1000], Loss: 14.5658\n",
      "Epoch [740/1000], Loss: 14.5635\n",
      "Epoch [750/1000], Loss: 14.5614\n",
      "Epoch [760/1000], Loss: 14.5595\n",
      "Epoch [770/1000], Loss: 14.5576\n",
      "Epoch [780/1000], Loss: 14.5559\n",
      "Epoch [790/1000], Loss: 14.5543\n",
      "Epoch [800/1000], Loss: 14.5527\n",
      "Epoch [810/1000], Loss: 14.5512\n",
      "Epoch [820/1000], Loss: 14.5497\n",
      "Epoch [830/1000], Loss: 14.5483\n",
      "Epoch [840/1000], Loss: 14.5469\n",
      "Epoch [850/1000], Loss: 14.5456\n",
      "Epoch [860/1000], Loss: 14.5443\n",
      "Epoch [870/1000], Loss: 14.5430\n",
      "Epoch [880/1000], Loss: 14.5417\n",
      "Epoch [890/1000], Loss: 14.5405\n",
      "Epoch [900/1000], Loss: 14.5392\n",
      "Epoch [910/1000], Loss: 14.5380\n",
      "Epoch [920/1000], Loss: 14.5368\n",
      "Epoch [930/1000], Loss: 14.5357\n",
      "Epoch [940/1000], Loss: 14.5345\n",
      "Epoch [950/1000], Loss: 14.5334\n",
      "Epoch [960/1000], Loss: 14.5323\n",
      "Epoch [970/1000], Loss: 14.5312\n",
      "Epoch [980/1000], Loss: 14.5301\n",
      "Epoch [990/1000], Loss: 14.5291\n",
      "Epoch [1000/1000], Loss: 14.5280\n",
      "Predicted days_remaining for parent_id 433: [21.954448699951172, 22.863317489624023, 22.86393165588379, 22.86379051208496, 22.8632755279541, 22.863534927368164, 22.8636474609375, 22.864368438720703]\n",
      "Training for parent_id 434...\n",
      "Epoch [10/1000], Loss: 117.6614\n",
      "Epoch [20/1000], Loss: 85.7535\n",
      "Epoch [30/1000], Loss: 66.2466\n",
      "Epoch [40/1000], Loss: 53.8718\n",
      "Epoch [50/1000], Loss: 44.9249\n",
      "Epoch [60/1000], Loss: 37.9521\n",
      "Epoch [70/1000], Loss: 32.4079\n",
      "Epoch [80/1000], Loss: 28.0190\n",
      "Epoch [90/1000], Loss: 24.5825\n",
      "Epoch [100/1000], Loss: 21.9273\n",
      "Epoch [110/1000], Loss: 19.9062\n",
      "Epoch [120/1000], Loss: 18.3923\n",
      "Epoch [130/1000], Loss: 17.2774\n",
      "Epoch [140/1000], Loss: 16.4702\n",
      "Epoch [150/1000], Loss: 15.8958\n",
      "Epoch [160/1000], Loss: 15.4938\n",
      "Epoch [170/1000], Loss: 15.2167\n",
      "Epoch [180/1000], Loss: 15.0284\n",
      "Epoch [190/1000], Loss: 14.9017\n",
      "Epoch [200/1000], Loss: 14.8169\n",
      "Epoch [210/1000], Loss: 14.7601\n",
      "Epoch [220/1000], Loss: 14.7216\n",
      "Epoch [230/1000], Loss: 14.6949\n",
      "Epoch [240/1000], Loss: 14.6757\n",
      "Epoch [250/1000], Loss: 14.6612\n",
      "Epoch [260/1000], Loss: 14.6497\n",
      "Epoch [270/1000], Loss: 14.6400\n",
      "Epoch [280/1000], Loss: 14.6316\n",
      "Epoch [290/1000], Loss: 14.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 14.6170\n",
      "Epoch [310/1000], Loss: 14.6105\n",
      "Epoch [320/1000], Loss: 14.6044\n",
      "Epoch [330/1000], Loss: 14.5987\n",
      "Epoch [340/1000], Loss: 14.5932\n",
      "Epoch [350/1000], Loss: 14.5880\n",
      "Epoch [360/1000], Loss: 14.5831\n",
      "Epoch [370/1000], Loss: 14.5784\n",
      "Epoch [380/1000], Loss: 14.5740\n",
      "Epoch [390/1000], Loss: 14.5697\n",
      "Epoch [400/1000], Loss: 14.5657\n",
      "Epoch [410/1000], Loss: 14.5618\n",
      "Epoch [420/1000], Loss: 14.5581\n",
      "Epoch [430/1000], Loss: 14.5546\n",
      "Epoch [440/1000], Loss: 14.5512\n",
      "Epoch [450/1000], Loss: 14.5480\n",
      "Epoch [460/1000], Loss: 14.5449\n",
      "Epoch [470/1000], Loss: 14.5419\n",
      "Epoch [480/1000], Loss: 14.5391\n",
      "Epoch [490/1000], Loss: 14.5364\n",
      "Epoch [500/1000], Loss: 14.5338\n",
      "Epoch [510/1000], Loss: 14.5313\n",
      "Epoch [520/1000], Loss: 14.5289\n",
      "Epoch [530/1000], Loss: 14.5265\n",
      "Epoch [540/1000], Loss: 14.5243\n",
      "Epoch [550/1000], Loss: 14.5222\n",
      "Epoch [560/1000], Loss: 14.5201\n",
      "Epoch [570/1000], Loss: 14.5181\n",
      "Epoch [580/1000], Loss: 14.5162\n",
      "Epoch [590/1000], Loss: 14.5143\n",
      "Epoch [600/1000], Loss: 14.5126\n",
      "Epoch [610/1000], Loss: 14.5108\n",
      "Epoch [620/1000], Loss: 14.5092\n",
      "Epoch [630/1000], Loss: 14.5076\n",
      "Epoch [640/1000], Loss: 14.5060\n",
      "Epoch [650/1000], Loss: 14.5045\n",
      "Epoch [660/1000], Loss: 14.5031\n",
      "Epoch [670/1000], Loss: 14.5017\n",
      "Epoch [680/1000], Loss: 14.5003\n",
      "Epoch [690/1000], Loss: 14.4990\n",
      "Epoch [700/1000], Loss: 14.4977\n",
      "Epoch [710/1000], Loss: 14.4965\n",
      "Epoch [720/1000], Loss: 14.4953\n",
      "Epoch [730/1000], Loss: 14.4941\n",
      "Epoch [740/1000], Loss: 14.4930\n",
      "Epoch [750/1000], Loss: 14.4919\n",
      "Epoch [760/1000], Loss: 14.4908\n",
      "Epoch [770/1000], Loss: 14.4898\n",
      "Epoch [780/1000], Loss: 14.4888\n",
      "Epoch [790/1000], Loss: 14.4878\n",
      "Epoch [800/1000], Loss: 14.4869\n",
      "Epoch [810/1000], Loss: 14.4860\n",
      "Epoch [820/1000], Loss: 14.4851\n",
      "Epoch [830/1000], Loss: 14.4842\n",
      "Epoch [840/1000], Loss: 14.4834\n",
      "Epoch [850/1000], Loss: 14.4826\n",
      "Epoch [860/1000], Loss: 14.4818\n",
      "Epoch [870/1000], Loss: 14.4810\n",
      "Epoch [880/1000], Loss: 14.4802\n",
      "Epoch [890/1000], Loss: 14.4795\n",
      "Epoch [900/1000], Loss: 14.4788\n",
      "Epoch [910/1000], Loss: 14.4781\n",
      "Epoch [920/1000], Loss: 14.4774\n",
      "Epoch [930/1000], Loss: 14.4767\n",
      "Epoch [940/1000], Loss: 14.4761\n",
      "Epoch [950/1000], Loss: 14.4754\n",
      "Epoch [960/1000], Loss: 14.4748\n",
      "Epoch [970/1000], Loss: 14.4742\n",
      "Epoch [980/1000], Loss: 14.4736\n",
      "Epoch [990/1000], Loss: 14.4731\n",
      "Epoch [1000/1000], Loss: 14.4725\n",
      "Predicted days_remaining for parent_id 434: [11.25367259979248, 11.819676399230957, 11.819121360778809, 11.817151069641113, 11.821284294128418, 11.819334030151367, 11.820425033569336, 11.815363883972168]\n",
      "Training for parent_id 437...\n",
      "Epoch [10/1000], Loss: 547.5644\n",
      "Epoch [20/1000], Loss: 474.6677\n",
      "Epoch [30/1000], Loss: 424.0497\n",
      "Epoch [40/1000], Loss: 387.8319\n",
      "Epoch [50/1000], Loss: 357.8607\n",
      "Epoch [60/1000], Loss: 331.0489\n",
      "Epoch [70/1000], Loss: 306.1467\n",
      "Epoch [80/1000], Loss: 282.9104\n",
      "Epoch [90/1000], Loss: 261.2745\n",
      "Epoch [100/1000], Loss: 241.1282\n",
      "Epoch [110/1000], Loss: 222.3712\n",
      "Epoch [120/1000], Loss: 204.9187\n",
      "Epoch [130/1000], Loss: 188.6969\n",
      "Epoch [140/1000], Loss: 173.6381\n",
      "Epoch [150/1000], Loss: 159.6781\n",
      "Epoch [160/1000], Loss: 146.7544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 134.8065\n",
      "Epoch [180/1000], Loss: 123.7757\n",
      "Epoch [190/1000], Loss: 113.6057\n",
      "Epoch [200/1000], Loss: 104.2427\n",
      "Epoch [210/1000], Loss: 95.6353\n",
      "Epoch [220/1000], Loss: 87.7345\n",
      "Epoch [230/1000], Loss: 80.4938\n",
      "Epoch [240/1000], Loss: 73.8687\n",
      "Epoch [250/1000], Loss: 67.8172\n",
      "Epoch [260/1000], Loss: 62.2990\n",
      "Epoch [270/1000], Loss: 57.2762\n",
      "Epoch [280/1000], Loss: 52.7126\n",
      "Epoch [290/1000], Loss: 48.5739\n",
      "Epoch [300/1000], Loss: 44.8277\n",
      "Epoch [310/1000], Loss: 41.4433\n",
      "Epoch [320/1000], Loss: 38.3919\n",
      "Epoch [330/1000], Loss: 35.6461\n",
      "Epoch [340/1000], Loss: 33.1804\n",
      "Epoch [350/1000], Loss: 30.9708\n",
      "Epoch [360/1000], Loss: 28.9948\n",
      "Epoch [370/1000], Loss: 27.2314\n",
      "Epoch [380/1000], Loss: 25.6612\n",
      "Epoch [390/1000], Loss: 24.2658\n",
      "Epoch [400/1000], Loss: 23.0286\n",
      "Epoch [410/1000], Loss: 21.9340\n",
      "Epoch [420/1000], Loss: 20.9677\n",
      "Epoch [430/1000], Loss: 20.1165\n",
      "Epoch [440/1000], Loss: 19.3683\n",
      "Epoch [450/1000], Loss: 18.7122\n",
      "Epoch [460/1000], Loss: 18.1380\n",
      "Epoch [470/1000], Loss: 17.6367\n",
      "Epoch [480/1000], Loss: 17.2000\n",
      "Epoch [490/1000], Loss: 16.8203\n",
      "Epoch [500/1000], Loss: 16.4911\n",
      "Epoch [510/1000], Loss: 16.2061\n",
      "Epoch [520/1000], Loss: 15.9600\n",
      "Epoch [530/1000], Loss: 15.7480\n",
      "Epoch [540/1000], Loss: 15.5657\n",
      "Epoch [550/1000], Loss: 15.4093\n",
      "Epoch [560/1000], Loss: 15.2754\n",
      "Epoch [570/1000], Loss: 15.1610\n",
      "Epoch [580/1000], Loss: 15.0634\n",
      "Epoch [590/1000], Loss: 14.9804\n",
      "Epoch [600/1000], Loss: 14.9099\n",
      "Epoch [610/1000], Loss: 14.8501\n",
      "Epoch [620/1000], Loss: 14.7996\n",
      "Epoch [630/1000], Loss: 14.7568\n",
      "Epoch [640/1000], Loss: 14.7208\n",
      "Epoch [650/1000], Loss: 14.6904\n",
      "Epoch [660/1000], Loss: 14.6649\n",
      "Epoch [670/1000], Loss: 14.6434\n",
      "Epoch [680/1000], Loss: 14.6254\n",
      "Epoch [690/1000], Loss: 14.6103\n",
      "Epoch [700/1000], Loss: 14.5975\n",
      "Epoch [710/1000], Loss: 14.5869\n",
      "Epoch [720/1000], Loss: 14.5779\n",
      "Epoch [730/1000], Loss: 14.5703\n",
      "Epoch [740/1000], Loss: 14.5640\n",
      "Epoch [750/1000], Loss: 14.5586\n",
      "Epoch [760/1000], Loss: 14.5540\n",
      "Epoch [770/1000], Loss: 14.5501\n",
      "Epoch [780/1000], Loss: 14.5467\n",
      "Epoch [790/1000], Loss: 14.5438\n",
      "Epoch [800/1000], Loss: 14.5413\n",
      "Epoch [810/1000], Loss: 14.5391\n",
      "Epoch [820/1000], Loss: 14.5371\n",
      "Epoch [830/1000], Loss: 14.5354\n",
      "Epoch [840/1000], Loss: 14.5338\n",
      "Epoch [850/1000], Loss: 14.5324\n",
      "Epoch [860/1000], Loss: 14.5310\n",
      "Epoch [870/1000], Loss: 14.5298\n",
      "Epoch [880/1000], Loss: 14.5287\n",
      "Epoch [890/1000], Loss: 14.5276\n",
      "Epoch [900/1000], Loss: 14.5265\n",
      "Epoch [910/1000], Loss: 14.5255\n",
      "Epoch [920/1000], Loss: 14.5246\n",
      "Epoch [930/1000], Loss: 14.5236\n",
      "Epoch [940/1000], Loss: 14.5227\n",
      "Epoch [950/1000], Loss: 14.5219\n",
      "Epoch [960/1000], Loss: 14.5210\n",
      "Epoch [970/1000], Loss: 14.5202\n",
      "Epoch [980/1000], Loss: 14.5193\n",
      "Epoch [990/1000], Loss: 14.5185\n",
      "Epoch [1000/1000], Loss: 14.5177\n",
      "Predicted days_remaining for parent_id 437: [23.996305465698242, 24.85133171081543, 24.85195541381836, 24.852144241333008, 24.852413177490234, 24.852357864379883, 24.852489471435547, 24.852313995361328]\n",
      "Training for parent_id 445...\n",
      "Epoch [10/1000], Loss: 324.3111\n",
      "Epoch [20/1000], Loss: 270.7939\n",
      "Epoch [30/1000], Loss: 226.7923\n",
      "Epoch [40/1000], Loss: 197.5631\n",
      "Epoch [50/1000], Loss: 175.5025\n",
      "Epoch [60/1000], Loss: 156.6266\n",
      "Epoch [70/1000], Loss: 139.7651\n",
      "Epoch [80/1000], Loss: 124.7097\n",
      "Epoch [90/1000], Loss: 111.2672\n",
      "Epoch [100/1000], Loss: 99.2544\n",
      "Epoch [110/1000], Loss: 88.5270\n",
      "Epoch [120/1000], Loss: 78.9667\n",
      "Epoch [130/1000], Loss: 70.4697\n",
      "Epoch [140/1000], Loss: 62.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/1000], Loss: 56.2897\n",
      "Epoch [160/1000], Loss: 50.4336\n",
      "Epoch [170/1000], Loss: 45.2943\n",
      "Epoch [180/1000], Loss: 40.7994\n",
      "Epoch [190/1000], Loss: 36.8821\n",
      "Epoch [200/1000], Loss: 33.4807\n",
      "Epoch [210/1000], Loss: 30.5385\n",
      "Epoch [220/1000], Loss: 28.0035\n",
      "Epoch [230/1000], Loss: 25.8281\n",
      "Epoch [240/1000], Loss: 23.9691\n",
      "Epoch [250/1000], Loss: 22.3870\n",
      "Epoch [260/1000], Loss: 21.0463\n",
      "Epoch [270/1000], Loss: 19.9152\n",
      "Epoch [280/1000], Loss: 18.9649\n",
      "Epoch [290/1000], Loss: 18.1700\n",
      "Epoch [300/1000], Loss: 17.5081\n",
      "Epoch [310/1000], Loss: 16.9594\n",
      "Epoch [320/1000], Loss: 16.5064\n",
      "Epoch [330/1000], Loss: 16.1340\n",
      "Epoch [340/1000], Loss: 15.8293\n",
      "Epoch [350/1000], Loss: 15.5810\n",
      "Epoch [360/1000], Loss: 15.3794\n",
      "Epoch [370/1000], Loss: 15.2165\n",
      "Epoch [380/1000], Loss: 15.0852\n",
      "Epoch [390/1000], Loss: 14.9799\n",
      "Epoch [400/1000], Loss: 14.8957\n",
      "Epoch [410/1000], Loss: 14.8284\n",
      "Epoch [420/1000], Loss: 14.7750\n",
      "Epoch [430/1000], Loss: 14.7325\n",
      "Epoch [440/1000], Loss: 14.6988\n",
      "Epoch [450/1000], Loss: 14.6721\n",
      "Epoch [460/1000], Loss: 14.6508\n",
      "Epoch [470/1000], Loss: 14.6339\n",
      "Epoch [480/1000], Loss: 14.6204\n",
      "Epoch [490/1000], Loss: 14.6095\n",
      "Epoch [500/1000], Loss: 14.6007\n",
      "Epoch [510/1000], Loss: 14.5935\n",
      "Epoch [520/1000], Loss: 14.5875\n",
      "Epoch [530/1000], Loss: 14.5824\n",
      "Epoch [540/1000], Loss: 14.5781\n",
      "Epoch [550/1000], Loss: 14.5744\n",
      "Epoch [560/1000], Loss: 14.5710\n",
      "Epoch [570/1000], Loss: 14.5681\n",
      "Epoch [580/1000], Loss: 14.5653\n",
      "Epoch [590/1000], Loss: 14.5628\n",
      "Epoch [600/1000], Loss: 14.5604\n",
      "Epoch [610/1000], Loss: 14.5582\n",
      "Epoch [620/1000], Loss: 14.5561\n",
      "Epoch [630/1000], Loss: 14.5541\n",
      "Epoch [640/1000], Loss: 14.5521\n",
      "Epoch [650/1000], Loss: 14.5502\n",
      "Epoch [660/1000], Loss: 14.5483\n",
      "Epoch [670/1000], Loss: 14.5466\n",
      "Epoch [680/1000], Loss: 14.5448\n",
      "Epoch [690/1000], Loss: 14.5431\n",
      "Epoch [700/1000], Loss: 14.5414\n",
      "Epoch [710/1000], Loss: 14.5398\n",
      "Epoch [720/1000], Loss: 14.5382\n",
      "Epoch [730/1000], Loss: 14.5366\n",
      "Epoch [740/1000], Loss: 14.5351\n",
      "Epoch [750/1000], Loss: 14.5336\n",
      "Epoch [760/1000], Loss: 14.5322\n",
      "Epoch [770/1000], Loss: 14.5307\n",
      "Epoch [780/1000], Loss: 14.5293\n",
      "Epoch [790/1000], Loss: 14.5280\n",
      "Epoch [800/1000], Loss: 14.5266\n",
      "Epoch [810/1000], Loss: 14.5253\n",
      "Epoch [820/1000], Loss: 14.5240\n",
      "Epoch [830/1000], Loss: 14.5228\n",
      "Epoch [840/1000], Loss: 14.5215\n",
      "Epoch [850/1000], Loss: 14.5203\n",
      "Epoch [860/1000], Loss: 14.5191\n",
      "Epoch [870/1000], Loss: 14.5180\n",
      "Epoch [880/1000], Loss: 14.5168\n",
      "Epoch [890/1000], Loss: 14.5157\n",
      "Epoch [900/1000], Loss: 14.5146\n",
      "Epoch [910/1000], Loss: 14.5135\n",
      "Epoch [920/1000], Loss: 14.5125\n",
      "Epoch [930/1000], Loss: 14.5114\n",
      "Epoch [940/1000], Loss: 14.5104\n",
      "Epoch [950/1000], Loss: 14.5094\n",
      "Epoch [960/1000], Loss: 14.5084\n",
      "Epoch [970/1000], Loss: 14.5075\n",
      "Epoch [980/1000], Loss: 14.5065\n",
      "Epoch [990/1000], Loss: 14.5056\n",
      "Epoch [1000/1000], Loss: 14.5047\n",
      "Predicted days_remaining for parent_id 445: [18.064390182495117, 18.847803115844727, 18.848770141601562, 18.848066329956055, 18.847620010375977, 18.84758949279785, 18.847972869873047, 18.846473693847656]\n",
      "Training for parent_id 458...\n",
      "Epoch [10/1000], Loss: 1076.2827\n",
      "Epoch [20/1000], Loss: 967.1545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/1000], Loss: 886.7318\n",
      "Epoch [40/1000], Loss: 828.3220\n",
      "Epoch [50/1000], Loss: 779.5271\n",
      "Epoch [60/1000], Loss: 735.9689\n",
      "Epoch [70/1000], Loss: 695.6788\n",
      "Epoch [80/1000], Loss: 657.8854\n",
      "Epoch [90/1000], Loss: 622.2251\n",
      "Epoch [100/1000], Loss: 588.4821\n",
      "Epoch [110/1000], Loss: 556.5035\n",
      "Epoch [120/1000], Loss: 526.1669\n",
      "Epoch [130/1000], Loss: 497.3683\n",
      "Epoch [140/1000], Loss: 470.0173\n",
      "Epoch [150/1000], Loss: 444.0338\n",
      "Epoch [160/1000], Loss: 419.3458\n",
      "Epoch [170/1000], Loss: 395.8881\n",
      "Epoch [180/1000], Loss: 373.6010\n",
      "Epoch [190/1000], Loss: 352.4293\n",
      "Epoch [200/1000], Loss: 332.3216\n",
      "Epoch [210/1000], Loss: 313.2299\n",
      "Epoch [220/1000], Loss: 295.1093\n",
      "Epoch [230/1000], Loss: 277.9173\n",
      "Epoch [240/1000], Loss: 261.6137\n",
      "Epoch [250/1000], Loss: 246.1602\n",
      "Epoch [260/1000], Loss: 231.5202\n",
      "Epoch [270/1000], Loss: 217.6590\n",
      "Epoch [280/1000], Loss: 204.5430\n",
      "Epoch [290/1000], Loss: 192.1400\n",
      "Epoch [300/1000], Loss: 180.4193\n",
      "Epoch [310/1000], Loss: 169.3508\n",
      "Epoch [320/1000], Loss: 158.9059\n",
      "Epoch [330/1000], Loss: 149.0567\n",
      "Epoch [340/1000], Loss: 139.7764\n",
      "Epoch [350/1000], Loss: 131.0390\n",
      "Epoch [360/1000], Loss: 122.8195\n",
      "Epoch [370/1000], Loss: 115.0934\n",
      "Epoch [380/1000], Loss: 107.8375\n",
      "Epoch [390/1000], Loss: 101.0288\n",
      "Epoch [400/1000], Loss: 94.6455\n",
      "Epoch [410/1000], Loss: 88.6664\n",
      "Epoch [420/1000], Loss: 83.0709\n",
      "Epoch [430/1000], Loss: 77.8394\n",
      "Epoch [440/1000], Loss: 72.9526\n",
      "Epoch [450/1000], Loss: 68.3924\n",
      "Epoch [460/1000], Loss: 64.1408\n",
      "Epoch [470/1000], Loss: 60.1809\n",
      "Epoch [480/1000], Loss: 56.4964\n",
      "Epoch [490/1000], Loss: 53.0716\n",
      "Epoch [500/1000], Loss: 49.8913\n",
      "Epoch [510/1000], Loss: 46.9411\n",
      "Epoch [520/1000], Loss: 44.2071\n",
      "Epoch [530/1000], Loss: 41.6763\n",
      "Epoch [540/1000], Loss: 39.3358\n",
      "Epoch [550/1000], Loss: 37.1738\n",
      "Epoch [560/1000], Loss: 35.1787\n",
      "Epoch [570/1000], Loss: 33.3396\n",
      "Epoch [580/1000], Loss: 31.6461\n",
      "Epoch [590/1000], Loss: 30.0884\n",
      "Epoch [600/1000], Loss: 28.6573\n",
      "Epoch [610/1000], Loss: 27.3437\n",
      "Epoch [620/1000], Loss: 26.1396\n",
      "Epoch [630/1000], Loss: 25.0368\n",
      "Epoch [640/1000], Loss: 24.0281\n",
      "Epoch [650/1000], Loss: 23.1065\n",
      "Epoch [660/1000], Loss: 22.2653\n",
      "Epoch [670/1000], Loss: 21.4985\n",
      "Epoch [680/1000], Loss: 20.8003\n",
      "Epoch [690/1000], Loss: 20.1652\n",
      "Epoch [700/1000], Loss: 19.5882\n",
      "Epoch [710/1000], Loss: 19.0646\n",
      "Epoch [720/1000], Loss: 18.5899\n",
      "Epoch [730/1000], Loss: 18.1603\n",
      "Epoch [740/1000], Loss: 17.7717\n",
      "Epoch [750/1000], Loss: 17.4207\n",
      "Epoch [760/1000], Loss: 17.1040\n",
      "Epoch [770/1000], Loss: 16.8187\n",
      "Epoch [780/1000], Loss: 16.5618\n",
      "Epoch [790/1000], Loss: 16.3309\n",
      "Epoch [800/1000], Loss: 16.1235\n",
      "Epoch [810/1000], Loss: 15.9375\n",
      "Epoch [820/1000], Loss: 15.7709\n",
      "Epoch [830/1000], Loss: 15.6218\n",
      "Epoch [840/1000], Loss: 15.4886\n",
      "Epoch [850/1000], Loss: 15.3696\n",
      "Epoch [860/1000], Loss: 15.2636\n",
      "Epoch [870/1000], Loss: 15.1691\n",
      "Epoch [880/1000], Loss: 15.0851\n",
      "Epoch [890/1000], Loss: 15.0105\n",
      "Epoch [900/1000], Loss: 14.9443\n",
      "Epoch [910/1000], Loss: 14.8856\n",
      "Epoch [920/1000], Loss: 14.8336\n",
      "Epoch [930/1000], Loss: 14.7877\n",
      "Epoch [940/1000], Loss: 14.7471\n",
      "Epoch [950/1000], Loss: 14.7113\n",
      "Epoch [960/1000], Loss: 14.6798\n",
      "Epoch [970/1000], Loss: 14.6520\n",
      "Epoch [980/1000], Loss: 14.6276\n",
      "Epoch [990/1000], Loss: 14.6062\n",
      "Epoch [1000/1000], Loss: 14.5875\n",
      "Predicted days_remaining for parent_id 458: [32.96607208251953, 33.46446990966797, 33.46450424194336, 33.46443176269531, 33.46418762207031, 33.46414566040039, 33.464271545410156, 33.46449661254883]\n",
      "Training for parent_id 460...\n",
      "Epoch [10/1000], Loss: 361.3670\n",
      "Epoch [20/1000], Loss: 304.8873\n",
      "Epoch [30/1000], Loss: 262.0387\n",
      "Epoch [40/1000], Loss: 231.6453\n",
      "Epoch [50/1000], Loss: 207.3066\n",
      "Epoch [60/1000], Loss: 186.5679\n",
      "Epoch [70/1000], Loss: 168.1544\n",
      "Epoch [80/1000], Loss: 151.5661\n",
      "Epoch [90/1000], Loss: 136.5524\n",
      "Epoch [100/1000], Loss: 122.9470\n",
      "Epoch [110/1000], Loss: 110.6255\n",
      "Epoch [120/1000], Loss: 99.4855\n",
      "Epoch [130/1000], Loss: 89.4369\n",
      "Epoch [140/1000], Loss: 80.3965\n",
      "Epoch [150/1000], Loss: 72.2859\n",
      "Epoch [160/1000], Loss: 65.0306\n",
      "Epoch [170/1000], Loss: 58.5596\n",
      "Epoch [180/1000], Loss: 52.8059\n",
      "Epoch [190/1000], Loss: 47.7058\n",
      "Epoch [200/1000], Loss: 43.1994\n",
      "Epoch [210/1000], Loss: 39.2307\n",
      "Epoch [220/1000], Loss: 35.7469\n",
      "Epoch [230/1000], Loss: 32.6992\n",
      "Epoch [240/1000], Loss: 30.0422\n",
      "Epoch [250/1000], Loss: 27.7338\n",
      "Epoch [260/1000], Loss: 25.7355\n",
      "Epoch [270/1000], Loss: 24.0117\n",
      "Epoch [280/1000], Loss: 22.5303\n",
      "Epoch [290/1000], Loss: 21.2617\n",
      "Epoch [300/1000], Loss: 20.1793\n",
      "Epoch [310/1000], Loss: 19.2593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/1000], Loss: 18.4802\n",
      "Epoch [330/1000], Loss: 17.8227\n",
      "Epoch [340/1000], Loss: 17.2700\n",
      "Epoch [350/1000], Loss: 16.8071\n",
      "Epoch [360/1000], Loss: 16.4206\n",
      "Epoch [370/1000], Loss: 16.0992\n",
      "Epoch [380/1000], Loss: 15.8328\n",
      "Epoch [390/1000], Loss: 15.6127\n",
      "Epoch [400/1000], Loss: 15.4314\n",
      "Epoch [410/1000], Loss: 15.2825\n",
      "Epoch [420/1000], Loss: 15.1606\n",
      "Epoch [430/1000], Loss: 15.0610\n",
      "Epoch [440/1000], Loss: 14.9798\n",
      "Epoch [450/1000], Loss: 14.9138\n",
      "Epoch [460/1000], Loss: 14.8600\n",
      "Epoch [470/1000], Loss: 14.8164\n",
      "Epoch [480/1000], Loss: 14.7809\n",
      "Epoch [490/1000], Loss: 14.7520\n",
      "Epoch [500/1000], Loss: 14.7285\n",
      "Epoch [510/1000], Loss: 14.7092\n",
      "Epoch [520/1000], Loss: 14.6933\n",
      "Epoch [530/1000], Loss: 14.6801\n",
      "Epoch [540/1000], Loss: 14.6692\n",
      "Epoch [550/1000], Loss: 14.6599\n",
      "Epoch [560/1000], Loss: 14.6520\n",
      "Epoch [570/1000], Loss: 14.6452\n",
      "Epoch [580/1000], Loss: 14.6392\n",
      "Epoch [590/1000], Loss: 14.6339\n",
      "Epoch [600/1000], Loss: 14.6292\n",
      "Epoch [610/1000], Loss: 14.6249\n",
      "Epoch [620/1000], Loss: 14.6209\n",
      "Epoch [630/1000], Loss: 14.6172\n",
      "Epoch [640/1000], Loss: 14.6137\n",
      "Epoch [650/1000], Loss: 14.6104\n",
      "Epoch [660/1000], Loss: 14.6073\n",
      "Epoch [670/1000], Loss: 14.6043\n",
      "Epoch [680/1000], Loss: 14.6014\n",
      "Epoch [690/1000], Loss: 14.5986\n",
      "Epoch [700/1000], Loss: 14.5959\n",
      "Epoch [710/1000], Loss: 14.5932\n",
      "Epoch [720/1000], Loss: 14.5907\n",
      "Epoch [730/1000], Loss: 14.5882\n",
      "Epoch [740/1000], Loss: 14.5858\n",
      "Epoch [750/1000], Loss: 14.5834\n",
      "Epoch [760/1000], Loss: 14.5811\n",
      "Epoch [770/1000], Loss: 14.5788\n",
      "Epoch [780/1000], Loss: 14.5766\n",
      "Epoch [790/1000], Loss: 14.5744\n",
      "Epoch [800/1000], Loss: 14.5723\n",
      "Epoch [810/1000], Loss: 14.5702\n",
      "Epoch [820/1000], Loss: 14.5682\n",
      "Epoch [830/1000], Loss: 14.5662\n",
      "Epoch [840/1000], Loss: 14.5643\n",
      "Epoch [850/1000], Loss: 14.5624\n",
      "Epoch [860/1000], Loss: 14.5605\n",
      "Epoch [870/1000], Loss: 14.5587\n",
      "Epoch [880/1000], Loss: 14.5569\n",
      "Epoch [890/1000], Loss: 14.5552\n",
      "Epoch [900/1000], Loss: 14.5535\n",
      "Epoch [910/1000], Loss: 14.5518\n",
      "Epoch [920/1000], Loss: 14.5501\n",
      "Epoch [930/1000], Loss: 14.5485\n",
      "Epoch [940/1000], Loss: 14.5470\n",
      "Epoch [950/1000], Loss: 14.5454\n",
      "Epoch [960/1000], Loss: 14.5439\n",
      "Epoch [970/1000], Loss: 14.5424\n",
      "Epoch [980/1000], Loss: 14.5410\n",
      "Epoch [990/1000], Loss: 14.5395\n",
      "Epoch [1000/1000], Loss: 14.5381\n",
      "Predicted days_remaining for parent_id 460: [18.91103744506836, 19.86851692199707, 19.870019912719727, 19.870628356933594, 19.868511199951172, 19.8695011138916, 19.86928367614746, 19.87042236328125]\n",
      "Training for parent_id 464...\n",
      "Epoch [10/1000], Loss: 790.2167\n",
      "Epoch [20/1000], Loss: 709.5082\n",
      "Epoch [30/1000], Loss: 643.2853\n",
      "Epoch [40/1000], Loss: 596.1119\n",
      "Epoch [50/1000], Loss: 556.7914\n",
      "Epoch [60/1000], Loss: 521.2857\n",
      "Epoch [70/1000], Loss: 488.3141\n",
      "Epoch [80/1000], Loss: 457.0357\n",
      "Epoch [90/1000], Loss: 427.5547\n",
      "Epoch [100/1000], Loss: 399.7954\n",
      "Epoch [110/1000], Loss: 373.6454\n",
      "Epoch [120/1000], Loss: 349.0138\n",
      "Epoch [130/1000], Loss: 325.8188\n",
      "Epoch [140/1000], Loss: 303.9845\n",
      "Epoch [150/1000], Loss: 283.4404\n",
      "Epoch [160/1000], Loss: 264.1207\n",
      "Epoch [170/1000], Loss: 245.9657\n",
      "Epoch [180/1000], Loss: 228.9227\n",
      "Epoch [190/1000], Loss: 212.9435\n",
      "Epoch [200/1000], Loss: 197.9802\n",
      "Epoch [210/1000], Loss: 183.9828\n",
      "Epoch [220/1000], Loss: 170.9008\n",
      "Epoch [230/1000], Loss: 158.6846\n",
      "Epoch [240/1000], Loss: 147.2863\n",
      "Epoch [250/1000], Loss: 136.6605\n",
      "Epoch [260/1000], Loss: 126.7638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/1000], Loss: 117.5550\n",
      "Epoch [280/1000], Loss: 108.9951\n",
      "Epoch [290/1000], Loss: 101.0466\n",
      "Epoch [300/1000], Loss: 93.6740\n",
      "Epoch [310/1000], Loss: 86.8432\n",
      "Epoch [320/1000], Loss: 80.5219\n",
      "Epoch [330/1000], Loss: 74.6791\n",
      "Epoch [340/1000], Loss: 69.2851\n",
      "Epoch [350/1000], Loss: 64.3119\n",
      "Epoch [360/1000], Loss: 59.7324\n",
      "Epoch [370/1000], Loss: 55.5210\n",
      "Epoch [380/1000], Loss: 51.6534\n",
      "Epoch [390/1000], Loss: 48.1062\n",
      "Epoch [400/1000], Loss: 44.8575\n",
      "Epoch [410/1000], Loss: 41.8862\n",
      "Epoch [420/1000], Loss: 39.1727\n",
      "Epoch [430/1000], Loss: 36.6979\n",
      "Epoch [440/1000], Loss: 34.4442\n",
      "Epoch [450/1000], Loss: 32.3950\n",
      "Epoch [460/1000], Loss: 30.5343\n",
      "Epoch [470/1000], Loss: 28.8474\n",
      "Epoch [480/1000], Loss: 27.3203\n",
      "Epoch [490/1000], Loss: 25.9400\n",
      "Epoch [500/1000], Loss: 24.6943\n",
      "Epoch [510/1000], Loss: 23.5717\n",
      "Epoch [520/1000], Loss: 22.5618\n",
      "Epoch [530/1000], Loss: 21.6545\n",
      "Epoch [540/1000], Loss: 20.8408\n",
      "Epoch [550/1000], Loss: 20.1120\n",
      "Epoch [560/1000], Loss: 19.4605\n",
      "Epoch [570/1000], Loss: 18.8788\n",
      "Epoch [580/1000], Loss: 18.3604\n",
      "Epoch [590/1000], Loss: 17.8990\n",
      "Epoch [600/1000], Loss: 17.4891\n",
      "Epoch [610/1000], Loss: 17.1254\n",
      "Epoch [620/1000], Loss: 16.8034\n",
      "Epoch [630/1000], Loss: 16.5186\n",
      "Epoch [640/1000], Loss: 16.2671\n",
      "Epoch [650/1000], Loss: 16.0455\n",
      "Epoch [660/1000], Loss: 15.8504\n",
      "Epoch [670/1000], Loss: 15.6789\n",
      "Epoch [680/1000], Loss: 15.5285\n",
      "Epoch [690/1000], Loss: 15.3967\n",
      "Epoch [700/1000], Loss: 15.2815\n",
      "Epoch [710/1000], Loss: 15.1808\n",
      "Epoch [720/1000], Loss: 15.0930\n",
      "Epoch [730/1000], Loss: 15.0165\n",
      "Epoch [740/1000], Loss: 14.9500\n",
      "Epoch [750/1000], Loss: 14.8923\n",
      "Epoch [760/1000], Loss: 14.8422\n",
      "Epoch [770/1000], Loss: 14.7989\n",
      "Epoch [780/1000], Loss: 14.7614\n",
      "Epoch [790/1000], Loss: 14.7290\n",
      "Epoch [800/1000], Loss: 14.7010\n",
      "Epoch [810/1000], Loss: 14.6769\n",
      "Epoch [820/1000], Loss: 14.6561\n",
      "Epoch [830/1000], Loss: 14.6383\n",
      "Epoch [840/1000], Loss: 14.6229\n",
      "Epoch [850/1000], Loss: 14.6097\n",
      "Epoch [860/1000], Loss: 14.5983\n",
      "Epoch [870/1000], Loss: 14.5885\n",
      "Epoch [880/1000], Loss: 14.5801\n",
      "Epoch [890/1000], Loss: 14.5729\n",
      "Epoch [900/1000], Loss: 14.5667\n",
      "Epoch [910/1000], Loss: 14.5613\n",
      "Epoch [920/1000], Loss: 14.5567\n",
      "Epoch [930/1000], Loss: 14.5527\n",
      "Epoch [940/1000], Loss: 14.5492\n",
      "Epoch [950/1000], Loss: 14.5462\n",
      "Epoch [960/1000], Loss: 14.5435\n",
      "Epoch [970/1000], Loss: 14.5412\n",
      "Epoch [980/1000], Loss: 14.5391\n",
      "Epoch [990/1000], Loss: 14.5373\n",
      "Epoch [1000/1000], Loss: 14.5356\n",
      "Predicted days_remaining for parent_id 464: [27.876075744628906, 28.803255081176758, 28.80356216430664, 28.8035831451416, 28.8033390045166, 28.803525924682617, 28.803647994995117, 28.803646087646484]\n",
      "Training for parent_id 481...\n",
      "Epoch [10/1000], Loss: 859.9858\n",
      "Epoch [20/1000], Loss: 758.6329\n",
      "Epoch [30/1000], Loss: 692.9575\n",
      "Epoch [40/1000], Loss: 645.4163\n",
      "Epoch [50/1000], Loss: 605.4437\n",
      "Epoch [60/1000], Loss: 569.2399\n",
      "Epoch [70/1000], Loss: 535.5599\n",
      "Epoch [80/1000], Loss: 503.9957\n",
      "Epoch [90/1000], Loss: 474.2847\n",
      "Epoch [100/1000], Loss: 446.2525\n",
      "Epoch [110/1000], Loss: 419.7703\n",
      "Epoch [120/1000], Loss: 394.7344\n",
      "Epoch [130/1000], Loss: 371.0567\n",
      "Epoch [140/1000], Loss: 348.6596\n",
      "Epoch [150/1000], Loss: 327.4740\n",
      "Epoch [160/1000], Loss: 307.4370\n",
      "Epoch [170/1000], Loss: 288.4907\n",
      "Epoch [180/1000], Loss: 270.5821\n",
      "Epoch [190/1000], Loss: 253.6612\n",
      "Epoch [200/1000], Loss: 237.6816\n",
      "Epoch [210/1000], Loss: 222.5995\n",
      "Epoch [220/1000], Loss: 208.3732\n",
      "Epoch [230/1000], Loss: 194.9634\n",
      "Epoch [240/1000], Loss: 182.3322\n",
      "Epoch [250/1000], Loss: 170.4435\n",
      "Epoch [260/1000], Loss: 159.2628\n",
      "Epoch [270/1000], Loss: 148.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/1000], Loss: 138.8934\n",
      "Epoch [290/1000], Loss: 129.6417\n",
      "Epoch [300/1000], Loss: 120.9720\n",
      "Epoch [310/1000], Loss: 112.8554\n",
      "Epoch [320/1000], Loss: 105.2642\n",
      "Epoch [330/1000], Loss: 98.1717\n",
      "Epoch [340/1000], Loss: 91.5520\n",
      "Epoch [350/1000], Loss: 85.3801\n",
      "Epoch [360/1000], Loss: 79.6320\n",
      "Epoch [370/1000], Loss: 74.2843\n",
      "Epoch [380/1000], Loss: 69.3149\n",
      "Epoch [390/1000], Loss: 64.7023\n",
      "Epoch [400/1000], Loss: 60.4257\n",
      "Epoch [410/1000], Loss: 56.4653\n",
      "Epoch [420/1000], Loss: 52.8021\n",
      "Epoch [430/1000], Loss: 49.4178\n",
      "Epoch [440/1000], Loss: 46.2951\n",
      "Epoch [450/1000], Loss: 43.4172\n",
      "Epoch [460/1000], Loss: 40.7682\n",
      "Epoch [470/1000], Loss: 38.3331\n",
      "Epoch [480/1000], Loss: 36.0972\n",
      "Epoch [490/1000], Loss: 34.0470\n",
      "Epoch [500/1000], Loss: 32.1694\n",
      "Epoch [510/1000], Loss: 30.4522\n",
      "Epoch [520/1000], Loss: 28.8837\n",
      "Epoch [530/1000], Loss: 27.4528\n",
      "Epoch [540/1000], Loss: 26.1493\n",
      "Epoch [550/1000], Loss: 24.9633\n",
      "Epoch [560/1000], Loss: 23.8857\n",
      "Epoch [570/1000], Loss: 22.9080\n",
      "Epoch [580/1000], Loss: 22.0219\n",
      "Epoch [590/1000], Loss: 21.2202\n",
      "Epoch [600/1000], Loss: 20.4956\n",
      "Epoch [610/1000], Loss: 19.8416\n",
      "Epoch [620/1000], Loss: 19.2523\n",
      "Epoch [630/1000], Loss: 18.7219\n",
      "Epoch [640/1000], Loss: 18.2452\n",
      "Epoch [650/1000], Loss: 17.8173\n",
      "Epoch [660/1000], Loss: 17.4338\n",
      "Epoch [670/1000], Loss: 17.0905\n",
      "Epoch [680/1000], Loss: 16.7838\n",
      "Epoch [690/1000], Loss: 16.5099\n",
      "Epoch [700/1000], Loss: 16.2658\n",
      "Epoch [710/1000], Loss: 16.0486\n",
      "Epoch [720/1000], Loss: 15.8555\n",
      "Epoch [730/1000], Loss: 15.6841\n",
      "Epoch [740/1000], Loss: 15.5323\n",
      "Epoch [750/1000], Loss: 15.3978\n",
      "Epoch [760/1000], Loss: 15.2790\n",
      "Epoch [770/1000], Loss: 15.1741\n",
      "Epoch [780/1000], Loss: 15.0817\n",
      "Epoch [790/1000], Loss: 15.0004\n",
      "Epoch [800/1000], Loss: 14.9289\n",
      "Epoch [810/1000], Loss: 14.8661\n",
      "Epoch [820/1000], Loss: 14.8112\n",
      "Epoch [830/1000], Loss: 14.7631\n",
      "Epoch [840/1000], Loss: 14.7210\n",
      "Epoch [850/1000], Loss: 14.6843\n",
      "Epoch [860/1000], Loss: 14.6524\n",
      "Epoch [870/1000], Loss: 14.6245\n",
      "Epoch [880/1000], Loss: 14.6004\n",
      "Epoch [890/1000], Loss: 14.5794\n",
      "Epoch [900/1000], Loss: 14.5612\n",
      "Epoch [910/1000], Loss: 14.5455\n",
      "Epoch [920/1000], Loss: 14.5319\n",
      "Epoch [930/1000], Loss: 14.5202\n",
      "Epoch [940/1000], Loss: 14.5101\n",
      "Epoch [950/1000], Loss: 14.5014\n",
      "Epoch [960/1000], Loss: 14.4940\n",
      "Epoch [970/1000], Loss: 14.4876\n",
      "Epoch [980/1000], Loss: 14.4821\n",
      "Epoch [990/1000], Loss: 14.4774\n",
      "Epoch [1000/1000], Loss: 14.4734\n",
      "Predicted days_remaining for parent_id 481: [30.28853416442871, 30.649080276489258, 30.648738861083984, 30.649362564086914, 30.650232315063477, 30.650508880615234, 30.649507522583008, 30.647920608520508]\n",
      "Training for parent_id 486...\n",
      "Epoch [10/1000], Loss: 389.5008\n",
      "Epoch [20/1000], Loss: 329.7739\n",
      "Epoch [30/1000], Loss: 283.2556\n",
      "Epoch [40/1000], Loss: 250.8858\n",
      "Epoch [50/1000], Loss: 225.7499\n",
      "Epoch [60/1000], Loss: 204.1146\n",
      "Epoch [70/1000], Loss: 184.5844\n",
      "Epoch [80/1000], Loss: 166.8123\n",
      "Epoch [90/1000], Loss: 150.6940\n",
      "Epoch [100/1000], Loss: 136.0736\n",
      "Epoch [110/1000], Loss: 122.8139\n",
      "Epoch [120/1000], Loss: 110.7999\n",
      "Epoch [130/1000], Loss: 99.9309\n",
      "Epoch [140/1000], Loss: 90.1157\n",
      "Epoch [150/1000], Loss: 81.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/1000], Loss: 73.3162\n",
      "Epoch [170/1000], Loss: 66.1801\n",
      "Epoch [180/1000], Loss: 59.7935\n",
      "Epoch [190/1000], Loss: 54.0919\n",
      "Epoch [200/1000], Loss: 49.0152\n",
      "Epoch [210/1000], Loss: 44.5073\n",
      "Epoch [220/1000], Loss: 40.5157\n",
      "Epoch [230/1000], Loss: 36.9915\n",
      "Epoch [240/1000], Loss: 33.8892\n",
      "Epoch [250/1000], Loss: 31.1668\n",
      "Epoch [260/1000], Loss: 28.7852\n",
      "Epoch [270/1000], Loss: 26.7083\n",
      "Epoch [280/1000], Loss: 24.9032\n",
      "Epoch [290/1000], Loss: 23.3393\n",
      "Epoch [300/1000], Loss: 21.9889\n",
      "Epoch [310/1000], Loss: 20.8269\n",
      "Epoch [320/1000], Loss: 19.8303\n",
      "Epoch [330/1000], Loss: 18.9785\n",
      "Epoch [340/1000], Loss: 18.2530\n",
      "Epoch [350/1000], Loss: 17.6372\n",
      "Epoch [360/1000], Loss: 17.1162\n",
      "Epoch [370/1000], Loss: 16.6770\n",
      "Epoch [380/1000], Loss: 16.3080\n",
      "Epoch [390/1000], Loss: 15.9991\n",
      "Epoch [400/1000], Loss: 15.7413\n",
      "Epoch [410/1000], Loss: 15.5268\n",
      "Epoch [420/1000], Loss: 15.3490\n",
      "Epoch [430/1000], Loss: 15.2021\n",
      "Epoch [440/1000], Loss: 15.0811\n",
      "Epoch [450/1000], Loss: 14.9817\n",
      "Epoch [460/1000], Loss: 14.9002\n",
      "Epoch [470/1000], Loss: 14.8337\n",
      "Epoch [480/1000], Loss: 14.7795\n",
      "Epoch [490/1000], Loss: 14.7354\n",
      "Epoch [500/1000], Loss: 14.6996\n",
      "Epoch [510/1000], Loss: 14.6706\n",
      "Epoch [520/1000], Loss: 14.6471\n",
      "Epoch [530/1000], Loss: 14.6280\n",
      "Epoch [540/1000], Loss: 14.6126\n",
      "Epoch [550/1000], Loss: 14.6000\n",
      "Epoch [560/1000], Loss: 14.5898\n",
      "Epoch [570/1000], Loss: 14.5814\n",
      "Epoch [580/1000], Loss: 14.5745\n",
      "Epoch [590/1000], Loss: 14.5688\n",
      "Epoch [600/1000], Loss: 14.5641\n",
      "Epoch [610/1000], Loss: 14.5600\n",
      "Epoch [620/1000], Loss: 14.5566\n",
      "Epoch [630/1000], Loss: 14.5536\n",
      "Epoch [640/1000], Loss: 14.5510\n",
      "Epoch [650/1000], Loss: 14.5487\n",
      "Epoch [660/1000], Loss: 14.5465\n",
      "Epoch [670/1000], Loss: 14.5446\n",
      "Epoch [680/1000], Loss: 14.5428\n",
      "Epoch [690/1000], Loss: 14.5411\n",
      "Epoch [700/1000], Loss: 14.5395\n",
      "Epoch [710/1000], Loss: 14.5380\n",
      "Epoch [720/1000], Loss: 14.5366\n",
      "Epoch [730/1000], Loss: 14.5352\n",
      "Epoch [740/1000], Loss: 14.5338\n",
      "Epoch [750/1000], Loss: 14.5325\n",
      "Epoch [760/1000], Loss: 14.5312\n",
      "Epoch [770/1000], Loss: 14.5300\n",
      "Epoch [780/1000], Loss: 14.5287\n",
      "Epoch [790/1000], Loss: 14.5275\n",
      "Epoch [800/1000], Loss: 14.5263\n",
      "Epoch [810/1000], Loss: 14.5252\n",
      "Epoch [820/1000], Loss: 14.5241\n",
      "Epoch [830/1000], Loss: 14.5229\n",
      "Epoch [840/1000], Loss: 14.5218\n",
      "Epoch [850/1000], Loss: 14.5208\n",
      "Epoch [860/1000], Loss: 14.5197\n",
      "Epoch [870/1000], Loss: 14.5187\n",
      "Epoch [880/1000], Loss: 14.5176\n",
      "Epoch [890/1000], Loss: 14.5166\n",
      "Epoch [900/1000], Loss: 14.5156\n",
      "Epoch [910/1000], Loss: 14.5147\n",
      "Epoch [920/1000], Loss: 14.5137\n",
      "Epoch [930/1000], Loss: 14.5128\n",
      "Epoch [940/1000], Loss: 14.5118\n",
      "Epoch [950/1000], Loss: 14.5109\n",
      "Epoch [960/1000], Loss: 14.5100\n",
      "Epoch [970/1000], Loss: 14.5091\n",
      "Epoch [980/1000], Loss: 14.5083\n",
      "Epoch [990/1000], Loss: 14.5074\n",
      "Epoch [1000/1000], Loss: 14.5066\n",
      "Predicted days_remaining for parent_id 486: [20.0555362701416, 20.850242614746094, 20.850074768066406, 20.850566864013672, 20.849401473999023, 20.84973907470703, 20.84933090209961, 20.848491668701172]\n",
      "Training for parent_id 511...\n",
      "Epoch [10/1000], Loss: 141.5917\n",
      "Epoch [20/1000], Loss: 108.4713\n",
      "Epoch [30/1000], Loss: 88.9348\n",
      "Epoch [40/1000], Loss: 75.4689\n",
      "Epoch [50/1000], Loss: 64.5609\n",
      "Epoch [60/1000], Loss: 55.3352\n",
      "Epoch [70/1000], Loss: 47.5611\n",
      "Epoch [80/1000], Loss: 41.0486\n",
      "Epoch [90/1000], Loss: 35.6316\n",
      "Epoch [100/1000], Loss: 31.1657\n",
      "Epoch [110/1000], Loss: 27.5218\n",
      "Epoch [120/1000], Loss: 24.5824\n",
      "Epoch [130/1000], Loss: 22.2393\n",
      "Epoch [140/1000], Loss: 20.3938\n",
      "Epoch [150/1000], Loss: 18.9574\n",
      "Epoch [160/1000], Loss: 17.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asheshlalshrestha/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 8])) that is different to the input size (torch.Size([1, 8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/1000], Loss: 17.0128\n",
      "Epoch [180/1000], Loss: 16.3816\n",
      "Epoch [190/1000], Loss: 15.9124\n",
      "Epoch [200/1000], Loss: 15.5672\n",
      "Epoch [210/1000], Loss: 15.3157\n",
      "Epoch [220/1000], Loss: 15.1338\n",
      "Epoch [230/1000], Loss: 15.0030\n",
      "Epoch [240/1000], Loss: 14.9093\n",
      "Epoch [250/1000], Loss: 14.8420\n",
      "Epoch [260/1000], Loss: 14.7935\n",
      "Epoch [270/1000], Loss: 14.7580\n",
      "Epoch [280/1000], Loss: 14.7315\n",
      "Epoch [290/1000], Loss: 14.7113\n",
      "Epoch [300/1000], Loss: 14.6953\n",
      "Epoch [310/1000], Loss: 14.6822\n",
      "Epoch [320/1000], Loss: 14.6712\n",
      "Epoch [330/1000], Loss: 14.6616\n",
      "Epoch [340/1000], Loss: 14.6530\n",
      "Epoch [350/1000], Loss: 14.6451\n",
      "Epoch [360/1000], Loss: 14.6379\n",
      "Epoch [370/1000], Loss: 14.6311\n",
      "Epoch [380/1000], Loss: 14.6247\n",
      "Epoch [390/1000], Loss: 14.6187\n",
      "Epoch [400/1000], Loss: 14.6129\n",
      "Epoch [410/1000], Loss: 14.6075\n",
      "Epoch [420/1000], Loss: 14.6023\n",
      "Epoch [430/1000], Loss: 14.5973\n",
      "Epoch [440/1000], Loss: 14.5926\n",
      "Epoch [450/1000], Loss: 14.5881\n",
      "Epoch [460/1000], Loss: 14.5837\n",
      "Epoch [470/1000], Loss: 14.5796\n",
      "Epoch [480/1000], Loss: 14.5756\n",
      "Epoch [490/1000], Loss: 14.5718\n",
      "Epoch [500/1000], Loss: 14.5682\n",
      "Epoch [510/1000], Loss: 14.5647\n",
      "Epoch [520/1000], Loss: 14.5613\n",
      "Epoch [530/1000], Loss: 14.5581\n",
      "Epoch [540/1000], Loss: 14.5550\n",
      "Epoch [550/1000], Loss: 14.5521\n",
      "Epoch [560/1000], Loss: 14.5492\n",
      "Epoch [570/1000], Loss: 14.5464\n",
      "Epoch [580/1000], Loss: 14.5438\n",
      "Epoch [590/1000], Loss: 14.5412\n",
      "Epoch [600/1000], Loss: 14.5388\n",
      "Epoch [610/1000], Loss: 14.5364\n",
      "Epoch [620/1000], Loss: 14.5341\n",
      "Epoch [630/1000], Loss: 14.5319\n",
      "Epoch [640/1000], Loss: 14.5297\n",
      "Epoch [650/1000], Loss: 14.5277\n",
      "Epoch [660/1000], Loss: 14.5257\n",
      "Epoch [670/1000], Loss: 14.5238\n",
      "Epoch [680/1000], Loss: 14.5219\n",
      "Epoch [690/1000], Loss: 14.5201\n",
      "Epoch [700/1000], Loss: 14.5183\n",
      "Epoch [710/1000], Loss: 14.5166\n",
      "Epoch [720/1000], Loss: 14.5150\n",
      "Epoch [730/1000], Loss: 14.5134\n",
      "Epoch [740/1000], Loss: 14.5119\n",
      "Epoch [750/1000], Loss: 14.5104\n",
      "Epoch [760/1000], Loss: 14.5089\n",
      "Epoch [770/1000], Loss: 14.5075\n",
      "Epoch [780/1000], Loss: 14.5062\n",
      "Epoch [790/1000], Loss: 14.5048\n",
      "Epoch [800/1000], Loss: 14.5036\n",
      "Epoch [810/1000], Loss: 14.5023\n",
      "Epoch [820/1000], Loss: 14.5011\n",
      "Epoch [830/1000], Loss: 14.4999\n",
      "Epoch [840/1000], Loss: 14.4988\n",
      "Epoch [850/1000], Loss: 14.4977\n",
      "Epoch [860/1000], Loss: 14.4966\n",
      "Epoch [870/1000], Loss: 14.4955\n",
      "Epoch [880/1000], Loss: 14.4945\n",
      "Epoch [890/1000], Loss: 14.4935\n",
      "Epoch [900/1000], Loss: 14.4925\n",
      "Epoch [910/1000], Loss: 14.4916\n",
      "Epoch [920/1000], Loss: 14.4906\n",
      "Epoch [930/1000], Loss: 14.4897\n",
      "Epoch [940/1000], Loss: 14.4889\n",
      "Epoch [950/1000], Loss: 14.4880\n",
      "Epoch [960/1000], Loss: 14.4872\n",
      "Epoch [970/1000], Loss: 14.4863\n",
      "Epoch [980/1000], Loss: 14.4856\n",
      "Epoch [990/1000], Loss: 14.4848\n",
      "Epoch [1000/1000], Loss: 14.4840\n",
      "Predicted days_remaining for parent_id 511: [12.177908897399902, 12.828946113586426, 12.829703330993652, 12.83017349243164, 12.82946491241455, 12.826764106750488, 12.83206844329834, 12.829504013061523]\n",
      "Total training time: 47.48 seconds\n",
      "Ensemble Model MSE on test data: 290.34200361699413\n",
      "Total evaluation time: 0.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "input_size = train_df.drop(columns=['parent_id', 'days_remaining']).shape[1]  # Number of features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Predicting a single value for each timestep\n",
    "num_epochs = 1000  # Number of epochs\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()  # Loss function\n",
    "models = []  # To store trained models\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"Models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each parent_id and train a separate model\n",
    "unique_parent_ids = train_df['parent_id'].unique()\n",
    "\n",
    "for parent_id in unique_parent_ids:\n",
    "    print(f\"Training for parent_id {parent_id}...\")\n",
    "    \n",
    "    # Get the data for the current parent_id\n",
    "    features, target = get_data_for_parent(train_df, parent_id)\n",
    "    \n",
    "    # Initialize the model and optimizer\n",
    "    model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop for the current parent_id\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(features)\n",
    "        \n",
    "        # Compute the loss (many-to-many)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(output_dir, f\"model_parent_{parent_id}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    models.append(model)  # Append the trained model to the list\n",
    "\n",
    "    # Optionally, evaluate the model or make predictions\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "        print(f\"Predicted days_remaining for parent_id {parent_id}: {predictions.squeeze().tolist()}\")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total training time\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Assuming df_test is prepared similarly to train_df with 'parent_id' and 'days_remaining'\n",
    "# Extract the relevant features and target from df_test\n",
    "X_test = test_df.drop(columns=['parent_id', 'days_remaining'])\n",
    "y_test = test_df['days_remaining']\n",
    "\n",
    "# Initialize lists for true and predicted values\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each row in the test set and get predictions\n",
    "for i in range(len(X_test)):\n",
    "    # Prepare each test sample with the correct dimensions\n",
    "    test_sample = torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0)  # Shape (1, 1, input_size)\n",
    "    true_value = y_test.iloc[i]  # Actual value for the test sample\n",
    "    \n",
    "    # Get the ensemble prediction\n",
    "    prediction = ensemble_prediction(models, test_sample)\n",
    "    \n",
    "    # Append to lists for MSE calculation\n",
    "    y_true.append(true_value)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate Mean Squared Error for the ensemble model\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Ensemble Model MSE on test data: {mse}\")\n",
    "\n",
    "# Calculate and print the total evaluation time\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d76fbf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_size=32, learning_rate=0.001, num_epochs=500...\n",
      "Mean MSE for configuration: 159.41375485511676\n",
      "Training with hidden_size=32, learning_rate=0.001, num_epochs=1000...\n",
      "Mean MSE for configuration: 55.84570055577445\n",
      "Training with hidden_size=32, learning_rate=0.005, num_epochs=500...\n",
      "Mean MSE for configuration: 14.884851904764567\n",
      "Training with hidden_size=32, learning_rate=0.005, num_epochs=1000...\n",
      "Mean MSE for configuration: 11.057279933328205\n",
      "Training with hidden_size=64, learning_rate=0.001, num_epochs=500...\n",
      "Mean MSE for configuration: 51.74333020391529\n",
      "Training with hidden_size=64, learning_rate=0.001, num_epochs=1000...\n",
      "Mean MSE for configuration: 15.08548268673093\n",
      "Training with hidden_size=64, learning_rate=0.005, num_epochs=500...\n",
      "Mean MSE for configuration: 12.85772388887732\n",
      "Training with hidden_size=64, learning_rate=0.005, num_epochs=1000...\n",
      "Mean MSE for configuration: 11.610962504626265\n",
      "Best Parameters: {'hidden_size': 32, 'learning_rate': 0.005, 'num_epochs': 1000}\n",
      "Total training time: 249.28 seconds\n",
      "Best Model MSE on test data: 276.8898258994092\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv(\"../Datasets/df_over_14.csv\")\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df['days_remaining'] = df['hospital_length_of_stay'] - df['day']\n",
    "df = df.drop(['hospital_length_of_stay', 'day'], axis=1)\n",
    "\n",
    "# Define the columns to scale\n",
    "columns_to_scale = df.select_dtypes(include=[np.number]).columns.difference(['parent_id', 'days_remaining'])\n",
    "\n",
    "# Scale relevant columns\n",
    "scaler = MinMaxScaler()\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Split based on unique parent_id\n",
    "unique_parent_ids = df['parent_id'].unique()\n",
    "train_parent_ids, test_parent_ids = train_test_split(unique_parent_ids, test_size=0.25, random_state=42)\n",
    "train_df = df[df['parent_id'].isin(train_parent_ids)]\n",
    "test_df = df[df['parent_id'].isin(test_parent_ids)]\n",
    "\n",
    "# Define the RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Data extraction function\n",
    "def get_data_for_parent(df, parent_id):\n",
    "    data = df[df['parent_id'] == parent_id]\n",
    "    features = data.drop(columns=['parent_id', 'days_remaining'])\n",
    "    target = data['days_remaining'].values\n",
    "    features_tensor = torch.tensor(features.values).float().unsqueeze(0)\n",
    "    target_tensor = torch.tensor(target).float().unsqueeze(0).unsqueeze(-1)  # Add feature dimension for target\n",
    "    return features_tensor, target_tensor\n",
    "\n",
    "# Define the ensemble prediction function\n",
    "def ensemble_prediction(models, input_data):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_data)\n",
    "            predictions.append(output.squeeze().tolist())\n",
    "    return torch.mean(torch.tensor(predictions), dim=0)\n",
    "\n",
    "# Hyperparameter tuning configuration\n",
    "tuning_params = {\n",
    "    'hidden_size': [32, 64],\n",
    "    'learning_rate': [0.001, 0.005],\n",
    "    'num_epochs': [500, 1000]\n",
    "}\n",
    "best_mse = float('inf')\n",
    "best_params = None\n",
    "models = []\n",
    "\n",
    "# Training loop with hyperparameter tuning\n",
    "input_size = train_df.drop(columns=['parent_id', 'days_remaining']).shape[1]\n",
    "output_size = 1  # Single output for each timestep\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "output_dir = \"Models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "start_time = time.time()\n",
    "\n",
    "for hidden_size in tuning_params['hidden_size']:\n",
    "    for learning_rate in tuning_params['learning_rate']:\n",
    "        for num_epochs in tuning_params['num_epochs']:\n",
    "            print(f\"Training with hidden_size={hidden_size}, learning_rate={learning_rate}, num_epochs={num_epochs}...\")\n",
    "            fold_models = []\n",
    "            mse_list = []\n",
    "            \n",
    "            for parent_id in train_df['parent_id'].unique():\n",
    "                # Data for current parent_id\n",
    "                features, target = get_data_for_parent(train_df, parent_id)\n",
    "\n",
    "                # Initialize model\n",
    "                model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                # Training\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(features)\n",
    "                    loss = criterion(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Save model and record loss\n",
    "                fold_models.append(model)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(features)\n",
    "                    mse = criterion(predictions, target).item()\n",
    "                    mse_list.append(mse)\n",
    "\n",
    "            # Compute mean MSE for this parameter configuration\n",
    "            mean_mse = np.mean(mse_list)\n",
    "            print(f\"Mean MSE for configuration: {mean_mse}\")\n",
    "\n",
    "            # Save the best configuration\n",
    "            if mean_mse < best_mse:\n",
    "                best_mse = mean_mse\n",
    "                best_params = {'hidden_size': hidden_size, 'learning_rate': learning_rate, 'num_epochs': num_epochs}\n",
    "                models = fold_models\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Evaluation on test data\n",
    "X_test = test_df.drop(columns=['parent_id', 'days_remaining'])\n",
    "y_test = test_df['days_remaining']\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    test_sample = torch.tensor(X_test.iloc[i].values).float().unsqueeze(0).unsqueeze(0)\n",
    "    true_value = y_test.iloc[i]\n",
    "    prediction = ensemble_prediction(models, test_sample)\n",
    "    y_true.append(true_value)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Best Model MSE on test data: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6d730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
